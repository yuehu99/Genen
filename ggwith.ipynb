{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cd537db-6b00-488b-91ca-079fce7074a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3951797/2362945497.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import obonet\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fed804b2-915d-4514-8563-c4631836c5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Source      Target\n",
      "0      GO:0000001  GO:0048308\n",
      "1      GO:0000001  GO:0048311\n",
      "2      GO:0000002  GO:0007005\n",
      "3      GO:0000003  GO:0008150\n",
      "4      GO:0000006  GO:0005385\n",
      "...           ...         ...\n",
      "83792  GO:2001317  GO:0034309\n",
      "83793  GO:2001317  GO:0042181\n",
      "83794  GO:2001317  GO:0120255\n",
      "83795  GO:2001317  GO:1901362\n",
      "83796  GO:2001317  GO:2001316\n",
      "\n",
      "[83797 rows x 2 columns]\n",
      "               GO  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
      "0      GO:0000001 -1.168093 -0.355214  0.265877 -0.710051  0.515028 -0.525165   \n",
      "1      GO:0000002 -1.185879 -0.098765  0.388240 -0.295556  0.327296 -0.119842   \n",
      "2      GO:0000003  0.063323 -0.199995  0.151511 -0.942141  0.109313  0.015316   \n",
      "3      GO:0000005  0.163135  0.301527  0.219680  0.094342 -0.129769  0.225696   \n",
      "4      GO:0000006 -0.641113 -0.541363  0.413941  0.699345  0.461507 -0.497388   \n",
      "...           ...       ...       ...       ...       ...       ...       ...   \n",
      "47590  GO:2001313  0.174428  0.194728 -0.284376  0.282102 -0.713190 -0.272055   \n",
      "47591  GO:2001314  0.025886  0.306214 -0.254303  0.253673 -0.533680 -0.269355   \n",
      "47592  GO:2001315  0.027134  0.241391 -0.227353  0.317366 -0.726657 -0.197968   \n",
      "47593  GO:2001316  0.139543  0.028883  0.899480  0.152932  0.576852  0.330342   \n",
      "47594  GO:2001317  0.083064  0.090899  0.888541  0.309920  0.403966  0.202783   \n",
      "\n",
      "       feature7  feature8  feature9  ...  feature759  feature760  feature761  \\\n",
      "0     -0.186588 -0.161192  0.186984  ...   -1.350874   -0.991801   -0.648123   \n",
      "1      0.399882 -0.035890  0.853417  ...   -1.086927   -0.842870   -0.385764   \n",
      "2      0.633298  0.507875  0.665548  ...    0.174185    0.351648    0.138497   \n",
      "3      0.357577  0.819992  0.852388  ...   -0.084025   -0.291103   -0.003621   \n",
      "4     -0.044589 -0.655766 -0.596647  ...   -0.561434    0.246475   -0.029871   \n",
      "...         ...       ...       ...  ...         ...         ...         ...   \n",
      "47590  0.121190  0.129901 -0.983496  ...    0.500545    0.429651   -0.292929   \n",
      "47591  0.150939 -0.229323 -1.078991  ...    0.042979    0.134560   -0.356661   \n",
      "47592  0.045653  0.038912 -0.954113  ...    0.349853    0.370059   -0.144606   \n",
      "47593  0.916943  0.012306 -0.020316  ...   -0.354748   -0.083168    0.043640   \n",
      "47594  0.706517 -0.017584 -0.171057  ...   -0.544680   -0.046654    0.262865   \n",
      "\n",
      "       feature762  feature763  feature764  feature765  feature766  feature767  \\\n",
      "0       -0.361629   -0.914965   -0.506993    0.389760    0.207266    0.070705   \n",
      "1        0.175797   -1.223772   -0.999628    0.101473   -0.051212    0.048775   \n",
      "2        0.119273   -0.295167   -0.331179    0.102570   -0.524301   -0.139264   \n",
      "3        0.245929   -0.443244    0.229245   -0.685159   -0.725621    0.285964   \n",
      "4       -0.212828   -0.985273    0.677472    0.582681    0.299317   -0.131577   \n",
      "...           ...         ...         ...         ...         ...         ...   \n",
      "47590   -0.464941   -0.740187    0.179149   -0.960807   -0.746958    1.069112   \n",
      "47591   -0.381828   -0.638338    0.077176   -0.788312   -0.683442    1.087031   \n",
      "47592   -0.493184   -0.655063    0.217335   -0.841272   -0.821077    1.036363   \n",
      "47593   -0.663565    0.543016   -0.652230   -1.427882   -0.985257    1.673561   \n",
      "47594   -0.767305    0.753788   -0.577503   -1.194910   -0.799556    1.519368   \n",
      "\n",
      "       feature768  \n",
      "0        0.938593  \n",
      "1        0.780470  \n",
      "2        0.761573  \n",
      "3        0.313211  \n",
      "4        0.739702  \n",
      "...           ...  \n",
      "47590   -0.848182  \n",
      "47591   -0.593092  \n",
      "47592   -0.836614  \n",
      "47593    0.109659  \n",
      "47594    0.263210  \n",
      "\n",
      "[47595 rows x 769 columns]\n"
     ]
    }
   ],
   "source": [
    "GO_graph = obonet.read_obo(\"GNN/go-basic.obo\")\n",
    "\n",
    "go_edges = []\n",
    "for u, v, data in GO_graph.edges(data=True):\n",
    "    go_edges.append([u, v])\n",
    "go_edges_df = pd.DataFrame(go_edges, columns=['Source', 'Target']).dropna()\n",
    "print(go_edges_df)\n",
    "col_name = ['GO']\n",
    "for i in range(1,769):\n",
    "  col_name.append('feature'+str(i))\n",
    "go_features_df = pd.read_csv(\"GNN/go_terms_embeddings.csv\", skiprows=1, names=col_name).dropna()\n",
    "#remove solubility\n",
    "\n",
    "with open('GNN/soluble_go_terms.txt', 'r') as file:\n",
    "    soluble_go_terms = file.read().splitlines()\n",
    "\n",
    "# 删除‘GO’列里名字在soluble_go_terms.txt中的行\n",
    "#go_features_df = go_features_df[~go_features_df['GO'].isin(soluble_go_terms)]\n",
    "\n",
    "# 用soluble_go_terms列表过滤go_features_df中的行\n",
    "#mask = go_features_df['GO'].isin(soluble_go_terms)\n",
    "\n",
    "# 除了'GO'列外，将所有列的值设置为0\n",
    "#go_features_df.loc[mask, go_features_df.columns != 'GO'] = 0\n",
    "# 选择除了'GO'列之外的所有列，并将其值设置为0\n",
    "#go_features_df.loc[:, go_features_df.columns != 'GO'] = 0\n",
    "\n",
    "# 保存结果到新的CSV文件（可选）\n",
    "go_features_df.to_csv(\"GNN/remove_solubility_go_terms_embeddings.csv\", index=False)\n",
    "print(go_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa95d6ce-e3a7-462c-8abc-ac64dc4243ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      protein  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
      "0         FES  0.339602 -0.030744 -0.901381  0.100888  0.886443  0.383596   \n",
      "1      HADHA  -0.131799 -0.025745 -0.677301 -0.053545  0.971046  0.180315   \n",
      "2      SLC7A7  0.385693 -0.070692 -0.847796 -0.022054  0.959772  0.085487   \n",
      "3        LCK   0.650428  0.014479 -0.866163  0.053508  0.951529  0.269402   \n",
      "4       HSPA2  0.322262  0.017484 -0.849302  0.046401  0.920429  0.463832   \n",
      "...       ...       ...       ...       ...       ...       ...       ...   \n",
      "14445   BPY2C -0.840158 -0.042814 -0.853394 -0.049438  0.943925  0.104337   \n",
      "14446    CLPS -0.270716 -0.036871 -0.915350 -0.013635  0.972046  0.016017   \n",
      "14447    DNER  0.228932 -0.033579 -0.907262  0.010446  0.961684  0.524211   \n",
      "14448    SOX7  0.140491  0.033339 -0.806014 -0.072016  0.938781  0.339959   \n",
      "14449  CXCL14 -0.570266 -0.011502 -0.741149 -0.096209  0.967244  0.426519   \n",
      "\n",
      "       feature7  feature8  feature9  ...  feature759  feature760  feature761  \\\n",
      "0     -0.192082 -0.032063 -0.154869  ...   -0.549204   -0.856123    0.714672   \n",
      "1     -0.028189 -0.077389 -0.095152  ...    0.927885   -0.817812    0.809631   \n",
      "2      0.076455 -0.003006 -0.032268  ...    0.941094   -0.912443    0.789828   \n",
      "3     -0.214788  0.045179 -0.506429  ...   -0.576739   -0.969558    0.916549   \n",
      "4     -0.050414 -0.033398  0.387791  ...    0.387301   -0.860696    0.678607   \n",
      "...         ...       ...       ...  ...         ...         ...         ...   \n",
      "14445 -0.132297 -0.000937 -0.709405  ...    0.923731   -0.709760    0.656196   \n",
      "14446 -0.018190 -0.008408  0.548612  ...    0.946926   -0.739839    0.755471   \n",
      "14447 -0.214318  0.039835  0.433141  ...    0.806205   -0.857853    0.223238   \n",
      "14448 -0.045201 -0.004075 -0.169557  ...    0.756285   -0.889968    0.837899   \n",
      "14449 -0.138793  0.028511 -0.785310  ...    0.979686   -0.660164    0.784033   \n",
      "\n",
      "       feature762  feature763  feature764  feature765  feature766  feature767  \\\n",
      "0       -0.046649   -0.894424   -0.001815    0.739485    0.015581   -0.023863   \n",
      "1       -0.005827   -0.848839    0.024516    0.526404   -0.039926   -0.102787   \n",
      "2        0.046979   -0.715636    0.085842    0.150494    0.025392   -0.066035   \n",
      "3       -0.080332   -0.927649   -0.047398    0.741663   -0.000096   -0.096318   \n",
      "4       -0.060695   -0.945793    0.040472    0.831079   -0.001711   -0.079842   \n",
      "...           ...         ...         ...         ...         ...         ...   \n",
      "14445   -0.029016   -0.879421    0.133864   -0.558606   -0.035379   -0.122373   \n",
      "14446    0.006047   -0.488539    0.009771   -0.261908   -0.048952   -0.062768   \n",
      "14447   -0.078767   -0.973104    0.026339    0.918282   -0.041604   -0.039557   \n",
      "14448   -0.030372   -0.969834   -0.054169    0.099373    0.018777   -0.166461   \n",
      "14449    0.043913   -0.958544    0.016069   -0.345766   -0.004606    0.074345   \n",
      "\n",
      "       feature768  \n",
      "0       -0.022002  \n",
      "1       -0.026980  \n",
      "2       -0.028283  \n",
      "3       -0.056501  \n",
      "4       -0.011189  \n",
      "...           ...  \n",
      "14445   -0.030907  \n",
      "14446    0.019626  \n",
      "14447   -0.014415  \n",
      "14448   -0.062177  \n",
      "14449   -0.093482  \n",
      "\n",
      "[14450 rows x 769 columns]\n",
      "         Target      Source\n",
      "0         MT-TF  GO:0030533\n",
      "1         MT-TF  GO:0006412\n",
      "4       MT-RNR2  GO:0003735\n",
      "5       MT-RNR2  GO:0005840\n",
      "6        MT-TL1  GO:0030533\n",
      "...         ...         ...\n",
      "456584  PLEKHM2  GO:0032880\n",
      "456585  PLEKHM2  GO:0010008\n",
      "456586  PLEKHM2  GO:0019894\n",
      "456587  PLEKHM2  GO:0032418\n",
      "456588  PLEKHM2  GO:0042267\n",
      "\n",
      "[393231 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "col_name = ['protein']\n",
    "for i in range(1,769):\n",
    "  col_name.append('feature'+str(i))\n",
    "gene_features_df = pd.read_csv('GNN/gene_embedding_GeneLLM_2.csv', header=None, names=col_name).dropna()\n",
    "#gene_features_df.loc[:, gene_features_df.columns != 'protein'] = 0\n",
    "print(gene_features_df)\n",
    "\n",
    "col_name = ['Target', 'Source']\n",
    "go_protein_df = pd.read_csv(\n",
    "    \"GNN/mart_export.txt\", \n",
    "    skiprows=1, \n",
    "    names=col_name, \n",
    "    usecols=[1, 2]  # 使用列的索引来指定\n",
    ").dropna()\n",
    "print(go_protein_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13b1d168-54ac-4a60-893c-495ab7c7a990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47595\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature759</th>\n",
       "      <th>feature760</th>\n",
       "      <th>feature761</th>\n",
       "      <th>feature762</th>\n",
       "      <th>feature763</th>\n",
       "      <th>feature764</th>\n",
       "      <th>feature765</th>\n",
       "      <th>feature766</th>\n",
       "      <th>feature767</th>\n",
       "      <th>feature768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FES</td>\n",
       "      <td>0.339602</td>\n",
       "      <td>-0.030744</td>\n",
       "      <td>-0.901381</td>\n",
       "      <td>0.100888</td>\n",
       "      <td>0.886443</td>\n",
       "      <td>0.383596</td>\n",
       "      <td>-0.192082</td>\n",
       "      <td>-0.032063</td>\n",
       "      <td>-0.154869</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.549204</td>\n",
       "      <td>-0.856123</td>\n",
       "      <td>0.714672</td>\n",
       "      <td>-0.046649</td>\n",
       "      <td>-0.894424</td>\n",
       "      <td>-0.001815</td>\n",
       "      <td>0.739485</td>\n",
       "      <td>0.015581</td>\n",
       "      <td>-0.023863</td>\n",
       "      <td>-0.022002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HADHA</td>\n",
       "      <td>-0.131799</td>\n",
       "      <td>-0.025745</td>\n",
       "      <td>-0.677301</td>\n",
       "      <td>-0.053545</td>\n",
       "      <td>0.971046</td>\n",
       "      <td>0.180315</td>\n",
       "      <td>-0.028189</td>\n",
       "      <td>-0.077389</td>\n",
       "      <td>-0.095152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.927885</td>\n",
       "      <td>-0.817812</td>\n",
       "      <td>0.809631</td>\n",
       "      <td>-0.005827</td>\n",
       "      <td>-0.848839</td>\n",
       "      <td>0.024516</td>\n",
       "      <td>0.526404</td>\n",
       "      <td>-0.039926</td>\n",
       "      <td>-0.102787</td>\n",
       "      <td>-0.026980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLC7A7</td>\n",
       "      <td>0.385693</td>\n",
       "      <td>-0.070692</td>\n",
       "      <td>-0.847796</td>\n",
       "      <td>-0.022054</td>\n",
       "      <td>0.959772</td>\n",
       "      <td>0.085487</td>\n",
       "      <td>0.076455</td>\n",
       "      <td>-0.003006</td>\n",
       "      <td>-0.032268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.941094</td>\n",
       "      <td>-0.912443</td>\n",
       "      <td>0.789828</td>\n",
       "      <td>0.046979</td>\n",
       "      <td>-0.715636</td>\n",
       "      <td>0.085842</td>\n",
       "      <td>0.150494</td>\n",
       "      <td>0.025392</td>\n",
       "      <td>-0.066035</td>\n",
       "      <td>-0.028283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LCK</td>\n",
       "      <td>0.650428</td>\n",
       "      <td>0.014479</td>\n",
       "      <td>-0.866163</td>\n",
       "      <td>0.053508</td>\n",
       "      <td>0.951529</td>\n",
       "      <td>0.269402</td>\n",
       "      <td>-0.214788</td>\n",
       "      <td>0.045179</td>\n",
       "      <td>-0.506429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.576739</td>\n",
       "      <td>-0.969558</td>\n",
       "      <td>0.916549</td>\n",
       "      <td>-0.080332</td>\n",
       "      <td>-0.927649</td>\n",
       "      <td>-0.047398</td>\n",
       "      <td>0.741663</td>\n",
       "      <td>-0.000096</td>\n",
       "      <td>-0.096318</td>\n",
       "      <td>-0.056501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HSPA2</td>\n",
       "      <td>0.322262</td>\n",
       "      <td>0.017484</td>\n",
       "      <td>-0.849302</td>\n",
       "      <td>0.046401</td>\n",
       "      <td>0.920429</td>\n",
       "      <td>0.463832</td>\n",
       "      <td>-0.050414</td>\n",
       "      <td>-0.033398</td>\n",
       "      <td>0.387791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387301</td>\n",
       "      <td>-0.860696</td>\n",
       "      <td>0.678607</td>\n",
       "      <td>-0.060695</td>\n",
       "      <td>-0.945793</td>\n",
       "      <td>0.040472</td>\n",
       "      <td>0.831079</td>\n",
       "      <td>-0.001711</td>\n",
       "      <td>-0.079842</td>\n",
       "      <td>-0.011189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47590</th>\n",
       "      <td>GO:2001313</td>\n",
       "      <td>0.174428</td>\n",
       "      <td>0.194728</td>\n",
       "      <td>-0.284376</td>\n",
       "      <td>0.282102</td>\n",
       "      <td>-0.713190</td>\n",
       "      <td>-0.272055</td>\n",
       "      <td>0.121190</td>\n",
       "      <td>0.129901</td>\n",
       "      <td>-0.983496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500545</td>\n",
       "      <td>0.429651</td>\n",
       "      <td>-0.292929</td>\n",
       "      <td>-0.464941</td>\n",
       "      <td>-0.740187</td>\n",
       "      <td>0.179149</td>\n",
       "      <td>-0.960807</td>\n",
       "      <td>-0.746958</td>\n",
       "      <td>1.069112</td>\n",
       "      <td>-0.848182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47591</th>\n",
       "      <td>GO:2001314</td>\n",
       "      <td>0.025886</td>\n",
       "      <td>0.306214</td>\n",
       "      <td>-0.254303</td>\n",
       "      <td>0.253673</td>\n",
       "      <td>-0.533680</td>\n",
       "      <td>-0.269355</td>\n",
       "      <td>0.150939</td>\n",
       "      <td>-0.229323</td>\n",
       "      <td>-1.078991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042979</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>-0.356661</td>\n",
       "      <td>-0.381828</td>\n",
       "      <td>-0.638338</td>\n",
       "      <td>0.077176</td>\n",
       "      <td>-0.788312</td>\n",
       "      <td>-0.683442</td>\n",
       "      <td>1.087031</td>\n",
       "      <td>-0.593092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47592</th>\n",
       "      <td>GO:2001315</td>\n",
       "      <td>0.027134</td>\n",
       "      <td>0.241391</td>\n",
       "      <td>-0.227353</td>\n",
       "      <td>0.317366</td>\n",
       "      <td>-0.726657</td>\n",
       "      <td>-0.197968</td>\n",
       "      <td>0.045653</td>\n",
       "      <td>0.038912</td>\n",
       "      <td>-0.954113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349853</td>\n",
       "      <td>0.370059</td>\n",
       "      <td>-0.144606</td>\n",
       "      <td>-0.493184</td>\n",
       "      <td>-0.655063</td>\n",
       "      <td>0.217335</td>\n",
       "      <td>-0.841272</td>\n",
       "      <td>-0.821077</td>\n",
       "      <td>1.036363</td>\n",
       "      <td>-0.836614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47593</th>\n",
       "      <td>GO:2001316</td>\n",
       "      <td>0.139543</td>\n",
       "      <td>0.028883</td>\n",
       "      <td>0.899480</td>\n",
       "      <td>0.152932</td>\n",
       "      <td>0.576852</td>\n",
       "      <td>0.330342</td>\n",
       "      <td>0.916943</td>\n",
       "      <td>0.012306</td>\n",
       "      <td>-0.020316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.354748</td>\n",
       "      <td>-0.083168</td>\n",
       "      <td>0.043640</td>\n",
       "      <td>-0.663565</td>\n",
       "      <td>0.543016</td>\n",
       "      <td>-0.652230</td>\n",
       "      <td>-1.427882</td>\n",
       "      <td>-0.985257</td>\n",
       "      <td>1.673561</td>\n",
       "      <td>0.109659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47594</th>\n",
       "      <td>GO:2001317</td>\n",
       "      <td>0.083064</td>\n",
       "      <td>0.090899</td>\n",
       "      <td>0.888541</td>\n",
       "      <td>0.309920</td>\n",
       "      <td>0.403966</td>\n",
       "      <td>0.202783</td>\n",
       "      <td>0.706517</td>\n",
       "      <td>-0.017584</td>\n",
       "      <td>-0.171057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.544680</td>\n",
       "      <td>-0.046654</td>\n",
       "      <td>0.262865</td>\n",
       "      <td>-0.767305</td>\n",
       "      <td>0.753788</td>\n",
       "      <td>-0.577503</td>\n",
       "      <td>-1.194910</td>\n",
       "      <td>-0.799556</td>\n",
       "      <td>1.519368</td>\n",
       "      <td>0.263210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62045 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          protein  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
       "0             FES  0.339602 -0.030744 -0.901381  0.100888  0.886443  0.383596   \n",
       "1          HADHA  -0.131799 -0.025745 -0.677301 -0.053545  0.971046  0.180315   \n",
       "2          SLC7A7  0.385693 -0.070692 -0.847796 -0.022054  0.959772  0.085487   \n",
       "3            LCK   0.650428  0.014479 -0.866163  0.053508  0.951529  0.269402   \n",
       "4           HSPA2  0.322262  0.017484 -0.849302  0.046401  0.920429  0.463832   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "47590  GO:2001313  0.174428  0.194728 -0.284376  0.282102 -0.713190 -0.272055   \n",
       "47591  GO:2001314  0.025886  0.306214 -0.254303  0.253673 -0.533680 -0.269355   \n",
       "47592  GO:2001315  0.027134  0.241391 -0.227353  0.317366 -0.726657 -0.197968   \n",
       "47593  GO:2001316  0.139543  0.028883  0.899480  0.152932  0.576852  0.330342   \n",
       "47594  GO:2001317  0.083064  0.090899  0.888541  0.309920  0.403966  0.202783   \n",
       "\n",
       "       feature7  feature8  feature9  ...  feature759  feature760  feature761  \\\n",
       "0     -0.192082 -0.032063 -0.154869  ...   -0.549204   -0.856123    0.714672   \n",
       "1     -0.028189 -0.077389 -0.095152  ...    0.927885   -0.817812    0.809631   \n",
       "2      0.076455 -0.003006 -0.032268  ...    0.941094   -0.912443    0.789828   \n",
       "3     -0.214788  0.045179 -0.506429  ...   -0.576739   -0.969558    0.916549   \n",
       "4     -0.050414 -0.033398  0.387791  ...    0.387301   -0.860696    0.678607   \n",
       "...         ...       ...       ...  ...         ...         ...         ...   \n",
       "47590  0.121190  0.129901 -0.983496  ...    0.500545    0.429651   -0.292929   \n",
       "47591  0.150939 -0.229323 -1.078991  ...    0.042979    0.134560   -0.356661   \n",
       "47592  0.045653  0.038912 -0.954113  ...    0.349853    0.370059   -0.144606   \n",
       "47593  0.916943  0.012306 -0.020316  ...   -0.354748   -0.083168    0.043640   \n",
       "47594  0.706517 -0.017584 -0.171057  ...   -0.544680   -0.046654    0.262865   \n",
       "\n",
       "       feature762  feature763  feature764  feature765  feature766  feature767  \\\n",
       "0       -0.046649   -0.894424   -0.001815    0.739485    0.015581   -0.023863   \n",
       "1       -0.005827   -0.848839    0.024516    0.526404   -0.039926   -0.102787   \n",
       "2        0.046979   -0.715636    0.085842    0.150494    0.025392   -0.066035   \n",
       "3       -0.080332   -0.927649   -0.047398    0.741663   -0.000096   -0.096318   \n",
       "4       -0.060695   -0.945793    0.040472    0.831079   -0.001711   -0.079842   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "47590   -0.464941   -0.740187    0.179149   -0.960807   -0.746958    1.069112   \n",
       "47591   -0.381828   -0.638338    0.077176   -0.788312   -0.683442    1.087031   \n",
       "47592   -0.493184   -0.655063    0.217335   -0.841272   -0.821077    1.036363   \n",
       "47593   -0.663565    0.543016   -0.652230   -1.427882   -0.985257    1.673561   \n",
       "47594   -0.767305    0.753788   -0.577503   -1.194910   -0.799556    1.519368   \n",
       "\n",
       "       feature768  \n",
       "0       -0.022002  \n",
       "1       -0.026980  \n",
       "2       -0.028283  \n",
       "3       -0.056501  \n",
       "4       -0.011189  \n",
       "...           ...  \n",
       "47590   -0.848182  \n",
       "47591   -0.593092  \n",
       "47592   -0.836614  \n",
       "47593    0.109659  \n",
       "47594    0.263210  \n",
       "\n",
       "[62045 rows x 769 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(go_features_df))\n",
    "go_features_df.rename(columns={'GO': 'protein'}, inplace=True)\n",
    "combined_features = pd.concat([gene_features_df, go_features_df])\n",
    "combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b04cf6a3-3fb2-4a10-bc36-097a15a2bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_edges_df = pd.read_csv('GNN/protein_interactions.csv', usecols=[0, 1], names=col_name).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b494146-3fa6-4224-962a-cdbece697a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>protein1</td>\n",
       "      <td>protein2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ARF5</td>\n",
       "      <td>RALGPS2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ARF5</td>\n",
       "      <td>FHDC1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ARF5</td>\n",
       "      <td>ATP6V1E1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARF5</td>\n",
       "      <td>CYTH2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13715124</th>\n",
       "      <td>LDB1</td>\n",
       "      <td>SAMD14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13715125</th>\n",
       "      <td>LDB1</td>\n",
       "      <td>KDM6B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13715126</th>\n",
       "      <td>LDB1</td>\n",
       "      <td>WWP2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13715127</th>\n",
       "      <td>LDB1</td>\n",
       "      <td>VPS33B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13715128</th>\n",
       "      <td>LDB1</td>\n",
       "      <td>NDST2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13067420 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Target    Source\n",
       "0         protein1  protein2\n",
       "1             ARF5   RALGPS2\n",
       "2             ARF5     FHDC1\n",
       "3             ARF5  ATP6V1E1\n",
       "4             ARF5     CYTH2\n",
       "...            ...       ...\n",
       "13715124      LDB1    SAMD14\n",
       "13715125      LDB1     KDM6B\n",
       "13715126      LDB1      WWP2\n",
       "13715127      LDB1    VPS33B\n",
       "13715128      LDB1     NDST2\n",
       "\n",
       "[13067420 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_edges_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "209ab288-b647-41c1-a14a-963e7e520b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MT-TF</td>\n",
       "      <td>GO:0030533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MT-TF</td>\n",
       "      <td>GO:0006412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MT-RNR2</td>\n",
       "      <td>GO:0003735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MT-RNR2</td>\n",
       "      <td>GO:0005840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MT-TL1</td>\n",
       "      <td>GO:0030533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13715124</th>\n",
       "      <td>LDB1</td>\n",
       "      <td>SAMD14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13715125</th>\n",
       "      <td>LDB1</td>\n",
       "      <td>KDM6B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13715126</th>\n",
       "      <td>LDB1</td>\n",
       "      <td>WWP2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13715127</th>\n",
       "      <td>LDB1</td>\n",
       "      <td>VPS33B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13715128</th>\n",
       "      <td>LDB1</td>\n",
       "      <td>NDST2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13544448 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Target      Source\n",
       "0           MT-TF  GO:0030533\n",
       "1           MT-TF  GO:0006412\n",
       "4         MT-RNR2  GO:0003735\n",
       "5         MT-RNR2  GO:0005840\n",
       "6          MT-TL1  GO:0030533\n",
       "...           ...         ...\n",
       "13715124     LDB1      SAMD14\n",
       "13715125     LDB1       KDM6B\n",
       "13715126     LDB1        WWP2\n",
       "13715127     LDB1      VPS33B\n",
       "13715128     LDB1       NDST2\n",
       "\n",
       "[13544448 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_edges = pd.concat([go_protein_df, go_edges_df, gene_edges_df])\n",
    "#combined_edges = combined_edges[['Source', 'Target']]\n",
    "combined_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7d6e661-65eb-4914-bcf0-4a0a4d56cbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_in_features = set(combined_features['protein'])\n",
    "\n",
    "filtered_edges_df = combined_edges[\n",
    "    combined_edges['Source'].isin(nodes_in_features) & combined_edges['Target'].isin(nodes_in_features)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88086f82-144e-421c-9e8f-98b8943a68a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>MT-CO1</td>\n",
       "      <td>GO:0016020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>MT-CO1</td>\n",
       "      <td>GO:0004129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>MT-CO1</td>\n",
       "      <td>GO:0020037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>MT-CO1</td>\n",
       "      <td>GO:0009060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>MT-CO1</td>\n",
       "      <td>GO:0045277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13715122</th>\n",
       "      <td>LDB1</td>\n",
       "      <td>PGAP6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13715125</th>\n",
       "      <td>LDB1</td>\n",
       "      <td>KDM6B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13715126</th>\n",
       "      <td>LDB1</td>\n",
       "      <td>WWP2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13715127</th>\n",
       "      <td>LDB1</td>\n",
       "      <td>VPS33B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13715128</th>\n",
       "      <td>LDB1</td>\n",
       "      <td>NDST2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9914754 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Target      Source\n",
       "48        MT-CO1  GO:0016020\n",
       "49        MT-CO1  GO:0004129\n",
       "50        MT-CO1  GO:0020037\n",
       "51        MT-CO1  GO:0009060\n",
       "52        MT-CO1  GO:0045277\n",
       "...          ...         ...\n",
       "13715122    LDB1       PGAP6\n",
       "13715125    LDB1       KDM6B\n",
       "13715126    LDB1        WWP2\n",
       "13715127    LDB1      VPS33B\n",
       "13715128    LDB1       NDST2\n",
       "\n",
       "[9914754 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_edges_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea4d1659-bd1d-47e2-9d03-6313ceb4ac32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24323, 17460, 27132,  ...,   947,  6874, 13222],\n",
       "        [ 2077,  2077,  2077,  ..., 10107, 10107, 10107]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_id_to_index = {node_id: i for i, node_id in enumerate(combined_features['protein'])}\n",
    "# 确保edge_index是按照这个新的索引顺序排列的\n",
    "source_indices = [node_id_to_index[node_id] for node_id in filtered_edges_df['Source']]\n",
    "target_indices = [node_id_to_index[node_id] for node_id in filtered_edges_df['Target']]\n",
    "edge_index = torch.tensor([source_indices, target_indices], dtype=torch.long)\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc17178b-d7a5-4f1f-ac63-3cfdde4d04e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1BG</td>\n",
       "      <td>0.062761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NAT2</td>\n",
       "      <td>0.118469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADA</td>\n",
       "      <td>0.253069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CDH2</td>\n",
       "      <td>0.694620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GAGE12F</td>\n",
       "      <td>0.088746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30992</th>\n",
       "      <td>SLC12A6</td>\n",
       "      <td>0.659845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30993</th>\n",
       "      <td>PTBP3</td>\n",
       "      <td>0.729372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30994</th>\n",
       "      <td>KCNE2</td>\n",
       "      <td>0.160752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30995</th>\n",
       "      <td>DGCR2</td>\n",
       "      <td>0.627946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30996</th>\n",
       "      <td>SCO2</td>\n",
       "      <td>0.229875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30997 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       protein     Label\n",
       "0         A1BG  0.062761\n",
       "1         NAT2  0.118469\n",
       "2          ADA  0.253069\n",
       "3         CDH2  0.694620\n",
       "4      GAGE12F  0.088746\n",
       "...        ...       ...\n",
       "30992  SLC12A6  0.659845\n",
       "30993    PTBP3  0.729372\n",
       "30994    KCNE2  0.160752\n",
       "30995    DGCR2  0.627946\n",
       "30996     SCO2  0.229875\n",
       "\n",
       "[30997 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#regression\n",
    "labels_df = pd.read_csv('GNN/phastcons.csv')#\n",
    "#labels_df.rename(columns={'Gene name': 'protein'}, inplace=True)\n",
    "labels_df.rename(columns={'GeneSymbol': 'protein'}, inplace=True)\n",
    "labels_df.rename(columns={'Conservation': 'Label'}, inplace=True)\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf562e3d-74ec-4f75-84f8-feca608153f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein</th>\n",
       "      <th>Subcellular_location</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OR7A10</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORC5</td>\n",
       "      <td>Nucleus</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZNF596</td>\n",
       "      <td>Nucleus</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SFI1</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SUCNR1</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8592</th>\n",
       "      <td>INSR</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8593</th>\n",
       "      <td>SLC7A1</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8594</th>\n",
       "      <td>SLC44A2</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8595</th>\n",
       "      <td>PARM1</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596</th>\n",
       "      <td>OR10C1</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8597 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      protein Subcellular_location  Label\n",
       "0      OR7A10        Cell membrane      0\n",
       "1        ORC5              Nucleus      1\n",
       "2      ZNF596              Nucleus      1\n",
       "3        SFI1            Cytoplasm      2\n",
       "4      SUCNR1        Cell membrane      0\n",
       "...       ...                  ...    ...\n",
       "8592     INSR        Cell membrane      0\n",
       "8593   SLC7A1        Cell membrane      0\n",
       "8594  SLC44A2        Cell membrane      0\n",
       "8595    PARM1        Cell membrane      0\n",
       "8596   OR10C1        Cell membrane      0\n",
       "\n",
       "[8597 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#localization\n",
    "labels_df = pd.read_csv('GNN/subcellular_location.csv')\n",
    "labels_df.dropna(inplace=True)\n",
    "location_mapping = {\n",
    "    'Cell membrane': 0,\n",
    "    'Nucleus': 1,\n",
    "    'Cytoplasm': 2\n",
    "}\n",
    "labels_df.rename(columns={'GeneSymbol': 'protein'}, inplace=True)\n",
    "labels_df['Label'] = labels_df['Subcellular_location'].map(location_mapping)\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3090baeb-ad7a-4920-a257-dab2efd701b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>protein</th>\n",
       "      <th>Solubility</th>\n",
       "      <th>Label</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>Count_Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ERAP2</td>\n",
       "      <td>Membrane</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ADAMTSL5</td>\n",
       "      <td>Soluble</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>TBC1D30</td>\n",
       "      <td>Membrane</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>KCNK18</td>\n",
       "      <td>Membrane</td>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NDNF</td>\n",
       "      <td>Soluble</td>\n",
       "      <td>1</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>1374</td>\n",
       "      <td>TRABD2B</td>\n",
       "      <td>Membrane</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>1375</td>\n",
       "      <td>RPS9</td>\n",
       "      <td>Soluble</td>\n",
       "      <td>1</td>\n",
       "      <td>205</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>1376</td>\n",
       "      <td>SLC22A16</td>\n",
       "      <td>Membrane</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>1377</td>\n",
       "      <td>FBN3</td>\n",
       "      <td>Soluble</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378</th>\n",
       "      <td>1378</td>\n",
       "      <td>BDH2</td>\n",
       "      <td>Soluble</td>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1379 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0   protein Solubility  Label  Word_Count  Count_Category\n",
       "0              0     ERAP2   Membrane      0         117               0\n",
       "1              1  ADAMTSL5    Soluble      1          28               1\n",
       "2              2   TBC1D30   Membrane      0          55               0\n",
       "3              3    KCNK18   Membrane      0         184               0\n",
       "4              4      NDNF    Soluble      1         129               0\n",
       "...          ...       ...        ...    ...         ...             ...\n",
       "1374        1374   TRABD2B   Membrane      0          96               0\n",
       "1375        1375      RPS9    Soluble      1         205               0\n",
       "1376        1376  SLC22A16   Membrane      0          93               0\n",
       "1377        1377      FBN3    Soluble      1          90               0\n",
       "1378        1378      BDH2    Soluble      1         102               0\n",
       "\n",
       "[1379 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df = pd.read_csv('GNN/new_labels.csv')#\n",
    "#labels_df.rename(columns={'Gene name': 'protein'}, inplace=True)\n",
    "labels_df.rename(columns={'GeneSymbol': 'protein'}, inplace=True)\n",
    "#labels_df.rename(columns={'Conservation': 'Label'}, inplace=True)\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00f837f1-3ab5-42ce-be9c-6d72a1aa9104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3392, 10432, 1559, 6014, 10283, 5428, 11077, 11045, 2705, 7346, 10444, 11680, 7739, 2174, 14092, 3834, 5029, 11211, 191, 2205, 5373, 14094, 11419, 1029, 9446, 8540, 2201, 9682, 5757, 4924, 1416, 1026, 4229, 11718, 8334, 7270, 965, 8936, 10708, 6729, 7873, 2757, 13311, 5238, 8190, 12285, 5863, 13831, 7462, 637, 4275, 9411, 181, 3228, 6842, 3374, 5143, 9340, 13098, 9714, 10022, 8470, 11157, 4626, 8026, 11597, 13583, 10757, 936, 4658, 11589, 2218, 8305, 255, 800, 5781, 10896, 2673, 12293, 6306, 8302, 7223, 4151, 5355, 4941, 6855, 7774, 10328, 7404, 11192, 8771, 1649, 10456, 7825, 8275, 6682, 6335, 8177, 3213, 1822, 534, 13881, 1211, 5187, 2379, 7554, 14125, 11969, 12711, 6294, 180, 441, 10195, 9326, 11374, 7671, 12388, 3585, 2736, 9453, 11331, 8802, 3201, 7385, 5843, 536, 9200, 2793, 13933, 12571, 4645, 10436, 729, 1021, 7979, 4561, 6730, 14374, 7230, 4845, 33, 8770, 9162, 5665, 7794, 12653, 12267, 615, 10181, 12714, 5965, 7472, 6691, 12985, 6349, 1069, 6061, 4880, 4895, 12981, 14301, 11820, 11988, 8116, 2533, 7983, 553, 1339, 8933, 9487, 5766, 8033, 47, 13288, 6182, 4875, 10356, 12978, 11388, 4280, 3399, 8348, 4987, 4769, 2103, 8670, 3193, 6460, 5704, 12943, 11882, 6839, 10377, 14169, 10431, 2869, 305, 7593, 3860, 11043, 146, 2433, 10643, 5540, 10143, 13590, 13578, 7095, 5397, 5759, 2060, 11655, 5058, 5181, 7435, 6132, 6310, 8640, 55, 2668, 3232, 14107, 3332, 12494, 12352, 1563, 5875, 327, 1886, 11725, 9641, 9829, 4183, 2996, 10518, 1397, 1740, 4615, 3019, 12927, 6885, 2860, 4963, 11265, 12182, 13581, 8477, 9405, 1018, 2812, 3667, 14194, 6223, 1350, 4200, 3295, 8704, 10759, 1244, 410, 7624, 2436, 12504, 5497, 4432, 3249, 6013, 2426, 3723, 8284, 14128, 4884, 1041, 13036, 9573, 1267, 934, 1155, 776, 11242, 11468, 1393, 11671, 11636, 11538, 7477, 377, 1575, 7489, 461, 5711, 3197, 10913, 1223, 8719, 13551, 8424, 9817, 7711, 3491, 9578, 7764, 7717, 892, 1440, 1450, 2586, 13580, 13114, 7870, 4286, 2070, 1697, 7590, 5251, 3419, 1213, 12232, 10392, 11396, 5903, 8964, 9508, 5489, 2030, 1417, 9019, 2311, 6742, 9771, 13918, 14265, 3350, 3881, 1753, 10306, 13568, 8763, 4420, 5305, 1003, 11214, 11181, 8574, 14347, 907, 510, 2579, 12067, 6939, 136, 5458, 8904, 6617, 12991, 10687, 4109, 10958, 8098, 374, 13356, 4011, 10233, 11921, 11546, 10213, 1141, 10091, 3659, 4782, 4128, 12716, 5694, 5047, 8641, 12634, 8344, 9422, 3349, 2698, 11635, 9247, 6816, 1937, 12662, 12704, 1730, 3733, 14145, 3612, 10723, 4545, 12474, 9500, 10974, 11842, 13349, 5126, 4375, 6951, 4874, 12757, 8627, 3358, 14027, 9295, 14104, 2440, 11826, 6744, 7964, 5496, 10684, 11644, 4105, 10384, 1002, 8444, 13517, 1589, 7914, 4702, 5804, 10724, 3606, 5986, 12655, 9400, 11528, 8149, 9866, 12761, 14342, 12316, 3880, 12315, 7100, 5661, 1547, 13026, 2878, 9672, 5802, 10954, 11173, 1867, 14217, 8352, 1349, 18, 8358, 8607, 13914, 9308, 11097, 5057, 11417, 8776, 10258, 10748, 9941, 7216, 4462, 13778, 1819, 9836, 3161, 13574, 2044, 13677, 4002, 7689, 5401, 10344, 4344, 12593, 7772, 4556, 2237, 2090, 10089, 11217, 11137, 2035, 6440, 910, 4954, 13263, 8111, 12596, 2258, 6800, 464, 264, 14404, 3858, 11050, 2410, 3148, 4752, 7995, 5876, 8401, 10156, 11128, 6414, 1360, 11872, 235, 10652, 6011, 4123, 8960, 13261, 13287, 3743, 12863, 9517, 3990, 12230, 9300, 5743, 4830, 6715, 9278, 10742, 1979, 1134, 2022, 10489, 1651, 1110, 4152, 596, 700, 7076, 12493, 1486, 9601, 11543, 8786, 6716, 13009, 11029, 11071, 11817, 5350, 14438, 12313, 7057, 6320, 13634, 5523, 9530, 8312, 6478, 11529, 8561, 13984, 10523, 5675, 2674, 10636, 10877, 4912, 9159, 11108, 8656, 7800, 10556, 4727, 5100, 1049, 5510, 5292, 7650, 11170, 1996, 8824, 3121, 9622, 4143, 8990, 11736, 970, 5883, 5344, 951, 10376, 2122, 10470, 9344, 7027, 445, 4573, 10689, 6120, 652, 6919, 2428, 11321, 11299, 2113, 12689, 3291, 6944, 4473, 7293, 6644, 11947, 5345, 8542, 10172, 9156, 5853, 12235, 9100, 8482, 4536, 3795, 10830, 4232, 9431, 6567, 8644, 2837, 5030, 10544, 5555, 8556, 10070, 1500, 7368, 514, 5313, 5331, 5302, 8283, 8461, 6249, 3849, 4732, 4357, 5632, 8038, 9131, 13837, 8204, 8975, 12828, 6547, 89, 3493, 8598, 416, 5257, 5702, 11371, 4116, 6187, 13555, 6065, 6922, 6138, 2398, 7694, 5003, 6695, 12607, 6831, 11638, 11430, 8777, 2695, 12323, 7746, 9140, 13913, 14270, 11269, 10102, 12527, 10767, 6544, 4057, 151, 7203, 1843, 10836, 11107, 13209, 11244, 8920, 13434, 1430, 9085, 10640, 7266, 8674, 9942, 5745, 6356, 6844, 10019, 10332, 9520, 4506, 7890, 1477, 4784, 6435, 8950, 10419, 5194, 11643, 13566, 2905, 14256, 4706, 12097, 8276, 2275, 5464, 7878, 245, 11456, 372, 13840, 13838, 3427, 6375, 3969, 8130, 5807, 12224, 11744, 7998, 5075, 5394, 6034, 236, 7987, 9800, 13937, 11090, 5814, 11221, 11378, 2789, 2697, 11573, 10650, 1093, 4091, 7748, 34, 4413, 13962, 14414, 2623, 7867, 7813, 9529, 13539, 12421, 12880, 10425, 5340, 7410, 8583, 9618, 12351, 3169, 7696, 9882, 8346, 8155, 10562, 10695, 10964, 4828, 9673, 5483, 6432, 6082, 14096, 5049, 5713, 10609, 484, 11319, 3348, 2170, 4382, 8129, 12412, 11625, 2848, 8663, 3675, 8386, 10026, 5405, 6507, 13864, 13627, 164, 5860, 6136, 521, 7432, 3768, 4439, 14005, 13152, 1599, 5991, 4921, 4790, 4560, 6290, 7349, 3439, 12326, 10785, 7771, 13759, 7244, 8077, 13506, 8963, 1139, 8185, 3979, 10683, 183, 6635, 633, 7103, 9762, 619, 909, 7339, 10772, 8071, 3340, 11729, 7621, 2787, 5440, 7389, 10525, 3477, 9126, 11619, 11523, 4646, 14299, 931, 10577, 1516, 3312, 3980, 3287, 4222, 3765, 6166, 6790, 10769, 3942, 10515, 11041, 6003, 8173, 10342, 3102, 4826, 2782, 6727, 8760, 4917, 438, 3045, 8117, 5614, 6222, 5838, 5604, 3481, 2678, 9289, 8119, 687, 6775, 5375, 7291, 7612, 7832, 12233, 6291, 1784, 7388, 6482, 7824, 7583, 9576, 13087, 6247, 7080, 4003, 11194, 6517, 1398, 9369, 29, 7559, 10648, 4957, 7320, 5482, 13234, 4127, 2093, 6590, 9847, 14039, 13331, 2136, 4653, 3553, 5421, 10217, 7259, 6684, 2313, 1254, 1601, 5896, 13559, 7101, 5429, 8420, 6745, 9633, 9425, 12614, 8946, 5306, 4469, 5620, 7516, 5846, 1618, 11464, 14240, 7709, 3257, 7072, 8759, 1612, 1846, 7175, 1905, 3771, 4353, 7405, 938, 13028, 8278, 4129, 6135, 7841, 11962, 2520, 9876, 741, 8628, 9382, 4981, 4244, 5586, 3112, 10267, 7930, 3216, 4430, 8338, 4929, 11139, 11359, 3150, 6550, 2409, 1511, 13325, 7569, 10948, 350, 11205, 949, 4569, 8527, 2131, 3452, 694, 13106, 5987, 11234, 9460, 82, 9323, 5451, 6602, 11435, 6858, 7565, 6908, 3716, 10498, 4441, 10138, 3304, 7138, 13594, 5445, 4860, 6832, 3657, 5996, 3077, 1478, 10438, 3024, 1698, 7526, 269, 6340, 7297, 1088, 3178, 6129, 4248, 5960, 474, 11847, 14273, 11098, 3552, 4006, 5168, 11702, 9448, 10010, 8251, 10183, 10642, 6091, 5316, 4897, 5104, 239, 5384, 13283, 13567, 2702, 14130, 11696, 2404, 13401, 9410, 987, 2300, 4773, 5048, 2445, 649, 1250, 5870, 12353, 7085, 1293, 6436, 11688, 10832, 5678, 8404, 7202, 9505, 13973, 12117, 9593, 3787, 3652, 6299, 10971, 469, 8617, 4343, 5629, 12800, 12197, 8136, 6412, 1752, 8218, 9120, 5712, 9937, 5746, 12935, 5321, 6931, 1597, 7551, 7836, 6236, 2855, 2866, 13048, 12221, 720, 4592, 3311, 10336, 1728, 5479, 4691, 12819, 6685, 7988, 5037, 2457, 278, 8951, 11150, 9114, 10197, 8285, 8113, 1521, 5907, 1761, 4160, 5415, 6025, 14155, 2737, 11015, 13731, 10352, 957, 5932, 1341, 1137, 2342, 5912, 8203, 7492, 5545, 8602, 4488, 240, 7820, 9701, 708, 12577, 11341, 11531, 13639, 3394, 8890, 3973, 4909, 5120, 6256, 10750, 10988, 2822, 5773, 14315, 2804, 1808, 5368, 3936, 13598, 6673, 3187, 14186, 9970, 4733, 3593, 7731, 3186, 6821, 1364, 1821, 13848, 2243, 14048, 8014, 9918, 9645, 8056, 1989, 2522, 5106, 4079, 2167, 7921, 7310, 389, 14, 10239, 5339, 7660, 14063, 14037, 6824, 1059, 14235, 6404, 2693, 12439, 11395, 9809, 13501, 5966, 9596, 8115, 5836, 6276, 10598, 13399, 5616, 10730, 7637, 12908, 2826, 4513, 3889, 10686, 5381, 14049, 5256, 7319, 72, 3115, 5283, 3666, 6147, 9669, 3499, 10784, 9744, 11457, 7158, 7502, 4568, 3760, 1391, 2390, 9219, 2228, 9963, 10410, 9730, 5301, 11390, 10864, 11601, 7125, 5118, 10427, 3856, 14098, 4219, 3463, 1908, 6759, 7949, 11510, 3097, 7916, 4340, 7893, 10754, 9385, 4285, 5690, 7262, 11845, 3895, 1736, 1439, 4673, 2004, 7037, 13999, 9718, 1496, 2624, 68, 6747, 587, 5658, 9301, 11255, 10085, 6761, 6219, 3360, 1507, 3299, 1687, 1785, 8101, 266, 4504, 1536, 2708, 11423, 11641, 6812, 9979, 8735, 4744, 669, 10505, 3049, 4872, 1658, 7418, 3129, 2947, 13724, 5173, 10439, 6911, 6196, 1208, 14229, 13689, 6099, 7237, 5045, 3616, 2892, 10706, 5815, 5842, 3742, 908, 307, 10953, 4524, 10473, 465, 11115, 12895, 7299, 585, 9412, 5923, 5342, 7290, 7629, 6779, 7060, 4288, 13542, 12656, 1114, 13765, 8589, 13271, 1901, 13376, 14278, 12790, 3166, 2314, 11381, 1564, 6535, 539, 13202, 2076, 3591, 12654, 11581, 10331, 12604, 10807, 6244, 1596, 2985, 11282, 1013, 4259, 11002, 8940, 10272, 661, 2099, 3269, 11604, 6426, 10895, 9063, 812, 6263, 943, 4755, 664, 889, 13322, 13907, 1127, 2198, 7452, 11453, 11713, 896, 11485, 9155, 7414, 10870, 5962, 9106, 4550, 6586, 6476, 6918, 11060, 5751, 2231, 12956, 285, 11980, 14269, 13820, 2715, 620, 9352, 12020, 9819, 6634, 11734, 4683, 8372, 10881, 10349, 11162, 4154, 251, 5528, 5334, 4920, 11393, 6936, 11033, 1970, 6932, 8522, 3095, 7599, 6688, 2349, 1779, 7537, 12204, 12215, 7314, 5753, 10557, 1064, 8134, 13776, 6279, 4084, 5705, 2583, 745, 10615, 2732, 9231, 9603, 12079, 1732, 10554, 13084, 7120, 10955, 6261, 5964, 11440, 11611, 802, 11377, 4241, 2152, 9000, 6976, 1930, 6615, 809, 8970, 13615, 12498, 8883, 3719, 9494, 1685, 13819, 11079, 4082, 1459, 2808, 7536, 12419, 7587, 14259, 2663, 6466, 12785, 7741, 9242, 4778, 839, 6218, 2487, 7378, 7888, 5566, 1270, 6300, 14276, 2183, 8491, 518, 15, 14436, 7808, 2561, 3874, 7743, 4718, 10758, 3680, 4820, 11964, 3089, 2497, 6171, 8145, 4512, 616, 8369, 8983, 10668, 3701, 926, 3396, 3811, 6137, 3496, 9373, 4764, 2562, 3402, 13183, 5446, 3531, 2282, 2555, 5400, 11905, 7527, 1092, 10993, 7868, 4521, 1717, 8007, 3947, 7777, 5411, 11917, 13307, 6191, 3515, 13593, 2669, 3418, 5564, 297, 14135, 3996, 1591, 13532, 2628, 14441, 12467, 6343, 7504, 11003, 7408, 8349, 10367, 5022, 3425, 11579, 8625, 8015, 6497, 4527, 12745, 6246, 2546, 8804, 2664, 4584, 9167, 10709, 4516, 2924, 9843, 7768, 3173, 3775, 4162, 4466, 11798, 12113, 5046, 6475, 2092, 7686, 7149, 3940, 6265, 1877, 5626, 5839, 8408, 630, 6686, 3577, 8833, 4554, 3647, 10771, 10539, 3030, 12146, 6738, 5598, 10441, 6242, 5498, 10353, 6396, 13589, 13614, 6701, 2318, 5320, 6271, 10641, 149, 14317, 10364, 13402, 9662, 13100, 6680, 9753, 12919, 8825, 1704, 11667, 8846, 4279, 2203, 4725, 10478, 5020, 7148, 4267, 2196, 8998, 7630, 2639, 11340, 11496, 6886, 4798, 1817, 5697, 4788, 344, 9592, 8840, 5796, 7681, 449, 3273, 2095, 7011, 8123, 12268, 8498, 9251, 13390, 10603, 14122, 9082, 12877, 10889, 11686, 7884, 13105, 3225, 13847, 11758, 12399, 4631, 4990, 3449, 9316, 9038, 9329, 7487, 8848, 1031, 6407, 10566, 2034, 9583, 6142, 4146, 3614, 2920, 5248, 12767, 3081, 4745, 6654, 8497, 3409, 12175, 130, 10001, 12332, 6388, 3122, 4192, 1058, 8622, 5938, 3663, 577, 4056, 7944, 4490, 1475, 1462, 2735, 11561, 3335, 8865, 9554, 8744, 10341, 8110, 3397, 11614, 2180, 5267, 1995, 9496, 3022, 5161, 11046, 14245, 2023, 11167, 10564, 6189, 8364, 4355, 9589, 7311, 9920, 9541, 975, 2296, 10157, 11898, 9670, 5396, 5537, 11712, 8330, 14176, 8979, 10284, 11500, 3485, 7350, 4549, 2718, 4071, 6638, 8025, 13459, 10253, 10844, 13901, 8127, 9746, 5909, 7901, 14045, 5589, 9504, 10326, 9272, 7032, 44, 5531, 3669, 650, 4703, 3735, 12234, 7355, 2986, 11372, 10495, 5721, 3498, 3118, 6444, 4064, 4813, 13902, 3041, 5279, 5201, 4898, 9657, 13921, 8212, 12264, 7665, 8773, 6628, 168, 13801, 6032, 11034, 7400, 13900, 5038, 4245, 2508, 13661, 10820, 4667, 14306, 4542, 4758, 2594, 3203, 4748, 3088, 5170, 12685, 8739, 10766, 4045, 4767, 9486, 6266, 10756, 3372, 13825, 1448, 12426, 50, 1181, 7239, 2107, 9430, 2068, 10422, 13256, 7993, 7981, 4193, 11065, 8630, 11746, 2466, 1903, 4104, 5882, 11336, 11545, 6808, 10055, 7179, 9720, 9659, 2665, 8084, 1672, 3307, 14350, 320, 6703, 9152, 7413, 10799, 12393, 12206, 5297, 4252, 8291, 4130, 1214, 5243, 4766, 9868, 7340, 7539, 2717, 9791, 14442, 7555, 6781, 5336, 1747, 6081, 7004, 9995, 3085, 11603, 3471, 2002, 5314, 2139, 6456, 9786, 8342, 3569, 1077, 9615, 14201, 6315, 7327, 6766, 7264, 7468, 6963, 8856, 9355, 14225, 9176, 9482, 13412, 4086, 1680, 11122, 2126, 11821, 12202, 11600, 13912, 6718, 770, 11695, 8761, 4001, 3763, 8189, 979, 1976, 9204, 10190, 10694, 1290, 14267, 4598, 5353, 10583, 9908, 3910, 9498, 7968, 347, 12885, 11651, 274, 1556, 7615, 2112, 10126, 9001, 11125, 13400, 5448, 4841, 3380, 9288, 4648, 13191, 604, 8789, 24, 9005, 2930, 12616, 12528, 791, 4388, 517, 2217, 5404, 14447, 7106, 8053, 9754, 12727, 5653, 267, 3773, 8112, 5122, 10145, 4429, 5071, 5890, 8722, 6639, 6022, 8451, 10692, 1310, 9853, 3794, 10142, 13751, 5136, 3457, 6636, 2459, 10370, 13478, 4674, 11681, 6658, 11027, 2512, 11433, 7872, 3917, 9741, 12076, 4837, 6689, 1111, 2327, 7220, 5554, 2052, 2538, 10775, 10854, 13378, 2294, 127, 12888, 830, 1502, 471, 2269, 2738, 3055, 2427, 10570, 6009, 10596, 4785, 3532, 8195, 1466, 8126, 10390, 13431, 6312, 11038, 12008, 14427, 13166, 1330, 10800, 4681, 8441, 1409, 7043, 4318, 6823, 10770, 2618, 5994, 8889, 8501, 11346, 9138, 13782, 10215, 12559, 3074, 11275, 2001, 6777, 4444, 7589, 4523, 1497, 2392, 989, 6245, 2407, 10774, 13435, 6430, 11126, 5293, 773, 6898, 12256, 9224, 6880, 8703, 12035, 8347, 13866, 13161, 5312, 7898, 3100, 10009, 10437, 7111, 8745, 11411, 2125, 4576, 2525, 2489, 2332, 9606, 7917, 12506, 1987, 3514, 5468, 10083, 5417, 13269, 5977, 3423, 418, 13822, 4710, 7546, 2138, 2834, 5152, 5323, 1616, 10588, 2105, 1324, 12454, 795, 9079, 3581, 5904, 6391, 10316, 2851, 7538, 10736, 8636, 10574, 1121, 6904, 7063, 13908, 10761, 8543, 6828, 8675, 12058, 11368, 61, 3099, 6959, 8266, 10415, 5841, 8299, 2406, 10053, 11766, 11000, 3731, 1991, 9130, 11249, 7903, 4054, 4722, 3708, 6903, 10369, 3896, 4715, 6164, 11701, 11427, 7469, 2926, 13800, 6940, 4212, 2734, 11063, 7994, 11567, 11369, 4548, 7282, 8449, 752, 13487, 6519, 13579, 4007, 12769, 11863, 10863, 4291, 13832, 7672, 2958, 8135, 11177, 986, 8495, 5183, 11895, 980, 12611, 5581, 7722, 4262, 8426, 8569, 2999, 3067, 594, 10660, 9183, 7697, 11739, 11888, 8220, 4087, 13893, 13695, 9993, 2387, 2291, 2439, 10374, 8808, 8146, 14388, 2303, 13268, 10926, 7558, 3192, 14379, 6318, 10325, 2137, 8022, 9467, 7325, 6225, 13301, 6657, 3378, 6104, 13131, 3270, 13830, 9518, 12834, 11857, 1333, 2279, 6887, 12821, 9198, 6462, 12126, 2499, 11996, 3159, 9325, 9728, 8775, 7738, 9377, 6669, 6232, 8642, 6282, 9013, 10819, 3483, 4794, 5015, 801, 13214, 9418, 1725, 1191, 1321, 4823, 5670, 13324, 10662, 988, 10967, 5192, 13043, 6870, 3476, 4900, 557, 5852, 3823, 10222, 4339, 5910, 10940, 2184, 1581, 5820, 10733, 6471, 11649, 2089, 993, 13042, 7377, 12122, 13398, 3487, 3442, 5572, 3821, 4915, 14116, 10141, 5456, 2007, 5002, 1560, 11209, 7480, 1741, 2530, 11222, 8987, 14406, 2743, 7763, 12172, 2161, 12782, 12884, 13780, 8143, 3850, 7372, 8985, 3383, 6157, 1578, 14387, 4423, 10602, 13798, 1929, 6942, 10919, 3518, 6503, 4831, 12773, 5689, 117, 1635, 439, 2108, 11001, 2704, 8813, 7556, 142, 5982, 6623, 9879, 5847, 12169, 8452, 948, 2998, 12976, 5467, 1824, 3819, 511, 1583, 8517, 11536, 6178, 11271, 11515, 7667, 6201, 4760, 7353, 7577, 4697, 6443, 4832, 10868, 3741, 8359, 11186, 14086, 195, 10207, 8727, 13081, 254, 13862, 7258, 5828, 537, 11894, 13439, 10116, 2874, 7342, 8207, 12743, 5490, 2476, 8322, 13327, 3285, 4543, 5549, 480, 4336, 2744, 11756, 12839, 6724, 13859, 11135, 11968, 8036, 12056, 6026, 11949, 8294, 4035, 769, 8140, 5070, 13675, 2424, 9443, 10677, 10021, 13734, 11190, 8004, 498, 7092, 5114, 10264, 5582, 5129, 2124, 959, 532, 7780, 4274, 7295, 7636, 4695, 13946, 1897, 2968, 1891, 4315, 4436, 8191, 11737, 4226, 11564, 4942, 7751, 9708, 10998, 8086, 10477, 4309, 5613, 9671, 2121, 4966, 10749, 10760, 5388, 8550, 13151, 10080, 10182, 2856, 11930, 978, 13118, 3138, 12975, 1474, 8700, 5596, 6055, 11185, 5107, 9356, 7074, 12213, 12903, 6618, 12156, 11927, 12239, 8363, 7416, 13019, 2230, 876, 4341, 7770, 12553, 3885, 10691, 14021, 11403, 8133, 13630, 393, 11653, 10522, 13971, 3915, 4498, 12627, 767, 5115, 3882, 10874, 8931, 4097, 7428, 6326, 4802, 4936, 6012, 5921, 10088, 5223, 2803, 7034, 9423, 6847, 11572, 2054, 10276, 10000, 12637, 2634, 6712, 2726, 422, 1688, 13789, 8560, 13147, 554, 12222, 7402, 10189, 6900, 10791, 7219, 9408, 1530, 8988, 5806, 4678, 5438, 1024, 7643, 4468, 6653, 10979, 14435, 10209, 3692, 386, 3562, 2037, 4904, 11522, 13809, 9992, 11506, 379, 11410, 9252, 12522, 5944, 4142, 7860, 7198, 8567, 6488, 8031, 14210, 10805, 3051, 8002, 3978, 9458, 2478, 6584, 6929, 13254, 1313, 1922, 12631, 8877, 3254, 1389, 2065, 12752, 4614, 10676, 6145, 2943, 1326, 9024, 7082, 10480, 8350, 3567, 12801, 8536, 4044, 7398, 13251, 3586, 12019, 11151, 2820, 9193, 8154, 11642, 1042, 4806, 6330, 5151, 9935, 10838, 13360, 968, 8379, 10054, 1983, 5993, 9466, 14368, 12931, 9093, 6737, 2142, 11763, 3691, 3512, 6668, 1386, 7124, 10448, 9045, 1934, 3721, 780, 11887, 6434, 12986, 11202, 7690, 8054, 8008, 8429, 11072, 9328, 3025, 3218, 5231, 3963, 9978, 5590, 10430, 6714, 10996, 7212, 3060, 8726, 11610, 1682, 4354, 1233, 4190, 7678, 5608, 14437, 12974, 3134, 5041, 10743, 10859, 2667, 12519, 1857, 13527, 6609, 12417, 11172, 194, 7608, 1089, 10486, 3305, 6431, 10610, 8506, 1941, 12784, 13783, 5955, 6066, 8509, 9018, 1082, 13126, 7649, 1797, 2222, 3513, 8048, 8443, 13034, 8296, 2253, 10824, 13655, 7815, 12568, 11384, 5886, 6118, 1408, 7936, 4132, 12802, 6146, 973, 12525, 3078, 8923, 14002, 13829, 963, 7153, 10582, 5764, 12082, 8578, 9469, 10203, 8513, 10713, 4008, 6748, 9269, 11508, 13865, 5325, 2405, 3142, 11216, 6778, 702, 1355, 13956, 1193, 10107, 11258, 6756, 6109, 9386, 12718, 6344, 9062, 9194, 7757, 5178, 6648, 9277, 4914, 6143, 515, 2882, 7089, 8412, 13138, 10025, 12010, 7463, 9899, 6804, 1542, 10219, 3846, 845, 2072, 0, 11138, 5166, 7614, 9187, 148, 9770, 12424, 7493, 218, 12641, 3373, 14295, 6169, 13564, 5988, 13164, 287, 5322, 197, 6733, 5930, 10169, 4149, 1319, 10533, 6706, 10484, 247, 2758, 7848, 8237, 13640, 11061, 4500, 804, 7189, 7251, 12713, 354, 4449, 5051, 1835, 9228, 6966, 11498, 5142, 2739, 6612, 1883, 157, 8709, 4633, 1919, 13490, 8157, 11053, 4235, 1759, 7214, 2776, 6419, 6075, 5204, 112, 5189, 5010, 12366, 11, 6566, 13430, 9371, 10396, 13605, 11120, 11332, 13917, 11853, 3962, 8247, 3180, 8012, 13421, 12000, 13355, 7274, 10481, 13067, 11851, 865, 1210, 6883, 9854, 2098, 8750, 3829, 2458, 2966, 8300, 206, 2879, 13936, 5928, 66, 6510, 12358, 528, 5061, 9490, 10590, 8753, 1400, 2049, 4847, 7900, 9165, 2380, 11854, 8752, 5561, 2549, 8009, 13177, 10746, 6455, 13525, 5144, 8471, 12775, 8153, 5553, 11670, 13744, 11439, 2047, 9403, 13014, 6888, 6226, 5618, 6051, 10856, 9575, 13482, 13107, 5736, 8613, 8428, 674, 4714, 5514, 7773, 4451, 8909, 12447, 6681, 10236, 578, 12534, 9912, 4406, 6982, 10194, 3246, 11631, 2474, 3528, 6084, 10067, 1594, 11161, 1637, 11679, 9190, 9856, 5016, 3539, 11100, 8526, 10655, 11552, 12201, 6367, 3938, 4400, 7451, 5259, 3631, 13295, 12165, 11096, 10499, 10818, 5158, 9620, 4928, 5635, 3359, 11738, 6449, 2452, 11013, 7795, 7417, 12021, 3505, 11840, 5714, 12362, 12896, 8029, 6390, 2904, 9680, 6152, 9655, 1047, 4394, 3050, 6382, 3859, 4663, 1608, 6352, 6721, 10867, 11924, 3754, 4983, 9205, 8982, 6988, 12803, 10318, 6710, 8684, 3694, 13888, 8427, 9608, 13150, 6321, 14420, 11971, 3688, 8039, 1916, 1911, 13874, 6372, 7054, 7959, 3044, 10433, 10812, 14250, 9291, 9381, 5610, 12661, 7347, 768, 9830, 7512, 3076, 11085, 11860, 5809, 8827, 9561, 10162, 941, 3094, 8254, 8976, 51, 1940, 999, 8535, 11855, 3729, 3426, 9965, 9844, 6016, 6971, 3722, 7514, 6967, 4431, 6004, 4346, 11163, 5873, 2251, 12190, 4159, 6100, 10242, 2740, 5163, 1726, 3686, 299, 7042, 7373, 9151, 4438, 6578, 7371, 2862, 4699, 3625, 8162, 3762, 5768, 6913, 432, 13977, 9370, 1277, 2531, 4475, 2036, 11838, 5832, 1494, 3386, 5734, 5228, 4680, 13824, 1011, 10072, 11694, 11793, 9435, 5935, 6994, 9653, 14413, 8494, 12025, 4927, 11451, 623, 9914, 2155, 6587, 4211, 6835, 7908, 4736, 2189, 9346, 8778, 2266, 6692, 9137, 902, 3170, 6543, 4342, 8202, 12597, 7818, 7425, 3937, 7423, 9196, 4381, 12060, 4595, 1395, 823, 3363, 6202, 4862, 10561, 5937, 4174, 5762, 9222, 14209, 5214, 1157, 13484, 12042, 2606, 5782, 6211, 7687, 947, 14112, 5348, 3098, 8024, 2891, 8215, 13236, 8880, 4731, 11731, 6483, 5364, 4630, 434, 6925, 12639, 2773, 4139, 5763, 10248, 10924, 212, 3160, 7505, 10710, 7277, 3174, 4295, 10869, 3772, 1251, 10500, 12269, 4563, 4713, 6548, 11260, 2350, 6052, 9016, 2961, 12897, 7419, 4533, 7333, 5936, 11436, 8286, 12835, 3344, 3379, 4952, 2419, 12862, 10741, 355, 3584, 4311, 10029, 1947, 7329, 10458, 588, 5172, 11533, 3650, 2903, 10688, 13396, 13817, 4976, 2788, 2685, 6585, 6901, 2261, 3799, 9378, 10986, 4347, 9816, 11018, 9427, 8391, 14205, 13316, 12184, 7958, 6915, 3671, 9275, 6973, 2786, 1299, 8328, 13903, 2412, 5449, 13418, 13758, 4937, 12436, 8337, 8592, 4270, 9298, 6206, 13146, 13157, 1260, 7094, 5465, 8723, 6739, 2534, 2048, 2846, 13673, 411, 8040, 6369, 6103, 5634, 3139, 7940, 10520, 10167, 9333, 2894, 7891, 10834, 8810, 1743, 9584, 10104, 10205, 8955, 10656, 4902, 3649, 2177, 243, 723, 12064, 7434, 1429, 11278, 12036, 7581, 6765, 6753, 11730, 4053, 13011, 13775, 3120, 9452, 9820, 4526, 10173, 4124, 2937, 10428, 6172, 9060, 9982, 9136, 4089, 3711, 8949, 9129, 2382, 7209, 11792, 6490, 10861, 1674, 4487, 685, 11994, 5715, 7177, 4988, 584, 9098, 3566, 1431, 13970, 7348, 8281, 14354, 3617, 11856, 6852, 5607, 3803, 9749, 5856, 13691, 8697, 8032, 11958, 6920, 10823, 5262, 4424, 4297, 8751, 6541, 5808, 13229, 11356, 6368, 11466, 11627, 1385, 5210, 3941, 1527, 13530, 3818, 864, 7503, 7692, 3241, 11465, 6017, 5999, 11520, 7499, 7976, 3017, 11293, 10579, 1780, 11754, 7549, 4768, 12973, 8994, 10829, 208, 9118, 5735, 6989, 11024, 8041, 5792, 4765, 2900, 2811, 8815, 12833, 5276, 10296, 5901, 7187, 12404, 3971, 11455, 10651, 2956, 1007, 13044, 7705, 8023, 7316, 5805, 1458, 3523, 1464, 3152, 10114, 13733, 904, 7412, 6978, 10591, 7912, 8899, 12858, 7381, 12356, 806, 856, 2532, 2802, 10003, 6148, 9705, 9696, 472, 7844, 3574, 9180, 5532, 12517, 6709, 13561, 8764, 11691, 11613, 7821, 8957, 7107, 8070, 224, 11379, 9230, 12397, 5286, 7666, 2847, 479, 10678, 5924, 2814, 7542, 4214, 1644, 12682, 11052, 3052, 1453, 6743, 7616, 14050, 2616, 3521, 5908, 1156, 7165, 2573, 12377, 227, 6358, 2042, 11040, 9859, 13497, 9537, 11302, 9580, 491, 6848, 9374, 8996, 2415, 11196, 14023, 12619, 7910, 2455, 7140, 9652, 14084, 4457, 316, 2085, 8082, 3127, 5108, 11298, 8181, 6043, 3814, 6267, 9432, 3732, 3164, 8563, 8240, 4407, 13531, 11367, 1799, 519, 14272, 1969, 10280, 6185, 873, 6707, 2292, 994, 3215, 11569, 1282, 2632, 6199, 14051, 6200, 6868, 6485, 13701, 706, 380, 3737, 9491, 7683, 3126, 13253, 3331, 10111, 207, 5141, 7816, 8208, 1701, 4923, 10250, 8073, 8685, 2331, 4728, 10347, 8310, 3233, 2351, 7735, 7118, 13154, 7211, 10228, 5791, 5913, 5425, 7343, 90, 6397, 9335, 248, 11640, 9838, 10977, 8954, 7857, 7594, 4088, 8546, 1634, 12282, 2994, 5226, 9872, 4664, 3945, 5602, 3780, 11010, 8934, 7843, 9758, 11441, 11438, 9276, 1274, 5877, 11022, 11431, 7506, 5775, 9147, 13480, 13179, 5703, 14335, 1652, 10883, 2824, 13631, 10700, 3175, 8398, 11511, 11705, 8914, 12638, 6891, 8263, 4693, 1999, 4499, 9888, 10600, 3167, 5017, 13680, 1709, 10559, 4546, 8270, 10028, 8431, 8370, 1086, 7448, 7098, 3807, 2061, 6696, 12989, 10123, 4977, 2885, 175, 12940, 1864, 4201, 2082, 13284, 10962, 5469, 4121, 10312, 995, 11416, 4780, 4061, 3767, 11521, 2890, 3770, 3892, 5858, 3624, 3957, 10608, 5246, 11062, 13145, 11663, 3119, 7154, 1072, 2456, 489, 11414, 5851, 13296, 2654, 4099, 7191, 8912, 1619, 6725, 10616, 2828, 3534, 3871, 7278, 11428, 2325, 4349, 2118, 7242, 6660, 5000, 679, 7942, 5211, 5701, 412, 7707, 4705, 11810, 14100, 8114, 9614, 4601, 2171, 6269, 10002, 7215, 1215, 9178, 13319, 10301, 6056, 12569, 5603, 14298, 5968, 10675, 5915, 2257, 10898, 4231, 2239, 13464, 7647, 102, 5205, 11859, 5430, 5035, 5833, 306, 2015, 1363, 4361, 11462, 7521, 2813, 748, 12645, 4319, 10527, 1476, 4590, 5857, 5949, 10795, 11449, 3976, 13915, 4282, 5409, 10888, 11812, 12554, 2750, 7121, 7253, 4497, 977, 7918, 9254, 5140, 9832, 9855, 4163, 9354, 7391, 13394, 5303, 5552, 8882, 4913, 5327, 7437, 4507, 331, 840, 10765, 1098, 9801, 4972, 12906, 13898, 10560, 11844, 4795, 1194, 2604, 8253, 9878, 10033, 3145, 2214, 8046, 8896, 4395, 3757, 1569, 7490, 13112, 14057, 9675, 8304, 2917, 8921, 8313, 1570, 13502, 11565, 11773, 12115, 133, 12334, 6453, 12558, 2114, 10202, 9244, 8811, 9264, 1185, 4814, 5533, 8757, 203, 3406, 7449, 8870, 4574, 13025, 7155, 2264, 750, 14322, 9047, 7324, 2472, 11591, 13033, 626, 12699, 10014, 283, 5926, 2597, 12463, 8765, 13920, 3578, 8992, 9237, 634, 3198, 14291, 654, 10382, 793, 12612, 603, 9527, 14185, 13232, 2978, 12034, 13877, 5788, 3844, 6962, 5044, 8730, 7786, 3952, 6394, 12095, 7433, 5575, 2204, 1054, 13128, 13078, 9690, 9117, 12640, 11199, 2127, 12971, 2883, 10310, 774, 6376, 12442, 1066, 80, 6945, 11322, 3361, 12297, 8616, 3703, 7022, 6551, 13722, 9677, 10893, 698, 924, 5254, 1661, 13654, 14356, 219, 1268, 14056, 12343, 6523, 4332, 8544, 11482, 3490, 6774, 10992, 11829, 9015, 4567, 13746, 8741, 10406, 4112, 7793, 6950, 7224, 13512, 3709, 956, 1810, 11839, 3488, 91, 13895, 2254, 7238, 682, 8186, 3111, 8069, 8937, 8844, 10572, 6580, 3634, 9115, 1945, 2541, 5160, 1437, 4203, 7116, 4570, 179, 4209, 3802, 8531, 5275, 7767, 14319, 3636, 5894, 13139, 6197, 4225, 1873, 6015, 5351, 7939, 11351, 11134, 9789, 2262, 4687, 12152, 3974, 7359, 10711, 13467, 11230, 10234, 12548, 2867, 3778, 1491, 3143, 6278, 2062, 9989, 3653, 5978, 1351, 10246, 4199, 10639, 12470, 10496, 8706, 94, 2227, 1136, 12854, 11352, 6036, 10804, 5947, 4579, 3981, 9314, 261, 4164, 7657, 12809, 7185, 10921, 2437, 9007, 2910, 2965, 13343, 5372, 765, 4050, 7276, 11227, 2927, 725, 4539, 11362, 2852, 12511, 4078, 4019, 9634, 1710, 9249, 8620, 4958, 10018, 6640, 3117, 641, 2889, 1387, 5818, 6018, 829, 11580, 9724, 12806, 5601, 1727, 10184, 9803, 12403, 8257, 11081, 296, 5398, 8941, 1869, 8831, 13099, 2615, 8973, 10127, 2226, 3296, 4611, 7056, 5509, 3104, 5452, 5789, 8079, 5950, 5067, 2361, 4821, 12818, 12676, 14325, 10788, 10892, 2175, 6791, 1338, 11513, 7178, 4991, 7081, 9760, 12296, 2976, 12980, 14033, 8503, 7907, 5432, 9833, 7473, 5064, 2150, 7336, 5123, 10786, 3933, 13137, 3736, 1166, 8199, 1249, 10292, 3253, 6364, 7913, 10908, 7102, 2711, 96, 12543, 5519, 13277, 6650, 7513, 11141, 8450, 5386, 6116, 7718, 8991, 3057, 2259, 5880, 2588, 93, 9893, 13173, 2832, 10240, 10087, 5559, 2509, 9938, 1796, 9033, 14151, 2397, 5155, 14257, 8323, 10704, 5529, 13227, 3384, 3494, 189, 10718, 7255, 8233, 7975, 6792, 12237, 736, 7750, 12911, 8387, 9784, 606, 1917, 10942, 14381, 3321, 14271, 11588, 3123, 3440, 12120, 3404, 13702, 12151, 10086, 1762, 1894, 3603, 13167, 4419, 12452, 4588, 10372, 3529, 13763, 7248, 13845, 3040, 8196, 5709, 3237, 1902, 121, 9685, 7544, 9954, 8647, 12251, 2288, 6954, 3355, 7009, 1849, 5153, 1105, 5750, 6949, 11268, 8476, 7638, 1415, 2009, 1197, 6028, 7779, 11944, 5269, 11380, 4173, 6001, 13176, 5028, 2363, 8065, 7296, 4489, 778, 11977, 2304, 10008, 9895, 11963, 5599, 4224, 836, 13113, 4887, 1882, 12788, 5622, 11197, 1017, 984, 2716, 13979, 8932, 11305, 6783, 7740, 10978, 3925, 11307, 8419, 8807, 4063, 3464, 805, 7015, 7691, 1921, 11509, 9774, 657, 1723, 10005, 8315, 4426, 4243, 10049, 204, 11987, 11408, 3852, 3264, 12137, 7693, 7582, 3975, 8169, 4508, 7617, 2078, 13750, 6626, 10744, 12374, 6937, 13352, 1412, 4621, 644, 10510, 4833, 11075, 1856, 7894, 8271, 8871, 10442, 13553, 4374, 4756, 2505, 488, 1572, 6760, 11662, 10383, 9823, 9702, 7351, 8554, 12529, 4028, 9683, 8780, 10612, 2193, 10275, 10287, 9042, 492, 783, 8120, 14143, 11548, 6853, 8819, 2931, 12799, 2898, 14367, 3697, 12080, 5891, 3410, 12130, 8925, 2432, 11700, 13279, 9837, 1099, 10112, 1997, 5587, 13310, 8236, 6504, 3064, 2240, 5487, 10252, 4712, 8474, 2268, 2272, 1506, 2656, 4903, 9519, 6241, 5646, 3548, 9636, 8005, 10909, 8168, 9551, 10682, 4119, 13560, 2515, 9988, 10502, 1538, 97, 11082, 5427, 10720, 6486, 710, 12413, 12258, 1461, 1216, 7899, 6530, 7673, 4059, 1792, 8772, 11103, 4314, 1932, 5663, 1696, 6229, 5099, 9824, 1532, 6021, 875, 1910, 6809, 6031, 4995, 3263, 11409, 409, 2877, 12970, 2094, 11405, 14432, 8711, 5784, 14068, 9379, 2807, 11091, 2083, 2104, 13012, 1212, 10939, 8459, 2298, 6096, 8021, 4606, 6418, 5756, 9565, 13197, 7749, 5611, 13698, 13637, 2946, 14147, 4604, 13445, 9368, 8664, 1262, 1192, 5360, 7467, 13839, 11019, 10132, 2948, 6446, 10914, 7700, 7632, 12701, 6098, 7379, 9987, 8447, 855, 4472, 6672, 10453, 5024, 7241, 13086, 962, 8325, 1981, 4106, 8576, 2638, 3028, 5648, 341, 5422, 2341, 7481, 7535, 13883, 2010, 2677, 13488, 7619, 4675, 6484, 11837, 4505, 3946, 7704, 11287, 6370, 11673, 14059, 7399, 9256, 11966, 187, 3877, 11337, 6894, 10062, 12284, 11237, 186, 628, 5250, 12870, 6393, 4320, 7728, 12308, 2570, 10066, 3934, 513, 5387, 5241, 10035, 2206, 3920, 3824, 10916, 5337, 11843, 9273, 7199, 6850, 6313, 7595, 7776, 10263, 9717, 10995, 11016, 463, 10777, 3367, 11721, 10976, 1232, 3366, 6125, 6970, 11950, 1246, 3970, 7974, 3993, 3524, 10811, 3930, 12693, 5774, 1265, 12420, 1862, 8074, 10082, 8790, 810, 1118, 5466, 3854, 12582, 4901, 6400, 11574, 1738, 9528, 162, 11985, 10937, 3007, 6041, 1229, 2753, 10463, 5844, 9944, 4090, 13051, 13443, 7271, 7855, 10147, 983, 1524, 2156, 6827, 4443, 3784, 5119, 6596, 8183, 6112, 920, 10095, 2574, 1667, 7020, 8720, 4417, 6910, 2360, 11248, 10395, 4051, 2324, 4, 10703, 5686, 3412, 7109, 8653, 4757, 2401, 10358, 4926, 10735, 704, 9640, 14242, 8725, 613, 11902, 3641, 1176, 2749, 119, 12065, 199, 9103, 5983, 11401, 398, 1361, 3465, 8945, 8662, 370, 7721, 7091, 5233, 5069, 5845, 3437, 8353, 3554, 7146, 3157, 5076, 9144, 12677, 8200, 11289, 3587, 9292, 2576, 11304, 6324, 6083, 5799, 2888, 10227, 12444, 10388, 8604, 8249, 9739, 7698, 4659, 13057, 6002, 11338, 1523, 12892, 12787, 1421, 3183, 7977, 6604, 940, 5299, 10876, 10302, 969, 9488, 13995, 9806, 7024, 13083, 5755, 6060, 3071, 13231, 11017, 11391, 1009, 867, 11550, 10592, 2689, 6058, 13652, 9493, 11502, 9080, 6207, 5113, 5311, 12979, 7628, 2209, 10985, 2833, 13996, 4153, 3916, 9976, 3224, 6985, 1433, 4062, 10745, 5801, 1826, 369, 1694, 1764, 8993, 8547, 11025, 7470, 3682, 9648, 1758, 960, 10787, 7822, 7411, 3370, 9087, 2815, 7573, 3239, 5027, 11761, 13223, 7495, 10585, 7221, 6619, 13546, 10130, 2394, 10991, 1153, 3506, 2055, 2754, 12174, 4628, 9928, 131, 923, 1775, 2280, 2908, 4881, 10291, 2554, 1306, 6788, 10408, 7028, 1913, 7902, 176, 3753, 573, 1275, 6877, 2102, 10179, 607, 10936, 10526, 12753, 3725, 6140, 2484, 7534, 8903, 6663, 8645, 2642, 9321, 6873, 4980, 10567, 5884, 6964, 7951, 11723, 5681, 3610, 4250, 9035, 11373, 5710, 7141, 10950, 9350, 11484, 7766, 4612, 8821, 314, 2967, 10094, 6666, 533, 10457, 4578, 9503, 2581, 3898, 8587, 8989, 4293, 13440, 7436, 10575, 2195, 6377, 1836, 2215, 7247, 9861, 14064, 1399, 12320, 4437, 6054, 14448, 11937, 5777, 7407, 8809, 1372, 2854, 5558, 13132, 10124, 10155, 5997, 1927, 11442, 8232, 13162, 9665, 6454, 13429, 4779, 11608, 13226, 903, 4141, 4403, 2645, 7656, 1012, 2147, 8308, 8099, 13069, 3416, 8138, 2032, 9282, 2151, 9567, 14443, 19, 4208, 13143, 4908, 11360, 10628, 2687, 12600, 7971, 582, 10649, 9693, 3141, 6565, 11425, 1298, 7627, 13021, 13975, 7227, 1555, 693, 5358, 821, 8872, 13237, 4679, 11899, 4623, 1972, 13711, 12486, 2672, 8141, 6048, 1060, 11310, 5098, 11225, 3791, 6063, 12448, 2858, 12045, 4278, 8260, 10513, 2893, 6357, 9972, 11628, 4939, 6228, 10848, 8655, 6998, 1342, 7802, 14177, 13779, 11884, 2287, 2179, 7752, 7598, 8180, 8453, 13168, 4960, 9375, 7110, 4458, 3252, 14163, 5636, 7923, 6281, 14332, 2714, 4283, 11555, 14334, 1777, 13685, 1295, 7322, 5504, 12866, 10090, 13370, 12848, 7439, 13453, 6829, 2649, 6333, 2165, 4529, 4661, 6805, 5485, 4017, 8800, 5625, 7828, 1955, 5121, 11483, 201, 2346, 6298, 3847, 43, 3172, 11353, 11087, 11262, 12422, 9399, 9956, 4450, 5263, 4369, 12133, 7895, 1529, 3632, 1426, 1484, 9440, 111, 14281, 8732, 10646, 7078, 9501, 375, 5431, 12610, 4625, 5054, 1872, 7931, 2444, 5855, 3149, 13526, 5109, 3890, 990, 5082, 8818, 4321, 2733, 11279, 13644, 9143, 1124, 8466, 2418, 6361, 4389, 356, 11394, 1874, 9737, 6007, 9777, 8702, 10420, 762, 1554, 8747, 11285, 1380, 12409, 6822, 991, 1602, 2470, 6856, 11495, 5006, 1449, 6675, 12078, 12162, 3855, 4448, 7145, 8273, 4338, 11659, 5175, 7826, 13489, 11865, 6694, 4072, 1716, 834, 5624, 3265, 3310, 8658, 7478, 3395, 8457, 5502, 4356, 8667, 7126, 3592, 6288, 7269, 5946, 8148, 2609, 12043, 6576, 3901, 10809, 7190, 1168, 5895, 12791, 8166, 1706, 5811, 6772, 3422, 4624, 13965, 483, 9071, 1358, 10797, 289, 13715, 9790, 3470, 7652, 10850, 7471, 9874, 12599, 8797, 4685, 6040, 4014, 10552, 6890, 7788, 2774, 2057, 5565, 1881, 5888, 12408, 6474, 4098, 9066, 13879, 13861, 3376, 7724, 7982, 10128, 5416, 1839, 12967, 2021, 7926, 11595, 9931, 4333, 3891, 5823, 6508, 3320, 7578, 1252, 12071, 3904, 3619, 8875, 7937, 14105, 6183, 8001, 10321, 8724, 6768, 13571, 4038, 12257, 2539, 12768, 13354, 6568, 5719, 11413, 8425, 10386, 11833, 8246, 5203, 3346, 13411, 10446, 9471, 6595, 2760, 5889, 10335, 7602, 2119, 444, 1613, 11497, 13870, 8016, 3314, 8411, 11443, 9250, 14219, 6130, 1327, 14006, 10289, 8564, 4296, 1548, 1956, 2934, 12386, 8357, 11238, 8184, 13806, 5092, 2176, 10381, 10927, 9319, 593, 13367, 2383, 319, 7194, 976, 14296, 8107, 10170, 8152, 2556, 13383, 4704, 4777, 4102, 4335, 8570, 1638, 4792, 696, 3728, 4911, 10324, 5125, 470, 11626, 2722, 4771, 1084, 3730, 2213, 13111, 11312, 3326, 6582, 4565, 2191, 12715, 1102, 12052, 5124, 3456, 6536, 12240, 7231, 4839, 4747, 2821, 13291, 7369, 625, 2875, 7050, 4440, 8248, 384, 3072, 5229, 8758, 6941, 13339, 13050, 1334, 3479, 14324, 9108, 3362, 14011, 6429, 4742, 10952, 6814, 2359, 7540, 8060, 11231, 5426, 6641, 9897, 11733, 794, 10196, 9811, 1259, 4948, 937, 3300, 6078, 12482, 7250, 1283, 11223, 4323, 9637, 1203, 1577, 2906, 123, 1369, 7429, 8456, 1126, 2270, 13037, 10193, 3432, 12990, 8792, 9507, 1598, 2759, 4974, 14399, 2016, 357, 7928, 6527, 2234, 8256, 8439, 6697, 827, 9900, 8986, 10117, 13843, 6416, 3029, 8850, 12081, 13927, 11250, 1711, 7765, 6027, 12786, 8030, 3727, 3336, 10293, 3931, 6479, 13045, 13708, 5657, 9311, 14302, 7267, 7205, 8402, 4479, 6969, 8806, 9782, 3921, 5729, 9600, 13449, 13944, 2941, 2005, 11646, 268, 12724, 13469, 5083, 11257, 3401, 2721, 8306, 13572, 5220, 4041, 10512, 1573, 10622, 2823, 11215, 7858, 10783, 4519, 10450, 9581, 9711, 7460, 8371, 11164, 6437, 1868, 10790, 6427, 10840, 558, 11039, 9470, 1337, 7654, 10530, 8521, 9065, 13710, 401, 4271, 7454, 11056, 11195, 9056, 7723, 11006, 2699, 6629, 3165, 4610, 10467, 1296, 5341, 4210, 8707, 8230, 2957, 1227, 13958, 5649, 7068, 9781, 7866, 10696, 41, 10224, 732, 2393, 3338, 7886, 5776, 4020, 1170, 3351, 4984, 5980, 7234, 9338, 8505, 10084, 3301, 1522, 5866, 7308, 4956, 8442, 8814, 10835, 3275, 3467, 5264, 364, 942, 7745, 3580, 11365, 11918, 8828, 7877, 1504, 4890, 699, 2707, 10716, 2982, 6593, 9647, 1769, 13377, 11806, 12124, 9798, 12480, 6283, 10621, 12011, 7031, 6534, 12475, 1221, 677, 2810, 12003, 13834, 671, 10204, 7440, 6656, 7954, 7059, 9040, 7734, 3648, 4217, 7861, 7104, 11668, 1446, 7835, 13053, 3654, 7522, 4668, 4829, 9053, 2641, 11684, 6751, 3207, 7497, 627, 11592, 3570, 684, 6156, 10046, 12142, 2983, 4460, 872, 13725, 7747, 5374, 3747, 6428, 5011, 1801, 1187, 10664, 9678, 3325, 2000, 5951, 3644, 11317, 3924, 10032, 4933, 10251, 9315, 2386, 8335, 4613, 6726, 9243, 12547, 10354, 10274, 13181, 1420, 919, 11366, 1893, 10073, 2633, 1737, 1520, 895, 13276, 6085, 249, 9813, 9270, 8966, 1850, 5252, 13158, 11344, 4825, 7986, 13444, 4871, 6603, 3594, 8922, 8217, 1915, 7653, 1773, 8078, 1129, 13729, 3489, 6341, 2755, 11073, 5684, 10282, 13318, 10935, 7249, 13093, 12723, 293, 1811, 3137, 4122, 8795, 2872, 8187, 13871, 7783, 4386, 3986, 2652, 11123, 10944, 6050, 4136, 6802, 6421, 3385, 8063, 4970, 2710, 4910, 2907, 9455, 1000, 14411, 4024, 113, 5378, 12766, 12301, 14115, 1096, 8694, 2839, 3836, 8898, 6049, 3525, 8519, 1200, 11281, 813, 4684, 8714, 10414, 5740, 10378, 1305, 11160, 11080, 2190, 8488, 4027, 8633, 1030, 12932, 10490, 4048, 4735, 7787, 1028, 10792, 4176, 6150, 12735, 1218, 8362, 311, 10277, 6575, 9663, 10416, 10402, 10630, 12134, 9712, 13454, 6693, 7062, 8394, 8572, 5198, 10519, 9358, 11175, 4184, 334, 10397, 318, 4770, 14385, 7338, 6614, 7, 6080, 1605, 6652, 11474, 5893, 4950, 2630, 7571, 1471, 14181, 5300, 7163, 4835, 7543, 6520, 12328, 11329, 1052, 7458, 3817, 1770, 10350, 13198, 8303, 10865, 3212, 5149, 1592, 6339, 11274, 4838, 8541, 9716, 13885, 4147, 7620, 5803, 7464, 4039, 14215, 5454, 691, 5645, 12584, 11724, 14233, 10329, 10569, 5700, 2913, 8067, 9233, 3810, 9768, 6980, 13130, 14426, 7756, 4585, 629, 10891, 9812, 8565, 11078, 12489, 8769, 2352, 419, 3583, 6722, 7360, 9286, 1734, 8221, 3912, 6230, 2577, 9186, 5437, 507, 8823, 5785, 7807, 3543, 10387, 2928, 9533, 2590, 981, 8267, 6501, 13342, 3955, 927, 14424, 8504, 9111, 5787, 1005, 10129, 6174, 9429, 11210, 549, 1904, 3209, 8259, 3550, 12379, 4501, 8635, 9058, 11370, 11280, 2100, 14081, 13554, 3280, 2507, 11491, 726, 3630, 8939, 2202, 6295, 8577, 12378, 2481, 6038, 10714, 8826, 192, 1629, 3110, 14199, 10355, 8959, 9885, 7651, 10343, 9532, 6257, 13657, 8713, 290, 2838, 7252, 5660, 11979, 10594, 11516, 10149, 8593, 5503, 11158, 4517, 12236, 5185, 1116, 4000, 10965, 6780, 5522, 6860, 2117, 11489, 10969, 3639, 4632, 1493, 3454, 1798, 6875, 646, 2216, 4849, 10333, 1942, 9674, 9107, 12324, 2513, 2516, 13346, 3238, 4540, 5005, 7932, 1744, 6209, 7935, 847, 12416, 11328, 13818, 4230, 3718, 4218, 5861, 12471, 10764, 2703, 9179, 13509, 5260, 2087, 8693, 9732, 4511, 14026, 4135, 2895, 10813, 5330, 8242, 781, 4885, 4113, 6342, 647, 4446, 7997, 2265, 10879, 8830, 8198, 4276, 11961, 7609, 5131, 13016, 9309, 6938, 2281, 11179, 5343, 884, 486, 7567, 1032, 6505, 13909, 5266, 1343, 4261, 10058, 1492, 12028, 141, 1379, 2447, 2320, 2779, 4196, 5240, 2503, 12635, 4157, 4198, 10945, 2396, 9246, 14359, 3217, 11586, 1312, 11076, 4299, 10550, 253, 10667, 8432, 7243, 13242, 4559, 10317, 3000, 11284, 1142, 14362, 12325, 3927, 2620, 7000, 6840, 14309, 2317, 4964, 9169, 14345, 13723, 1469, 12557, 7382, 4385, 13842, 5329, 8907, 2971, 12938, 104, 10517, 2510, 3608, 5783, 10538, 2310, 9772, 1993, 11672, 13500, 4882, 10629, 9428, 9934, 5795, 3717, 2159, 407, 4694, 8422, 12824, 7272, 8331, 653, 2024, 14121, 11722, 10137, 225, 6843, 12842, 6594, 11363, 4257, 11716, 1912, 11345, 8487, 4557, 13719, 11869, 1006, 5897, 7017, 8137, 9723, 3704, 751, 3199, 13742, 2312, 10404, 8677, 10580, 2378, 9694, 10548, 14148, 12193, 7553, 7625, 7635, 7201, 14067, 11099, 1455, 5906, 2870, 3319, 12073, 1432, 3282, 10065, 10601, 5424, 3668, 12533, 7960, 2277, 9735, 8736, 6552, 2123, 2384, 11560, 2028, 3922, 7115, 11220, 13108, 12532, 13420, 4787, 5592, 2336, 6024, 11203, 2051, 7204, 3136, 2859, 4187, 6889, 11232, 9444, 5557, 3655, 2876, 12030, 9091, 7151, 5671, 6704, 9211, 632, 7530, 8573, 10626, 8847, 6348, 1348, 2355, 12014, 5724, 2011, 7864, 10843, 6184, 6946, 13876, 3595, 6556, 10474, 7356, 10423, 1388, 11277, 1879, 11910, 6637, 14293, 8486, 322, 13414, 1731, 1280, 10098, 5461, 13610, 8687, 2395, 1825, 8440, 9303, 2286, 2780, 3943, 6168, 6515, 8515, 1855, 6461, 8389, 10511, 12536, 6158, 11355, 13713, 7363, 12455, 2464, 12813, 5717, 10605, 7018, 5065, 2181, 9564, 6175, 12384, 8849, 14289, 9825, 4510, 8692, 13039, 14262, 5096, 7289, 6463, 2344, 5687, 11470, 9764, 3408, 10268, 14196, 8356, 11893, 571, 12481, 10294, 11198, 10440, 4372, 11382, 11682, 8397, 546, 14197, 10810, 11909, 3107, 8042, 8435, 2770, 5093, 3502, 450, 6731, 2694, 4373, 10637, 10304, 8118, 10411, 4077, 13297, 5501, 4177, 12292, 4647, 753, 6035, 4759, 3698, 8798, 3082, 14095, 6458, 4305, 3443, 6651, 764, 13257, 4906, 12830, 7453, 6451, 6531, 10339, 11398, 9124, 6683, 4032, 4855, 13712, 2746, 6076, 9497, 7978, 7658, 382, 9009, 13413, 8319, 9021, 3558, 4762, 10947, 8984, 3242, 6630, 1617, 8076, 7592, 2274, 3651, 1292, 7456, 3306, 5326, 2375, 8339, 5177, 9265, 1365, 4858, 11907, 1620, 1122, 6525, 7520, 13095, 13163, 10983, 14102, 10120, 3329, 2959, 1603, 12939, 814, 4434, 14377, 13952, 5524, 4959, 7380, 2777, 8037, 12595, 9849, 5008, 8631, 3706, 12322, 12405, 5925, 4306, 756, 7663, 4477, 7053, 10620, 3797, 1377, 8055, 5892, 11044, 11777, 6876, 5414, 4022, 12183, 7233, 1654, 6477, 11891, 5217, 4240, 7273, 11666, 8160, 5871, 6857, 8064, 3210, 10529, 6248, 8381, 2974, 1539, 5609, 1075, 13199, 7685, 2039, 11069, 5271, 10821, 14038, 4049, 12765, 5542, 8409, 2364, 8548, 5213, 12964, 8605, 9263, 11481, 10654, 6365, 3131, 3785, 11755, 5817, 2595, 1632, 1245, 5952, 9112, 2449, 12541, 13397, 883, 11226, 11113, 3913, 9217, 6093, 11026, 1255, 2319, 8094, 5668, 11605, 11294, 4083, 7925, 10906, 7850, 12087, 161, 11180, 6089, 9721, 6545, 9280, 8468, 8158, 7576, 10434, 4410, 7143, 13219, 8615, 1472, 13649, 6968, 185, 11999, 13646, 9154, 7782, 10732, 4746, 6384, 4863, 11389, 10597, 4883, 9192, 6728, 12836, 3789, 6798, 13981, 1390, 2914, 5666, 7280, 10223, 1172, 6632, 10680, 5902, 7829, 2358, 12728, 1078, 7626, 1837, 11311, 3842, 3267, 12669, 3510, 1422, 3016, 10884, 14053, 6008, 10899, 5253, 6616, 8691, 3495, 9049, 13931, 1887, 10115, 10308, 4111, 6837, 10393, 10726, 12625, 260, 169, 1774, 383, 4167, 12298, 3066, 4655, 10303, 2559, 27, 4456, 7058, 9925, 6558, 8222, 10469, 8423, 4622, 12883, 4435, 12176, 8839, 5810, 5436, 6625, 13313, 2578, 13851, 6522, 5752, 2954, 4055, 4115, 13992, 12294, 6301, 3354, 6631, 7639, 2921, 6445, 10295, 10256, 3011, 998, 13447, 1549, 10487, 10855, 4810, 4204, 789, 11621, 271, 3315, 5227, 3323, 13416, 11121, 8918, 5695, 4052, 11539, 1022, 14428, 8733, 3601, 11858, 11657, 1610, 258, 12658, 11960, 3822, 4997, 2263, 1107, 11886, 1739, 7139, 11747, 2046, 10794, 9775, 3798, 12524, 2144, 4709, 2164, 6574, 10781, 4033, 9719, 10186, 4398, 7669, 5357, 338, 7305, 6305, 10216, 10524, 8469, 4776, 1158, 4471, 11519, 6687, 3853, 2431, 5798, 12889, 11252, 9828, 1322, 7494, 5959, 3618, 7256, 8665, 4967, 12107, 14234, 9465, 3501, 4273, 7105, 7562, 3906, 11273, 12365, 1020, 11229, 9006, 2267, 12879, 8317, 1966, 451, 13510, 7033, 13068, 10348, 13348, 45, 3665, 11241, 1574, 9101, 5541, 8887, 5032, 11490, 6345, 13740, 8968, 5073, 4330, 6514, 777, 13211, 10659, 7550, 13518, 1633, 13365, 4478, 13596, 2696, 5128, 1957, 7511, 7482, 7601, 7761, 13867, 9597, 12406, 9588, 4596, 13645, 3607, 14416, 1441, 10571, 6865, 4922, 9036, 11772, 12808, 742, 3206, 5725, 4476, 8159, 5918, 7945, 3693, 8843, 7950, 9160, 6810, 1679, 2653, 5939, 3676, 10042, 10997, 12007, 4819, 13194, 2842, 13119, 4301, 12210, 6115, 8590, 6262, 8595, 11678, 9998, 1070, 13017, 796, 3568, 2018, 9348, 11191, 5418, 9075, 12957, 5056, 8668, 4246, 1225, 4483, 5914, 1990, 2629, 6560, 7260, 10110, 1436, 2423, 612, 3059, 4412, 1677, 5840, 13910, 4724, 12780, 2728, 7708, 10198, 2602, 4726, 6735, 8879, 10728, 6311, 2829, 4946, 6360, 11647, 12111, 4940, 4597, 7003, 11618, 7924, 10413, 12695, 12678, 13281, 8295, 5797, 7364, 3559, 294, 8638, 12922, 13963, 13643, 11116, 1318, 9948, 1600, 13604, 1040, 3572, 1678, 4114, 3600, 2536, 4737, 4620, 7706, 4593, 9002, 2679, 4797, 8403, 10266, 6167, 2373, 13570, 7010, 9084, 6907, 14188, 3545, 4016, 5207, 3604, 10365, 9649, 13474, 7459, 12955, 3573, 10255, 1689, 4101, 13766, 1735, 673, 2140, 12461, 6111, 3679, 5597, 5453, 13982, 6845, 10507, 14227, 5145, 7447, 11021, 9686, 6113, 7719, 6773, 11698, 5963, 7386, 11783, 7285, 4721, 10466, 242, 5068, 7093, 1396, 6711, 10199, 4493, 1378, 8614, 57, 5410, 6108, 9195, 5584, 11168, 10299, 10491, 6286, 5948, 10319, 73, 7781, 9027, 5878, 2462, 7882, 6984, 11165, 3031, 1923, 10923, 4989, 12277, 5289, 10966, 13366, 2987, 7012, 10475, 6417, 1683, 4852, 2817, 8318, 8384, 10624, 9475, 2245, 6591, 12123, 7426, 4879, 12674, 7344, 13878, 7014, 1148, 2188, 8282, 14160, 2565, 6502, 3809, 8223, 10779, 1648, 6470, 1053, 6649, 4641, 5760, 9197, 7580, 788, 10514, 7090, 4442, 1513, 2980, 8150, 10168, 8109, 6678, 12565, 6674, 1207, 11849, 2241, 8097, 3576, 2547, 1975, 9635, 13230, 3867, 6807, 8910, 12243, 4310, 11532, 10653, 6190, 13387, 13676, 3522, 6961, 8219, 2643, 6797, 8971, 1568, 13550, 7361, 6465, 14131, 13767, 5972, 2526, 3848, 5039, 3043, 9936, 8383, 9158, 3756, 648, 4492, 3015, 6762, 12273, 8787, 12469, 2809, 3012, 1939, 8596, 599, 2551, 6995, 139, 13781, 9751, 3900, 9342, 8072, 7127, 8390, 9865, 8891, 11571, 4869, 11499, 8125, 7021, 1336, 10822, 7025, 10232, 4635, 12572, 13804, 5197, 12977, 1063, 11324, 5771, 8418, 11253, 11361, 9171, 2012, 5484, 10933, 7525, 5579, 9050, 6227, 13987, 14024, 5036, 10476, 11801, 3902, 3597, 6601, 8767, 11880, 5931, 6664, 853, 8321, 8225, 4953, 7854, 2308, 7733, 8539, 5619, 526, 7136, 11458, 3179, 9014, 2769, 5691, 10731, 7394, 5268, 12758, 13353, 6690, 4023, 4754, 6633, 4266, 1445, 4961, 4195, 105, 4979, 7337, 10599, 8051, 3670, 5672, 6645, 14183, 3308, 4918, 3113, 13049, 14307, 1261, 2322, 3341, 598, 11007, 7006, 8287, 4074, 9389, 10901, 7561, 13575, 11306, 9553, 7298, 14326, 7213, 5990, 11606, 12270, 7335, 7300, 6005, 8085, 60, 2791, 3678, 1383, 12846, 6127, 7727, 7552, 12487, 11630, 2026, 1702, 2598, 7045, 5813, 1180, 14054, 13220, 1703, 13071, 10338, 5481, 8599, 6732, 4126, 10238, 8479, 11665, 4896, 4994, 3434, 5319, 1899, 3184, 2338, 12918, 4878, 11706, 5, 4272, 2211, 8774, 10725, 13395, 13103, 12129, 692, 3013, 8407, 7732, 6155, 198, 4720, 13403, 3259, 7610, 3196, 11558, 5517, 8502, 2064, 14226, 378, 8562, 7703, 10421, 966, 4824, 14417, 6165, 1712, 4312, 1715, 8500, 3557, 5631, 10400, 2290, 8100, 11412, 6571, 871, 349, 14409, 1806, 12680, 1562, 9241, 5086, 14314, 3640, 7052, 1545, 7634, 11992, 1499, 6144, 9010, 4103, 3146, 4817, 11463, 13753, 6302, 13683, 6564, 3826, 8869, 6702, 961, 7990, 202, 6176, 3083, 3863, 3433, 1235, 7871, 7019, 6799, 2494, 2153, 10465, 11563, 6947, 6897, 6255, 12205, 1008, 7742, 193, 3413, 7785, 9905, 996, 10373, 459, 10633, 10981, 2831, 2775, 6046, 4654, 4730, 9023, 12288, 11283, 8557, 6613, 165, 12811, 12950, 9877, 2752, 4520, 7876, 8406, 8108, 4337, 6433, 12091, 7605, 9590, 5349, 3702, 1509, 7946, 11397, 9923, 11901, 9113, 13991, 8096, 11587, 6928, 3991, 8817, 3888, 4069, 10887, 5274, 13771, 1748, 8393, 14283, 3661, 4491, 4992, 6383, 7354, 4935, 14410, 11615, 6381, 13656, 14036, 11152, 10931, 5674, 8545, 2599, 5224, 11145, 4791, 10139, 1444, 9409, 1418, 8956, 10778, 2767, 4251, 4284, 11480, 3954, 8049, 4562, 2977, 3909, 1926, 6956, 404, 4700, 12996, 9032, 3274, 14277, 7232, 10782, 6071, 7973, 2540, 3005, 4973, 11235, 6221, 12694, 13790, 4213, 4934, 7507, 757, 2069, 14144, 12725, 7597, 2973, 6540, 9586, 4366, 8106, 13027, 12562, 3156, 8151, 13868, 3879, 6336, 13217, 3155, 5605, 7622, 540, 2247, 2651, 5595, 11460, 13441, 6859, 9664, 4392, 4496, 5521, 3003, 1653, 11264, 12959, 9871, 6679, 6351, 11402, 7147, 9783, 3391, 559, 6447, 4399, 6974, 8484, 8507, 8182, 13957, 3511, 12223, 7805, 14077, 8231, 5495, 3279, 13821, 9070, 13272, 5043, 6493, 5338, 3204, 3929, 8228, 6149, 5812, 6581, 7679, 1495, 2283, 12917, 718, 12501, 971, 4669, 7302, 6195, 5385, 1541, 14019, 11708, 12650, 11187, 955, 10161, 10273, 13101, 10153, 724, 12588, 1038, 13632, 4191, 3598, 8646, 4408, 12432, 5393, 222, 13451, 9605, 3744, 10079, 4165, 4324, 4605, 7114, 12579, 8530, 6750, 13308, 8121, 3958, 5816, 1123, 7169, 12733, 7306, 4525, 2120, 3547, 4660, 13120, 7532, 11932, 11291, 8822, 12546, 921, 5945, 5025, 9392, 1907, 11906, 107, 1561, 6914, 12266, 7475, 1094, 10558, 11593, 4783, 3837, 1304, 9571, 3989, 5669, 11183, 826, 8052, 6062, 406, 1404, 6935, 11690, 3317, 3375, 12988, 2901, 3342, 12882, 13693, 13880, 11830, 2276, 4465, 11193, 5494, 2911, 1890, 8699, 10175, 13556, 785, 2798, 2762, 3387, 8618, 6188, 5723, 12103, 6053, 8090, 8538, 985, 317, 9980, 4716, 3035, 4092, 5080, 4350, 565, 1844, 5989, 5769, 3327, 3046, 6237]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1191971/1731568721.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_tensor = torch.tensor(labels)\n"
     ]
    }
   ],
   "source": [
    "labels_df = labels_df[\n",
    "    labels_df['protein'].isin(nodes_in_features)]\n",
    "labels_df = labels_df.reset_index(drop=True)\n",
    "\n",
    "label_indices = [node_id_to_index[node_id] for node_id in labels_df['protein']]\n",
    "print(label_indices)\n",
    "num_nodes = len(combined_features)\n",
    "labels = torch.full((num_nodes,), -1, dtype=torch.long)\n",
    "#labels = torch.full((num_nodes,), -1.0)\n",
    "for i, index in enumerate(labels_df['Label']):\n",
    "    labels[label_indices[i]] = index\n",
    "\n",
    "labels_tensor = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1be38d1a-41c3-42f3-a84e-e74866855c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33960226, -0.03074448, -0.90138096, ...,  0.01558092,\n",
       "        -0.02386307, -0.02200161],\n",
       "       [-0.13179901, -0.02574519, -0.67730105, ..., -0.03992649,\n",
       "        -0.10278717, -0.02697964],\n",
       "       [ 0.38569278, -0.07069244, -0.8477959 , ...,  0.0253919 ,\n",
       "        -0.06603534, -0.02828273],\n",
       "       ...,\n",
       "       [ 0.02713387,  0.24139147, -0.22735251, ..., -0.82107705,\n",
       "         1.036363  , -0.83661443],\n",
       "       [ 0.13954346,  0.02888298,  0.89947975, ..., -0.9852566 ,\n",
       "         1.6735605 ,  0.10965873],\n",
       "       [ 0.08306409,  0.09089889,  0.8885408 , ..., -0.79955566,\n",
       "         1.5193683 ,  0.2632099 ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = combined_features.iloc[:, 1:].values\n",
    "features_tensor = torch.tensor(features, dtype=torch.float)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a4d73a0-a8af-475d-a452-f02fe5922250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([62045, 768]) torch.float32\n",
      "edge_index: torch.Size([2, 9914754]) torch.int64\n",
      "labels: torch.Size([62045]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "data = Data(x=features_tensor, edge_index=edge_index, y=labels_tensor)\n",
    "\n",
    "print(\"x:\", data.x.shape, data.x.dtype)\n",
    "print(\"edge_index:\", data.edge_index.shape, data.edge_index.dtype)\n",
    "print(\"labels:\", data.y.shape, data.y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c490be94-f0a5-4c98-94dc-fe5cfd48155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, num_layers, in_dim, num_hidden, num_classes, heads, activation, dropout, negative_slope, residual):\n",
    "        super(GAT, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        self.activation = activation\n",
    "\n",
    "        # Input projection (no residual)\n",
    "        self.gat_layers.append(GATConv(\n",
    "            in_dim, num_hidden, heads=heads[0],\n",
    "            dropout=dropout, negative_slope=negative_slope, concat=True, add_self_loops=True))\n",
    "\n",
    "        # Hidden layers\n",
    "        for l in range(1, num_layers):\n",
    "            # Due to multi-head, the in_dim = num_hidden * num_heads\n",
    "            self.gat_layers.append(GATConv(\n",
    "                num_hidden * heads[l-1], num_hidden, heads=heads[l],\n",
    "                dropout=dropout, negative_slope=negative_slope, concat=True, add_self_loops=True))\n",
    "\n",
    "        # Output projection\n",
    "        self.gat_layers.append(GATConv(\n",
    "            num_hidden * heads[-2], num_classes, heads=heads[-1],\n",
    "            dropout=dropout, negative_slope=negative_slope, concat=False, add_self_loops=True))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = x\n",
    "        for l, layer in enumerate(self.gat_layers[:-1]):\n",
    "            h = layer(h, edge_index)\n",
    "            if self.activation:\n",
    "                h = self.activation(h)\n",
    "            if l < self.num_layers - 1:\n",
    "                h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Output projection\n",
    "        logits = self.gat_layers[-1](h, edge_index)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7202077b-9e4b-46f3-8790-2dfc9838ffcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport torch.nn.functional as F\\nfrom torch_geometric.nn import GCNConv\\nfrom torch.nn import Linear, ModuleList, Dropout\\n\\nclass GCN(torch.nn.Module):\\n    def __init__(self, num_features, hidden_dim, num_classes, num_layers, activation, dropout):\\n        super(GCN, self).__init__()\\n        self.conv1 = GCNConv(num_features, hidden_dim)\\n        self.convs = ModuleList([GCNConv(hidden_dim, hidden_dim) for _ in range(num_layers - 2)])\\n        self.conv_last = GCNConv(hidden_dim, num_classes)\\n        self.activation = activation\\n        self.dropout = Dropout(dropout)\\n        self.num_layers = num_layers\\n        self.fc = torch.nn.Linear(hidden_dim, num_classes)\\n    \\n    def forward(self, x, edge_index):\\n        # 输入层\\n        x = self.conv1(x, edge_index)\\n        x = self.activation(x)\\n        x = self.dropout(x)\\n        \\n        # 隐藏层\\n        for conv in self.convs:\\n            x = conv(x, edge_index)\\n            x = self.activation(x)\\n            x = self.dropout(x)\\n\\n        # 输出层\\n        #x = self.conv_last(x, edge_index)\\n        x = self.fc(x)\\n        return x\\n    def get_embedding(self, x, edge_index):\\n        # 输入层\\n        x = self.conv1(x, edge_index)\\n        x = self.activation(x)\\n        x = self.dropout(x)\\n        \\n        # 隐藏层\\n        for conv in self.convs:\\n            x = conv(x, edge_index)\\n            x = self.activation(x)\\n            x = self.dropout(x)\\n        return x\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch_geometric.nn import GCNConv\\nfrom torch.nn import ModuleList, Dropout, Linear\\n\\nclass GCN(torch.nn.Module):\\n    def __init__(self, num_features, hidden_dim, num_classes, num_layers,activation, dropout):\\n        super(GCN, self).__init__()\\n        # 为每种节点类型初始化一个全连接层\\n        self.fc_list = nn.ModuleList([nn.Linear(feats_dim, hidden_dim) for feats_dim in num_features])\\n        \\n        # 图卷积层\\n        self.conv1 = GCNConv(hidden_dim, hidden_dim)\\n        self.convs = ModuleList([GCNConv(hidden_dim, hidden_dim) for _ in range(num_layers - 2)])\\n        self.fc = nn.Linear(hidden_dim, num_classes)\\n        \\n        self.activation = activation\\n        self.dropout = Dropout(dropout)\\n        \\n        self.conv_last = GCNConv(hidden_dim, num_classes)\\n\\n    def forward(self, x_list, edge_index):\\n        # 节点类型特定的特征转换\\n        x = torch.cat([fc(x) for fc, x in zip(self.fc_list, x_list)], dim=0)\\n        # 图卷积层\\n        x = self.conv1(x, edge_index)\\n        x = self.activation(x)\\n        x = self.dropout(x)\\n        \\n        for conv in self.convs:\\n            x = conv(x, edge_index)\\n            x = self.activation(x)\\n            x = self.dropout(x)\\n\\n        x = self.fc(x)  # 最终分类层\\n        return x'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear, ModuleList, Dropout\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, num_classes, num_layers, activation, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.convs = ModuleList([GCNConv(hidden_dim, hidden_dim) for _ in range(num_layers - 2)])\n",
    "        self.conv_last = GCNConv(hidden_dim, num_classes)\n",
    "        self.activation = activation\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # 输入层\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 隐藏层\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # 输出层\n",
    "        x = self.conv_last(x, edge_index)\n",
    "        return x\n",
    "\"\"\"\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear, ModuleList, Dropout\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, num_classes, num_layers, activation, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.convs = ModuleList([GCNConv(hidden_dim, hidden_dim) for _ in range(num_layers - 2)])\n",
    "        self.conv_last = GCNConv(hidden_dim, num_classes)\n",
    "        self.activation = activation\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.num_layers = num_layers\n",
    "        self.fc = torch.nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # 输入层\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 隐藏层\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # 输出层\n",
    "        #x = self.conv_last(x, edge_index)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    def get_embedding(self, x, edge_index):\n",
    "        # 输入层\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 隐藏层\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import ModuleList, Dropout, Linear\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, num_classes, num_layers,activation, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        # 为每种节点类型初始化一个全连接层\n",
    "        self.fc_list = nn.ModuleList([nn.Linear(feats_dim, hidden_dim) for feats_dim in num_features])\n",
    "        \n",
    "        # 图卷积层\n",
    "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.convs = ModuleList([GCNConv(hidden_dim, hidden_dim) for _ in range(num_layers - 2)])\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "        self.conv_last = GCNConv(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x_list, edge_index):\n",
    "        # 节点类型特定的特征转换\n",
    "        x = torch.cat([fc(x) for fc, x in zip(self.fc_list, x_list)], dim=0)\n",
    "        # 图卷积层\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.fc(x)  # 最终分类层\n",
    "        return x\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c34a42c-54c5-4439-82e0-08663e1ecddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def accuracy(pred, target):\n",
    "    r\"\"\"Computes the accuracy of correct predictions.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    return (pred == target).sum().item() / target.numel()\n",
    "\n",
    "\n",
    "\n",
    "def true_positive(pred, target, num_classes):\n",
    "    r\"\"\"Computes the number of true positive predictions.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`LongTensor`\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred == i) & (target == i)).sum())\n",
    "\n",
    "    return torch.tensor(out)\n",
    "\n",
    "\n",
    "\n",
    "def true_negative(pred, target, num_classes):\n",
    "    r\"\"\"Computes the number of true negative predictions.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`LongTensor`\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred != i) & (target != i)).sum())\n",
    "\n",
    "    return torch.tensor(out)\n",
    "\n",
    "\n",
    "\n",
    "def false_positive(pred, target, num_classes):\n",
    "    r\"\"\"Computes the number of false positive predictions.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`LongTensor`\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred == i) & (target != i)).sum())\n",
    "\n",
    "    return torch.tensor(out)\n",
    "\n",
    "\n",
    "\n",
    "def false_negative(pred, target, num_classes):\n",
    "    r\"\"\"Computes the number of false negative predictions.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`LongTensor`\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred != i) & (target == i)).sum())\n",
    "\n",
    "    return torch.tensor(out)\n",
    "\n",
    "\n",
    "\n",
    "def precision(pred, target, num_classes):\n",
    "    r\"\"\"Computes the precision:\n",
    "    :math:`\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}}`.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`Tensor`\n",
    "    \"\"\"\n",
    "    tp = true_positive(pred, target, num_classes).to(torch.float)\n",
    "    fp = false_positive(pred, target, num_classes).to(torch.float)\n",
    "\n",
    "    out = tp / (tp + fp)\n",
    "    out[torch.isnan(out)] = 0\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def recall(pred, target, num_classes):\n",
    "    r\"\"\"Computes the recall:\n",
    "    :math:`\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}`.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`Tensor`\n",
    "    \"\"\"\n",
    "    tp = true_positive(pred, target, num_classes).to(torch.float)\n",
    "    fn = false_negative(pred, target, num_classes).to(torch.float)\n",
    "\n",
    "    out = tp / (tp + fn)\n",
    "    out[torch.isnan(out)] = 0\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def f1_score(pred, target, num_classes):\n",
    "    r\"\"\"Computes the :math:`F_1` score:\n",
    "    :math:`2 \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}\n",
    "    {\\mathrm{precision}+\\mathrm{recall}}`.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`Tensor`\n",
    "    \"\"\"\n",
    "    prec = precision(pred, target, num_classes)\n",
    "    rec = recall(pred, target, num_classes)\n",
    "\n",
    "    score = 2 * (prec * rec) / (prec + rec)\n",
    "    score[torch.isnan(score)] = 0\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d8805ce-77e7-4081-b888-8c35b50f883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "def train_model_scheduler(model, masked_features, labels, edge_index, optimizer, criterion, scheduler, train_mask):\n",
    "    model.train()  # 设置模型为训练模\n",
    "    optimizer.zero_grad()  # 清空梯度\n",
    "    out = model(masked_features, edge_index)  # 获取模型输出\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])  # 计算损失值，只针对训练集的节点\n",
    "    loss.backward()  # 反向传播\n",
    "    optimizer.step()  # 更新模型参数\n",
    "    scheduler.step(loss)\n",
    "    return loss.item()\n",
    "\n",
    "def train_model(model, masked_features, labels, edge_index, optimizer, criterion, train_mask):\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    optimizer.zero_grad()  # 清空梯度\n",
    "    out = model(masked_features, edge_index) # 获取模型输出\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])  # 计算损失值，只针对训练集的节点\n",
    "    loss.backward()  # 反向传播\n",
    "    optimizer.step()  # 更新模型参数\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate_model(model, features, labels, edge_index, mask):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    with torch.no_grad():  # 关闭梯度计算\n",
    "        # 获取模型输出，这里假设输出已经是经过sigmoid的概率\n",
    "        probabilities = model(features, edge_index)\n",
    "        if probabilities.shape[1] == 2:  # 假设有两个输出（每个类一个概率）\n",
    "            positive_probs = probabilities[mask, 1]  # 选择正类概率\n",
    "        else:\n",
    "            positive_probs = probabilities[mask]  # 如果只有一个输出，假设已经是正类概率\n",
    "        val_f1 = torch.mean(f1_score(torch.argmax(probabilities[mask],dim=1), labels[mask], num_classes=3)).cpu().numpy()\n",
    "        auc_score = roc_auc_score(labels[mask].cpu().numpy(), positive_probs.cpu().numpy())\n",
    "\n",
    "    return val_f1, auc_score\n",
    "def evaluate_regression(model, features, labels, edge_index, mask):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    with torch.no_grad():  # 关闭梯度计算\n",
    "        predictions = model(features, edge_index).squeeze()  # 获取模型输出\n",
    "        print(predictions[mask])\n",
    "        mse = torch.mean((predictions[mask] - labels[mask]) ** 2)  # 计算均方误差\n",
    "        mae = torch.mean(torch.abs(predictions[mask] - labels[mask]))  # 计算平均绝对误差\n",
    "        rmse = torch.sqrt(mse)  # 计算均方根误差\n",
    "    return mse.item(), mae.item(), rmse.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "902d45b4-f0f5-4852-bb71-4aa066088776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature759</th>\n",
       "      <th>feature760</th>\n",
       "      <th>feature761</th>\n",
       "      <th>feature762</th>\n",
       "      <th>feature763</th>\n",
       "      <th>feature764</th>\n",
       "      <th>feature765</th>\n",
       "      <th>feature766</th>\n",
       "      <th>feature767</th>\n",
       "      <th>feature768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FES</td>\n",
       "      <td>0.339602</td>\n",
       "      <td>-0.030744</td>\n",
       "      <td>-0.901381</td>\n",
       "      <td>0.100888</td>\n",
       "      <td>0.886443</td>\n",
       "      <td>0.383596</td>\n",
       "      <td>-0.192082</td>\n",
       "      <td>-0.032063</td>\n",
       "      <td>-0.154869</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.549204</td>\n",
       "      <td>-0.856123</td>\n",
       "      <td>0.714672</td>\n",
       "      <td>-0.046649</td>\n",
       "      <td>-0.894424</td>\n",
       "      <td>-0.001815</td>\n",
       "      <td>0.739485</td>\n",
       "      <td>0.015581</td>\n",
       "      <td>-0.023863</td>\n",
       "      <td>-0.022002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HADHA</td>\n",
       "      <td>-0.131799</td>\n",
       "      <td>-0.025745</td>\n",
       "      <td>-0.677301</td>\n",
       "      <td>-0.053545</td>\n",
       "      <td>0.971046</td>\n",
       "      <td>0.180315</td>\n",
       "      <td>-0.028189</td>\n",
       "      <td>-0.077389</td>\n",
       "      <td>-0.095152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.927885</td>\n",
       "      <td>-0.817812</td>\n",
       "      <td>0.809631</td>\n",
       "      <td>-0.005827</td>\n",
       "      <td>-0.848839</td>\n",
       "      <td>0.024516</td>\n",
       "      <td>0.526404</td>\n",
       "      <td>-0.039926</td>\n",
       "      <td>-0.102787</td>\n",
       "      <td>-0.026980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLC7A7</td>\n",
       "      <td>0.385693</td>\n",
       "      <td>-0.070692</td>\n",
       "      <td>-0.847796</td>\n",
       "      <td>-0.022054</td>\n",
       "      <td>0.959772</td>\n",
       "      <td>0.085487</td>\n",
       "      <td>0.076455</td>\n",
       "      <td>-0.003006</td>\n",
       "      <td>-0.032268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.941094</td>\n",
       "      <td>-0.912443</td>\n",
       "      <td>0.789828</td>\n",
       "      <td>0.046979</td>\n",
       "      <td>-0.715636</td>\n",
       "      <td>0.085842</td>\n",
       "      <td>0.150494</td>\n",
       "      <td>0.025392</td>\n",
       "      <td>-0.066035</td>\n",
       "      <td>-0.028283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LCK</td>\n",
       "      <td>0.650428</td>\n",
       "      <td>0.014479</td>\n",
       "      <td>-0.866163</td>\n",
       "      <td>0.053508</td>\n",
       "      <td>0.951529</td>\n",
       "      <td>0.269402</td>\n",
       "      <td>-0.214788</td>\n",
       "      <td>0.045179</td>\n",
       "      <td>-0.506429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.576739</td>\n",
       "      <td>-0.969558</td>\n",
       "      <td>0.916549</td>\n",
       "      <td>-0.080332</td>\n",
       "      <td>-0.927649</td>\n",
       "      <td>-0.047398</td>\n",
       "      <td>0.741663</td>\n",
       "      <td>-0.000096</td>\n",
       "      <td>-0.096318</td>\n",
       "      <td>-0.056501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HSPA2</td>\n",
       "      <td>0.322262</td>\n",
       "      <td>0.017484</td>\n",
       "      <td>-0.849302</td>\n",
       "      <td>0.046401</td>\n",
       "      <td>0.920429</td>\n",
       "      <td>0.463832</td>\n",
       "      <td>-0.050414</td>\n",
       "      <td>-0.033398</td>\n",
       "      <td>0.387791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387301</td>\n",
       "      <td>-0.860696</td>\n",
       "      <td>0.678607</td>\n",
       "      <td>-0.060695</td>\n",
       "      <td>-0.945793</td>\n",
       "      <td>0.040472</td>\n",
       "      <td>0.831079</td>\n",
       "      <td>-0.001711</td>\n",
       "      <td>-0.079842</td>\n",
       "      <td>-0.011189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47590</th>\n",
       "      <td>GO:2001313</td>\n",
       "      <td>0.174428</td>\n",
       "      <td>0.194728</td>\n",
       "      <td>-0.284376</td>\n",
       "      <td>0.282102</td>\n",
       "      <td>-0.713190</td>\n",
       "      <td>-0.272055</td>\n",
       "      <td>0.121190</td>\n",
       "      <td>0.129901</td>\n",
       "      <td>-0.983496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500545</td>\n",
       "      <td>0.429651</td>\n",
       "      <td>-0.292929</td>\n",
       "      <td>-0.464941</td>\n",
       "      <td>-0.740187</td>\n",
       "      <td>0.179149</td>\n",
       "      <td>-0.960807</td>\n",
       "      <td>-0.746958</td>\n",
       "      <td>1.069112</td>\n",
       "      <td>-0.848182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47591</th>\n",
       "      <td>GO:2001314</td>\n",
       "      <td>0.025886</td>\n",
       "      <td>0.306214</td>\n",
       "      <td>-0.254303</td>\n",
       "      <td>0.253673</td>\n",
       "      <td>-0.533680</td>\n",
       "      <td>-0.269355</td>\n",
       "      <td>0.150939</td>\n",
       "      <td>-0.229323</td>\n",
       "      <td>-1.078991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042979</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>-0.356661</td>\n",
       "      <td>-0.381828</td>\n",
       "      <td>-0.638338</td>\n",
       "      <td>0.077176</td>\n",
       "      <td>-0.788312</td>\n",
       "      <td>-0.683442</td>\n",
       "      <td>1.087031</td>\n",
       "      <td>-0.593092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47592</th>\n",
       "      <td>GO:2001315</td>\n",
       "      <td>0.027134</td>\n",
       "      <td>0.241391</td>\n",
       "      <td>-0.227353</td>\n",
       "      <td>0.317366</td>\n",
       "      <td>-0.726657</td>\n",
       "      <td>-0.197968</td>\n",
       "      <td>0.045653</td>\n",
       "      <td>0.038912</td>\n",
       "      <td>-0.954113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349853</td>\n",
       "      <td>0.370059</td>\n",
       "      <td>-0.144606</td>\n",
       "      <td>-0.493184</td>\n",
       "      <td>-0.655063</td>\n",
       "      <td>0.217335</td>\n",
       "      <td>-0.841272</td>\n",
       "      <td>-0.821077</td>\n",
       "      <td>1.036363</td>\n",
       "      <td>-0.836614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47593</th>\n",
       "      <td>GO:2001316</td>\n",
       "      <td>0.139543</td>\n",
       "      <td>0.028883</td>\n",
       "      <td>0.899480</td>\n",
       "      <td>0.152932</td>\n",
       "      <td>0.576852</td>\n",
       "      <td>0.330342</td>\n",
       "      <td>0.916943</td>\n",
       "      <td>0.012306</td>\n",
       "      <td>-0.020316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.354748</td>\n",
       "      <td>-0.083168</td>\n",
       "      <td>0.043640</td>\n",
       "      <td>-0.663565</td>\n",
       "      <td>0.543016</td>\n",
       "      <td>-0.652230</td>\n",
       "      <td>-1.427882</td>\n",
       "      <td>-0.985257</td>\n",
       "      <td>1.673561</td>\n",
       "      <td>0.109659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47594</th>\n",
       "      <td>GO:2001317</td>\n",
       "      <td>0.083064</td>\n",
       "      <td>0.090899</td>\n",
       "      <td>0.888541</td>\n",
       "      <td>0.309920</td>\n",
       "      <td>0.403966</td>\n",
       "      <td>0.202783</td>\n",
       "      <td>0.706517</td>\n",
       "      <td>-0.017584</td>\n",
       "      <td>-0.171057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.544680</td>\n",
       "      <td>-0.046654</td>\n",
       "      <td>0.262865</td>\n",
       "      <td>-0.767305</td>\n",
       "      <td>0.753788</td>\n",
       "      <td>-0.577503</td>\n",
       "      <td>-1.194910</td>\n",
       "      <td>-0.799556</td>\n",
       "      <td>1.519368</td>\n",
       "      <td>0.263210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62045 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          protein  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
       "0             FES  0.339602 -0.030744 -0.901381  0.100888  0.886443  0.383596   \n",
       "1          HADHA  -0.131799 -0.025745 -0.677301 -0.053545  0.971046  0.180315   \n",
       "2          SLC7A7  0.385693 -0.070692 -0.847796 -0.022054  0.959772  0.085487   \n",
       "3            LCK   0.650428  0.014479 -0.866163  0.053508  0.951529  0.269402   \n",
       "4           HSPA2  0.322262  0.017484 -0.849302  0.046401  0.920429  0.463832   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "47590  GO:2001313  0.174428  0.194728 -0.284376  0.282102 -0.713190 -0.272055   \n",
       "47591  GO:2001314  0.025886  0.306214 -0.254303  0.253673 -0.533680 -0.269355   \n",
       "47592  GO:2001315  0.027134  0.241391 -0.227353  0.317366 -0.726657 -0.197968   \n",
       "47593  GO:2001316  0.139543  0.028883  0.899480  0.152932  0.576852  0.330342   \n",
       "47594  GO:2001317  0.083064  0.090899  0.888541  0.309920  0.403966  0.202783   \n",
       "\n",
       "       feature7  feature8  feature9  ...  feature759  feature760  feature761  \\\n",
       "0     -0.192082 -0.032063 -0.154869  ...   -0.549204   -0.856123    0.714672   \n",
       "1     -0.028189 -0.077389 -0.095152  ...    0.927885   -0.817812    0.809631   \n",
       "2      0.076455 -0.003006 -0.032268  ...    0.941094   -0.912443    0.789828   \n",
       "3     -0.214788  0.045179 -0.506429  ...   -0.576739   -0.969558    0.916549   \n",
       "4     -0.050414 -0.033398  0.387791  ...    0.387301   -0.860696    0.678607   \n",
       "...         ...       ...       ...  ...         ...         ...         ...   \n",
       "47590  0.121190  0.129901 -0.983496  ...    0.500545    0.429651   -0.292929   \n",
       "47591  0.150939 -0.229323 -1.078991  ...    0.042979    0.134560   -0.356661   \n",
       "47592  0.045653  0.038912 -0.954113  ...    0.349853    0.370059   -0.144606   \n",
       "47593  0.916943  0.012306 -0.020316  ...   -0.354748   -0.083168    0.043640   \n",
       "47594  0.706517 -0.017584 -0.171057  ...   -0.544680   -0.046654    0.262865   \n",
       "\n",
       "       feature762  feature763  feature764  feature765  feature766  feature767  \\\n",
       "0       -0.046649   -0.894424   -0.001815    0.739485    0.015581   -0.023863   \n",
       "1       -0.005827   -0.848839    0.024516    0.526404   -0.039926   -0.102787   \n",
       "2        0.046979   -0.715636    0.085842    0.150494    0.025392   -0.066035   \n",
       "3       -0.080332   -0.927649   -0.047398    0.741663   -0.000096   -0.096318   \n",
       "4       -0.060695   -0.945793    0.040472    0.831079   -0.001711   -0.079842   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "47590   -0.464941   -0.740187    0.179149   -0.960807   -0.746958    1.069112   \n",
       "47591   -0.381828   -0.638338    0.077176   -0.788312   -0.683442    1.087031   \n",
       "47592   -0.493184   -0.655063    0.217335   -0.841272   -0.821077    1.036363   \n",
       "47593   -0.663565    0.543016   -0.652230   -1.427882   -0.985257    1.673561   \n",
       "47594   -0.767305    0.753788   -0.577503   -1.194910   -0.799556    1.519368   \n",
       "\n",
       "       feature768  \n",
       "0       -0.022002  \n",
       "1       -0.026980  \n",
       "2       -0.028283  \n",
       "3       -0.056501  \n",
       "4       -0.011189  \n",
       "...           ...  \n",
       "47590   -0.848182  \n",
       "47591   -0.593092  \n",
       "47592   -0.836614  \n",
       "47593    0.109659  \n",
       "47594    0.263210  \n",
       "\n",
       "[62045 rows x 769 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "648c83d3-0003-46d5-a9be-8b1ec6537c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "protein       GO:0000001\n",
       "feature1       -1.168093\n",
       "feature2       -0.355214\n",
       "feature3        0.265877\n",
       "feature4       -0.710051\n",
       "                 ...    \n",
       "feature764     -0.506993\n",
       "feature765       0.38976\n",
       "feature766      0.207266\n",
       "feature767      0.070705\n",
       "feature768      0.938593\n",
       "Name: 0, Length: 769, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_features.iloc[14450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0c87c39-ce11-4f24-bd32-c8eae20ef2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276\n",
      "tensor([False, False, False,  ..., False, False, False])\n"
     ]
    }
   ],
   "source": [
    "#regression\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 实例化模型\n",
    "device = torch.device('cuda:1')\n",
    "data = data.to(device)\n",
    "#model = GCN(num_features=[768,768], hidden_dim=64, num_classes=2, num_layers=3, activation=F.relu, dropout=0.5)\n",
    "#model = GCN(num_features=data.x.shape[1], hidden_dim=64, num_classes=2, num_layers=2, activation=F.relu, dropout=0.5)\n",
    "# 假设是一个单变量回归问题\n",
    "model = GCN(num_features=data.x.shape[1], hidden_dim=64, output_dim=1)\n",
    "\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=50, verbose=True)\n",
    "\n",
    "labeled_indices = label_indices\n",
    "random.shuffle(labeled_indices)\n",
    "num_labeled = len(labeled_indices)\n",
    "num_train = int(num_labeled * 0.8)\n",
    "num_test = num_labeled - num_train\n",
    "print(num_test)\n",
    "\n",
    "# 创建训练和测试掩码\n",
    "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "\n",
    "train_mask[labeled_indices[:num_train]] = True\n",
    "test_mask[labeled_indices[num_train:num_train+num_test]] = True\n",
    "print(test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb4f3afe-f4f3-4a39-8f15-a9b2c474775c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1541\n",
      "tensor([False, False, False,  ..., False, False, False])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 实例化模型\n",
    "device = torch.device('cuda:1')\n",
    "data = data.to(device)\n",
    "#model = GCN(num_features=[768,768], hidden_dim=64, num_classes=2, num_layers=3, activation=F.relu, dropout=0.5)\n",
    "model = GCN(num_features=data.x.shape[1], hidden_dim=64, num_classes=3, num_layers=2, activation=F.relu, dropout=0.5)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=50, verbose=True)\n",
    "\n",
    "labeled_indices = label_indices\n",
    "random.shuffle(labeled_indices)\n",
    "num_labeled = len(labeled_indices)\n",
    "num_train = int(num_labeled * 0.8)\n",
    "num_test = num_labeled - num_train\n",
    "print(num_test)\n",
    "\n",
    "# 创建训练和测试掩码\n",
    "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "\n",
    "train_mask[labeled_indices[:num_train]] = True\n",
    "test_mask[labeled_indices[num_train:num_train+num_test]] = True\n",
    "print(test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd152838-7ee8-4e86-a8b8-ece5d76faeaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'GCNConv' received a tuple of node features as input while this layer does not support bipartite message passing. Please try other layers such as 'SAGEConv' or 'GraphConv' instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m x_list \u001b[38;5;241m=\u001b[39m [x_type1, x_type2]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 7\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     test_f1, test_auc \u001b[38;5;241m=\u001b[39m evaluate_model(model, x_list, data\u001b[38;5;241m.\u001b[39my, data\u001b[38;5;241m.\u001b[39medge_index, test_mask)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m, in \u001b[0;36mtrain_model_scheduler\u001b[0;34m(model, masked_features, labels, edge_index, optimizer, criterion, scheduler, train_mask)\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# 设置模型为训练模\u001b[39;00m\n\u001b[1;32m      4\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# 清空梯度\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 获取模型输出\u001b[39;00m\n\u001b[1;32m      6\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out[train_mask], data\u001b[38;5;241m.\u001b[39my[train_mask])  \u001b[38;5;66;03m# 计算损失值，只针对训练集的节点\u001b[39;00m\n\u001b[1;32m      7\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# 反向传播\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/python310/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/python310/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m, in \u001b[0;36mGCN.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# 输入层\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[0;32m~/.conda/envs/python310/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/python310/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/python310/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py:212\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, edge_index: Adj,\n\u001b[1;32m    209\u001b[0m             edge_weight: OptTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m--> 212\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m received a tuple \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    213\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof node features as input while this layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    214\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not support bipartite message passing. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    215\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease try other layers such as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAGEConv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGraphConv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize:\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, Tensor):\n",
      "\u001b[0;31mValueError\u001b[0m: 'GCNConv' received a tuple of node features as input while this layer does not support bipartite message passing. Please try other layers such as 'SAGEConv' or 'GraphConv' instead"
     ]
    }
   ],
   "source": [
    "#fully connection\n",
    "num_epochs = 1000\n",
    "x_type1 = data.x[:14450]\n",
    "x_type2 = data.x[14450:]\n",
    "x_list = [x_type1, x_type2]\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_model_scheduler(model, x_list, data.y, data.edge_index, optimizer, loss_fn, scheduler, train_mask)\n",
    "    test_f1, test_auc = evaluate_model(model, x_list, data.y, data.edge_index, test_mask)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0: \n",
    "        print(f'Epoch {epoch}: Train Loss:{train_loss:.4f}, Macro_F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94065e17-07d4-49f5-967d-791a7dbb1a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss:0.4135, Macro_F1: 0.8323, AUC_score: 0.8807\n",
      "Epoch 39: Train Loss:0.3151, Macro_F1: 0.8652, AUC_score: 0.9145\n",
      "Epoch 59: Train Loss:0.2829, Macro_F1: 0.8910, AUC_score: 0.9336\n",
      "Epoch 79: Train Loss:0.2524, Macro_F1: 0.8982, AUC_score: 0.9435\n",
      "Epoch 99: Train Loss:0.2432, Macro_F1: 0.9053, AUC_score: 0.9468\n",
      "Epoch 119: Train Loss:0.2050, Macro_F1: 0.9015, AUC_score: 0.9495\n",
      "Epoch 139: Train Loss:0.1900, Macro_F1: 0.9274, AUC_score: 0.9528\n",
      "Epoch 159: Train Loss:0.1855, Macro_F1: 0.9273, AUC_score: 0.9532\n",
      "Epoch 179: Train Loss:0.1761, Macro_F1: 0.9202, AUC_score: 0.9540\n",
      "Epoch 199: Train Loss:0.1732, Macro_F1: 0.9273, AUC_score: 0.9547\n",
      "Epoch 219: Train Loss:0.1510, Macro_F1: 0.9163, AUC_score: 0.9533\n",
      "Epoch 239: Train Loss:0.1416, Macro_F1: 0.9163, AUC_score: 0.9531\n",
      "Epoch 259: Train Loss:0.1537, Macro_F1: 0.9163, AUC_score: 0.9549\n",
      "Epoch 279: Train Loss:0.1351, Macro_F1: 0.9199, AUC_score: 0.9555\n",
      "Epoch 299: Train Loss:0.1530, Macro_F1: 0.9237, AUC_score: 0.9577\n",
      "Epoch 319: Train Loss:0.1353, Macro_F1: 0.9166, AUC_score: 0.9590\n",
      "Epoch 339: Train Loss:0.1355, Macro_F1: 0.9310, AUC_score: 0.9574\n",
      "Epoch 359: Train Loss:0.1230, Macro_F1: 0.9199, AUC_score: 0.9572\n",
      "Epoch 379: Train Loss:0.1182, Macro_F1: 0.9273, AUC_score: 0.9583\n",
      "Epoch 399: Train Loss:0.1266, Macro_F1: 0.9310, AUC_score: 0.9590\n",
      "Epoch 419: Train Loss:0.1286, Macro_F1: 0.9199, AUC_score: 0.9581\n",
      "Epoch 439: Train Loss:0.1183, Macro_F1: 0.9199, AUC_score: 0.9575\n",
      "Epoch 00456: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 459: Train Loss:0.1041, Macro_F1: 0.9236, AUC_score: 0.9584\n",
      "Epoch 479: Train Loss:0.1193, Macro_F1: 0.9310, AUC_score: 0.9587\n",
      "Epoch 499: Train Loss:0.0940, Macro_F1: 0.9346, AUC_score: 0.9590\n",
      "Epoch 519: Train Loss:0.1074, Macro_F1: 0.9199, AUC_score: 0.9583\n",
      "Epoch 539: Train Loss:0.0934, Macro_F1: 0.9273, AUC_score: 0.9587\n",
      "Epoch 00557: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 559: Train Loss:0.1100, Macro_F1: 0.9310, AUC_score: 0.9590\n",
      "Epoch 579: Train Loss:0.1011, Macro_F1: 0.9310, AUC_score: 0.9591\n",
      "Epoch 599: Train Loss:0.0941, Macro_F1: 0.9310, AUC_score: 0.9591\n",
      "Epoch 00608: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 619: Train Loss:0.1014, Macro_F1: 0.9346, AUC_score: 0.9591\n",
      "Epoch 639: Train Loss:0.1063, Macro_F1: 0.9273, AUC_score: 0.9586\n",
      "Epoch 659: Train Loss:0.1010, Macro_F1: 0.9310, AUC_score: 0.9590\n",
      "Epoch 679: Train Loss:0.0983, Macro_F1: 0.9310, AUC_score: 0.9591\n",
      "Epoch 00693: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 699: Train Loss:0.1056, Macro_F1: 0.9310, AUC_score: 0.9591\n",
      "Epoch 719: Train Loss:0.1056, Macro_F1: 0.9310, AUC_score: 0.9591\n",
      "Epoch 739: Train Loss:0.1060, Macro_F1: 0.9310, AUC_score: 0.9591\n",
      "Epoch 759: Train Loss:0.1158, Macro_F1: 0.9310, AUC_score: 0.9591\n",
      "Epoch 00775: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 779: Train Loss:0.0943, Macro_F1: 0.9310, AUC_score: 0.9591\n",
      "Epoch 799: Train Loss:0.1073, Macro_F1: 0.9310, AUC_score: 0.9591\n",
      "Epoch 819: Train Loss:0.1004, Macro_F1: 0.9310, AUC_score: 0.9591\n",
      "Epoch 00826: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 839: Train Loss:0.1118, Macro_F1: 0.9310, AUC_score: 0.9591\n",
      "Epoch 859: Train Loss:0.1154, Macro_F1: 0.9310, AUC_score: 0.9591\n",
      "Epoch 879: Train Loss:0.0941, Macro_F1: 0.9310, AUC_score: 0.9591\n",
      "Epoch 00889: reducing learning rate of group 0 to 1.2800e-08.\n",
      "Epoch 899: Train Loss:0.1117, Macro_F1: 0.9310, AUC_score: 0.9591\n",
      "Epoch 919: Train Loss:0.0959, Macro_F1: 0.9310, AUC_score: 0.9591\n",
      "Epoch 00940: reducing learning rate of group 0 to 2.5600e-09.\n",
      "Epoch 939: Train Loss:0.1021, Macro_F1: 0.9310, AUC_score: 0.9591\n",
      "Epoch 959: Train Loss:0.1053, Macro_F1: 0.9310, AUC_score: 0.9591\n",
      "Epoch 979: Train Loss:0.0970, Macro_F1: 0.9310, AUC_score: 0.9591\n",
      "Epoch 999: Train Loss:0.1020, Macro_F1: 0.9310, AUC_score: 0.9591\n"
     ]
    }
   ],
   "source": [
    "#linear\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_model_scheduler(model, data.x, data.y, data.edge_index, optimizer, loss_fn, scheduler, train_mask)\n",
    "    test_f1, test_auc = evaluate_model(model, data.x, data.y, data.edge_index, test_mask)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0: \n",
    "        print(f'Epoch {epoch}: Train Loss:{train_loss:.4f}, Macro_F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f289eb9-716b-467a-9cd5-94c03ad90cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1103\n",
      "tensor([False, False, False,  ..., False, False, False])\n"
     ]
    }
   ],
   "source": [
    "print(num_train)\n",
    "print(train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d44f0e71-deb6-49ad-a222-5250ab6799f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yue/.conda/envs/python310/lib/python3.10/site-packages/sklearn/utils/_array_api.py:290: RuntimeWarning: invalid value encountered in cast\n",
      "  return x.astype(dtype, copy=copy, casting=casting)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input y_true contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      3\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_model_scheduler(model, data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39my, data\u001b[38;5;241m.\u001b[39medge_index, optimizer, loss_fn, scheduler, train_mask)\n\u001b[0;32m----> 4\u001b[0m     test_f1, test_auc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Train Loss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Macro_F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, AUC_score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_auc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[49], line 31\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, features, labels, edge_index, mask)\u001b[0m\n\u001b[1;32m     29\u001b[0m         positive_probs \u001b[38;5;241m=\u001b[39m probabilities[mask]  \u001b[38;5;66;03m# 如果只有一个输出，假设已经是正类概率\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     val_f1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(f1_score(torch\u001b[38;5;241m.\u001b[39margmax(probabilities[mask],dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), labels[mask], num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 31\u001b[0m     auc_score \u001b[38;5;241m=\u001b[39m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_probs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m val_f1, auc_score\n",
      "File \u001b[0;32m~/.conda/envs/python310/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/python310/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:604\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    409\u001b[0m     {\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    428\u001b[0m     labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    429\u001b[0m ):\n\u001b[1;32m    430\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) \\\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03m    from prediction scores.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;124;03m    array([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 604\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_true\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    606\u001b[0m     y_score \u001b[38;5;241m=\u001b[39m check_array(y_score, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/python310/lib/python3.10/site-packages/sklearn/utils/multiclass.py:389\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name)\u001b[0m\n\u001b[1;32m    387\u001b[0m     data \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mif\u001b[39;00m issparse(y) \u001b[38;5;28;01melse\u001b[39;00m y\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39many(data \u001b[38;5;241m!=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(data, \u001b[38;5;28mint\u001b[39m)):\n\u001b[0;32m--> 389\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m suffix\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# Check multiclass\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/python310/lib/python3.10/site-packages/sklearn/utils/validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/python310/lib/python3.10/site-packages/sklearn/utils/validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m     )\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input y_true contains NaN."
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_model_scheduler(model, data.x, data.y, data.edge_index, optimizer, loss_fn, scheduler, train_mask)\n",
    "    test_f1, test_auc = evaluate_model(model, data.x, data.y, data.edge_index, test_mask)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0: \n",
    "        print(f'Epoch {epoch}: Train Loss:{train_loss:.4f}, Macro_F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "955b5b31-05e0-4877-a455-10411dd9f3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Features NaN Check: False\n",
      "Input Features Inf Check: False\n",
      "Edge Index NaN Check: False\n",
      "Edge Index Inf Check: False\n"
     ]
    }
   ],
   "source": [
    "print(\"Input Features NaN Check:\", torch.isnan(data.x).any().item())\n",
    "print(\"Input Features Inf Check:\", torch.isinf(data.x).any().item())\n",
    "print(\"Edge Index NaN Check:\", torch.isnan(data.edge_index).any().item())\n",
    "print(\"Edge Index Inf Check:\", torch.isinf(data.edge_index).any().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3b8557aa-b855-40ad-94ec-abb4005fa699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yue/.conda/envs/python310/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([10853])) that is different to the input size (torch.Size([10853, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 19: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 39: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 00051: reducing learning rate of group 0 to 2.0000e-04.\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 59: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 79: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 99: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "Epoch 00102: reducing learning rate of group 0 to 4.0000e-05.\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 119: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 139: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 00153: reducing learning rate of group 0 to 8.0000e-06.\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 159: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 179: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 199: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 00204: reducing learning rate of group 0 to 1.6000e-06.\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 219: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 239: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 00255: reducing learning rate of group 0 to 3.2000e-07.\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 259: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 279: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 299: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 00306: reducing learning rate of group 0 to 6.4000e-08.\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 319: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 339: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 00357: reducing learning rate of group 0 to 1.2800e-08.\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 359: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 379: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 399: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 00408: reducing learning rate of group 0 to 2.5600e-09.\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 419: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 439: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 459: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 479: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 499: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 519: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 539: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 559: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 579: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 599: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 619: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 639: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 659: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 679: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 699: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 719: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 739: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 759: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 779: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 799: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 819: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 839: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 859: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 879: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 899: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 919: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 939: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 959: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 979: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')\n",
      "Epoch 999: Train Loss:nan, mse: nan, mae: nan, rmse: nan\n"
     ]
    }
   ],
   "source": [
    "#regresion\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_model_scheduler(model, data.x, data.y, data.edge_index, optimizer, loss_fn, scheduler, train_mask)\n",
    "    mse, mae, rmse = evaluate_regression(model, data.x, data.y, data.edge_index, test_mask)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0: \n",
    "        print(f'Epoch {epoch}: Train Loss:{train_loss:.4f}, mse: {mse:.4f}, mae: {mae:.4f}, rmse: {rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5430647d-8540-48ac-9134-afacf2131251",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'GGwith_linear.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e5e513c-5624-4c8b-9c0c-4ca63ebbe491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 1]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_layers=1\n",
    "n_heads = 6\n",
    "heads = ([n_heads] * num_layers) + [1]\n",
    "heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ada2a5fe-9393-4de0-9f0d-b70a0e429c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4052808/384139741.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_indices = torch.tensor(label_indices, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   67,    81,    93,  ..., 14398, 14407, 14426])\n",
      "276\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 0: Train Loss: 0.6935, Macro_F1: 0.3897, AUC_score: 0.7263\n",
      "Epoch 10: Train Loss: 0.6867, Macro_F1: 0.2977, AUC_score: 0.7515\n",
      "Epoch 20: Train Loss: 0.6775, Macro_F1: 0.3710, AUC_score: 0.7702\n",
      "Epoch 30: Train Loss: 0.6668, Macro_F1: 0.4810, AUC_score: 0.7854\n",
      "Epoch 40: Train Loss: 0.6377, Macro_F1: 0.6844, AUC_score: 0.7661\n",
      "Epoch 50: Train Loss: 0.5642, Macro_F1: 0.6831, AUC_score: 0.7506\n",
      "Epoch 60: Train Loss: 0.4668, Macro_F1: 0.8629, AUC_score: 0.8021\n",
      "Epoch 70: Train Loss: 0.4418, Macro_F1: 0.9191, AUC_score: 0.8227\n",
      "Epoch 80: Train Loss: 0.3400, Macro_F1: 0.9376, AUC_score: 0.8663\n",
      "Epoch 90: Train Loss: 0.3410, Macro_F1: 0.9630, AUC_score: 0.8997\n",
      "Epoch 100: Train Loss: 0.3227, Macro_F1: 0.9594, AUC_score: 0.9387\n",
      "Epoch 110: Train Loss: 0.2961, Macro_F1: 0.9666, AUC_score: 0.8963\n",
      "Epoch 120: Train Loss: 0.3108, Macro_F1: 0.9666, AUC_score: 0.9101\n",
      "Epoch 130: Train Loss: 0.3075, Macro_F1: 0.9594, AUC_score: 0.9242\n",
      "Epoch 140: Train Loss: 0.2744, Macro_F1: 0.9592, AUC_score: 0.9338\n",
      "Epoch 150: Train Loss: 0.2936, Macro_F1: 0.9631, AUC_score: 0.9535\n",
      "Epoch 160: Train Loss: 0.2779, Macro_F1: 0.9666, AUC_score: 0.9481\n",
      "Epoch 170: Train Loss: 0.3022, Macro_F1: 0.9593, AUC_score: 0.9448\n",
      "Epoch 180: Train Loss: 0.2781, Macro_F1: 0.9667, AUC_score: 0.9389\n",
      "Epoch 190: Train Loss: 0.2839, Macro_F1: 0.9630, AUC_score: 0.9374\n",
      "Epoch 200: Train Loss: 0.2562, Macro_F1: 0.9630, AUC_score: 0.9352\n",
      "Epoch 210: Train Loss: 0.2688, Macro_F1: 0.9630, AUC_score: 0.9281\n",
      "Epoch 220: Train Loss: 0.2646, Macro_F1: 0.9667, AUC_score: 0.9328\n",
      "Epoch 230: Train Loss: 0.2643, Macro_F1: 0.9667, AUC_score: 0.9454\n",
      "Epoch 240: Train Loss: 0.2655, Macro_F1: 0.9667, AUC_score: 0.9228\n",
      "Epoch 250: Train Loss: 0.2902, Macro_F1: 0.9665, AUC_score: 0.9308\n",
      "Epoch 260: Train Loss: 0.2618, Macro_F1: 0.9631, AUC_score: 0.9541\n",
      "Epoch 270: Train Loss: 0.2654, Macro_F1: 0.9667, AUC_score: 0.9503\n",
      "Epoch 280: Train Loss: 0.2659, Macro_F1: 0.9667, AUC_score: 0.9226\n",
      "Epoch 290: Train Loss: 0.2448, Macro_F1: 0.9629, AUC_score: 0.9204\n",
      "Epoch 300: Train Loss: 0.2727, Macro_F1: 0.9667, AUC_score: 0.9462\n",
      "Epoch 00308: reducing learning rate of group 0 to 2.0000e-05.\n",
      "Epoch 310: Train Loss: 0.2579, Macro_F1: 0.9667, AUC_score: 0.9516\n",
      "Epoch 320: Train Loss: 0.2511, Macro_F1: 0.9667, AUC_score: 0.9464\n",
      "Epoch 330: Train Loss: 0.2685, Macro_F1: 0.9667, AUC_score: 0.9346\n",
      "Epoch 340: Train Loss: 0.2475, Macro_F1: 0.9667, AUC_score: 0.9228\n",
      "Epoch 350: Train Loss: 0.2531, Macro_F1: 0.9667, AUC_score: 0.9194\n",
      "Epoch 360: Train Loss: 0.2550, Macro_F1: 0.9667, AUC_score: 0.9273\n",
      "Epoch 370: Train Loss: 0.2483, Macro_F1: 0.9667, AUC_score: 0.9307\n",
      "Epoch 380: Train Loss: 0.2573, Macro_F1: 0.9667, AUC_score: 0.9307\n",
      "Epoch 390: Train Loss: 0.2343, Macro_F1: 0.9630, AUC_score: 0.9290\n",
      "Epoch 400: Train Loss: 0.2632, Macro_F1: 0.9667, AUC_score: 0.9237\n",
      "Epoch 00403: reducing learning rate of group 0 to 4.0000e-06.\n",
      "Epoch 410: Train Loss: 0.2661, Macro_F1: 0.9630, AUC_score: 0.9200\n",
      "Epoch 420: Train Loss: 0.2314, Macro_F1: 0.9630, AUC_score: 0.9186\n",
      "Epoch 430: Train Loss: 0.2790, Macro_F1: 0.9630, AUC_score: 0.9189\n",
      "Epoch 440: Train Loss: 0.2484, Macro_F1: 0.9630, AUC_score: 0.9195\n",
      "Epoch 450: Train Loss: 0.2378, Macro_F1: 0.9630, AUC_score: 0.9195\n",
      "Epoch 460: Train Loss: 0.2534, Macro_F1: 0.9630, AUC_score: 0.9177\n",
      "Epoch 470: Train Loss: 0.2443, Macro_F1: 0.9630, AUC_score: 0.9172\n",
      "Epoch 480: Train Loss: 0.2514, Macro_F1: 0.9630, AUC_score: 0.9167\n",
      "Epoch 490: Train Loss: 0.2745, Macro_F1: 0.9630, AUC_score: 0.9173\n",
      "Epoch 500: Train Loss: 0.2582, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 510: Train Loss: 0.2407, Macro_F1: 0.9630, AUC_score: 0.9163\n",
      "Epoch 00519: reducing learning rate of group 0 to 8.0000e-07.\n",
      "Epoch 520: Train Loss: 0.2517, Macro_F1: 0.9630, AUC_score: 0.9161\n",
      "Epoch 530: Train Loss: 0.2575, Macro_F1: 0.9630, AUC_score: 0.9163\n",
      "Epoch 540: Train Loss: 0.2321, Macro_F1: 0.9630, AUC_score: 0.9167\n",
      "Epoch 550: Train Loss: 0.2466, Macro_F1: 0.9630, AUC_score: 0.9167\n",
      "Epoch 560: Train Loss: 0.2450, Macro_F1: 0.9630, AUC_score: 0.9167\n",
      "Epoch 570: Train Loss: 0.2453, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 580: Train Loss: 0.2367, Macro_F1: 0.9630, AUC_score: 0.9163\n",
      "Epoch 590: Train Loss: 0.2321, Macro_F1: 0.9630, AUC_score: 0.9164\n",
      "Epoch 600: Train Loss: 0.2454, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 00611: reducing learning rate of group 0 to 1.6000e-07.\n",
      "Epoch 610: Train Loss: 0.2385, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 620: Train Loss: 0.2542, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 630: Train Loss: 0.2282, Macro_F1: 0.9630, AUC_score: 0.9164\n",
      "Epoch 640: Train Loss: 0.2559, Macro_F1: 0.9630, AUC_score: 0.9164\n",
      "Epoch 650: Train Loss: 0.2588, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 660: Train Loss: 0.2436, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 00662: reducing learning rate of group 0 to 3.2000e-08.\n",
      "Epoch 670: Train Loss: 0.2335, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 680: Train Loss: 0.2383, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 690: Train Loss: 0.2483, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 700: Train Loss: 0.2471, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 710: Train Loss: 0.2434, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 00713: reducing learning rate of group 0 to 6.4000e-09.\n",
      "Epoch 720: Train Loss: 0.2492, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 730: Train Loss: 0.2494, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 740: Train Loss: 0.2658, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 750: Train Loss: 0.2332, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 760: Train Loss: 0.2845, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 770: Train Loss: 0.2471, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 780: Train Loss: 0.2492, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 790: Train Loss: 0.2339, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 800: Train Loss: 0.2472, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 810: Train Loss: 0.2487, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 820: Train Loss: 0.2493, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 830: Train Loss: 0.2323, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 840: Train Loss: 0.2652, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 850: Train Loss: 0.2357, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 860: Train Loss: 0.2376, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 870: Train Loss: 0.2840, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 880: Train Loss: 0.2593, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 890: Train Loss: 0.2497, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 900: Train Loss: 0.2531, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 910: Train Loss: 0.2581, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 920: Train Loss: 0.2471, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 930: Train Loss: 0.2555, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 940: Train Loss: 0.2354, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 950: Train Loss: 0.2483, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 960: Train Loss: 0.2509, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 970: Train Loss: 0.2536, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 980: Train Loss: 0.2400, Macro_F1: 0.9630, AUC_score: 0.9165\n",
      "Epoch 990: Train Loss: 0.2461, Macro_F1: 0.9630, AUC_score: 0.9165\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "device = torch.device('cuda:1')\n",
    "data = data.to(device)\n",
    "num_layers=1\n",
    "heads = ([n_heads] * num_layers) + [1]\n",
    "\n",
    "model = GAT(num_layers=num_layers, \n",
    "            in_dim=features.shape[1], \n",
    "            num_hidden=64, \n",
    "            num_classes=2, \n",
    "            heads = heads, \n",
    "            activation=F.elu, dropout=0.6, negative_slope=0.2, residual=True).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.001)#0.0005\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=50, verbose=True)\n",
    "\n",
    "label_indices = torch.tensor(label_indices, dtype=torch.long)\n",
    "print(label_indices)\n",
    "# 随机打乱有标签的节点索引\n",
    "labeled_indices = label_indices[torch.randperm(label_indices.size(0))]\n",
    "#print(labeled_indices)\n",
    "labeled_indices = label_indices\n",
    "\n",
    "# 定义训练和测试集的大小\n",
    "num_labeled = labeled_indices.size(0)\n",
    "num_train = int(num_labeled * 0.8)\n",
    "num_test = num_labeled - num_train\n",
    "print(num_test)\n",
    "\n",
    "# 创建训练和测试掩码\n",
    "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "\n",
    "train_mask[labeled_indices[:num_train]] = True\n",
    "test_mask[labeled_indices[num_train:num_train+num_test]] = True\n",
    "print(test_mask)\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_model_scheduler(model, data.x, data.y, data.edge_index, optimizer, loss_fn, scheduler, train_mask)\n",
    "    #train_loss = train_model(model, data.x, data.y, data.edge_index, optimizer, loss_fn, train_mask)\n",
    "    test_acc, test_auc = evaluate_model(model, data.x, data.y, data.edge_index, test_mask)\n",
    "    \n",
    "    if epoch % 10 == 0: \n",
    "        print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Macro_F1: {test_acc:.4f}, AUC_score: {test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06fba91f-567b-4434-8fde-056219f27b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'GGwith_GAT.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe5ffc1d-7890-4f3f-afc6-1d21a1ba24b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, num_layers, in_dim, num_hidden, num_classes, heads, activation, dropout, negative_slope, residual):\n",
    "        super(GAT, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        self.activation = activation\n",
    "\n",
    "        # Input projection (no residual)\n",
    "        self.gat_layers.append(GATConv(\n",
    "            in_dim, num_hidden, heads=heads[0],\n",
    "            dropout=dropout, negative_slope=negative_slope, concat=True, add_self_loops=True))\n",
    "\n",
    "        # Hidden layers\n",
    "        for l in range(1, num_layers):\n",
    "            # Due to multi-head, the in_dim = num_hidden * num_heads\n",
    "            self.gat_layers.append(GATConv(\n",
    "                num_hidden * heads[l-1], num_hidden, heads=heads[l],\n",
    "                dropout=dropout, negative_slope=negative_slope, concat=True, add_self_loops=True))\n",
    "\n",
    "        # Output projection\n",
    "        self.gat_layers.append(GATConv(\n",
    "            num_hidden * heads[-2], num_classes, heads=heads[-1],\n",
    "            dropout=dropout, negative_slope=negative_slope, concat=False, add_self_loops=True))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = x\n",
    "        for l, layer in enumerate(self.gat_layers[:-1]):\n",
    "            h = layer(h, edge_index)\n",
    "            if self.activation:\n",
    "                h = self.activation(h)\n",
    "            if l < self.num_layers - 1:\n",
    "                h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Output projection\n",
    "        logits = self.gat_layers[-1](h, edge_index)\n",
    "        return logits\n",
    "    def get_embeddings(self, x, edge_index):\n",
    "        h = x\n",
    "        for l, layer in enumerate(self.gat_layers[:-1]):\n",
    "            h = layer(h, edge_index)\n",
    "            if self.activation:\n",
    "                h = self.activation(h)\n",
    "            if l GGwith_GAT.pth< self.num_layers - 1:\n",
    "                h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        return h\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import ModuleList, Dropout, Linear\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim, num_classes, num_layers, num_node_types, activation, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        # 为每种节点类型初始化一个全连接层\n",
    "        self.fc_list = nn.ModuleList([nn.Linear(feats_dim, hidden_dim) for feats_dim in in_feats])\n",
    "        \n",
    "        # 图卷积层\n",
    "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.convs = ModuleList([GCNConv(hidden_dim, hidden_dim) for _ in range(num_layers - 2)])\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "        # 权重初始化（可选）\n",
    "        for fc in self.fc_list:\n",
    "            nn.init.xavier_normal_(fc.weight, gain=1.414)\n",
    "        for layer in self.convs:\n",
    "            nn.init.xavier_normal_(layer.weight, gain=1.4)\n",
    "\n",
    "    def forward(self, x, edge_index, node_types):\n",
    "        # 节点类型特定的特征转换\n",
    "        h = [fc(x_i) for fc, x_i in zip(self.fc_list, x)]\n",
    "        x = torch.cat(h, 0)  # 将不同类型的节点特征合并\n",
    "        \n",
    "        # 图卷积层\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.fc(x)  # 最终分类层\n",
    "        return x\n",
    "\n",
    "    def get_embedding(self, x, edge_index, node_types):\n",
    "        # 重复前向传播中的处理，但通常不包括最后的分类层\n",
    "        h = [fc(x_i) for fc, x_i in zip(self.fc_list, x)]\n",
    "        x = torch.cat(h, 0)\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5be799b9-1ef9-4573-b4df-c6c0bd77d7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Feature_0  Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  \\\n",
      "0       0.146868        0.0   0.027175   0.107902   0.161667   0.000000   \n",
      "1       0.000000        0.0   0.310793   0.095509   0.278071   0.000000   \n",
      "2       0.000000        0.0   0.000000   0.751667   0.000000   0.000000   \n",
      "3       0.000000        0.0   0.349353   0.073857   0.288762   0.000000   \n",
      "4       0.000000        0.0   0.538568   0.000000   0.581786   0.001704   \n",
      "...          ...        ...        ...        ...        ...        ...   \n",
      "62040   0.122980        0.0   0.000000   0.883799   0.000000   0.000000   \n",
      "62041   0.080433        0.0   0.000000   0.404349   0.000750   0.000206   \n",
      "62042   0.021762        0.0   0.000000   0.702997   0.000000   0.000000   \n",
      "62043   0.509305        0.0   0.785769   0.000000   0.666547   0.003337   \n",
      "62044   1.305182        0.0   0.866692   0.000000   0.683970   0.000000   \n",
      "\n",
      "       Feature_6  Feature_7  Feature_8  Feature_9  ...  Feature_54  \\\n",
      "0       0.214584   0.065325   0.079564   0.113648  ...    0.134436   \n",
      "1       0.132546   0.312597   0.325196   0.091759  ...    0.250705   \n",
      "2       0.463399   0.000000   0.000000   0.632237  ...    0.000000   \n",
      "3       0.197156   0.284295   0.297394   0.101600  ...    0.278942   \n",
      "4       0.155134   0.567168   0.546199   0.000000  ...    0.548186   \n",
      "...          ...        ...        ...        ...  ...         ...   \n",
      "62040   0.400400   0.000000   0.000000   0.744805  ...    0.000000   \n",
      "62041   0.202458   0.000000   0.000000   0.345257  ...    0.000000   \n",
      "62042   0.295922   0.000000   0.000000   0.580830  ...    0.000000   \n",
      "62043   0.000000   0.731725   0.641647   0.000000  ...    0.726363   \n",
      "62044   0.000000   0.755405   0.529496   0.000000  ...    0.769195   \n",
      "\n",
      "       Feature_55  Feature_56  Feature_57  Feature_58  Feature_59  Feature_60  \\\n",
      "0        0.151516    0.138373    0.139669         0.0    0.000000    0.062650   \n",
      "1        0.217503    0.081036    0.280833         0.0    0.000000    0.281917   \n",
      "2        0.000000    0.624111    0.000000         0.0    0.000000    0.000000   \n",
      "3        0.280359    0.141396    0.269250         0.0    0.000000    0.339418   \n",
      "4        0.516534    0.000000    0.536494         0.0    0.000000    0.536474   \n",
      "...           ...         ...         ...         ...         ...         ...   \n",
      "62040    0.000000    0.660870    0.000000         0.0    0.000178    0.000000   \n",
      "62041    0.000000    0.306465    0.025778         0.0    0.000107    0.030815   \n",
      "62042    0.000000    0.480347    0.000000         0.0    0.000110    0.000000   \n",
      "62043    0.731937    0.000000    0.710831         0.0    0.000000    0.715655   \n",
      "62044    1.007962    0.000000    0.838710         0.0    0.000000    0.699373   \n",
      "\n",
      "       Feature_61  Feature_62  Feature_63  \n",
      "0        0.008044    0.160141    0.118033  \n",
      "1        0.311451    0.039210    0.088547  \n",
      "2        0.000000    0.658979    0.740995  \n",
      "3        0.377494    0.183116    0.111606  \n",
      "4        0.519704    0.000000    0.000000  \n",
      "...           ...         ...         ...  \n",
      "62040    0.000000    0.408723    0.870005  \n",
      "62041    0.000000    0.140077    0.392710  \n",
      "62042    0.000000    0.337681    0.696167  \n",
      "62043    0.456516    0.000000    0.000000  \n",
      "62044    0.272547    0.000000    0.000000  \n",
      "\n",
      "[62045 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#model = GCN(num_features=data.x.shape[1], hidden_dim=64, num_classes=2, num_layers=2, activation=F.relu, dropout=0.5).to(device)\n",
    "#model.load_state_dict(torch.load('GGwith.pth'))\n",
    "model.eval()  # 切换模型到评估模式\n",
    "\n",
    "with torch.no_grad():  # 在评估模式下不需要计算梯度\n",
    "    embeddings = model.get_embedding(data.x, data.edge_index).to('cpu')\n",
    "\n",
    "# 将嵌入转换为 DataFrame\n",
    "embeddings_df = pd.DataFrame(embeddings.numpy(), columns=[f'Feature_{i}' for i in range(embeddings.shape[1])])\n",
    "\n",
    "# 打印嵌入结果 DataFrame\n",
    "print(embeddings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b497835-1da2-4a2e-8599-aae9988e8996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature759</th>\n",
       "      <th>feature760</th>\n",
       "      <th>feature761</th>\n",
       "      <th>feature762</th>\n",
       "      <th>feature763</th>\n",
       "      <th>feature764</th>\n",
       "      <th>feature765</th>\n",
       "      <th>feature766</th>\n",
       "      <th>feature767</th>\n",
       "      <th>feature768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FES</td>\n",
       "      <td>0.339602</td>\n",
       "      <td>-0.030744</td>\n",
       "      <td>-0.901381</td>\n",
       "      <td>0.100888</td>\n",
       "      <td>0.886443</td>\n",
       "      <td>0.383596</td>\n",
       "      <td>-0.192082</td>\n",
       "      <td>-0.032063</td>\n",
       "      <td>-0.154869</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.549204</td>\n",
       "      <td>-0.856123</td>\n",
       "      <td>0.714672</td>\n",
       "      <td>-0.046649</td>\n",
       "      <td>-0.894424</td>\n",
       "      <td>-0.001815</td>\n",
       "      <td>0.739485</td>\n",
       "      <td>0.015581</td>\n",
       "      <td>-0.023863</td>\n",
       "      <td>-0.022002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HADHA</td>\n",
       "      <td>-0.131799</td>\n",
       "      <td>-0.025745</td>\n",
       "      <td>-0.677301</td>\n",
       "      <td>-0.053545</td>\n",
       "      <td>0.971046</td>\n",
       "      <td>0.180315</td>\n",
       "      <td>-0.028189</td>\n",
       "      <td>-0.077389</td>\n",
       "      <td>-0.095152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.927885</td>\n",
       "      <td>-0.817812</td>\n",
       "      <td>0.809631</td>\n",
       "      <td>-0.005827</td>\n",
       "      <td>-0.848839</td>\n",
       "      <td>0.024516</td>\n",
       "      <td>0.526404</td>\n",
       "      <td>-0.039926</td>\n",
       "      <td>-0.102787</td>\n",
       "      <td>-0.026980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLC7A7</td>\n",
       "      <td>0.385693</td>\n",
       "      <td>-0.070692</td>\n",
       "      <td>-0.847796</td>\n",
       "      <td>-0.022054</td>\n",
       "      <td>0.959772</td>\n",
       "      <td>0.085487</td>\n",
       "      <td>0.076455</td>\n",
       "      <td>-0.003006</td>\n",
       "      <td>-0.032268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.941094</td>\n",
       "      <td>-0.912443</td>\n",
       "      <td>0.789828</td>\n",
       "      <td>0.046979</td>\n",
       "      <td>-0.715636</td>\n",
       "      <td>0.085842</td>\n",
       "      <td>0.150494</td>\n",
       "      <td>0.025392</td>\n",
       "      <td>-0.066035</td>\n",
       "      <td>-0.028283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LCK</td>\n",
       "      <td>0.650428</td>\n",
       "      <td>0.014479</td>\n",
       "      <td>-0.866163</td>\n",
       "      <td>0.053508</td>\n",
       "      <td>0.951529</td>\n",
       "      <td>0.269402</td>\n",
       "      <td>-0.214788</td>\n",
       "      <td>0.045179</td>\n",
       "      <td>-0.506429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.576739</td>\n",
       "      <td>-0.969558</td>\n",
       "      <td>0.916549</td>\n",
       "      <td>-0.080332</td>\n",
       "      <td>-0.927649</td>\n",
       "      <td>-0.047398</td>\n",
       "      <td>0.741663</td>\n",
       "      <td>-0.000096</td>\n",
       "      <td>-0.096318</td>\n",
       "      <td>-0.056501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HSPA2</td>\n",
       "      <td>0.322262</td>\n",
       "      <td>0.017484</td>\n",
       "      <td>-0.849302</td>\n",
       "      <td>0.046401</td>\n",
       "      <td>0.920429</td>\n",
       "      <td>0.463832</td>\n",
       "      <td>-0.050414</td>\n",
       "      <td>-0.033398</td>\n",
       "      <td>0.387791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387301</td>\n",
       "      <td>-0.860696</td>\n",
       "      <td>0.678607</td>\n",
       "      <td>-0.060695</td>\n",
       "      <td>-0.945793</td>\n",
       "      <td>0.040472</td>\n",
       "      <td>0.831079</td>\n",
       "      <td>-0.001711</td>\n",
       "      <td>-0.079842</td>\n",
       "      <td>-0.011189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47590</th>\n",
       "      <td>GO:2001313</td>\n",
       "      <td>0.174428</td>\n",
       "      <td>0.194728</td>\n",
       "      <td>-0.284376</td>\n",
       "      <td>0.282102</td>\n",
       "      <td>-0.713190</td>\n",
       "      <td>-0.272055</td>\n",
       "      <td>0.121190</td>\n",
       "      <td>0.129901</td>\n",
       "      <td>-0.983496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500545</td>\n",
       "      <td>0.429651</td>\n",
       "      <td>-0.292929</td>\n",
       "      <td>-0.464941</td>\n",
       "      <td>-0.740187</td>\n",
       "      <td>0.179149</td>\n",
       "      <td>-0.960807</td>\n",
       "      <td>-0.746958</td>\n",
       "      <td>1.069112</td>\n",
       "      <td>-0.848182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47591</th>\n",
       "      <td>GO:2001314</td>\n",
       "      <td>0.025886</td>\n",
       "      <td>0.306214</td>\n",
       "      <td>-0.254303</td>\n",
       "      <td>0.253673</td>\n",
       "      <td>-0.533680</td>\n",
       "      <td>-0.269355</td>\n",
       "      <td>0.150939</td>\n",
       "      <td>-0.229323</td>\n",
       "      <td>-1.078991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042979</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>-0.356661</td>\n",
       "      <td>-0.381828</td>\n",
       "      <td>-0.638338</td>\n",
       "      <td>0.077176</td>\n",
       "      <td>-0.788312</td>\n",
       "      <td>-0.683442</td>\n",
       "      <td>1.087031</td>\n",
       "      <td>-0.593092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47592</th>\n",
       "      <td>GO:2001315</td>\n",
       "      <td>0.027134</td>\n",
       "      <td>0.241391</td>\n",
       "      <td>-0.227353</td>\n",
       "      <td>0.317366</td>\n",
       "      <td>-0.726657</td>\n",
       "      <td>-0.197968</td>\n",
       "      <td>0.045653</td>\n",
       "      <td>0.038912</td>\n",
       "      <td>-0.954113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349853</td>\n",
       "      <td>0.370059</td>\n",
       "      <td>-0.144606</td>\n",
       "      <td>-0.493184</td>\n",
       "      <td>-0.655063</td>\n",
       "      <td>0.217335</td>\n",
       "      <td>-0.841272</td>\n",
       "      <td>-0.821077</td>\n",
       "      <td>1.036363</td>\n",
       "      <td>-0.836614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47593</th>\n",
       "      <td>GO:2001316</td>\n",
       "      <td>0.139543</td>\n",
       "      <td>0.028883</td>\n",
       "      <td>0.899480</td>\n",
       "      <td>0.152932</td>\n",
       "      <td>0.576852</td>\n",
       "      <td>0.330342</td>\n",
       "      <td>0.916943</td>\n",
       "      <td>0.012306</td>\n",
       "      <td>-0.020316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.354748</td>\n",
       "      <td>-0.083168</td>\n",
       "      <td>0.043640</td>\n",
       "      <td>-0.663565</td>\n",
       "      <td>0.543016</td>\n",
       "      <td>-0.652230</td>\n",
       "      <td>-1.427882</td>\n",
       "      <td>-0.985257</td>\n",
       "      <td>1.673561</td>\n",
       "      <td>0.109659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47594</th>\n",
       "      <td>GO:2001317</td>\n",
       "      <td>0.083064</td>\n",
       "      <td>0.090899</td>\n",
       "      <td>0.888541</td>\n",
       "      <td>0.309920</td>\n",
       "      <td>0.403966</td>\n",
       "      <td>0.202783</td>\n",
       "      <td>0.706517</td>\n",
       "      <td>-0.017584</td>\n",
       "      <td>-0.171057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.544680</td>\n",
       "      <td>-0.046654</td>\n",
       "      <td>0.262865</td>\n",
       "      <td>-0.767305</td>\n",
       "      <td>0.753788</td>\n",
       "      <td>-0.577503</td>\n",
       "      <td>-1.194910</td>\n",
       "      <td>-0.799556</td>\n",
       "      <td>1.519368</td>\n",
       "      <td>0.263210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62045 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          protein  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
       "0             FES  0.339602 -0.030744 -0.901381  0.100888  0.886443  0.383596   \n",
       "1          HADHA  -0.131799 -0.025745 -0.677301 -0.053545  0.971046  0.180315   \n",
       "2          SLC7A7  0.385693 -0.070692 -0.847796 -0.022054  0.959772  0.085487   \n",
       "3            LCK   0.650428  0.014479 -0.866163  0.053508  0.951529  0.269402   \n",
       "4           HSPA2  0.322262  0.017484 -0.849302  0.046401  0.920429  0.463832   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "47590  GO:2001313  0.174428  0.194728 -0.284376  0.282102 -0.713190 -0.272055   \n",
       "47591  GO:2001314  0.025886  0.306214 -0.254303  0.253673 -0.533680 -0.269355   \n",
       "47592  GO:2001315  0.027134  0.241391 -0.227353  0.317366 -0.726657 -0.197968   \n",
       "47593  GO:2001316  0.139543  0.028883  0.899480  0.152932  0.576852  0.330342   \n",
       "47594  GO:2001317  0.083064  0.090899  0.888541  0.309920  0.403966  0.202783   \n",
       "\n",
       "       feature7  feature8  feature9  ...  feature759  feature760  feature761  \\\n",
       "0     -0.192082 -0.032063 -0.154869  ...   -0.549204   -0.856123    0.714672   \n",
       "1     -0.028189 -0.077389 -0.095152  ...    0.927885   -0.817812    0.809631   \n",
       "2      0.076455 -0.003006 -0.032268  ...    0.941094   -0.912443    0.789828   \n",
       "3     -0.214788  0.045179 -0.506429  ...   -0.576739   -0.969558    0.916549   \n",
       "4     -0.050414 -0.033398  0.387791  ...    0.387301   -0.860696    0.678607   \n",
       "...         ...       ...       ...  ...         ...         ...         ...   \n",
       "47590  0.121190  0.129901 -0.983496  ...    0.500545    0.429651   -0.292929   \n",
       "47591  0.150939 -0.229323 -1.078991  ...    0.042979    0.134560   -0.356661   \n",
       "47592  0.045653  0.038912 -0.954113  ...    0.349853    0.370059   -0.144606   \n",
       "47593  0.916943  0.012306 -0.020316  ...   -0.354748   -0.083168    0.043640   \n",
       "47594  0.706517 -0.017584 -0.171057  ...   -0.544680   -0.046654    0.262865   \n",
       "\n",
       "       feature762  feature763  feature764  feature765  feature766  feature767  \\\n",
       "0       -0.046649   -0.894424   -0.001815    0.739485    0.015581   -0.023863   \n",
       "1       -0.005827   -0.848839    0.024516    0.526404   -0.039926   -0.102787   \n",
       "2        0.046979   -0.715636    0.085842    0.150494    0.025392   -0.066035   \n",
       "3       -0.080332   -0.927649   -0.047398    0.741663   -0.000096   -0.096318   \n",
       "4       -0.060695   -0.945793    0.040472    0.831079   -0.001711   -0.079842   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "47590   -0.464941   -0.740187    0.179149   -0.960807   -0.746958    1.069112   \n",
       "47591   -0.381828   -0.638338    0.077176   -0.788312   -0.683442    1.087031   \n",
       "47592   -0.493184   -0.655063    0.217335   -0.841272   -0.821077    1.036363   \n",
       "47593   -0.663565    0.543016   -0.652230   -1.427882   -0.985257    1.673561   \n",
       "47594   -0.767305    0.753788   -0.577503   -1.194910   -0.799556    1.519368   \n",
       "\n",
       "       feature768  \n",
       "0       -0.022002  \n",
       "1       -0.026980  \n",
       "2       -0.028283  \n",
       "3       -0.056501  \n",
       "4       -0.011189  \n",
       "...           ...  \n",
       "47590   -0.848182  \n",
       "47591   -0.593092  \n",
       "47592   -0.836614  \n",
       "47593    0.109659  \n",
       "47594    0.263210  \n",
       "\n",
       "[62045 rows x 769 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93b03dc4-bdd2-4501-a826-5780bf94c09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last key: GO:2001317, Last value: 62044\n"
     ]
    }
   ],
   "source": [
    "\n",
    "last_key = list(node_id_to_index.keys())[-1]\n",
    "last_value = node_id_to_index[last_key]\n",
    "\n",
    "print(f\"Last key: {last_key}, Last value: {last_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ea8c542-0def-4e41-a05e-66436d4c9560",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot insert name, already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_63598/3805924712.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcombined_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0membeddings_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'protein'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0membeddings_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'embeddings_with_GG.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/python310/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   5141\u001b[0m                 \u001b[0;34m\"'self.flags.allows_duplicate_labels' is False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5142\u001b[0m             )\n\u001b[1;32m   5143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_duplicates\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5144\u001b[0m             \u001b[0;31m# Should this be a different kind of error??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5145\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cannot insert {column}, already exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5147\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loc must be int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5148\u001b[0m         \u001b[0;31m# convert non stdlib ints to satisfy typing checks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot insert name, already exists"
     ]
    }
   ],
   "source": [
    "combined_features = combined_features.reset_index(drop=True)\n",
    "embeddings_df.insert(0, 'name', combined_features['protein'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25c28b0c-e3dc-4cc0-96df-d5062cee70c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df.to_csv('embeddings_with_GGlinear.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98d68e9f-04af-4a31-8231-ffb0f4b801f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Score: 0.9670\n",
      "ROC AUC Score: 0.9931\n"
     ]
    }
   ],
   "source": [
    "features = data.x  # 示例特征\n",
    "edge_index = data.edge_index  # 示例边缘索引\n",
    "labels = data.y  # 示例标签\n",
    "# 评估模型\n",
    "val_f1, auc_score = evaluate_model(model, features, labels, edge_index, test_mask)\n",
    "print(f'Validation F1 Score: {val_f1:.4f}')\n",
    "print(f'ROC AUC Score: {auc_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a41578-7861-45e3-b1a8-83602c7695b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1338021/1920998772.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_indices = torch.tensor(label_indices, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1019,  8632, 10776,  ...,  7349,  1137,  4545])\n",
      "276\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 9: Train Loss: 0.6529, Macro_F1: 0.8006, AUC_score: 0.3963\n",
      "Epoch 19: Train Loss: 0.5299, Macro_F1: 0.7897, AUC_score: 0.1746\n",
      "Epoch 29: Train Loss: 0.4589, Macro_F1: 0.8202, AUC_score: 0.0999\n",
      "Epoch 39: Train Loss: 0.4406, Macro_F1: 0.8985, AUC_score: 0.0841\n",
      "Epoch 49: Train Loss: 0.5106, Macro_F1: 0.9130, AUC_score: 0.0822\n",
      "Epoch 59: Train Loss: 0.4014, Macro_F1: 0.9238, AUC_score: 0.0830\n",
      "Epoch 69: Train Loss: 0.3948, Macro_F1: 0.9165, AUC_score: 0.0837\n",
      "Epoch 79: Train Loss: 0.3624, Macro_F1: 0.9239, AUC_score: 0.0807\n",
      "Epoch 89: Train Loss: 0.3454, Macro_F1: 0.9130, AUC_score: 0.0833\n",
      "Epoch 99: Train Loss: 0.3451, Macro_F1: 0.9092, AUC_score: 0.0970\n",
      "Epoch 109: Train Loss: 0.3832, Macro_F1: 0.9203, AUC_score: 0.0929\n",
      "Epoch 119: Train Loss: 0.3633, Macro_F1: 0.9130, AUC_score: 0.0854\n",
      "Epoch 129: Train Loss: 0.3709, Macro_F1: 0.9201, AUC_score: 0.1003\n",
      "Epoch 139: Train Loss: 0.3262, Macro_F1: 0.9239, AUC_score: 0.0962\n",
      "Epoch 149: Train Loss: 0.3208, Macro_F1: 0.9203, AUC_score: 0.1089\n",
      "Epoch 159: Train Loss: 0.3359, Macro_F1: 0.9201, AUC_score: 0.1269\n",
      "Epoch 169: Train Loss: 0.3190, Macro_F1: 0.9094, AUC_score: 0.1167\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "device = torch.device('cuda')\n",
    "data = data.to(device)\n",
    "\n",
    "model = GAT(num_layers=1, \n",
    "            in_dim=features.shape[1], \n",
    "            num_hidden=64, \n",
    "            num_classes=2, \n",
    "            heads = heads, \n",
    "            activation=F.elu, dropout=0.6, negative_slope=0.2, residual=True).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.001)#0.0005\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=100, verbose=True)\n",
    "\n",
    "label_indices = torch.tensor(label_indices, dtype=torch.long)\n",
    "print(label_indices)\n",
    "# 随机打乱有标签的节点索引\n",
    "labeled_indices = label_indices[torch.randperm(label_indices.size(0))]\n",
    "#print(labeled_indices)\n",
    "labeled_indices = label_indices\n",
    "\n",
    "# 定义训练和测试集的大小\n",
    "num_labeled = labeled_indices.size(0)\n",
    "num_train = int(num_labeled * 0.8)\n",
    "num_test = num_labeled - num_train\n",
    "print(num_test)\n",
    "\n",
    "# 创建训练和测试掩码\n",
    "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "\n",
    "train_mask[labeled_indices[:num_train]] = True\n",
    "test_mask[labeled_indices[num_train:num_train+num_test]] = True\n",
    "print(test_mask)\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_model_scheduler(model, data.x, data.y, data.edge_index, optimizer, loss_fn, scheduler, train_mask)\n",
    "    #train_loss = train_model(model, data.x, data.y, data.edge_index, optimizer, loss_fn, train_mask)\n",
    "    test_acc, test_auc = evaluate_model(model, data.x, data.y, data.edge_index, test_mask)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0: \n",
    "        print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Macro_F1: {test_acc:.4f}, AUC_score: {test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc54ede0-9e20-45ae-b403-d705a40c77cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear, ModuleList, Dropout\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, num_classes, num_layers, activation, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.convs = ModuleList([GCNConv(hidden_dim, hidden_dim) for _ in range(num_layers - 2)])\n",
    "        self.conv_last = GCNConv(hidden_dim, num_classes)\n",
    "        self.activation = activation\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # 输入层\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 隐藏层\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # 输出层\n",
    "        x = self.conv_last(x, edge_index)\n",
    "        return x\n",
    "    def get_embeddings(self, x, edge_index):\n",
    "        # 输入层\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 隐藏层\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # 返回最后一层之前的嵌入\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e530a1c7-1137-4161-b102-64c5b282f771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         name  Feature_0  Feature_1  Feature_2  Feature_3  Feature_4  \\\n",
      "0         FES        0.0        0.0        0.0        0.0        0.0   \n",
      "1      HADHA         0.0        0.0        0.0        0.0        0.0   \n",
      "2      SLC7A7        0.0        0.0        0.0        0.0        0.0   \n",
      "3        LCK         0.0        0.0        0.0        0.0        0.0   \n",
      "4       HSPA2        0.0        0.0        0.0        0.0        0.0   \n",
      "...       ...        ...        ...        ...        ...        ...   \n",
      "14445   BPY2C        0.0        0.0        0.0        0.0        0.0   \n",
      "14446    CLPS        0.0        0.0        0.0        0.0        0.0   \n",
      "14447    DNER        0.0        0.0        0.0        0.0        0.0   \n",
      "14448    SOX7        0.0        0.0        0.0        0.0        0.0   \n",
      "14449  CXCL14        0.0        0.0        0.0        0.0        0.0   \n",
      "\n",
      "       Feature_5  Feature_6  Feature_7  Feature_8  ...  Feature_54  \\\n",
      "0            0.0   0.000000   0.000000   0.000000  ...    0.000000   \n",
      "1            0.0   0.012943   0.000000   0.000000  ...    0.000000   \n",
      "2            0.0   0.000000   0.000000   0.000000  ...    0.010094   \n",
      "3            0.0   0.000000   0.000000   0.000000  ...    0.017528   \n",
      "4            0.0   0.000000   0.000000   0.000000  ...    0.000000   \n",
      "...          ...        ...        ...        ...  ...         ...   \n",
      "14445        0.0   0.000000   0.136369   0.084607  ...    0.000000   \n",
      "14446        0.0   0.000000   0.000000   0.000000  ...    0.000000   \n",
      "14447        0.0   0.000000   0.000000   0.000000  ...    0.000000   \n",
      "14448        0.0   0.000000   0.000000   0.000000  ...    0.000000   \n",
      "14449        0.0   0.000000   0.000000   0.000000  ...    0.000000   \n",
      "\n",
      "       Feature_55  Feature_56  Feature_57  Feature_58  Feature_59  Feature_60  \\\n",
      "0             0.0         0.0         0.0    0.000000    0.000000    0.000000   \n",
      "1             0.0         0.0         0.0    0.000000    0.000000    0.000000   \n",
      "2             0.0         0.0         0.0    0.000000    0.000000    0.000000   \n",
      "3             0.0         0.0         0.0    0.000000    0.000000    0.000000   \n",
      "4             0.0         0.0         0.0    0.000000    0.000000    0.000000   \n",
      "...           ...         ...         ...         ...         ...         ...   \n",
      "14445         0.0         0.0         0.0    0.147961    0.262111    0.000974   \n",
      "14446         0.0         0.0         0.0    0.000000    0.000000    0.000000   \n",
      "14447         0.0         0.0         0.0    0.000000    0.000000    0.000000   \n",
      "14448         0.0         0.0         0.0    0.000000    0.000000    0.000000   \n",
      "14449         0.0         0.0         0.0    0.000000    0.000000    0.000000   \n",
      "\n",
      "       Feature_61  Feature_62  Feature_63  \n",
      "0             0.0         0.0     0.00000  \n",
      "1             0.0         0.0     0.00000  \n",
      "2             0.0         0.0     0.00000  \n",
      "3             0.0         0.0     0.00000  \n",
      "4             0.0         0.0     0.00000  \n",
      "...           ...         ...         ...  \n",
      "14445         0.0         0.0     0.15596  \n",
      "14446         0.0         0.0     0.00000  \n",
      "14447         0.0         0.0     0.00000  \n",
      "14448         0.0         0.0     0.00000  \n",
      "14449         0.0         0.0     0.00000  \n",
      "\n",
      "[14450 rows x 65 columns]\n"
     ]
    }
   ],
   "source": [
    "num_rows = len(embeddings_df)\n",
    "if num_rows > 47595:\n",
    "    filtered_df = embeddings_df.iloc[:num_rows - 47595]\n",
    "else:\n",
    "    filtered_df = embeddings_df.copy()  # 如果行数不够，复制整个 DataFrame\n",
    "\n",
    "# 打印过滤后的 DataFrame\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4ef14cc7-af81-47f3-b0e4-5cdadbacc437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 提取嵌入向量\n",
    "X = filtered_df.drop(columns=['name'])\n",
    "\n",
    "# 进行 PCA\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(X)\n",
    "\n",
    "# 创建 PCA 结果的 DataFrame\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['Principal Component 1', 'Principal Component 2'])\n",
    "pca_df['name'] = filtered_df['name']\n",
    "\"\"\"\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(pca_df['Principal Component 1'], pca_df['Principal Component 2'], alpha=0.5)\n",
    "for i, txt in enumerate(pca_df['name']):\n",
    "    plt.annotate(txt, (pca_df['Principal Component 1'][i], pca_df['Principal Component 2'][i]), fontsize=8, alpha=0.7)\n",
    "plt.title('PCA of Embeddings')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()\"\"\"\n",
    "\n",
    "# 选择放大区域的范围\n",
    "x_min, x_max = -0.5, 2  # 根据初始图确定的范围\n",
    "y_min, y_max = -2, 4 # 根据初始图确定的范围\n",
    "\n",
    "# 绘制放大后的 PCA 图\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(pca_df['Principal Component 1'], pca_df['Principal Component 2'], alpha=0.5)\n",
    "#for i, txt in enumerate(pca_df['name']):\n",
    "#    plt.annotate(txt, (pca_df['Principal Component 1'][i], pca_df['Principal Component 2'][i]), fontsize=8, alpha=0.7)\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.title('Zoomed PCA of Embeddings')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e6e6ec71-5279-47aa-a70e-035c04961486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>Feature_0</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_55</th>\n",
       "      <th>Feature_56</th>\n",
       "      <th>Feature_57</th>\n",
       "      <th>Feature_58</th>\n",
       "      <th>Feature_59</th>\n",
       "      <th>Feature_60</th>\n",
       "      <th>Feature_61</th>\n",
       "      <th>Feature_62</th>\n",
       "      <th>Feature_63</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ERAP2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADAMTSL5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TBC1D30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KCNK18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NDNF</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>TRABD2B</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>RPS9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.534929</td>\n",
       "      <td>1.434695</td>\n",
       "      <td>0.804825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.487443</td>\n",
       "      <td>1.025692</td>\n",
       "      <td>3.750202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.518183</td>\n",
       "      <td>2.788001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.35715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.186884</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>SLC22A16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>FBN3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378</th>\n",
       "      <td>BDH2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1379 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          name  Feature_0  Feature_1  Feature_2  Feature_3  Feature_4  \\\n",
       "0        ERAP2        0.0        0.0   0.000000   0.000000   0.000000   \n",
       "1     ADAMTSL5        0.0        0.0   0.000000   0.000000   0.000000   \n",
       "2      TBC1D30        0.0        0.0   0.000000   0.000000   0.000000   \n",
       "3       KCNK18        0.0        0.0   0.000000   0.000000   0.000000   \n",
       "4         NDNF        0.0        0.0   0.000000   0.000000   0.000000   \n",
       "...        ...        ...        ...        ...        ...        ...   \n",
       "1374   TRABD2B        0.0        0.0   0.000000   0.000000   0.000000   \n",
       "1375      RPS9        0.0        0.0   0.534929   1.434695   0.804825   \n",
       "1376  SLC22A16        0.0        0.0   0.000000   0.000000   0.000000   \n",
       "1377      FBN3        0.0        0.0   0.000000   0.000000   0.000000   \n",
       "1378      BDH2        0.0        0.0   0.000000   0.000000   0.000000   \n",
       "\n",
       "      Feature_5  Feature_6  Feature_7  Feature_8  ...  Feature_55  Feature_56  \\\n",
       "0           0.0   0.000000   0.000000   0.000000  ...    0.000000         0.0   \n",
       "1           0.0   0.000000   0.000000   0.000000  ...    0.000000         0.0   \n",
       "2           0.0   0.000000   0.000000   0.000000  ...    0.000000         0.0   \n",
       "3           0.0   0.000000   0.000000   0.000000  ...    0.000000         0.0   \n",
       "4           0.0   0.000000   0.000000   0.000000  ...    0.000000         0.0   \n",
       "...         ...        ...        ...        ...  ...         ...         ...   \n",
       "1374        0.0   0.000000   0.000000   0.052822  ...    0.000000         0.0   \n",
       "1375        0.0   3.487443   1.025692   3.750202  ...    0.000000         0.0   \n",
       "1376        0.0   0.000000   0.000000   0.000000  ...    0.117923         0.0   \n",
       "1377        0.0   0.059236   0.000000   0.000000  ...    0.000000         0.0   \n",
       "1378        0.0   0.000000   0.000000   0.000000  ...    0.000000         0.0   \n",
       "\n",
       "      Feature_57  Feature_58  Feature_59  Feature_60  Feature_61  Feature_62  \\\n",
       "0            0.0    0.000000    0.000000         0.0     0.00000         0.0   \n",
       "1            0.0    0.000000    0.000000         0.0     0.00000         0.0   \n",
       "2            0.0    0.000000    0.000000         0.0     0.00000         0.0   \n",
       "3            0.0    0.000000    0.000000         0.0     0.00000         0.0   \n",
       "4            0.0    0.000000    0.000000         0.0     0.00000         0.0   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "1374         0.0    0.000000    0.000000         0.0     0.00000         0.0   \n",
       "1375         0.0    0.518183    2.788001         0.0     1.35715         0.0   \n",
       "1376         0.0    0.000000    0.000000         0.0     0.00000         0.0   \n",
       "1377         0.0    0.000000    0.000000         0.0     0.00000         0.0   \n",
       "1378         0.0    0.000000    0.000000         0.0     0.00000         0.0   \n",
       "\n",
       "      Feature_63  Label  \n",
       "0       0.000000      0  \n",
       "1       0.000000      1  \n",
       "2       0.000000      0  \n",
       "3       0.000000      0  \n",
       "4       0.000000      1  \n",
       "...          ...    ...  \n",
       "1374    0.000000      0  \n",
       "1375    3.186884      1  \n",
       "1376    0.000000      0  \n",
       "1377    0.000000      1  \n",
       "1378    0.000000      1  \n",
       "\n",
       "[1379 rows x 66 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df.rename(columns={'protein':'name'}, inplace=True)\n",
    "label_df = labels_df[['name', 'Label']]\n",
    "merged_df = pd.merge(filtered_df, label_df, on='name', how='inner')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1de91452-db1f-4129-8fb1-ff1c5a79733f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.6086956521739131\n",
      "Logistic Regression AUC: 0.7230257601351351\n",
      "Linear Regression Accuracy: 0.5036231884057971\n",
      "Linear Regression AUC: 0.7171135979729729\n"
     ]
    }
   ],
   "source": [
    "X = merged_df.drop(columns=['name', 'Label'])\n",
    "y = merged_df['Label']\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# 标准化 embeddings\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 降维（可选）\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# 拆分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 训练逻辑回归模型\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# 进行预测\n",
    "y_pred_logistic = logistic_model.predict(X_test)\n",
    "y_pred_prob_logistic = logistic_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 计算逻辑回归的准确率和AUC\n",
    "accuracy_logistic = accuracy_score(y_test, y_pred_logistic)\n",
    "auc_logistic = roc_auc_score(y_test, y_pred_prob_logistic)\n",
    "print(f'Logistic Regression Accuracy: {accuracy_logistic}')\n",
    "print(f'Logistic Regression AUC: {auc_logistic}')\n",
    "\n",
    "# 训练线性回归模型\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# 进行预测\n",
    "y_pred_prob_linear = linear_model.predict(X_test)\n",
    "\n",
    "# 将预测的连续值转换为二分类结果\n",
    "y_pred_linear = (y_pred_prob_linear >= 0.5).astype(int)\n",
    "\n",
    "# 计算线性回归的准确率和AUC\n",
    "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
    "auc_linear = roc_auc_score(y_test, y_pred_prob_linear)\n",
    "print(f'Linear Regression Accuracy: {accuracy_linear}')\n",
    "print(f'Linear Regression AUC: {auc_linear}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c4626498-50ba-4cbf-914b-0afc1cdd7308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 0,  ..., 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "model = GCN(num_features=data.x.shape[1], hidden_dim=64, num_classes=2, num_layers=2, activation=F.relu, dropout=0.5).to(device)\n",
    "model.load_state_dict(torch.load('GGwith.pth'))\n",
    "model.eval()  # 切换模型到评估模式\n",
    "\n",
    "with torch.no_grad():  # 在评估模式下不需要计算梯度\n",
    "    logits = model(data.x, data.edge_index)\n",
    "    probabilities = torch.sigmoid(logits)\n",
    "    predicted_labels = (probabilities >= 0.5).int()\n",
    "\n",
    "predicted_labels_1d = torch.argmax(predicted_labels, dim=1)\n",
    "\n",
    "# 打印结果\n",
    "print(predicted_labels_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "121dd56f-8c8a-4c79-9220-f7ad28505387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.5382396e-01, 6.5293795e-01, 1.0555269e-04, ..., 3.9361306e-03,\n",
       "       3.0122927e-01, 5.2143085e-09], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities = F.softmax(logits, dim=1)[:, 1].numpy()\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8c5b2dc0-085e-4b1b-9b52-82be9955c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'protein': combined_features['protein'],\n",
    "    'predicted_label': probabilities\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "41143da6-a1f2-4abd-8462-a7651932e195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FES</td>\n",
       "      <td>8.538240e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HADHA</td>\n",
       "      <td>6.529379e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLC7A7</td>\n",
       "      <td>1.055527e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LCK</td>\n",
       "      <td>7.084623e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HSPA2</td>\n",
       "      <td>9.923551e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62040</th>\n",
       "      <td>GO:2001313</td>\n",
       "      <td>2.497066e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62041</th>\n",
       "      <td>GO:2001314</td>\n",
       "      <td>2.186424e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62042</th>\n",
       "      <td>GO:2001315</td>\n",
       "      <td>3.936131e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62043</th>\n",
       "      <td>GO:2001316</td>\n",
       "      <td>3.012293e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62044</th>\n",
       "      <td>GO:2001317</td>\n",
       "      <td>5.214309e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62045 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          protein  predicted_label\n",
       "0             FES     8.538240e-01\n",
       "1          HADHA      6.529379e-01\n",
       "2          SLC7A7     1.055527e-04\n",
       "3            LCK      7.084623e-01\n",
       "4           HSPA2     9.923551e-01\n",
       "...           ...              ...\n",
       "62040  GO:2001313     2.497066e-04\n",
       "62041  GO:2001314     2.186424e-02\n",
       "62042  GO:2001315     3.936131e-03\n",
       "62043  GO:2001316     3.012293e-01\n",
       "62044  GO:2001317     5.214309e-09\n",
       "\n",
       "[62045 rows x 2 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "03fc2281-ec20-44fc-b087-030c1290ed7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ERAP2</td>\n",
       "      <td>0.381655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADAMTSL5</td>\n",
       "      <td>0.996719</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TBC1D30</td>\n",
       "      <td>0.488628</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KCNK18</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NDNF</td>\n",
       "      <td>0.999265</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>TRABD2B</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>RPS9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>SLC22A16</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>FBN3</td>\n",
       "      <td>0.965023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378</th>\n",
       "      <td>BDH2</td>\n",
       "      <td>0.936908</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1379 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       protein  predicted_label  Label\n",
       "0        ERAP2         0.381655      0\n",
       "1     ADAMTSL5         0.996719      1\n",
       "2      TBC1D30         0.488628      0\n",
       "3       KCNK18         0.000004      0\n",
       "4         NDNF         0.999265      1\n",
       "...        ...              ...    ...\n",
       "1374   TRABD2B         0.000329      0\n",
       "1375      RPS9         1.000000      1\n",
       "1376  SLC22A16         0.000376      0\n",
       "1377      FBN3         0.965023      1\n",
       "1378      BDH2         0.936908      1\n",
       "\n",
       "[1379 rows x 3 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df.rename(columns={'name':'protein'}, inplace=True)\n",
    "label_df = labels_df[['protein', 'Label']]\n",
    "merged_df = pd.merge(results_df, label_df, on='protein', how='inner')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bd6e98d5-8a06-4b16-8dd0-432bb859cff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9969159842536262\n"
     ]
    }
   ],
   "source": [
    "auc = roc_auc_score(merged_df['Label'], merged_df['predicted_label'])\n",
    "print(f'AUC: {auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82566bfe-deb4-4879-b218-01770874659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1ca7baa-fba7-4a61-bcf3-8751b530b56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5183,\n",
       " 11072,\n",
       " 6534,\n",
       " 13349,\n",
       " 7020,\n",
       " 12799,\n",
       " 9405,\n",
       " 8993,\n",
       " 13287,\n",
       " 5417,\n",
       " 5828,\n",
       " 5797,\n",
       " 9036,\n",
       " 7974,\n",
       " 3909,\n",
       " 10223,\n",
       " 8391,\n",
       " 3933,\n",
       " 6545,\n",
       " 3523,\n",
       " 13643,\n",
       " 2750,\n",
       " 6434,\n",
       " 4091,\n",
       " 10829,\n",
       " 11222,\n",
       " 10266,\n",
       " 1541,\n",
       " 10879,\n",
       " 2481,\n",
       " 1905,\n",
       " 1350,\n",
       " 587,\n",
       " 4382,\n",
       " 4903,\n",
       " 10782,\n",
       " 4712,\n",
       " 13719,\n",
       " 13346,\n",
       " 9138,\n",
       " 7296,\n",
       " 10003,\n",
       " 1810,\n",
       " 6113,\n",
       " 13151,\n",
       " 11853,\n",
       " 2241,\n",
       " 4211,\n",
       " 5149,\n",
       " 11081,\n",
       " 6877,\n",
       " 8719,\n",
       " 762,\n",
       " 4540,\n",
       " 6653,\n",
       " 6460,\n",
       " 539,\n",
       " 2325,\n",
       " 5330,\n",
       " 13878,\n",
       " 10310,\n",
       " 9151,\n",
       " 10636,\n",
       " 11328,\n",
       " 3161,\n",
       " 8871,\n",
       " 2732,\n",
       " 8627,\n",
       " 293,\n",
       " 3920,\n",
       " 11402,\n",
       " 10303,\n",
       " 6672,\n",
       " 10505,\n",
       " 4045,\n",
       " 12097,\n",
       " 13713,\n",
       " 3335,\n",
       " 12493,\n",
       " 8889,\n",
       " 5750,\n",
       " 13801,\n",
       " 6781,\n",
       " 2205,\n",
       " 6001,\n",
       " 2852,\n",
       " 10486,\n",
       " 9475,\n",
       " 13067,\n",
       " 5465,\n",
       " 8614,\n",
       " 8687,\n",
       " 646,\n",
       " 11899,\n",
       " 5076,\n",
       " 8824,\n",
       " 2039,\n",
       " 2762,\n",
       " 10193,\n",
       " 5098,\n",
       " 3945,\n",
       " 2538,\n",
       " 5811,\n",
       " 1589,\n",
       " 14413,\n",
       " 4259,\n",
       " 11161,\n",
       " 10287,\n",
       " 6446,\n",
       " 10302,\n",
       " 9490,\n",
       " 7459,\n",
       " 5271,\n",
       " 4007,\n",
       " 6751,\n",
       " 3930,\n",
       " 8429,\n",
       " 9443,\n",
       " 9194,\n",
       " 4056,\n",
       " 10107,\n",
       " 12559,\n",
       " 941,\n",
       " 14242,\n",
       " 5119,\n",
       " 5983,\n",
       " 2867,\n",
       " 2894,\n",
       " 2204,\n",
       " 13859,\n",
       " 9789,\n",
       " 13012,\n",
       " 5214,\n",
       " 12985,\n",
       " 1856,\n",
       " 13559,\n",
       " 12210,\n",
       " 11700,\n",
       " 12467,\n",
       " 8294,\n",
       " 287,\n",
       " 9178,\n",
       " 12332,\n",
       " 8592,\n",
       " 7717,\n",
       " 11115,\n",
       " 2737,\n",
       " 9197,\n",
       " 704,\n",
       " 7054,\n",
       " 11497,\n",
       " 6244,\n",
       " 4267,\n",
       " 2588,\n",
       " 5745,\n",
       " 380,\n",
       " 5710,\n",
       " 3587,\n",
       " 2996,\n",
       " 4519,\n",
       " 10141,\n",
       " 3423,\n",
       " 12897,\n",
       " 1492,\n",
       " 11910,\n",
       " 13646,\n",
       " 2595,\n",
       " 5323,\n",
       " 3618,\n",
       " 2373,\n",
       " 2379,\n",
       " 11587,\n",
       " 6937,\n",
       " 9386,\n",
       " 10656,\n",
       " 13420,\n",
       " 10301,\n",
       " 3708,\n",
       " 10384,\n",
       " 829,\n",
       " 13365,\n",
       " 4831,\n",
       " 1972,\n",
       " 13360,\n",
       " 11191,\n",
       " 3603,\n",
       " 1506,\n",
       " 10795,\n",
       " 6928,\n",
       " 6115,\n",
       " 7351,\n",
       " 13229,\n",
       " 11016,\n",
       " 13418,\n",
       " 10083,\n",
       " 7056,\n",
       " 1446,\n",
       " 11855,\n",
       " 13632,\n",
       " 3895,\n",
       " 11618,\n",
       " 225,\n",
       " 8029,\n",
       " 6685,\n",
       " 10810,\n",
       " 963,\n",
       " 7772,\n",
       " 8031,\n",
       " 6814,\n",
       " 10684,\n",
       " 4713,\n",
       " 5955,\n",
       " 4284,\n",
       " 10181,\n",
       " 11906,\n",
       " 1555,\n",
       " 3978,\n",
       " 8184,\n",
       " 14388,\n",
       " 6100,\n",
       " 8317,\n",
       " 9608,\n",
       " 9503,\n",
       " 6028,\n",
       " 5073,\n",
       " 3666,\n",
       " 5241,\n",
       " 6639,\n",
       " 8228,\n",
       " 9091,\n",
       " 7270,\n",
       " 2061,\n",
       " 4251,\n",
       " 1599,\n",
       " 11126,\n",
       " 5809,\n",
       " 6772,\n",
       " 5037,\n",
       " 3778,\n",
       " 4852,\n",
       " 4054,\n",
       " 2813,\n",
       " 1134,\n",
       " 6313,\n",
       " 1697,\n",
       " 11173,\n",
       " 3880,\n",
       " 7690,\n",
       " 14432,\n",
       " 10491,\n",
       " 9865,\n",
       " 5320,\n",
       " 9784,\n",
       " 2533,\n",
       " 12447,\n",
       " 12233,\n",
       " 2267,\n",
       " 6017,\n",
       " 14293,\n",
       " 2213,\n",
       " 9049,\n",
       " 1118,\n",
       " 9980,\n",
       " 1563,\n",
       " 2364,\n",
       " 3384,\n",
       " 4926,\n",
       " 10646,\n",
       " 3300,\n",
       " 7272,\n",
       " 1616,\n",
       " 11100,\n",
       " 10523,\n",
       " 726,\n",
       " 3608,\n",
       " 11641,\n",
       " 4881,\n",
       " 7916,\n",
       " 8112,\n",
       " 4167,\n",
       " 3860,\n",
       " 12631,\n",
       " 4222,\n",
       " 12694,\n",
       " 3233,\n",
       " 6431,\n",
       " 2530,\n",
       " 1351,\n",
       " 2181,\n",
       " 10465,\n",
       " 7502,\n",
       " 12487,\n",
       " 5723,\n",
       " 5891,\n",
       " 2673,\n",
       " 839,\n",
       " 7413,\n",
       " 6143,\n",
       " 2002,\n",
       " 9758,\n",
       " 10102,\n",
       " 7239,\n",
       " 10440,\n",
       " 13307,\n",
       " 7481,\n",
       " 12120,\n",
       " 3525,\n",
       " 809,\n",
       " 9923,\n",
       " 5932,\n",
       " 11457,\n",
       " 347,\n",
       " 5858,\n",
       " 12766,\n",
       " 6055,\n",
       " 14063,\n",
       " 3479,\n",
       " 3648,\n",
       " 10746,\n",
       " 6873,\n",
       " 6206,\n",
       " 970,\n",
       " 5263,\n",
       " 4177,\n",
       " 10115,\n",
       " 7233,\n",
       " 7890,\n",
       " 11304,\n",
       " 1704,\n",
       " 204,\n",
       " 781,\n",
       " 926,\n",
       " 249,\n",
       " 14077,\n",
       " 8474,\n",
       " 3169,\n",
       " 1618,\n",
       " 10123,\n",
       " 7872,\n",
       " 2598,\n",
       " 7514,\n",
       " 11248,\n",
       " 4330,\n",
       " 5322,\n",
       " 5490,\n",
       " 1873,\n",
       " 4511,\n",
       " 8344,\n",
       " 14281,\n",
       " 13128,\n",
       " 14269,\n",
       " 1598,\n",
       " 4492,\n",
       " 6715,\n",
       " 12067,\n",
       " 3514,\n",
       " 7903,\n",
       " 3802,\n",
       " 4974,\n",
       " 8435,\n",
       " 4159,\n",
       " 4232,\n",
       " 12627,\n",
       " 8428,\n",
       " 11417,\n",
       " 5806,\n",
       " 1513,\n",
       " 3902,\n",
       " 2457,\n",
       " 11595,\n",
       " 2597,\n",
       " 3305,\n",
       " 11380,\n",
       " 3110,\n",
       " 4605,\n",
       " 8774,\n",
       " 5228,\n",
       " 12021,\n",
       " 625,\n",
       " 3794,\n",
       " 5125,\n",
       " 1296,\n",
       " 8865,\n",
       " 13214,\n",
       " 271,\n",
       " 12883,\n",
       " 8692,\n",
       " 8694,\n",
       " 9553,\n",
       " 6890,\n",
       " 5246,\n",
       " 2237,\n",
       " 6791,\n",
       " 800,\n",
       " 412,\n",
       " 464,\n",
       " 4913,\n",
       " 10711,\n",
       " 13567,\n",
       " 11893,\n",
       " 12600,\n",
       " 10641,\n",
       " 8120,\n",
       " 6414,\n",
       " 5145,\n",
       " 10571,\n",
       " 6108,\n",
       " 14201,\n",
       " 12743,\n",
       " 5947,\n",
       " 965,\n",
       " 11546,\n",
       " 6675,\n",
       " 2290,\n",
       " 4917,\n",
       " 318,\n",
       " 7405,\n",
       " 7940,\n",
       " 6891,\n",
       " 9113,\n",
       " 685,\n",
       " 1996,\n",
       " 4895,\n",
       " 7734,\n",
       " 4343,\n",
       " 2095,\n",
       " 3938,\n",
       " 11017,\n",
       " 7127,\n",
       " 7458,\n",
       " 9277,\n",
       " 9979,\n",
       " 1913,\n",
       " 4164,\n",
       " 1725,\n",
       " 1491,\n",
       " 6880,\n",
       " 14177,\n",
       " 10652,\n",
       " 13530,\n",
       " 11179,\n",
       " 7679,\n",
       " 3771,\n",
       " 12519,\n",
       " 13708,\n",
       " 1341,\n",
       " 9196,\n",
       " 11651,\n",
       " 7452,\n",
       " 8733,\n",
       " 12833,\n",
       " 10560,\n",
       " 6283,\n",
       " 11398,\n",
       " 7868,\n",
       " 2433,\n",
       " 2104,\n",
       " 11382,\n",
       " 4628,\n",
       " 4271,\n",
       " 2663,\n",
       " 7530,\n",
       " 9126,\n",
       " 9819,\n",
       " 6456,\n",
       " 3625,\n",
       " 6973,\n",
       " 3847,\n",
       " 4837,\n",
       " 1702,\n",
       " 7346,\n",
       " 2276,\n",
       " 1674,\n",
       " 10735,\n",
       " 1054,\n",
       " 8115,\n",
       " 2057,\n",
       " 12959,\n",
       " 3858,\n",
       " 2877,\n",
       " 8390,\n",
       " 14128,\n",
       " 5853,\n",
       " 11668,\n",
       " 11992,\n",
       " 10094,\n",
       " 5715,\n",
       " 3199,\n",
       " 4419,\n",
       " 12565,\n",
       " 11856,\n",
       " 4664,\n",
       " 10899,\n",
       " 8814,\n",
       " 355,\n",
       " 97,\n",
       " 2035,\n",
       " 9230,\n",
       " 588,\n",
       " 9790,\n",
       " 9292,\n",
       " 7898,\n",
       " 12791,\n",
       " 2769,\n",
       " 7059,\n",
       " 14250,\n",
       " 1504,\n",
       " 2332,\n",
       " 10194,\n",
       " 8424,\n",
       " 6228,\n",
       " 6024,\n",
       " 9244,\n",
       " 6694,\n",
       " 5041,\n",
       " 1975,\n",
       " 1200,\n",
       " 10756,\n",
       " 12413,\n",
       " 11708,\n",
       " 10953,\n",
       " 9323,\n",
       " 6437,\n",
       " 6330,\n",
       " 13918,\n",
       " 14084,\n",
       " 2757,\n",
       " 6278,\n",
       " 13434,\n",
       " 9992,\n",
       " 2139,\n",
       " 3150,\n",
       " 7271,\n",
       " 8477,\n",
       " 7551,\n",
       " 1910,\n",
       " 8203,\n",
       " 13880,\n",
       " 11691,\n",
       " 7410,\n",
       " 11141,\n",
       " 10466,\n",
       " 4019,\n",
       " 3976,\n",
       " 10283,\n",
       " 6565,\n",
       " 7556,\n",
       " 6828,\n",
       " 948,\n",
       " 7369,\n",
       " 1792,\n",
       " 10317,\n",
       " 9369,\n",
       " 9045,\n",
       " 4779,\n",
       " 6484,\n",
       " 1330,\n",
       " 5890,\n",
       " 8016,\n",
       " 7360,\n",
       " 13111,\n",
       " 2924,\n",
       " 4661,\n",
       " 5756,\n",
       " 10649,\n",
       " 14319,\n",
       " 117,\n",
       " 8509,\n",
       " 7733,\n",
       " 4048,\n",
       " 10415,\n",
       " 14131,\n",
       " 3257,\n",
       " 693,\n",
       " 12316,\n",
       " 12917,\n",
       " 1274,\n",
       " 3111,\n",
       " 6302,\n",
       " 10781,\n",
       " 2005,\n",
       " 1260,\n",
       " 1493,\n",
       " 3931,\n",
       " 14143,\n",
       " 4897,\n",
       " 13366,\n",
       " 9888,\n",
       " 1282,\n",
       " 11644,\n",
       " 11655,\n",
       " 3351,\n",
       " 10377,\n",
       " 3207,\n",
       " 14306,\n",
       " 8491,\n",
       " 5448,\n",
       " 4279,\n",
       " 6802,\n",
       " 1801,\n",
       " 3072,\n",
       " 2652,\n",
       " 8260,\n",
       " 10458,\n",
       " 4102,\n",
       " 3855,\n",
       " 5866,\n",
       " 4432,\n",
       " 10896,\n",
       " 7506,\n",
       " 11431,\n",
       " 4832,\n",
       " 3419,\n",
       " 11299,\n",
       " 2159,\n",
       " 10761,\n",
       " 6994,\n",
       " 10574,\n",
       " 12424,\n",
       " 1919,\n",
       " 12784,\n",
       " 10511,\n",
       " 2904,\n",
       " 8002,\n",
       " 10216,\n",
       " 10239,\n",
       " 441,\n",
       " 7555,\n",
       " 5418,\n",
       " 4921,\n",
       " 626,\n",
       " 2715,\n",
       " 5451,\n",
       " 3265,\n",
       " 14442,\n",
       " 7259,\n",
       " 2300,\n",
       " 12927,\n",
       " 4388,\n",
       " 334,\n",
       " 1613,\n",
       " 6651,\n",
       " 7003,\n",
       " 13870,\n",
       " 11886,\n",
       " 3432,\n",
       " 13281,\n",
       " 240,\n",
       " 14409,\n",
       " 6718,\n",
       " 8287,\n",
       " 5104,\n",
       " 1022,\n",
       " 8004,\n",
       " 10142,\n",
       " 2439,\n",
       " 13390,\n",
       " 9160,\n",
       " 7139,\n",
       " 7191,\n",
       " 11838,\n",
       " 1166,\n",
       " 13158,\n",
       " 1554,\n",
       " 8412,\n",
       " 4823,\n",
       " 8363,\n",
       " 13050,\n",
       " 13683,\n",
       " 2891,\n",
       " 1893,\n",
       " 7100,\n",
       " 10569,\n",
       " 9739,\n",
       " 8190,\n",
       " 11391,\n",
       " 7808,\n",
       " 14309,\n",
       " 6189,\n",
       " 10253,\n",
       " 3457,\n",
       " 2262,\n",
       " 1520,\n",
       " 7815,\n",
       " 6633,\n",
       " 6022,\n",
       " 11519,\n",
       " 10055,\n",
       " 14151,\n",
       " 6774,\n",
       " 13848,\n",
       " 1096,\n",
       " 3917,\n",
       " 10529,\n",
       " 3340,\n",
       " 5782,\n",
       " 6237,\n",
       " 7534,\n",
       " 10940,\n",
       " 6750,\n",
       " 4883,\n",
       " 10338,\n",
       " 4647,\n",
       " 4721,\n",
       " 940,\n",
       " 10771,\n",
       " 9356,\n",
       " 8037,\n",
       " 1333,\n",
       " 1194,\n",
       " 3896,\n",
       " 630,\n",
       " 5846,\n",
       " 8904,\n",
       " 4472,\n",
       " 13798,\n",
       " 5948,\n",
       " 61,\n",
       " 10664,\n",
       " 8441,\n",
       " 6615,\n",
       " 5963,\n",
       " 10639,\n",
       " 8173,\n",
       " 613,\n",
       " 2740,\n",
       " 5398,\n",
       " 10678,\n",
       " 11336,\n",
       " 2775,\n",
       " 4373,\n",
       " 2561,\n",
       " 80,\n",
       " 1441,\n",
       " 5043,\n",
       " 6684,\n",
       " 11235,\n",
       " 278,\n",
       " 9006,\n",
       " 4119,\n",
       " 5977,\n",
       " 14096,\n",
       " 1679,\n",
       " 5262,\n",
       " 12892,\n",
       " 10306,\n",
       " 10219,\n",
       " 11271,\n",
       " 10572,\n",
       " 2546,\n",
       " 7650,\n",
       " 13234,\n",
       " 7504,\n",
       " 3630,\n",
       " 3412,\n",
       " 11684,\n",
       " 7115,\n",
       " 8322,\n",
       " 5233,\n",
       " 698,\n",
       " 7320,\n",
       " 12422,\n",
       " 9042,\n",
       " 5293,\n",
       " 6603,\n",
       " 5816,\n",
       " 4332,\n",
       " 13649,\n",
       " 9828,\n",
       " 5989,\n",
       " 6279,\n",
       " 10513,\n",
       " 14278,\n",
       " 3651,\n",
       " 3269,\n",
       " 2559,\n",
       " 3210,\n",
       " 11366,\n",
       " 7471,\n",
       " 12745,\n",
       " 419,\n",
       " 8312,\n",
       " 5203,\n",
       " 2392,\n",
       " 1070,\n",
       " 12042,\n",
       " 11716,\n",
       " 2470,\n",
       " 12569,\n",
       " 8065,\n",
       " 8283,\n",
       " 694,\n",
       " 13146,\n",
       " 8970,\n",
       " 11150,\n",
       " 5411,\n",
       " 14387,\n",
       " 7582,\n",
       " 7870,\n",
       " 13572,\n",
       " 4562,\n",
       " 2708,\n",
       " 9412,\n",
       " 3224,\n",
       " 8404,\n",
       " 5743,\n",
       " 2412,\n",
       " 7189,\n",
       " 7278,\n",
       " 2728,\n",
       " 7825,\n",
       " 5852,\n",
       " 5360,\n",
       " 1990,\n",
       " 4213,\n",
       " 14024,\n",
       " 8565,\n",
       " 4956,\n",
       " 14145,\n",
       " 5757,\n",
       " 285,\n",
       " 2711,\n",
       " 5427,\n",
       " 9663,\n",
       " 6031,\n",
       " 13725,\n",
       " 2920,\n",
       " 12060,\n",
       " 5902,\n",
       " 10743,\n",
       " 2735,\n",
       " 7359,\n",
       " 1536,\n",
       " 3370,\n",
       " 13742,\n",
       " 7285,\n",
       " 9988,\n",
       " 2253,\n",
       " 11253,\n",
       " 8572,\n",
       " 4061,\n",
       " 13154,\n",
       " 3443,\n",
       " 8522,\n",
       " 6056,\n",
       " 5039,\n",
       " 4266,\n",
       " 4057,\n",
       " 7111,\n",
       " 1883,\n",
       " 1548,\n",
       " 669,\n",
       " 4674,\n",
       " 6341,\n",
       " 2042,\n",
       " 3784,\n",
       " 8251,\n",
       " 8573,\n",
       " 12988,\n",
       " 13291,\n",
       " 955,\n",
       " 684,\n",
       " 996,\n",
       " 3826,\n",
       " 5047,\n",
       " 5445,\n",
       " 13915,\n",
       " 612,\n",
       " 5128,\n",
       " 7732,\n",
       " 9498,\n",
       " 11427,\n",
       " 2383,\n",
       " 10686,\n",
       " 4429,\n",
       " 7721,\n",
       " 14272,\n",
       " 2254,\n",
       " 9728,\n",
       " 8137,\n",
       " 7609,\n",
       " 12677,\n",
       " 821,\n",
       " 6082,\n",
       " 8133,\n",
       " 7683,\n",
       " 9050,\n",
       " 10731,\n",
       " 7578,\n",
       " 13045,\n",
       " 13914,\n",
       " 6165,\n",
       " 1562,\n",
       " 6167,\n",
       " 1744,\n",
       " 11588,\n",
       " 13167,\n",
       " 13874,\n",
       " 14205,\n",
       " 8741,\n",
       " 12964,\n",
       " 13776,\n",
       " 13143,\n",
       " 3925,\n",
       " 3566,\n",
       " 8593,\n",
       " 10784,\n",
       " 751,\n",
       " 4451,\n",
       " 1304,\n",
       " 5855,\n",
       " 12528,\n",
       " 7267,\n",
       " 3539,\n",
       " 10014,\n",
       " 470,\n",
       " 12727,\n",
       " 1478,\n",
       " 7512,\n",
       " 8991,\n",
       " 11522,\n",
       " 2313,\n",
       " 10597,\n",
       " 11964,\n",
       " 10544,\n",
       " 9982,\n",
       " 1110,\n",
       " 9518,\n",
       " 7735,\n",
       " 4584,\n",
       " 2161,\n",
       " 4276,\n",
       " 13888,\n",
       " 7520,\n",
       " 8109,\n",
       " 9047,\n",
       " 13614,\n",
       " 7025,\n",
       " 2520,\n",
       " 2049,\n",
       " 13759,\n",
       " 8064,\n",
       " 11370,\n",
       " 4841,\n",
       " 7751,\n",
       " 5143,\n",
       " 12008,\n",
       " 1093,\n",
       " 10147,\n",
       " 14359,\n",
       " 3890,\n",
       " 12957,\n",
       " 12728,\n",
       " 3900,\n",
       " 6547,\n",
       " 3505,\n",
       " 2516,\n",
       " 10066,\n",
       " 12374,\n",
       " 2428,\n",
       " 9315,\n",
       " 6221,\n",
       " 8697,\n",
       " 13383,\n",
       " 13263,\n",
       " 10246,\n",
       " 2833,\n",
       " 7764,\n",
       " 4318,\n",
       " 6091,\n",
       " 11500,\n",
       " 5469,\n",
       " 11076,\n",
       " 5781,\n",
       " 2102,\n",
       " 692,\n",
       " 13605,\n",
       " 4966,\n",
       " 999,\n",
       " 14265,\n",
       " 11847,\n",
       " 12256,\n",
       " 6735,\n",
       " 2948,\n",
       " 2576,\n",
       " 4024,\n",
       " 8182,\n",
       " 13701,\n",
       " 8723,\n",
       " 10339,\n",
       " 11285,\n",
       " 6157,\n",
       " 10704,\n",
       " 11215,\n",
       " 3175,\n",
       " 11260,\n",
       " 8452,\n",
       " 9368,\n",
       " 4560,\n",
       " 13296,\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_indices = label_indices\n",
    "labeled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7989b928-1cf0-47f3-9995-c6b7d1763bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5183, 11072, 6534, 13349, 7020, 12799, 9405, 8993, 13287, 5417, 5828, 5797, 9036, 7974, 3909, 10223, 8391, 3933, 6545, 3523, 13643, 2750, 6434, 4091, 10829, 11222, 10266, 1541, 10879, 2481, 1905, 1350, 587, 4382, 4903, 10782, 4712, 13719, 13346, 9138, 7296, 10003, 1810, 6113, 13151, 11853, 2241, 4211, 5149, 11081, 6877, 8719, 762, 4540, 6653, 6460, 539, 2325, 5330, 13878, 10310, 9151, 10636, 11328, 3161, 8871, 2732, 8627, 293, 3920, 11402, 10303, 6672, 10505, 4045, 12097, 13713, 3335, 12493, 8889, 5750, 13801, 6781, 2205, 6001, 2852, 10486, 9475, 13067, 5465, 8614, 8687, 646, 11899, 5076, 8824, 2039, 2762, 10193, 5098, 3945, 2538, 5811, 1589, 14413, 4259, 11161, 10287, 6446, 10302, 9490, 7459, 5271, 4007, 6751, 3930, 8429, 9443, 9194, 4056, 10107, 12559, 941, 14242, 5119, 5983, 2867, 2894, 2204, 13859, 9789, 13012, 5214, 12985, 1856, 13559, 12210, 11700, 12467, 8294, 287, 9178, 12332, 8592, 7717, 11115, 2737, 9197, 704, 7054, 11497, 6244, 4267, 2588, 5745, 380, 5710, 3587, 2996, 4519, 10141, 3423, 12897, 1492, 11910, 13646, 2595, 5323, 3618, 2373, 2379, 11587, 6937, 9386, 10656, 13420, 10301, 3708, 10384, 829, 13365, 4831, 1972, 13360, 11191, 3603, 1506, 10795, 6928, 6115, 7351, 13229, 11016, 13418, 10083, 7056, 1446, 11855, 13632, 3895, 11618, 225, 8029, 6685, 10810, 963, 7772, 8031, 6814, 10684, 4713, 5955, 4284, 10181, 11906, 1555, 3978, 8184, 14388, 6100, 8317, 9608, 9503, 6028, 5073, 3666, 5241, 6639, 8228, 9091, 7270, 2061, 4251, 1599, 11126, 5809, 6772, 5037, 3778, 4852, 4054, 2813, 1134, 6313, 1697, 11173, 3880, 7690, 14432, 10491, 9865, 5320, 9784, 2533, 12447, 12233, 2267, 6017, 14293, 2213, 9049, 1118, 9980, 1563, 2364, 3384, 4926, 10646, 3300, 7272, 1616, 11100, 10523, 726, 3608, 11641, 4881, 7916, 8112, 4167, 3860, 12631, 4222, 12694, 3233, 6431, 2530, 1351, 2181, 10465, 7502, 12487, 5723, 5891, 2673, 839, 7413, 6143, 2002, 9758, 10102, 7239, 10440, 13307, 7481, 12120, 3525, 809, 9923, 5932, 11457, 347, 5858, 12766, 6055, 14063, 3479, 3648, 10746, 6873, 6206, 970, 5263, 4177, 10115, 7233, 7890, 11304, 1704, 204, 781, 926, 249, 14077, 8474, 3169, 1618, 10123, 7872, 2598, 7514, 11248, 4330, 5322, 5490, 1873, 4511, 8344, 14281, 13128, 14269, 1598, 4492, 6715, 12067, 3514, 7903, 3802, 4974, 8435, 4159, 4232, 12627, 8428, 11417, 5806, 1513, 3902, 2457, 11595, 2597, 3305, 11380, 3110, 4605, 8774, 5228, 12021, 625, 3794, 5125, 1296, 8865, 13214, 271, 12883, 8692, 8694, 9553, 6890, 5246, 2237, 6791, 800, 412, 464, 4913, 10711, 13567, 11893, 12600, 10641, 8120, 6414, 5145, 10571, 6108, 14201, 12743, 5947, 965, 11546, 6675, 2290, 4917, 318, 7405, 7940, 6891, 9113, 685, 1996, 4895, 7734, 4343, 2095, 3938, 11017, 7127, 7458, 9277, 9979, 1913, 4164, 1725, 1491, 6880, 14177, 10652, 13530, 11179, 7679, 3771, 12519, 13708, 1341, 9196, 11651, 7452, 8733, 12833, 10560, 6283, 11398, 7868, 2433, 2104, 11382, 4628, 4271, 2663, 7530, 9126, 9819, 6456, 3625, 6973, 3847, 4837, 1702, 7346, 2276, 1674, 10735, 1054, 8115, 2057, 12959, 3858, 2877, 8390, 14128, 5853, 11668, 11992, 10094, 5715, 3199, 4419, 12565, 11856, 4664, 10899, 8814, 355, 97, 2035, 9230, 588, 9790, 9292, 7898, 12791, 2769, 7059, 14250, 1504, 2332, 10194, 8424, 6228, 6024, 9244, 6694, 5041, 1975, 1200, 10756, 12413, 11708, 10953, 9323, 6437, 6330, 13918, 14084, 2757, 6278, 13434, 9992, 2139, 3150, 7271, 8477, 7551, 1910, 8203, 13880, 11691, 7410, 11141, 10466, 4019, 3976, 10283, 6565, 7556, 6828, 948, 7369, 1792, 10317, 9369, 9045, 4779, 6484, 1330, 5890, 8016, 7360, 13111, 2924, 4661, 5756, 10649, 14319, 117, 8509, 7733, 4048, 10415, 14131, 3257, 693, 12316, 12917, 1274, 3111, 6302, 10781, 2005, 1260, 1493, 3931, 14143, 4897, 13366, 9888, 1282, 11644, 11655, 3351, 10377, 3207, 14306, 8491, 5448, 4279, 6802, 1801, 3072, 2652, 8260, 10458, 4102, 3855, 5866, 4432, 10896, 7506, 11431, 4832, 3419, 11299, 2159, 10761, 6994, 10574, 12424, 1919, 12784, 10511, 2904, 8002, 10216, 10239, 441, 7555, 5418, 4921, 626, 2715, 5451, 3265, 14442, 7259, 2300, 12927, 4388, 334, 1613, 6651, 7003, 13870, 11886, 3432, 13281, 240, 14409, 6718, 8287, 5104, 1022, 8004, 10142, 2439, 13390, 9160, 7139, 7191, 11838, 1166, 13158, 1554, 8412, 4823, 8363, 13050, 13683, 2891, 1893, 7100, 10569, 9739, 8190, 11391, 7808, 14309, 6189, 10253, 3457, 2262, 1520, 7815, 6633, 6022, 11519, 10055, 14151, 6774, 13848, 1096, 3917, 10529, 3340, 5782, 6237, 7534, 10940, 6750, 4883, 10338, 4647, 4721, 940, 10771, 9356, 8037, 1333, 1194, 3896, 630, 5846, 8904, 4472, 13798, 5948, 61, 10664, 8441, 6615, 5963, 10639, 8173, 613, 2740, 5398, 10678, 11336, 2775, 4373, 2561, 80, 1441, 5043, 6684, 11235, 278, 9006, 4119, 5977, 14096, 1679, 5262, 12892, 10306, 10219, 11271, 10572, 2546, 7650, 13234, 7504, 3630, 3412, 11684, 7115, 8322, 5233, 698, 7320, 12422, 9042, 5293, 6603, 5816, 4332, 13649, 9828, 5989, 6279, 10513, 14278, 3651, 3269, 2559, 3210, 11366, 7471, 12745, 419, 8312, 5203, 2392, 1070, 12042, 11716, 2470, 12569, 8065, 8283, 694, 13146, 8970, 11150, 5411, 14387, 7582, 7870, 13572, 4562, 2708, 9412, 3224, 8404, 5743, 2412, 7189, 7278, 2728, 7825, 5852, 5360, 1990, 4213, 14024, 8565, 4956, 14145, 5757, 285, 2711, 5427, 9663, 6031, 13725, 2920, 12060, 5902, 10743, 2735, 7359, 1536, 3370, 13742, 7285, 9988, 2253, 11253, 8572, 4061, 13154, 3443, 8522, 6056, 5039, 4266, 4057, 7111, 1883, 1548, 669, 4674, 6341, 2042, 3784, 8251, 8573, 12988, 13291, 955, 684, 996, 3826, 5047, 5445, 13915, 612, 5128, 7732, 9498, 11427, 2383, 10686, 4429, 7721, 14272, 2254, 9728, 8137, 7609, 12677, 821, 6082, 8133, 7683, 9050, 10731, 7578, 13045, 13914, 6165, 1562, 6167, 1744, 11588, 13167, 13874, 14205, 8741, 12964, 13776, 13143, 3925, 3566, 8593, 10784, 751, 4451, 1304, 5855, 12528, 7267, 3539, 10014, 470, 12727, 1478, 7512, 8991, 11522, 2313, 10597, 11964, 10544, 9982, 1110, 9518, 7735, 4584, 2161, 4276, 13888, 7520, 8109, 9047, 13614, 7025, 2520, 2049, 13759, 8064, 11370, 4841, 7751, 5143, 12008, 1093, 10147, 14359, 3890, 12957, 12728, 3900, 6547, 3505, 2516, 10066, 12374, 2428, 9315, 6221, 8697, 13383, 13263, 10246, 2833, 7764, 4318, 6091, 11500, 5469, 11076, 5781, 2102, 692, 13605, 4966, 999, 14265, 11847, 12256, 6735, 2948, 2576, 4024, 8182, 13701, 8723, 10339, 11285, 6157, 10704, 11215, 3175, 11260, 8452, 9368, 4560, 13296, 557, 12758, 3780, 4810, 8966, 3358, 11000, 131, 2721, 3929, 10974, 10128, 10155, 1524, 7936, 10637, 3502, 2153, 3736, 4757, 1141, 3818, 4465, 4114, 13532, 12828, 5860, 3138, 13084, 5106, 1569, 1698, 11024, 6914, 11027, 3489, 8786, 6609, 2487, 2437, 5163, 3891, 2266, 1969, 2734, 4632, 9882, 5292, 9080, 6125, 2726, 13862, 12358, 3232, 5839, 14291, 12701, 5877, 4496, 11322, 4257, 13596, 3354, 6178, 12842, 8789, 8769, 3731, 2167, 1825, 2851, 10427, 8204, 13818, 3709, 3043, 3349, 7105, 6477, 4151, 9768, 8023, 1098, 4434, 5142, 13981, 752, 757, 6152, 13217, 7835, 13331, 11227, 9448, 6886, 11181, 5648, 7028, 11898, 5996, 13936, 1532, 1923, 9832, 11371, 2638, 12896, 222, 2322, 6200, 5712, 13553, 6601, 11755, 12420, 11, 4111, 13081, 4756, 2076, 6058, 9117, 10598, 9482, 5624, 8918, 1153, 895, 10861, 1774, 4460, 13804, 8996, 8536, 7439, 3863, 5467, 4008, 14295, 2554, 471, 14169, 4915, 6581, 2860, 10548, 9375, 7402, 2645, 13645, 13933, 2629, 5704, 68, 6852, 2349, 11317, 3688, 11502, 7800, 724, 13068, 9816, 9762, 537, 4309, 10196, 3922, 14147, 12903, 2452, 8039, 6660, 5785, 357, 9466, 10687, 12082, 8577, 6098, 836, 11468, 4469, 6839, 2288, 12230, 8931, 13842, 10499, 12813, 6364, 8809, 3619, 2279, 450, 7190, 13071, 112, 14342, 6419, 7419, 12267, 6416, 1064, 9249, 12234, 14176, 6590, 9425, 7624, 7937, 12315, 5925, 5224, 3165, 8560, 5071, 796, 2150, 374, 4440, 7212, 8713, 6586, 8500, 8127, 4565, 10621, 7293, 10760, 989, 6745, 5070, 6076, 3797, 10570, 10353, 5815, 10757, 6587, 12956, 2677, 11523, 9124, 11244, 13497, 4825, 10725, 9446, 7711, 9648, 677, 9825, 2047, 9243, 266, 12322, 6800, 1157, 9344, 6558, 5660, 7353, 2404, 12480, 11702, 10293, 11591, 12206, 12614, 5396, 261, 7158, 5799, 9680, 2444, 12653, 1426, 9664, 3686, 2839, 8618, 8121, 9115, 7697, 1632, 11346, 13973, 7685, 6721, 1667, 6693, 2696, 2507, 9760, 3029, 8100, 10001, 6988, 986, 2140, 13403, 4794, 11007, 8282, 12071, 3394, 1365, 8051, 7355, 7597, 3336, 195, 5818, 2082, 5759, 1777, 13885, 4274, 4121, 4933, 13053, 14270, 13199, 9575, 3212, 13510, 2346, 10935, 5118, 11282, 1502, 449, 2264, 13594, 12107, 2898, 4063, 13809, 6792, 7935, 199, 11405, 5980, 4631, 1561, 7913, 3742, 13396, 7503, 10386, 10035, 186, 2311, 10168, 6910, 4299, 1310, 6390, 1486, 201, 8113, 3204, 5541, 10026, 6191, 1991, 9683, 14104, 7651, 3098, 9423, 5845, 13453, 4970, 10343, 8578, 1459, 5552, 4319, 2930, 3496, 10382, 5762, 3692, 7373, 10577, 10783, 3640, 4062, 3361, 12611, 3718, 4958, 3989, 10877, 7340, 805, 10651, 10383, 3543, 2393, 5028, 2510, 8726, 11937, 8107, 5436, 5789, 11667, 10691, 1511, 8276, 9231, 11167, 8644, 11419, 5456, 7165, 4244, 9717, 8300, 6907, 8383, 1849, 3559, 13542, 7616, 4691, 10668, 8348, 7432, 8263, 9019, 10958, 11414, 7052, 2774, 1254, 11532, 11164, 9010, 6925, 11310, 2282, 4762, 2396, 7364, 6065, 12020, 3829, 4204, 2464, 187, 11761, 10416, 6527, 2028, 9222, 8758, 13288, 1899, 2195, 1245, 10730, 12939, 14296, 1395, 9399, 10770, 4927, 3456, 12270, 11495, 700, 7497, 11340, 5057, 4153, 1235, 6707, 1339, 14443, 11635, 10561, 3066, 11826, 936, 11409, 2668, 7475, 2743, 6858, 6171, 2269, 4333, 13527, 5986, 9273, 1233, 5256, 5798, 559, 9866, 3253, 208, 4320, 4301, 8777, 3259, 1299, 7629, 6551, 13817, 2286, 12184, 1601, 9751, 13917, 9118, 11613, 8662, 9431, 1306, 13734, 1727, 11177, 7237, 7429, 14115, 8830, 11994, 3064, 7770, 1077, 2085, 7348, 13783, 11116, 1321, 4130, 12397, 5616, 7599, 9400, 8699, 8685, 13173, 9905, 5537, 6855, 12554, 11050, 5158, 13685, 4773, 6002, 11186, 11203, 6929, 12846, 6998, 5049, 11515, 12258, 6376, 3129, 5080, 3716, 6807, 13845, 8195, 7619, 6276, 13138, 801, 12733, 3846, 10559, 2216, 9467, 2228, 11367, 6245, 11606, 12193, 8787, 8256, 8135, 7347, 5894, 2331, 7335, 10981, 12240, 8642, 11298, 7155, 13106, 480, 13767, 5113, 10277, 4196, 9382, 769, 5842, 4768, 18, 1408, 7379, 8976, 8145, 9899, 6048, 10374, 571, 4090, 9471, 66, 1214, 8038, 1826, 6312, 6207, 1246, 2272, 1908, 10058, 10439, 9171, 9458, 1075, 10480, 11332, 1251, 3045, 4014, 3118, 9486, 12266, 856, 3824, 8574, 5843, 11352, 10473, 4219, 5906, 6147, 7636, 2618, 1904, 8846, 14196, 11498, 12768, 6404, 10282, 13489, 2685, 5820, 11294, 3152, 327, 13261, 780, 1455, 13295, 3584, 13145, 8488, 2803, 1139, 12821, 2551, 8005, 6060, 8831, 8427, 5479, 3823, 11175, 7463, 7042, 7750, 12152, 5353, 10139, 6629, 3320, 5701, 3374, 7456, 3730, 750, 9342, 13352, 10971, 6829, 8303, 9114, 7153, 1761, 968, 8451, 9200, 1155, 8191, 959, 10933, 5938, 979, 2206, 10481, 8870, 10091, 3725, 484, 5123, 11845, 9636, 11435, 3747, 8541, 5666, 2651, 7276, 5096, 9573, 5936, 12325, 11830, 4536, 14379, 13011, 7749, 14048, 4659, 3531, 3280, 7926, 11123, 6195, 6497, 11006, 3657, 923, 14112, 4039, 5908, 13843, 9033, 7302, 12183, 10955, 5401, 3706, 14410, 3359, 5921, 623, 11411, 4113, 3433, 10533, 13194, 10948, 7339, 5166, 10073, 255, 10373, 8384, 8497, 6686, 8270, 1369, 5880, 11372, 3391, 8053, 2098, 1780, 11563, 2060, 3572, 4001, 9674, 2977, 12028, 876, 8073, 2474, 1603, 6116, 8949, 13057, 2718, 3126, 8140, 4735, 7053, 5331, 11087, 8964, 7, 11924, 6444, 274, 8246, 1377, 377, 4820, 1877, 3703, 2052, 389, 9497, 13581, 1738, 10640, 12030, 1102, 8304, 14326, 13566, 7647, 7700, 9007, 311, 13921, 6604, 7977, 12547, 2188, 6381, 3287, 5432, 1327, 3107, 3892, 3515, 2318, 6043, 6187, 3030, 11034, 1343, 5529, 14054, 1250, 13430, 3418, 11097, 10082, 11801, 6286, 6475, 9355, 2114, 11438, 12889, 11157, 13026, 5393, 3583, 4, 6004, 317, 12351, 11666, 3639, 7912, 1191, 11640, 5000, 8403, 7583, 12836, 2876, 11806, 14335, 6898, 2630, 7931, 9137, 8982, 2807, 4790, 9677, 8505, 9874, 7731, 7763, 247, 8339, 10954, 5523, 4443, 2746, 14217, 13995, 8703, 6199, 8030, 3410, 14094, 14002, 5962, 1610, 4176, 4499, 5608, 11077, 10248, 12616, 2907, 5904, 10387, 11829, 8658, 10724, 12935, 10524, 8116, 1881, 4210, 5231, 2361, 10258, 7264, 4417, 1124, 11706, 5357, 7050, 7290, 4403, 1779, 12461, 931, 7453, 6762, 9772, 13876, 4561, 12938, 1759, 11205, 7093, 4374, 4059, 8668, 1129, 14130, 192, 2068, 3067, 892, 3094, 2359, 8285, 10267, 2415, 991, 5054, 6281, 3225, 11250, 12638, 9252, 11474, 11490, 11135, 1461, 4285, 13120, 6383, 9014, 7511, 11766, 5358, 510, 1976, 3554, 2036, 3319, 7428, 14227, 3164, 12880, 8330, 4994, 4493, 9665, 6894, 11932, 10855, 3197, 1305, 2973, 11416, 3198, 9340, 7822, 5833, 3141, 3179, 10067, 3676, 6407, 11128, 5703, 9422, 10029, 2298, 1981, 10189, 12313, 7768, 2787, 11968, 9507, 12285, 13958, 3552, 6515, 13268, 13927, 806, 5201, 7622, 404, 4863, 4315, 691, 3550, 393, 6732, 13821, 3585, 13723, 7273, 181, 4596, 12604, 3213, 10733, 6860, 3184, 11921, 10084, 8984, 5857, 5636, 11694, 4727, 105, 5923, 8732, 8411, 4112, 7469, 9316, 5589, 5005, 855, 1915, 8983, 7349, 2358, 1784, 4141, 11712, 3366, 10653, 8647, 1770, 7014, 11680, 4909, 9156, 8519, 4798, 5901, 7807, 4630, 10268, 3323, 13920, 12471, 5997, 6812, 3528, 12562, 2170, 6490, 5965, 13322, 12800, 10316, 949, 10906, 1415, 2440, 5368, 11231, 11917, 7060, 14416, 6075, 13119, 12334, 5024, 884, 2100, 8021, 6449, 2384, 4612, 9254, 2113, 2214, 4032, 6748, 5487, 5514, 10114, 6640, 7689, 5517, 6440, 1292, 1499, 9854, 11442, 13832, 10580, 10864, 1003, 13750, 6267, 11950, 418, 8196, 5489, 4635, 3854, 10296, 573, 14038, 3003, 1922, 9645, 687, 10372, 3974, 2986, 13631, 179, 14437, 1547, 10822, 1058, 9303, 2985, 3756, 13867, 316, 1066, 8153, 4523, 8517, 1399, 7327, 3380, 294, 7116, 2556, 5259, 13838, 5751, 3402, 7694, 3049, 13751, 6142, 350, 4477, 10250, 7451, 5966, 2184, 3012, 3567, 6918, 5812, 7423, 5663, 9309, 11362, 7553, 9620, 11773, 5160, 13937, 9308, 11901, 12404, 2503, 9211, 11705, 6054, 1012, 971, 7477, 1111, 2203, 2418, 6032, 136, 9465, 10428, 4476, 5210, 4976, 11987, 8154, 251, 4759, 10850, 7385, 10347, 5414, 6901, 13834, 12103, 9741, 5334, 6174, 2378, 1082, 7378, 11963, 4828, 7343, 6777, 5892, 14229, 5755, 1894, 5760, 5803, 10696, 12908, 920, 8531, 14438, 10983, 5928, 3678, 7412, 13157, 6711, 6710, 2999, 9250, 3573, 8630, 4539, 10198, 5312, 9935, 11312, 2883, 11280, 51, 10005, 1530, 518, 2375, 1575, 3591, 5672, 7126, 2578, 7829, 8138, 6335, 2767, 4624, 4559, 14259, 82, 11733, 111, 8674, 1380, 8751, 1433, 4104, 7844, 6343, 14436, 2856, 5665, 5189, 6964, 1581, 7143, 4195, 4123, 11075, 9403, 8550, 8331, 10355, 12393, 8389, 14095, 11003, 1416, 6945, 8123, 6824, 8797, 2409, 6298, 3852, 13574, 9877, 7854, 4438, 9529, 10169, 2189, 3946, 2704, 3338, 10090, 5670, 3392, 12399, 4280, 3671, 11306, 7342, 6578, 6582, 3325, 6915, 7590, 11464, 1387, 2793, 2247, 3859, 2812, 4506, 12785, 127, 10475, 6326, 12498, 5173, 5566, 4098, 4914, 8535, 10629, 6211, 1439, 10976, 13639, 7179, 7652, 1837, 10713, 10676, 13181, 5586, 13509, 9241, 12130, 13377, 5689, 6482, 9588, 4683, 12365, 1494, 12087, 9378, 12656, 2954, 6145, 3885, 1596, 4722, 7630, 9021, 10182, 865, 12308, 13970, 7761, 2054, 12269, 1637, 3097, 6560, 6136, 9143, 260, 8745, 4718, 12971, 2021, 370, 1658, 11601, 1123, 7724, 6903, 11460, 11456, 139, 4684, 9280, 3215, 11679, 9024, 10433, 10927, 4625, 11390, 13355, 4512, 5313, 3452, 2804, 641, 9801, 5438, 12115, 7824, 10022, 12235, 11225, 4407, 682, 7554, 11172, 11653, 8442, 1412, 13256, 7377, 903, 5068, 4475, 8420, 9193, 107, 7786, 7785, 3624, 11681, 10609, 11193, 14417, 4576, 11858, 11725, 2136, 5485, 10887, 4508, 1464, 4505, 10654, 8169, 6155, 3807, 4771, 13439, 3669, 9278, 1661, 11237, 9155, 5695, 8759, 3485, 11508, 2478, 7964, 1957, 13746, 8635, 4972, 2817, 10522, 10909, 5048, 6176, 8773, 5897, 50, 4830, 2777, 13370, 14428, 11384, 2387, 11052, 5400, 6447, 3437, 7464, 11355, 6038, 11192, 6140, 7783, 6742, 13219, 3434, 7628, 10774, 11284, 8225, 7620, 9685, 13484, 1728, 8808, 12408, 9286, 3927, 5326, 3217, 12078, 10985, 3741, 7386, 8090, 5464, 13502, 8393, 6488, 13078, 9606, 12579, 4439, 4458, 10518, 1032, 6843, 7990, 10517, 12000, 13319, 13354, 519, 7728, 8798, 3604, 1038, 7741, 7080, 8923, 5266, 5522, 3511, 13691, 5838, 8308, 8484, 8223, 13765, 10197, 9288, 3148, 13441, 12486, 383, 1005, 11361, 13397, 6548, 6788, 12932, 13101, 5777, 7567, 4529, 5161, 9071, 4435, 3408, 3957, 11812, 1638, 5844, 2431, 13444, 5564, 2832, 3011, 3146, 5558, 13105, 13824, 4714, 7742, 5082, 8471, 5705, 6436, 12650, 407, 11511, 1355, 6349, 11053, 2667, 6062, 2714, 7602, 2926, 13137, 11647, 7319, 4083, 3663, 9647, 6120, 11199, 4086, 764, 8776, 4754, 6947, 921, 2513, 4212, 6465, 5344, 6984, 6315, 756, 3077, 3723, 14194, 10145, 7202, 9470, 13198, 9311, 8620, 11269, 4578, 13474, 8168, 11238, 14215, 1262, 9786, 12809, 14332, 9075, 8941, 12803, 2770, 3425, 2023, 10947, 11581, 6135, 11293, 4288, 11145, 3970, 11536, 1911, 3990, 7968, 13583, 10680, 12323, 8084, 627, 9868, 8494, 11232, 198, 4291, 6544, 6266, 5913, 2921, 962, 5069, 10709, 5283, 6462, 10610, 8752, 4742, 4410, 9605, 7826, 11321, 1259, 10421, 6543, 5022, 4230, 3267, 7249, 2642, 6683, 8476, 4935, 12282, 7917, 5394, 7295, 8097, 3545, 7888, 5540, 4356, 10000, 10901, 13478, 6050, 4934, 12076, 3634, 3655, 5614, 14163, 5220, 197, 12981, 8372, 8877, 13166, 5275, 12221, 11891, 7102, 10395, 13028, 13343, 3187, 6505, 3856, 840, 12761, 11160, 9885, 7381, 13126, 7973, 1114, 1799, 7473, 1379, 10832, 9085, 7449, 11671, 1989, 990, 10919, 133, 13087, 8547, 9444, 13387, 11258, 2015, 864, 11229, 10236, 9183, 11980, 8143, 2957, 1396, 6664, 1556, 810, 924, 7462, 2240, 5596, 469, 438, 4678, 11158, 6463, 795, 4849, 6007, 2689, 10750, 4882, 4027, 1072, 3401, 4295, 10843, 2822, 11449, 10157, 12625, 8466, 8379, 2623, 299, 5729, 8117, 802, 8872, 4797, 11470, 4250, 1516, 13490, 7673, 13819, 12146, 4780, 8987, 13401, 5099, 47, 12884, 11971, 11099, 8503, 3558, 11277, 9079, 9653, 10952, 14155, 5599, 5108, 3312, 10945, 10500, 10775, 14219, 7163, 7291, 13877, 10895, 6256, 3644, 1137, 7472, 12452, 7774, 11408, 8049, 5337, 6724, 6367, 8887, 1078, 6681, 1216, 9527, 4653, 10708, 7693, 3301, 7298, 14067, 9824, 1644, 5690, 7747, 7418, 598, 5801, 2296, 4424, 1993, 4983, 5533, 10304, 10978, 6832, 10582, 4517, 8955, 4869, 2121, 185, 3888, 7389, 1549, 9879, 5725, 2092, 12655, 2838, 11369, 8933, 10272, 9242, 9711, 9140, 10390, 3178, 2579, 2352, 8747, 12990, 104, 5951, 10325, 5114, 12801, 11810, 649, 1364, 8157, 10800, 8009, 13710, 12455, 9732, 9918, 10381, 10893, 12818, 10378, 2405, 2030, 10205, 12417, 1148, 6523, 11533, 13982, 7946, 2678, 12264, 8275, 8704, 1731, 671, 7356, 12123, 372, 11319, 2419, 3142, 5700, 5461, 202, 2753, 11268, 6046, 7705, 11255, 296, 702, 7569, 9428, 826, 7939, 2609, 4262, 2083, 11516, 9192, 13839, 7882, 4224, 6970, 5604, 4839, 7098, 9289, 6870, 11341, 4192, 619, 5631, 11311, 9103, 7266, 1633, 10813, 9770, 8118, 14262, 8543, 718, 10117, 7537, 2913, 919, 8364, 1966, 6429, 4765, 5773, 13324, 7850, 2201, 3172, 1042, 3728, 10291, 10628, 12426, 8022, 8744, 6626, 6808, 8111, 4335, 653, 8338, 1006, 4937, 10153, 8985, 9746, 5185, 409, 11046, 8284, 4980, 4338, 827, 11249, 8720, 3941, 11211, 1215, 2555, 9346, 6370, 9601, 12204, 4084, 554, 2466, 6566, 7565, 1817, 3924, 5299, 13042, 4143, 3598, 3264, 6904, 10575, 5882, 1822, 13449, 2044, 10689, 12284, 10053, 7617, 5949, 9694, 3350, 4556, 6418, 7417, 3442, 10276, 7805, 1127, 9720, 3031, 3667, 5521, 10706, 6827, 11482, 12111, 13831, 11403, 6695, 13501, 1385, 12058, 1617, 4201, 12328, 3495, 12718, 7562, 10116, 13931, 11659, 3373, 11305, 5248, 6129, 6722, 2512, 3440, 4392, 1444, 6761, 6885, 180, 11063, 4778, 6853, 3773, 10072, 9809, 5223, 10838, 410, 9678, 8989, 12474, 9528, 8775, 13908, 9754, 1945, 12232, 3586, 9718, 10741, 6804, 7121, 6209, 1855, 194, 2386, 7925, 13644, 9920, 2099, 6517, 5926, 7653, 12584, 12536, 1560, 3789, 7490, 13488, 11586, 9352, 13223, 2226, 5083, 7177, 2277, 2093, 578, 11273, 8079, 8425, 4135, 10028, 7211, 72, 10370, 2570, 8259, 5886, 1850, 871, 7910, 4003, 2716, 6673, 7408, 4770, 2733, 4399, 6617, 224, 11573, 9179, 4744, 7718, 13348, 1835, 4872, 9897, 553, 5177, 1017, 12122, 648, 3958, 7248, 7089, 8125, 3498, 7203, 3733, 11107, 11283, 4648, 4887, 11401, 12558, 2744, 8640, 4574, 11944, 1652, 6850, 1785, 9053, 11626, 2211, 6008, 8449, 13996, 5410, 7138, 6099, 12517, 558, 6691, 12202, 742, 10474, 8882, 8222, 6430, 2257, 4507, 4946, 10088, 12906, 3940, 7467, 4386, 10599, 12025, 444, 5211, 11281, 14197, 3051, 9829, 8811, 5931, 8735, 10195, 13656, 2165, 13652, 1378, 4163, 11363, 6365, 3304, 8155, 11564, 13269, 9321, 10936, 4579, 11425, 11022, 5736, 11918, 3610, 7154, 10616, 12268, 11754, 2791, 3104, 9144, 10848, 2350, 3693, 10997, 2900, 7752, 9256, 7776, 3694, 3242, 11631, 10600, 6697, 3754, 5124, 6230, 13781, 4491, 10583, 10079, 1084, 11069, 3000, 9856, 1509, 6232, 5792, 8583, 13589, 7687, 3650, 2196, 6585, 10432, 7634, 9777, 5064, 1798, 3607, 9379, 9836, 4610, 13487, 1421, 8910, 289, 8899, 8036, 13722, 11723, 9989, 3679, 6648, 341, 657, 2599, 1471, 73, 3216, 12215, 4550, 7658, 14356, 4339, 943, 5884, 10667, 4920, 11393, 6567, 9381, 5217, 9430, 13318, 5338, 9263, 11103, 7669, 3016, 10554, 10333, 2590, 8526, 3238, 9859, 9993, 3310, 11210, 4361, 2209, 6768, 7201, 7841, 3100, 1322, 1735, 3762, 227, 382, 10564, 11458, 13861, 3722, 10070, 12366, 7559, 2447, 3367, 8032, 13482, 13231, 3719, 4520, 1445, 6932, 6172, 1955, 1397, 7766, 3321, 9005, 6089, 5032, 9270, 8815, 3019, 10112, 4685, 14116, 11571, 6433, 3534, 2946, 1211, 4704, 3203, 6922, 5519, 2395, 1225, 1436, 3119, 4526, 8349, 11737, 10111, 338, 12974, 13416, 8817, 6016, 10804, 7274, 6242, 1086, 12641, 768, 2138, 10292, 6531, 8313, 1739, 9900, 8386, 4323, 2120, 1280, 5740, 4457, 6111, 3916, 451, 2649, 5713, 9843, 8198, 4788, 10758, 4554, 3836, 7765, 9593, 13780, 4700, 4069, 11379, 7960, 5807, 9914, 8963, 6654, 11909, 3842, 2855, 9670, 5131, 7907, 889, 10487, 1999, 9830, 9246, 7107, 8046, 5602, 11152, 43, 6701, 2532, 10995, 6897, 7382, 6348, 168, 6951, 6035, 8230, 13800, 6954, 7544, 3915, 253, 9716, 8896, 10280, 4615, 12714, 12436, 14368, 14098, 6021, 4590, 3562, 8753, 8302, 2890, 13014, 2217, 7258, 8903, 7893, 11413, 2308, 4050, 7058, 2310, 1897, 11721, 812, 12501, 13093, 3743, 11513, 11999, 7372, 7308, 2846, 9537, 11628, 984, 4767, 10222, 7009, 5714, 2914, 7414, 7494, 2087, 5687, 11190, 8406, 349, 3578, 3180, 4990, 1063, 1500, 4802, 13866, 12384, 10777, 13702, 10594, 4633, 2156, 4706, 13367, 319, 1388, 4936, 776, 11859, 8060, 3282, 7627, 8334, 3131, 10331, 248, 7867, 6728, 10450, 9411, 1041, 6822, 96, 4035, 6950, 2541, 13912, 6169, 11021, 6397, 2814, 813, 1372, 9496, 8026, 1168, 12713, 10889, 1268, 8106, 9429, 1979, 2281, 4006, 3614, 6680, 5257, 13837, 8589, 8849, 1521, 1682, 11096, 10988, 1404, 4953, 5269, 6980, 11552, 4311, 305, 8221, 7513, 10402, 6470, 5669, 5746, 8827, 5404, 13946, 8501, 10854, 11018, 9517, 2423, 2304, 3524, 13016, 5243, 12522, 12014, 12699, 11665, 5340, 3102, 4611, 2628, 10410, 5937, 6453, 11747, 9597, 1872, 4621, 7194, 7269, 1527, 1349, 3593, 4314, 10785, 1275, 3007, 7337, 8979, 13556, 3173, 3652, 11545, 1648, 5061, 6282, 6071, 4126, 4922, 2280, 2351, 6417, 2540, 3263, 10888, 14324, 6594, 1400, 10797, 1422, 3937, 11736, 11833, 1265, 5915, 6649, 12877, 258, 10736, 2193, 9070, 7981, 14245, 6014, 8267, 2275, 2472, 3139, 4855, 8968, 2180, 3547, 13454, 14023, 141, 1559, 12156, 11758, 11506, 8040, 11082, 1462, 11360, 4426, 4089, 14056, 12978, 12213, 12223, 12239, 6443, 12129, 3853, 9058, 5502, 14317, 4875, 3512, 11262, 6687, 3046, 1677, 6575, 5311, 12454, 11531, 8328, 12442, 14064, 647, 9269, 8939, 11636, 5734, 7918, 10552, 9409, 5823, 1594, 4423, 9600, 2888, 6962, 654, 10476, 4139, 9501, 4245, 14345, 594, 1937, 12788, 1902, 12806, 2594, 14057, 8804, 10354, 8540, 10179, 6013, 8617, 10799, 1732, 3557, 791, 4604, 14026, 1391, 5870, 1386, 5684, 8001, 94, 9657, 7407, 9872, 9494, 10490, 9730, 5994, 804, 5686, 13782, 3612, 7706, 11381, 3898, 11368, 8604, 973, 7242, 13956, 6669, 3702, 5409, 3083, 9082, 4372, 1193, 10149, 10939, 12757, 11223, 1797, 11882, 6149, 5674, 14059, 4954, 4595, 4071, 2773, 5155, 13342, 3785, 2090, 11484, 4924, 398, 632, 10213, 8007, 4513, 1142, 6085, 2108, 6493, 4655, 13242, 45, 10779, 11961, 6508, 2831, 4879, 8631, 11121, 11686, 11019, 10332, 1608, 13027, 8616, 664, 4231, 6530, 5025, 13879, 12463, 10367, 10591, 514, 11718, 11120, 4342, 3647, 1469, 7398, 5678, 14226, 4147, 13790, 5152, 748, 1474, 10874, 10002, 872, 6501, 4901, 5888, 11026, 6888, 9015, 6847, 2739, 1687, 4283, 6196, 3137, 1389, 1107, 2967, 8200, 0, 6580, 7425, 4077, 13279, 2760, 11729, 9791, 10965, 10456, 9435, 5126, 2265, 11378, 9813, 11548, 13443, 2016, 10592, 3057, 10716, 1094, 4442, 6949, 1207, 12007, 6644, 9551, 8015, 3617, 5109, 5595, 33, 2124, 10062, 5351, 1649, 10819, 4821, 7032, 8605, 1796, 2615, 3078, 11331, 10392, 8310, 12126, 7015, 10463, 10869, 4784, 6291, 4350, 1255, 283, 1267, 3532, 9453, 13098, 6236, 5251, 7612, 6051, 10032, 8459, 1437, 10252, 7975, 4286, 10021, 8956, 6294, 2499, 7499, 6445, 4444, 9187, 7921, 5814, 2338, 577, 2062, 5610, 2829, 13579, 11033, 11682, 11170, 2458, 12543, 845, 8207, 12695, 1572, 3581, 8850, 2847, 7282, 1336, 9580, 5381, 9838, 3134, 4549, 10264, 3580, 6696, 2892, 9167, 10923, 1383, 4336, 6739, 12548, 2928, 12595, 14209, 1726, 5364, 5717, 8844, 2190, 11627, 2319, 7388, 5950, 3491, 2459, 2828, 6887, 14, 6394, 8994, 7610, 9853, 12151, 534, 5590, 8299, 3246, 8069, 6564, 10227, 4620, 10251, 459, 3732, 6760, 11643, 12911, 3143, 1477, 9350, 3127, 11198, 8443, 1277, 5429, 14362, 6324, 9775, 7000, 5136, 10349, 10393, 7858, 10423, 7950, 6875, 10967, 9224, 5561, 1529, 11078, 6454, 9251, 1466, 7593, 4533, 2895, 1939, 5896, 13554, 994, 11695, 2117, 4337, 7336, 12421, 10898, 12378, 5524, 13025, 2198, 5769, 3822, 9169, 13580, 7843, 11216, 2574, 3595, 411, 5430, 956, 14399, 7891, 11463, 1053, 7818, 1824, 1522, 12635, 6985, 12802, 9659, 10811, 8771, 8548, 8370, 3074, 13893, 5496, 7748, 8067, 3936, 14411, 7594, 13500, 10996, 4488, 60, 13435, 8248, 11521, 1775, 8141, 8461, 7027, 10240, 7719, 6919, 9954, 1573, 10942, 834, 6156, 3454, 12273, 474, 6011, 11792, 10434, 1947, 1393, 11275, 465, 6318, 4747, 13118, 7227, 9272, 5466, 13711, 4038, 1916, 9721, 11966, 8527, 13778, 6041, 6568, 4052, 4028, 6027, 12081, 6805, 8539, 2901, 5878, 8714, 6249, 10422, 9561, 2927, 3017, 12689, 7101, 5657, 9806, 6427, 10824, 207, 8085, 11649, 11569, 102, 5598, 5847, 2360, 5531, 4726, 11183, 9358, 9978, 1717, 7777, 6756, 489, 1121, 331, 479, 7828, 13108, 11614, 1934, 8335, 11663, 774, 7576, 1417, 3426, 7361, 8159, 2606, 5788, 7788, 2089, 2994, 12034, 8498, 6730, 12922, 2151, 3510, 8185, 6333, 910, 11895, 7649, 2494, 5319, 11927, 1741, 4776, 2336, 7371, 6432, 5607, 2643, 13984, 14100, 3463, 13907, 11979, 7092, 3721, 11113, 7435, 13615, 4829, 9040, 8220, 5378, 7998, 2664, 14005, 10562, 4660, 7109, 7767, 4412, 6911, 11180, 4011, 3088, 4000, 2164, 6831, 2522, 5027, 7540, 13179, 10767, 13712, 7125, 8730, 3465, 6083, 8353, 9389, 5075, 432, 13590, 3167, 8598, 10457, 4654, 3089, 6288, 10856, 5260, 7571, 1170, 6361, 7230, 11397, 14186, 8041, 8934, 14160, 7400, 8453, 12172, 12619, 2382, 11085, 11001, 5876, 9154, 10086, 12830, 4449, 4152, 5935, 5804, 1574, 8096, 12052, 14301, 9159, 5893, 1811, 13048, 5805, 3867, 10608, 6356, 4406, 11688, 5709, 3906, 5620, 2961, 4200, 3145, 6185, 8940, 644, 6859, 235, 4504, 7034, 7289, 9942, 4078, 11730, 2789, 5883, 633, 7542, 9690, 4918, 6666, 6799, 12639, 3518, 10110, 7082, 9176, 6306, 11960, 4248, 2624, 9247, 13806, 7543, 12834, 10436, 11165, 6729, 2064, 13561, 11597, 3123, 488, 6132, 4366, 7954, 11543, 9488, 6104, 6248, 978, 10840, 12079, 7958, 8556, 2292, 5907, 1497, 14289, 5205, 3252, 11040, 3942, 8469, 1907, 306, 11202, 3632, 6857, 14210, 7255, 4487, 10408, 3594, 7146, 4606, 8992, 6502, 4622, 375, 4791, 1941, 5011, 7704, 10344, 11485, 8693, 3306, 7595, 11252, 10992, 8402, 176, 9108, 9934, 607, 12973, 7983, 12243, 13017, 7416, 3449, 13963, 11483, 961, 4122, 6966, 10441, 7277, 5204, 6744, 10519, 9686, 10769, 7707, 8602, 1052, 4041, 1564, 3848, 3576, 7552, 10612, 8764, 11713, 11274, 8664, 1122, 7085, 13451, 6889, 10602, 4997, 6357, 4471, 9876, 14314, 6525, 8936, 8457, 6428, 4680, 8063, 593, 10358, 5254, 6865, 5213, 6290, 8780, 10944, 8444, 7638, 7535, 13775, 1249, 11209, 3606, 9578, 1917, 8638, 13575, 7979, 11603, 2654, 4977, 4992, 3904, 8806, 4745, 13693, 9162, 4847, 11777, 11772, 7832, 6848, 565, 7709, 6656, 12819, 7436, 14273, 13327, 9800, 13176, 13356, 6222, 5067, 11455, 7532, 1706, 2424, 12174, 8242, 9603, 12682, 10274, 7654, 13630, 3274, 4079, 5252, 13840, 5632, 10132, 12678, 10624, 5274, 2505, 5207, 4755, 5238, 3901, 8237, 245, 13881, 5691, 1092, 9013, 13284, 7214, 191, 11529, 7671, 7141, 778, 151, 8347, 3737, 7573, 2320, 1940, 10510, 3326, 10295, 1764, 2251, 4835, 6009, 2103, 5553, 9519, 7175, 5030, 9633, 9823, 5426, 8423, 13139, 10161, 8166, 3947, 6066, 3395, 2602, 6716, 10438, 9106, 12593, 4592, 536, 11220, 12888, 3488, 10204, 10865, 7493, 13051, 1000, 3383, 8986, 89, 3028, 9333, 10694, 1758, 993, 10520, 7615, 4088, 7247, 13871, 463, 5375, 5895, 8641, 2837, 4911, 2177, 11462, 10926, 11196, 13965, 9408, 13161, 11377, 13253, 12377, 1409, 12403, 9948, 5373, 4501, 7045, 13431, 1619, 10080, 1136, 985, 8750, 6375, 7987, 6704, 3592, 1261, 11489, 11996, 9002, 6971, 6372, 12582, 2577, 7204, 7492, 4527, 13740, 8183, 5605, 9675, 7213, 11615, 1318, 113, 10234, 8231, 6657, 6536, 3971, 13313, 11894, 2327, 9165, 8362, 8359, 13640, 13517, 13376, 5783, 5170, 2171, 11572, 2497, 5584, 5611, 9348, 13152, 8149, 2859, 7895, 12386, 2268, 7738, 5107, 10655, 14233, 13237, 2717, 6025, 9652, 12895, 909, 8266, 9912, 14105, 9702, 4478, 1223, 8607, 8628, 2866, 7667, 13868, 8507, 8725, 5425, 12035, 9931, 11798, 10590, 13715, 13518, 10167, 7480, 10764, 12142, 10217, 6519, 6883, 6218, 4838, 2070, 6295, 6844, 13069, 6081, 10294, 2131, 9469, 13297, 4979, 5100, 596, 9735, 13610, 6034, 5003, 12444, 7482, 8813, 3760, 4340, 4896, 13400, 2911, 777, 8286, 6118, 3772, 5, 439, 2815, 8760, 2669, 6976, 8356, 1715, 6809, 2230, 4569, 11746, 11580, 14121, 13251, 4306, 980, 1930, 14374, 9696, 2978, 2616, 12986, 4769, 1244, 1577, 7489, 7708, 4613, 10042, 5384, 2656, 2215, 788, 3698, 5017, 7698, 11565, 8094, 6510, 14081, 11958, 5305, 14037, 13311, 6602, 9098, 1458, 1740, 6765, 14144, 8706, 13162, 9298, 1672, 6636, 8486, 5721, 12637, 3729, 6137, 2858, 8236, 10748, 7884, 13112, 6377, 8875, 3717, 10444, 3404, 7924, 1711, 320, 10791, 8622, 5646, 11880, 6773, 8653, 3499, 13033, 6821, 5575, 14354, 8358, 3889, 4341, 4545, 2759, 3697, 4906, 11162, 7241, 12610, 9205, 8296, 5343, 10700, 6576, 8151, 386, 10538, 10683, 9282, 2908, 5329, 8663, 8856, 6351, 11443, 11481, 5875, 5991, 3378, 661, 7756, 11329, 4479, 13464, 10397, 11428, 10170, 3121, 11073, 7691, 3963, 1295, 1229, 10046, 5192, 8898, 4483, 6823, 2526, 9129, 2032, 11738, 9508, 7078, 4543, 8971, 10489, 9319, 7147, 8357, 9928, 9615, 8208, 8014, 3071, 2956, 12808, 4357, 6616, 5510, 11567, 2078, 10588, 212, 1185, 8325, 14021, 1181, 7861, 401, 12572, 9995, 1747, 9871, 8012, 10406, 13531, 10913, 6345, 6908, 8024, 2155, 10336, 4792, 13021, 5978, 3962, 4349, 6012, 11436, 3117, 9753, 7252, 7231, 1921, 8108, 4826, 2697, 4819, 5503, 3035, 13150, 789, 12011, 8840, 907, 9655, 9937, 3636, 1360, 13655, 13525, 1926, 7787, 3487, 5625, 4623, 10692, 4354, 8702, 6614, 12658, 14302, 7930, 13197, 7011, 14019, 4950, 1703, 3427, 12568, 6959, 6941, 7696, 1886, 12977, 10567, 13480, 2144, 6913, 13353, 6837, 4441, 1361, 6201, 10215, 7951, 3952, 6842, 3183, 7995, 11851, 11090, 2842, 8736, 10010, 11739, 3493, 8055, 2118, 8110, 12765, 4446, 2407, 3332, 10207, 13568, 2693, 4814, 7781, 4728, 7209, 9837, 5428, 1252, 6584, 8546, 12858, 7103, 9749, 7256, 5635, 13209, 11722, 5141, 10350, 19, 11947, 12975, 938, 1634, 2878, 4758, 3186, 3490, 11168, 6109, 8677, 6369, 14125, 12645, 4952, 13202, 11604, 4214, 10914, 5481, 11560, 8960, 6946, 513, 5572, 4109, 2317, 7813, 498, 9338, 11630, 12525, 5856, 1730, 10130, 6052, 10765, 5286, 422, 11696, 4908, 1059, 5036, 6779, 1868, 14051, 6339, 13310, 8807, 3668, 12553, 9925, 3819, 4948, 8564, 2755, 8130, 3120, 2398, 794, 4667, 11029, 2710, 3206, 5889, 4871, 1031, 5020, 1570, 604, 6344, 13673, 3979, 2287, 3981, 7723, 4939, 9737, 12296, 9671, 3874, 3237, 3631, 7886, 1734, 10835, 4737, 13901, 9893, 4929, 6049, 9038, 8056, 13272, 11056, 13276, 1232, 634, 9276, 13634, 1420, 8912, 5276, 8371, 9295, 8847, 12989, 11122, 3308, 9460, 8506, 11860, 6974, 5946, 1011, 7692, 2983, 7820, 5045, 5609, 10342, 4890, 7151, 5120, 2069, 472, 6637, 4116, 6168, 11187, 4973, 10630, 7821, 7949, 5990, 9592, 5301, 6190, 3921, 9152, 6391, 4381, 10790, 5645, 4430, 4191, 6269, 4246, 5504, 8595, 6944, 6726, 6540, 11907, 10527, 142, 6396, 8925, 1995, 4687, 8763, 947, 650, 2917, 11139, 9504, 2065, 3193, 12379, 3024, 10162, 7350, 11600, 13130, 11592, 4942, 7505, 3787, 5289, 8950, 416, 12557, 4884, 492, 13277, 7538, 5253, 6368, 6535, 3548, 7311, 14188, 5939, 9724, 3529, 12753, 13039, 4273, 8973, 2583, 2562, 5787, 7031, 14199, 14267, 13551, 10908, 4733, 1210, 169, 7322, 2738, 5993, 1808, 4732, 3375, 1187, 10530, 6229, 4450, 13883, 11264, 8273, 11863, 706, 11134, 2943, 998, 1912, 7394, 9000, 4190, 3680, 10937, 13680, 10008, 5649, 2048, 8848, 2885, 9972, 10126, 9640, 6040, 8408, 7549, 6301, 7220, 8099, 7873, 10365, 8988, 9614, 9084, 3040, 1864, 6845, 1337, 10335, 6714, 8077, 8544, 6321, 3682, 4016, 8315, 7614, 13226, 5912, 1736, 2174, 9291, 10098, 5453, 9455, 344, 14033, 1099, 8563, 3476, 10275, 9009, 7333, 10821, 9065, 2547, 9111, 6026, 937, 13830, 3022, 11395, 11374, 9101, 13220, 2222, 1484, 10430, 6550, 814, 6166, 10470, 1612, 3149, 10659, 10209, 5581, 10818, 8253, 1116, 4663, 3879, 11962, 14053, 5671, 8432, 9976, 5415, 6737, 10596, 10184, 4694, 6393, 5752, 12950, 13763, 10805, 4353, 11079, 11163, 673, 7663, 6112, 10127, 13086, 12251, 7848, 269, 12533, 13107, 2874, 3553, 4278, 14448, 11344, 10881, 2971, 10579, 7251, 7187, 6503, 13729, 11060, 4960, 6183, 11353, 6175, 517, 5016, 582, 3765, 11466, 3986, 1680, 2259, 7081, 5355, 10966, 4400, 10660, 3912, 7722, 12711, 12866, 767, 2176, 9998, 175, 7982, 8114, 13677, 3422, 10095, 10605, 11338, 3711, 9855, 6479, 3980, 1592, 14181, 5501, 3201, 12735, 6595, 1007, 6146, 11359, 1158, 7299, 3192, 6003, 9131, 533, 12680, 12976, 6628, 5044, 8470, 4785, 7914, 9649, 12725, 7876, 8802, 1156, 1956, 3670, 3295, 10993, 5065, 3285, 1890, 3155, 11593, 4217, 13546, 10420, 11611, 3399, 7219, 10477, 14322, 1290, 1221, 6618, 206, 8291, 3341, 1701, 1874, 3481, 7232, 3159, 6641, 11062, 24, 5753, 4347, 8818, 5421, 6478, 1882, 7632, 927, 8233, 1743, 13571, 11555, 7994, 8033, 14102, 8700, 11396, 7860, 7487, 8146, 10863, 7536, 6963, 2509, 8098, 11291, 584, 3991, 7587, 4044, 4910, 6458, 4940, 1942, 10318, 7354, 11185, 4160, 10328, 3767, 6320, 7976, 793, 7297, 6797, 1710, 5944, 823, 9565, 11234, 1026, 5658, 14334, 4777, 2620, 8212, 4385, 10726, 4473, 13459, 10484, 7344, 4679, 12940, 12919, 11608, 3279, 12236, 10823, 4174, 3270, 9440, 5545, 4270, 3821, 8439, 6093, 1018, 7221, 9589, 10172, 9190, 3675, 10868, 7095, 9500, 599, 6514, 1844, 12439, 4963, 4468, 13829, 1126, 4129, 13733, 3871, 5618, 1218, 7527, 12294, 9970, 2363, 11373, 123, 11520, 9158, 11108, 12588, 3314, 679, 3574, 4389, 3727, 13903, 7072, 674, 3850, 10448, 12416, 8278, 11619, 4764, 12716, 7605, 6255, 13766, 8711, 1651, 12162, 9505, 10065, 1176, 307, 4055, 7329, 5768, 1987, 5321, 3329, 3348, 12676, 2820, 2573, 3209, 7525, 12494, 2406, 7561, 4198, 8665, 6790, 2549, 9326, 10695, 4312, 4466, 12469, 254, 5836, 511, 908, 7816, 6310, 12388, 4103, 9112, 11579, 7120, 9107, 8189, 14092, 5498, 2653, 11440, 1857, 5603, 4413, 7404, 2274, 10714, 4225, 9354, 10703, 8770, 12780, 1013, 896, 2183, 10794, 10744, 10183, 12640, 3691, 13211, 12767, 5086, 4128, 12967, 13526, 11731, 5982, 10766, 934, 2633, 12003, 3483, 3943, 4995, 1819, 1597, 2698, 10772, 4593, 9622, 9584, 8218, 7855, 2776, 9811, 4209, 6798, 7314, 3296, 3041, 8513, 7325, 1753, 987, 10352, 5115, 8567, 10175, 7550, 7074, 3616, 2722, 1653, 7507, 5987, 732, 7006, 2705, 1862, 13469, 5909, 2786, 13822, 4282, 8076, 2959, 8521, 8158, 4542, 11499, 4510, 4394, 11701, 3055, 8160, 10404, 4165, 13132, 9576, 4748, 13049, 10049, 164, 1879, 873, 12356, 14299, 8350, 7878, 10087, 741, 243, 9328, 14185, 7110, 1180, 3993, 4817, 378, 4087, 2875, 2072, 1683, 14036, 8823, 13447, 7178, 9586, 6939, 6360, 2808, 3653, 5668, 2191, 4462, 10732, 4601, 12704, 8767, 1002, 6689, 12534, 10369, 11453, 7740, 6778, 10437, 2231, 4912, 1089, 1029, 9861, 12769, 9204, 2055, 5468, 8323, 4585, 4229, 7660, 5914, 6150, 4023, 12931, 3273, 4355, 5122, 10525, 9847, 6856, 8199, 9427, 7124, 6631, 4573, 13593, 6593, 13637, 5336, 4099, 268, 5763, 13975, 4420, 3376, 2834, 2489, 10550, 6780, 5227, 13099, 5372, 14406, 9370, 9965, 10566, 5795, 3275, 2142, 2456, 12293, 1430, 8070, 11902, 3494, 2694, 11854, 1390, 995, 5764, 11884, 6938, 1227, 8707, 8655, 4702, 2889, 4162, 8909, 4226, 637, 6265, 5724, 4187, 5038, 696, 4681, 10745, 9060, 12432, 11002, 14367, 12918, 1685, 9714, 7923, 8645, 3136, 13731, 5046, 2390, 3363, 8232, 6690, 13991, 8078, 765, 2202, 2586, 12165, 6876, 10986, 4149, 853, 5494, 4567, 4710, 988, 11605, 10969, 3814, 13467, 11091, 10977, 8008, 4020, 7262, 1298, 7900, 4051, 5397, 148, 8739, 1929, 8177, 9682, 4456, 10186, 9066, 3809, 2007, 11539, 9708, 6995, 165, 1040, 7727, 1869, 4183, 5314, 11496, 8538, 867, 13698, 8792, 4546, 9062, 2870, 6455, 9300, 10585, 2703, 3654, 6591, 8790, 5766, 2736, 7945, 708, 6645, 8398, 12205, 5841, 12352, 1983, 7426, 2034, 10891, 9418, 5058, 4928, 532, 11242, 2401, 12190, 13044, 13421, 7681, 1475, 2026, 5006, 11039, 7780, 6931, 7118, 10138, 10495, 13163, 10203, 13564, 5776, 13539, 3344, 12693, 7468, 4033, 10512, 6940, 8319, 11905, 13412, 1324, 4142, 5497, 6920, 12824, 585, 11673, 7678, 157, 10124, 5316, 7908, 3477, 3387, 4885, 11930, 7215, 5121, 5187, 12752, 1476, 11839, 6342, 5388, 4641, 5952, 13898, 710, 1208, 3954, 13661, 4193, 7068, 10754, 4824, 1008, 2906, 4787, 13902, 6978, 11041, 3406, 9987, 10830, 7363, 8468, 11840, 11217, 12409, 6679, 1192, 8215, 7899, 3577, 8181, 8590, 14315, 1448, 12175, 770, 1903, 7149, 9032, 8397, 10526, 5629, 628, 5557, 8557, 9782, 4241, 9554, 9377, 13325, 13164, 4297, 7635, 10224, 7581, 7145, 6223, 5051, 3568, 10156, 7243, 10284, 3409, 12529, 8502, 1846, 12839, 6816, 10496, 369, 720, 11125, 8254, 6638, 384, 6227, 9198, 1821, 2218, 12362, 3513, 4597, 10498, 5587, 239, 7306, 8724, 7539, 13043, 14045, 8072, 10453, 2879, 12326, 4668, 5542, 9771, 6738, 4904, 12787, 6652, 4310, 11988]\n",
      "[146, 11698, 7558, 5840, 13865, 1105, 9233, 11817, 13301, 8932, 8675, 3005, 13979, 6753, 5964, 4715, 7757, 3910, 14107, 2004, 5002, 7771, 6840, 5555, 6733, 830, 11394, 8054, 12036, 11843, 10964, 10682, 4398, 1542, 1583, 7310, 9093, 1748, 4324, 7454, 6612, 11670, 10018, 13971, 2987, 8048, 2937, 10742, 4703, 7253, 11763, 4252, 1024, 5482, 4900, 12599, 7932, 6743, 4898, 3506, 7216, 6688, 2539, 12320, 5863, 7795, 6520, 4987, 218, 7021, 6466, 6632, 9265, 2239, 6552, 8990, 4097, 2848, 5303, 12607, 7198, 10502, 6080, 4049, 12943, 12527, 10289, 4275, 4646, 10228, 13271, 2094, 3665, 11869, 8975, 12277, 12412, 6340, 6305, 8570, 12353, 8487, 12835, 7665, 10321, 7437, 8951, 9120, 10324, 3291, 13395, 4730, 5910, 11528, 2998, 7901, 5495, 8495, 12786, 4658, 6188, 4115, 2010, 1334, 3735, 5386, 3701, 11465, 3385, 7928, 9147, 4173, 12019, 11657, 10085, 12043, 1694, 10413, 2674, 8407, 2672, 13604, 7094, 11837, 6541, 1088, 3113, 461, 8998, 3795, 7495, 9812, 2699, 3413, 8342, 1342, 9491, 13512, 6184, 4626, 14381, 3379, 2009, 5661, 6421, 12724, 1688, 7656, 11430, 8042, 5342, 13598, 6504, 2227, 12511, 4092, 13168, 10414, 13402, 5810, 267, 11849, 4699, 7280, 8337, 8545, 8954, 12201, 3342, 9908, 10129, 2679, 3834, 14225, 11439, 8656, 11257, 2811, 4709, 4524, 6810, 7169, 10812, 12546, 14240, 4766, 9564, 6158, 526, 3346, 10749, 12045, 6634, 5349, 12169, 3013, 5168, 6663, 189, 773, 11821, 5010, 4344, 4833, 7062, 6382, 2968, 8576, 9195, 13183, 6747, 745, 5549, 12955, 7185, 14435, 4448, 9662, 4598, 7971, 2125, 4489, 1319, 11356, 7601, 13952, 4146, 14257, 1773, 6619, 10892, 2341, 2752, 3115, 6138, 1523, 7944, 9849, 9520, 3799, 1495, 12292, 8810, 483, 8646, 8596, 13036, 6471, 5861, 14050, 9314, 7877, 7447, 10539, 1472, 5387, 13820, 13864, 11857, 7460, 3241, 2432, 7368, 7743, 4074, 11977, 8074, 11045, 13009, 1843, 10025, 606, 6942, 4240, 4154, 4272, 3085, 11550, 6731, 12715, 9087, 12470, 616, 8880, 12448, 2427, 7779, 162, 7666, 6005, 11558, 3315, 11638, 1762, 960, 11690, 7324, 9590, 1020, 55, 5424, 236, 5579, 11480, 6507, 7836, 2344, 8450, 10364, 9301, 6969, 5327, 2179, 5374, 5178, 8826, 7580, 11538, 8599, 6336, 3597, 27, 9186, 11389, 3844, 8409, 12406, 4408, 10834, 10054, 8727, 11844, 4961, 7136, 8217, 1927, 4395, 2312, 121, 2294, 5268, 9596, 10688, 5871, 8819, 5796, 8136, 4490, 12095, 6936, 546, 1600, 13114, 356, 10190, 14427, 2037, 5181, 1568, 3397, 1769, 13113, 5924, 10728, 1283, 10507, 8869, 3770, 9672, 14256, 2122, 10870, 1545, 8615, 5350, 12134, 7434, 1806, 6483, 3775, 3307, 6968, 3196, 11214, 3881, 975, 9023, 6084, 5582, 9219, 3060, 8219, 6299, 2476, 1737, 951, 3973, 11065, 11872, 5626, 3050, 7433, 13789, 130, 5267, 10718, 3471, 7657, 1431, 9063, 13895, 9373, 8247, 9895, 5339, 10650, 5198, 9487, 9774, 5264, 9018, 6263, 12481, 7022, 13900, 2702, 7516, 8957, 11756, 7043, 11820, 8162, 11412, 13555, 9637, 5483, 7244, 7794, 11025, 5622, 4988, 2018, 2826, 4795, 847, 5302, 264, 4208, 615, 13627, 966, 753, 13398, 8504, 3817, 7478, 11672, 2243, 6261, 5988, 4570, 9432, 6623, 10419, 322, 2695, 4293, 183, 6197, 10232, 6219, 7440, 8946, 2324, 11887, 5172, 445, 3025, 8305, 10431, 4022, 8440, 977, 4500, 7037, 7739, 11783, 1450, 5930, 9803, 2798, 4548, 2639, 1836, 10642, 9130, 13411, 14298, 1709, 8959, 7411, 10759, 6226, 9567, 1496, 10467, 8636, 3641, 11662, 3501, 4157, 8530, 4984, 3601, 3362, 4845, 93, 1440, 10786, 5416, 12662, 1449, 7773, 10329, 7024, 3170, 13230, 2862, 3082, 8052, 10263, 1047, 5144, 3239, 9944, 2982, 6164, 7621, 1049, 4218, 6835, 9956, 13962, 2123, 4563, 9371, 13316, 1602, 8321, 11451, 2126, 12669, 2872, 7686, 6461, 11985, 7522, 10620, 8479, 8387, 8401, 10962, 9335, 7978, 13779, 11678, 8587, 7091, 4991, 13977, 6674, 2445, 364, 1689, 2934, 5681, 9581, 8126, 8447, 11289, 11287, 6725, 12524, 12419, 6961, 7589, 3317, 11888, 57, 3913, 8625, 10478, 5619, 2821, 6148, 5175, 2127, 13992, 2823, 14135, 10924, 2484, 2137, 4375, 1060, 5634, 3166, 2397, 8187, 5774, 10104, 8542, 13413, 1891, 14277, 5959, 5458, 13987, 5229, 4568, 7782, 12489, 3521, 9817, 603, 6435, 4724, 1901, 10348, 14235, 44, 12133, 4720, 3522, 3396, 8822, 10723, 1887, 2175, 12297, 10820, 2632, 12298, 11194, 5092, 2455, 7986, 10319, 7234, 10202, 2291, 10778, 7866, 6650, 1578, 10120, 5197, 6982, 2779, 13283, 7224, 1398, 9237, 12475, 3803, 7993, 2531, 8150, 193, 5802, 2508, 10998, 7942, 8757, 2112, 4243, 8722, 7997, 6078, 1348, 91, 7205, 5559, 4695, 10677, 7399, 3157, 5345, 1293, 8318, 5431, 3969, 10033, 4957, 406, 11574, 3882, 149, 11324, 2283, 10341, 507, 8418, 12723, 5532, 4557, 2782, 11138, 3467, 9723, 7802, 6596, 4072, 8306, 12863, 10809, 3095, 3015, 9027, 2565, 13147, 6727, 5348, 11621, 7864, 11646, 6257, 9878, 1716, 5140, 10255, 7470, 10859, 15, 10326, 7592, 8134, 1453, 4002, 969, 10019, 8561, 2011, 2436, 7793, 8795, 10256, 1363, 2707, 7004, 9541, 7857, 5675, 8346, 4497, 5029, 8419, 4923, 14183, 5484, 9844, 11279, 4296, 9781, 5702, 3311, 2903, 12506, 11307, 13394, 6358, 4132, 1654, 7871, 7019, 3600, 14347, 9275, 9136, 13654, 1009, 3744, 11491, 11610, 13227, 14426, 9374, 10556, 981, 7114, 12324, 8765, 13236, 1839, 11744, 2303, 12790, 5153, 10376, 11509, 1629, 10242, 9533, 5697, 6712, 6096, 11388, 7672, 9941, 5446, 1030, 540, 13254, 13445, 3849, 5385, 4752, 6703, 3372, 12237, 12224, 5808, 14011, 2941, 4964, 8249, 8369, 1605, 904, 2261, 4878, 5653, 2958, 10643, 3996, 6967, 8890, 2965, 6668, 8119, 11043, 9798, 7338, 6635, 3112, 10238, 8082, 10883, 2245, 11098, 2258, 699, 12176, 10388, 1723, 5093, 5813, 12541, 3934, 7104, 13657, 8186, 6202, 9669, 10089, 7637, 7988, 2234, 14325, 491, 8071, 3160, 6474, 5784, 2515, 6130, 2788, 13232, 8691, 4959, 8920, 14404, 7106, 9001, 13771, 14276, 5945, 2980, 3649, 2893, 5250, 3757, 8431, 12848, 10515, 6053, 5297, 2976, 8922, 7300, 13191, 434, 9100, 90, 4806, 8562, 6682, 5151, 10615, 7639, 2687, 5791, 8569, 9963, 9392, 10836, 7746, 10626, 2758, 6262, 8271, 6451, 7017, 4874, 12862, 10991, 14049, 1418, 785, 7012, 13676, 2525, 6783, 12504, 10514, 10662, 8101, 6127, 2147, 3228, 10557, 12970, 3464, 12197, 10720, 6388, 7238, 883, 10931, 4064, 12980, 6522, 12597, 119, 8839, 6311, 6574, 5449, 379, 2046, 13339, 12870, 521, 11080, 1752, 10137, 8148, 2001, 8152, 5918, 5056, 3768, 6678, 10199, 12571, 8667, 7148, 5341, 9325, 8921, 11038, 13944, 2974, 9833, 5509, 8515, 5711, 8381, 12654, 9493, 10807, 902, 6225, 8295, 6384, 9571, 7076, 12773, 13825, 8281, 5240, 13909, 6247, 4902, 12661, 620, 8914, 6182, 3044, 14148, 11410, 8129, 10273, 12882, 1507, 6935, 4862, 5452, 13999, 5422, 14068, 4614, 7703, 2641, 13177, 528, 4346, 4941, 9820, 41, 3763, 2152, 2270, 4369, 4760, 5735, 11195, 5226, 3416, 10396, 354, 1712, 5454, 5597, 875, 3122, 8633, 290, 5592, 1326, 8833, 297, 8426, 6613, 12073, 13257, 5194, 4783, 13847, 2105, 13131, 6061, 12775, 8907, 10356, 10411, 2051, 5775, 12064, 3470, 6018, 13034, 34, 13695, 4199, 14377, 515, 2634, 12674, 5279, 1696, 7643, 14447, 10876, 11151, 5694, 2809, 5832, 3254, 2910, 13414, 2536, 7894, 2802, 4588, 2410, 2749, 736, 11865, 12482, 6476, 4705, 13429, 203, 12612, 2824, 8879, 4858, 4431, 3299, 12056, 4437, 7626, 5613, 10622, 13675, 12991, 12010, 8240, 11010, 4645, 7010, 11423, 3218, 1338, 5440, 6241, 12634, 2534, 9712, 7391, 4981, 5300, 12685, 14086, 5817, 11071, 1635, 9452, 10601, 1997, 5999, 10916, 14414, 2426, 7223, 11241, 4967, 10308, 6246, 8937, 4880, 486, 5565, 1539, 13378, 12065, 14350, 4305, 4731, 5528, 4136, 13308, 8422, 6709, 11793, 8180, 6036, 11230, 9016, 976, 11642, 9744, 3975, 4736, 4725, 3439, 13957, 3355, 9532, 723, 6485, 7033, 11365, 2462, 12222, 4101, 12137, 7577, 10648, 3791, 4716, 5129, 5405, 9936, 9180, 12532, 11842, 8670, 9719, 13570, 5771, 2355, 4782, 13689, 2854, 9701, 5851, 7380, 6571, 7063, 11724, 9035, 12979, 14385, 10979, 6956, 8772, 1867, 3753, 2604, 3837, 4082, 7305, 942, 5972, 3360, 1197, 4436, 3052, 4516, 1678, 7090, 12343, 13095, 11351, 13851, 11278, 2022, 13440, 12854, 9618, 13083, 5903, 8828, 2449, 7546, 11302, 9217, 8825, 1270, 8821, 5960, 11265, 3249, 14006, 14424, 11044, 11061, 14039, 242, 12405, 3156, 1620, 9056, 10884, 4321, 8456, 9329, 10710, 5873, 8709, 7608, 652, 8554, 8202, 6103, 14441, 7448, 12288, 11345, 7959, 10299, 7260, 1591, 6352, 13910, 2869, 14271, 7140, 11015, 14234, 161, 5601, 14307, 2780, 4235, 8800, 1312, 12782, 4521, 6658, 6766, 5306, 4127, 13037, 10143, 3076, 13399, 2012, 3810, 8025, 12596, 6775, 7057, 11510, 9583, 12577, 3059, 2905, 2024, 3081, 11561, 783, 1970, 1028, 10787, 10312, 11221, 10867, 4525, 10675, 12301, 7521, 12182, 5008, 3798, 3331, 7250, 13019, 3659, 7316, 1069, 13560, 9228, 4675, 8352, 6144, 13100, 1021, 10950, 4106, 2119, 6486, 6630, 4498, 2947, 549, 8778, 983, 12113, 1358, 4697, 4693, 11969, 2931, 12080, 10400, 2342, 11625, 7526, 8257, 8891, 13758, 6063, 725, 11441, 10788, 1932, 629, 7745, 729, 6900, 9641, 3811, 6400, 4669, 12885, 219, 9410, 4203, 6868, 3099, 12257, 6556, 11137, 3570, 6692, 3877, 10446, 3174, 4746, 5437, 8843, 10009, 1172, 13913, 10921, 2754, 314, 2380, 13753, 2314, 6989, 11337, 12879, 6625, 13578, 3386, 9764, 10603, 4813, 14122, 10558, 4184, 6300, 8883, 13103, 5719, 9634, 13724, 5554, 5325, 1538, 1203, 2882, 10233, 2263, 10425, 3955, 9673, 6706, 4673, 7199, 12124, 11949, 9938, 8684, 14420, 1432, 11013, 8482, 12996, 10792, 14283, 7018, 9705, 12091, 13744, 4261, 14027, 6426, 8761, 4860, 10633, 2665, 6015, 6702, 10173, 10469, 29, 7902, 9693, 8086, 2394, 5035, 1429, 13550, 5968, 1212, 4017, 11226, 8945, 9385, 6271, 6412, 9635, 2000, 11433, 2810, 2966, 13506, 10442, 6759, 7598, 3704, 3327, 5015, 1313, 3569, 8613, 4105, 957, 12811, 8394, 4053, 9264, 4124, 10844, 7625, 4989, 1213, 12117, 2107, 11589, 2581, 9783, 11197, 3661, 11734, 9530]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "train_indices = labeled_indices[:num_train]\n",
    "test_indices = labeled_indices[num_train:num_train+num_test]\n",
    "print(train_indices)\n",
    "print(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fcc75b7-0162-4070-bda0-46c044b01719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature759</th>\n",
       "      <th>feature760</th>\n",
       "      <th>feature761</th>\n",
       "      <th>feature762</th>\n",
       "      <th>feature763</th>\n",
       "      <th>feature764</th>\n",
       "      <th>feature765</th>\n",
       "      <th>feature766</th>\n",
       "      <th>feature767</th>\n",
       "      <th>feature768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FES</td>\n",
       "      <td>0.339602</td>\n",
       "      <td>-0.030744</td>\n",
       "      <td>-0.901381</td>\n",
       "      <td>0.100888</td>\n",
       "      <td>0.886443</td>\n",
       "      <td>0.383596</td>\n",
       "      <td>-0.192082</td>\n",
       "      <td>-0.032063</td>\n",
       "      <td>-0.154869</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.549204</td>\n",
       "      <td>-0.856123</td>\n",
       "      <td>0.714672</td>\n",
       "      <td>-0.046649</td>\n",
       "      <td>-0.894424</td>\n",
       "      <td>-0.001815</td>\n",
       "      <td>0.739485</td>\n",
       "      <td>0.015581</td>\n",
       "      <td>-0.023863</td>\n",
       "      <td>-0.022002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HADHA</td>\n",
       "      <td>-0.131799</td>\n",
       "      <td>-0.025745</td>\n",
       "      <td>-0.677301</td>\n",
       "      <td>-0.053545</td>\n",
       "      <td>0.971046</td>\n",
       "      <td>0.180315</td>\n",
       "      <td>-0.028189</td>\n",
       "      <td>-0.077389</td>\n",
       "      <td>-0.095152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.927885</td>\n",
       "      <td>-0.817812</td>\n",
       "      <td>0.809631</td>\n",
       "      <td>-0.005827</td>\n",
       "      <td>-0.848839</td>\n",
       "      <td>0.024516</td>\n",
       "      <td>0.526404</td>\n",
       "      <td>-0.039926</td>\n",
       "      <td>-0.102787</td>\n",
       "      <td>-0.026980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLC7A7</td>\n",
       "      <td>0.385693</td>\n",
       "      <td>-0.070692</td>\n",
       "      <td>-0.847796</td>\n",
       "      <td>-0.022054</td>\n",
       "      <td>0.959772</td>\n",
       "      <td>0.085487</td>\n",
       "      <td>0.076455</td>\n",
       "      <td>-0.003006</td>\n",
       "      <td>-0.032268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.941094</td>\n",
       "      <td>-0.912443</td>\n",
       "      <td>0.789828</td>\n",
       "      <td>0.046979</td>\n",
       "      <td>-0.715636</td>\n",
       "      <td>0.085842</td>\n",
       "      <td>0.150494</td>\n",
       "      <td>0.025392</td>\n",
       "      <td>-0.066035</td>\n",
       "      <td>-0.028283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LCK</td>\n",
       "      <td>0.650428</td>\n",
       "      <td>0.014479</td>\n",
       "      <td>-0.866163</td>\n",
       "      <td>0.053508</td>\n",
       "      <td>0.951529</td>\n",
       "      <td>0.269402</td>\n",
       "      <td>-0.214788</td>\n",
       "      <td>0.045179</td>\n",
       "      <td>-0.506429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.576739</td>\n",
       "      <td>-0.969558</td>\n",
       "      <td>0.916549</td>\n",
       "      <td>-0.080332</td>\n",
       "      <td>-0.927649</td>\n",
       "      <td>-0.047398</td>\n",
       "      <td>0.741663</td>\n",
       "      <td>-0.000096</td>\n",
       "      <td>-0.096318</td>\n",
       "      <td>-0.056501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HSPA2</td>\n",
       "      <td>0.322262</td>\n",
       "      <td>0.017484</td>\n",
       "      <td>-0.849302</td>\n",
       "      <td>0.046401</td>\n",
       "      <td>0.920429</td>\n",
       "      <td>0.463832</td>\n",
       "      <td>-0.050414</td>\n",
       "      <td>-0.033398</td>\n",
       "      <td>0.387791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387301</td>\n",
       "      <td>-0.860696</td>\n",
       "      <td>0.678607</td>\n",
       "      <td>-0.060695</td>\n",
       "      <td>-0.945793</td>\n",
       "      <td>0.040472</td>\n",
       "      <td>0.831079</td>\n",
       "      <td>-0.001711</td>\n",
       "      <td>-0.079842</td>\n",
       "      <td>-0.011189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47590</th>\n",
       "      <td>GO:2001313</td>\n",
       "      <td>0.174428</td>\n",
       "      <td>0.194728</td>\n",
       "      <td>-0.284376</td>\n",
       "      <td>0.282102</td>\n",
       "      <td>-0.713190</td>\n",
       "      <td>-0.272055</td>\n",
       "      <td>0.121190</td>\n",
       "      <td>0.129901</td>\n",
       "      <td>-0.983496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500545</td>\n",
       "      <td>0.429651</td>\n",
       "      <td>-0.292929</td>\n",
       "      <td>-0.464941</td>\n",
       "      <td>-0.740187</td>\n",
       "      <td>0.179149</td>\n",
       "      <td>-0.960807</td>\n",
       "      <td>-0.746958</td>\n",
       "      <td>1.069112</td>\n",
       "      <td>-0.848182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47591</th>\n",
       "      <td>GO:2001314</td>\n",
       "      <td>0.025886</td>\n",
       "      <td>0.306214</td>\n",
       "      <td>-0.254303</td>\n",
       "      <td>0.253673</td>\n",
       "      <td>-0.533680</td>\n",
       "      <td>-0.269355</td>\n",
       "      <td>0.150939</td>\n",
       "      <td>-0.229323</td>\n",
       "      <td>-1.078991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042979</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>-0.356661</td>\n",
       "      <td>-0.381828</td>\n",
       "      <td>-0.638338</td>\n",
       "      <td>0.077176</td>\n",
       "      <td>-0.788312</td>\n",
       "      <td>-0.683442</td>\n",
       "      <td>1.087031</td>\n",
       "      <td>-0.593092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47592</th>\n",
       "      <td>GO:2001315</td>\n",
       "      <td>0.027134</td>\n",
       "      <td>0.241391</td>\n",
       "      <td>-0.227353</td>\n",
       "      <td>0.317366</td>\n",
       "      <td>-0.726657</td>\n",
       "      <td>-0.197968</td>\n",
       "      <td>0.045653</td>\n",
       "      <td>0.038912</td>\n",
       "      <td>-0.954113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349853</td>\n",
       "      <td>0.370059</td>\n",
       "      <td>-0.144606</td>\n",
       "      <td>-0.493184</td>\n",
       "      <td>-0.655063</td>\n",
       "      <td>0.217335</td>\n",
       "      <td>-0.841272</td>\n",
       "      <td>-0.821077</td>\n",
       "      <td>1.036363</td>\n",
       "      <td>-0.836614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47593</th>\n",
       "      <td>GO:2001316</td>\n",
       "      <td>0.139543</td>\n",
       "      <td>0.028883</td>\n",
       "      <td>0.899480</td>\n",
       "      <td>0.152932</td>\n",
       "      <td>0.576852</td>\n",
       "      <td>0.330342</td>\n",
       "      <td>0.916943</td>\n",
       "      <td>0.012306</td>\n",
       "      <td>-0.020316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.354748</td>\n",
       "      <td>-0.083168</td>\n",
       "      <td>0.043640</td>\n",
       "      <td>-0.663565</td>\n",
       "      <td>0.543016</td>\n",
       "      <td>-0.652230</td>\n",
       "      <td>-1.427882</td>\n",
       "      <td>-0.985257</td>\n",
       "      <td>1.673561</td>\n",
       "      <td>0.109659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47594</th>\n",
       "      <td>GO:2001317</td>\n",
       "      <td>0.083064</td>\n",
       "      <td>0.090899</td>\n",
       "      <td>0.888541</td>\n",
       "      <td>0.309920</td>\n",
       "      <td>0.403966</td>\n",
       "      <td>0.202783</td>\n",
       "      <td>0.706517</td>\n",
       "      <td>-0.017584</td>\n",
       "      <td>-0.171057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.544680</td>\n",
       "      <td>-0.046654</td>\n",
       "      <td>0.262865</td>\n",
       "      <td>-0.767305</td>\n",
       "      <td>0.753788</td>\n",
       "      <td>-0.577503</td>\n",
       "      <td>-1.194910</td>\n",
       "      <td>-0.799556</td>\n",
       "      <td>1.519368</td>\n",
       "      <td>0.263210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62045 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          protein  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
       "0             FES  0.339602 -0.030744 -0.901381  0.100888  0.886443  0.383596   \n",
       "1          HADHA  -0.131799 -0.025745 -0.677301 -0.053545  0.971046  0.180315   \n",
       "2          SLC7A7  0.385693 -0.070692 -0.847796 -0.022054  0.959772  0.085487   \n",
       "3            LCK   0.650428  0.014479 -0.866163  0.053508  0.951529  0.269402   \n",
       "4           HSPA2  0.322262  0.017484 -0.849302  0.046401  0.920429  0.463832   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "47590  GO:2001313  0.174428  0.194728 -0.284376  0.282102 -0.713190 -0.272055   \n",
       "47591  GO:2001314  0.025886  0.306214 -0.254303  0.253673 -0.533680 -0.269355   \n",
       "47592  GO:2001315  0.027134  0.241391 -0.227353  0.317366 -0.726657 -0.197968   \n",
       "47593  GO:2001316  0.139543  0.028883  0.899480  0.152932  0.576852  0.330342   \n",
       "47594  GO:2001317  0.083064  0.090899  0.888541  0.309920  0.403966  0.202783   \n",
       "\n",
       "       feature7  feature8  feature9  ...  feature759  feature760  feature761  \\\n",
       "0     -0.192082 -0.032063 -0.154869  ...   -0.549204   -0.856123    0.714672   \n",
       "1     -0.028189 -0.077389 -0.095152  ...    0.927885   -0.817812    0.809631   \n",
       "2      0.076455 -0.003006 -0.032268  ...    0.941094   -0.912443    0.789828   \n",
       "3     -0.214788  0.045179 -0.506429  ...   -0.576739   -0.969558    0.916549   \n",
       "4     -0.050414 -0.033398  0.387791  ...    0.387301   -0.860696    0.678607   \n",
       "...         ...       ...       ...  ...         ...         ...         ...   \n",
       "47590  0.121190  0.129901 -0.983496  ...    0.500545    0.429651   -0.292929   \n",
       "47591  0.150939 -0.229323 -1.078991  ...    0.042979    0.134560   -0.356661   \n",
       "47592  0.045653  0.038912 -0.954113  ...    0.349853    0.370059   -0.144606   \n",
       "47593  0.916943  0.012306 -0.020316  ...   -0.354748   -0.083168    0.043640   \n",
       "47594  0.706517 -0.017584 -0.171057  ...   -0.544680   -0.046654    0.262865   \n",
       "\n",
       "       feature762  feature763  feature764  feature765  feature766  feature767  \\\n",
       "0       -0.046649   -0.894424   -0.001815    0.739485    0.015581   -0.023863   \n",
       "1       -0.005827   -0.848839    0.024516    0.526404   -0.039926   -0.102787   \n",
       "2        0.046979   -0.715636    0.085842    0.150494    0.025392   -0.066035   \n",
       "3       -0.080332   -0.927649   -0.047398    0.741663   -0.000096   -0.096318   \n",
       "4       -0.060695   -0.945793    0.040472    0.831079   -0.001711   -0.079842   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "47590   -0.464941   -0.740187    0.179149   -0.960807   -0.746958    1.069112   \n",
       "47591   -0.381828   -0.638338    0.077176   -0.788312   -0.683442    1.087031   \n",
       "47592   -0.493184   -0.655063    0.217335   -0.841272   -0.821077    1.036363   \n",
       "47593   -0.663565    0.543016   -0.652230   -1.427882   -0.985257    1.673561   \n",
       "47594   -0.767305    0.753788   -0.577503   -1.194910   -0.799556    1.519368   \n",
       "\n",
       "       feature768  \n",
       "0       -0.022002  \n",
       "1       -0.026980  \n",
       "2       -0.028283  \n",
       "3       -0.056501  \n",
       "4       -0.011189  \n",
       "...           ...  \n",
       "47590   -0.848182  \n",
       "47591   -0.593092  \n",
       "47592   -0.836614  \n",
       "47593    0.109659  \n",
       "47594    0.263210  \n",
       "\n",
       "[62045 rows x 769 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "644d0d56-23a9-40aa-b884-42cd998fca31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47595\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 100\n",
    "count = 1 / num_iterations\n",
    "\n",
    "# 总节点数\n",
    "num_nodes = data.x.shape[0]\n",
    "# 所有节点的索引\n",
    "all_indices = np.arange(num_nodes)\n",
    "mask_out = torch.ones(num_nodes, dtype=torch.bool)\n",
    "# 将测试集索引处的掩码设为False\n",
    "#mask_out[test_indices] = False\n",
    "end = len(combined_features) - 47595\n",
    "mask_out[-47595:] = False\n",
    "#mask_out[0:end] = False\n",
    "# 使用掩码获取剩余的索引\n",
    "remaining_indices = all_indices[mask_out]\n",
    "complement_mask = ~mask_out\n",
    "complement_mask[test_indices] = False\n",
    "\n",
    "# 得到既不在 remaining_indices 也不在 test_indices 中的索引\n",
    "complement_indices = all_indices[complement_mask]\n",
    "print(len(complement_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ee52309-76c8-4276-adfe-f3e095a31fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: 47595\n",
      "s: 0\n",
      "c: 47120\n",
      "s: 475\n",
      "c: 46645\n",
      "s: 475\n",
      "c: 46170\n",
      "s: 475\n",
      "c: 45695\n",
      "s: 475\n",
      "c: 45220\n",
      "s: 475\n",
      "c: 44745\n",
      "s: 475\n",
      "c: 44270\n",
      "s: 475\n",
      "c: 43795\n",
      "s: 475\n",
      "c: 43320\n",
      "s: 475\n",
      "c: 42845\n",
      "s: 475\n",
      "c: 42370\n",
      "s: 475\n",
      "c: 41895\n",
      "s: 475\n",
      "c: 41420\n",
      "s: 475\n",
      "c: 40945\n",
      "s: 475\n",
      "c: 40470\n",
      "s: 475\n",
      "c: 39995\n",
      "s: 475\n",
      "c: 39520\n",
      "s: 475\n",
      "c: 39045\n",
      "s: 475\n",
      "c: 38570\n",
      "s: 475\n",
      "c: 38095\n",
      "s: 475\n",
      "c: 37620\n",
      "s: 475\n",
      "c: 37145\n",
      "s: 475\n",
      "c: 36670\n",
      "s: 475\n",
      "c: 36195\n",
      "s: 475\n",
      "c: 35720\n",
      "s: 475\n",
      "c: 35245\n",
      "s: 475\n",
      "c: 34770\n",
      "s: 475\n",
      "c: 34295\n",
      "s: 475\n",
      "c: 33820\n",
      "s: 475\n",
      "c: 33345\n",
      "s: 475\n",
      "c: 32870\n",
      "s: 475\n",
      "c: 32395\n",
      "s: 475\n",
      "c: 31920\n",
      "s: 475\n",
      "c: 31445\n",
      "s: 475\n",
      "c: 30970\n",
      "s: 475\n",
      "c: 30495\n",
      "s: 475\n",
      "c: 30020\n",
      "s: 475\n",
      "c: 29545\n",
      "s: 475\n",
      "c: 29070\n",
      "s: 475\n",
      "c: 28595\n",
      "s: 475\n",
      "c: 28120\n",
      "s: 475\n",
      "c: 27645\n",
      "s: 475\n",
      "c: 27170\n",
      "s: 475\n",
      "c: 26695\n",
      "s: 475\n",
      "c: 26220\n",
      "s: 475\n",
      "c: 25745\n",
      "s: 475\n",
      "c: 25270\n",
      "s: 475\n",
      "c: 24795\n",
      "s: 475\n",
      "c: 24320\n",
      "s: 475\n",
      "c: 23845\n",
      "s: 475\n",
      "c: 23370\n",
      "s: 475\n",
      "c: 22895\n",
      "s: 475\n",
      "c: 22420\n",
      "s: 475\n",
      "c: 21945\n",
      "s: 475\n",
      "c: 21470\n",
      "s: 475\n",
      "c: 20995\n",
      "s: 475\n",
      "c: 20520\n",
      "s: 475\n",
      "c: 20045\n",
      "s: 475\n",
      "c: 19570\n",
      "s: 475\n",
      "c: 19095\n",
      "s: 475\n",
      "c: 18620\n",
      "s: 475\n",
      "c: 18145\n",
      "s: 475\n",
      "c: 17670\n",
      "s: 475\n",
      "c: 17195\n",
      "s: 475\n",
      "c: 16720\n",
      "s: 475\n",
      "c: 16245\n",
      "s: 475\n",
      "c: 15770\n",
      "s: 475\n",
      "c: 15295\n",
      "s: 475\n",
      "c: 14820\n",
      "s: 475\n",
      "c: 14345\n",
      "s: 475\n",
      "c: 13870\n",
      "s: 475\n",
      "c: 13395\n",
      "s: 475\n",
      "c: 12920\n",
      "s: 475\n",
      "c: 12445\n",
      "s: 475\n",
      "c: 11970\n",
      "s: 475\n",
      "c: 11495\n",
      "s: 475\n",
      "c: 11020\n",
      "s: 475\n",
      "c: 10545\n",
      "s: 475\n",
      "c: 10070\n",
      "s: 475\n",
      "c: 9595\n",
      "s: 475\n",
      "c: 9120\n",
      "s: 475\n",
      "c: 8645\n",
      "s: 475\n",
      "c: 8170\n",
      "s: 475\n",
      "c: 7695\n",
      "s: 475\n",
      "c: 7220\n",
      "s: 475\n",
      "c: 6745\n",
      "s: 475\n",
      "c: 6270\n",
      "s: 475\n",
      "c: 5795\n",
      "s: 475\n",
      "c: 5320\n",
      "s: 475\n",
      "c: 4845\n",
      "s: 475\n",
      "c: 4370\n",
      "s: 475\n",
      "c: 3895\n",
      "s: 475\n",
      "c: 3420\n",
      "s: 475\n",
      "c: 2945\n",
      "s: 475\n",
      "c: 2470\n",
      "s: 475\n",
      "c: 1995\n",
      "s: 475\n",
      "c: 1520\n",
      "s: 475\n",
      "c: 1045\n",
      "s: 475\n",
      "c: 570\n",
      "s: 475\n",
      "c: 95\n",
      "s: 475\n",
      "所有迭代的selected_indices和remaining_indices已保存到文件。\n"
     ]
    }
   ],
   "source": [
    "num_nodes_out = len(complement_indices)\n",
    "num_to_select = int(num_nodes_out * count)\n",
    "for i in range(num_iterations+1):\n",
    "    if i > 0:\n",
    "    # 随机选择节点\n",
    "        selected_indices = np.random.choice(complement_indices, num_to_select, replace=False)\n",
    "        # 更新剩余节点列表\n",
    "        complement_indices = np.setdiff1d(complement_indices, selected_indices)\n",
    "    else:\n",
    "        selected_indices = np.random.choice(complement_indices, 0, replace=False)\n",
    "        complement_indices = np.setdiff1d(complement_indices, selected_indices)\n",
    "    print(\"c:\",len(complement_indices))\n",
    "    # 保存到文件\n",
    "    print(\"s:\",len(selected_indices))\n",
    "    np.save(f'DIVIDED_DATA/GOwith_{i}%.npy', complement_indices)  # 修改保存路径\n",
    "\n",
    "print(\"所有迭代的selected_indices和remaining_indices已保存到文件。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d263fb2-4ce6-491d-9cfc-86bd2d633573",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"早停机制，用于在验证损失停止改善时终止训练。\"\"\"\n",
    "    def __init__(self, patience=200, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            patience (int): 损失没有改善的迭代次数，在这之后训练将会被停止。\n",
    "            verbose (bool): 如果为True，则打印一条消息表明早停被触发。\n",
    "            delta (float): 损失的最小改变，被认为是改善。\n",
    "            path (str): 最佳模型保存路径。\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    print(\"Early stopping triggered\")\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "    \n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        '''保存模型当验证损失减少时'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e0d15fa-c526-48af-858e-05cd2f8ea2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33960226, -0.03074448, -0.90138096, ...,  0.01558092,\n",
       "        -0.02386307, -0.02200161],\n",
       "       [-0.13179901, -0.02574519, -0.67730105, ..., -0.03992649,\n",
       "        -0.10278717, -0.02697964],\n",
       "       [ 0.38569278, -0.07069244, -0.8477959 , ...,  0.0253919 ,\n",
       "        -0.06603534, -0.02828273],\n",
       "       ...,\n",
       "       [ 0.02713387,  0.24139147, -0.22735251, ..., -0.82107705,\n",
       "         1.036363  , -0.83661443],\n",
       "       [ 0.13954346,  0.02888298,  0.89947975, ..., -0.9852566 ,\n",
       "         1.6735605 ,  0.10965873],\n",
       "       [ 0.08306409,  0.09089889,  0.8885408 , ..., -0.79955566,\n",
       "         1.5193683 ,  0.2632099 ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_features = features\n",
    "masked_features[torch.tensor(complement_indices)] = 0\n",
    "masked_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "552a9b32-9a2b-40e2-b069-dabe9c967c66",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Count_Category'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/python310/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Count_Category'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m highinfo_indices \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     node_id_to_index[node_id]\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node_id, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(labels_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotein\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43mlabels_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCount_Category\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m ]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(highinfo_indices)\n\u001b[1;32m      7\u001b[0m lowinfo_indices \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      8\u001b[0m     node_id_to_index[node_id]\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node_id, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(labels_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotein\u001b[39m\u001b[38;5;124m'\u001b[39m], labels_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCount_Category\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     11\u001b[0m ]\n",
      "File \u001b[0;32m~/.conda/envs/python310/lib/python3.10/site-packages/pandas/core/frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.conda/envs/python310/lib/python3.10/site-packages/pandas/core/indexes/base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3805\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3806\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3807\u001b[0m     ):\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Count_Category'"
     ]
    }
   ],
   "source": [
    "highinfo_indices = [\n",
    "    node_id_to_index[node_id]\n",
    "    for node_id, weight in zip(labels_df['protein'], labels_df['Count_Category'])\n",
    "    if weight == 0\n",
    "]\n",
    "print(highinfo_indices)\n",
    "lowinfo_indices = [\n",
    "    node_id_to_index[node_id]\n",
    "    for node_id, weight in zip(labels_df['protein'], labels_df['Count_Category'])\n",
    "    if weight == 1\n",
    "]\n",
    "print(lowinfo_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b938b01a-9cea-43d7-97e0-3f657bf55bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_indices = torch.tensor(highinfo_indices, dtype=torch.long)\n",
    "high_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "high_mask[high_indices] = True\n",
    "test_high = high_mask & test_mask\n",
    "low_indices = torch.tensor(lowinfo_indices, dtype=torch.long)\n",
    "low_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "low_mask[low_indices] = True\n",
    "test_low = low_mask & test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45ceaf3c-9724-4cf1-810d-27cd576c42c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/python310/lib/python3.10/site-packages/torch_geometric/data/data.py:297\u001b[0m, in \u001b[0;36mBaseData.to\u001b[0;34m(self, device, non_blocking, *args)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, device: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m], \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    294\u001b[0m        non_blocking: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    295\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Performs tensor device conversion, either for all attributes or\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03m    only the ones given in :obj:`*args`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/python310/lib/python3.10/site-packages/torch_geometric/data/data.py:280\u001b[0m, in \u001b[0;36mBaseData.apply\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies the function :obj:`func`, either to all attributes or only\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;03mthe ones given in :obj:`*args`.\"\"\"\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m store \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstores:\n\u001b[0;32m--> 280\u001b[0m     \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/python310/lib/python3.10/site-packages/torch_geometric/data/storage.py:191\u001b[0m, in \u001b[0;36mBaseStorage.apply\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies the function :obj:`func`, either to all attributes or only\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mthe ones given in :obj:`*args`.\"\"\"\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems(\u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28mself\u001b[39m[key] \u001b[38;5;241m=\u001b[39m \u001b[43mrecursive_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/python310/lib/python3.10/site-packages/torch_geometric/data/storage.py:743\u001b[0m, in \u001b[0;36mrecursive_apply\u001b[0;34m(data, func)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecursive_apply\u001b[39m(data: Any, func: Callable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Tensor):\n\u001b[0;32m--> 743\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mPackedSequence):\n\u001b[1;32m    745\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(data)\n",
      "File \u001b[0;32m~/.conda/envs/python310/lib/python3.10/site-packages/torch_geometric/data/data.py:298\u001b[0m, in \u001b[0;36mBaseData.to.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, device: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m], \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    294\u001b[0m        non_blocking: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    295\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Performs tensor device conversion, either for all attributes or\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03m    only the ones given in :obj:`*args`.\"\"\"\u001b[39;00m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m--> 298\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1')\n",
    "data = data.to(device)\n",
    "results = []\n",
    "num_epochs = 1000\n",
    "# 加载数据\n",
    "features = data.x\n",
    "labels = data.y # 根据你的数据加载函数进行调整\n",
    "num_iterations = 100\n",
    "original_features = features.clone()\n",
    "# 总共需要迭代的次数，这里以逐步增加5%为例，直到100%\n",
    "model = GCN(num_features=data.x.shape[1], hidden_dim=64, num_classes=3, num_layers=3, activation=F.relu, dropout=0.5)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=50, verbose=True)\n",
    "for i in range(num_iterations):\n",
    "    # 加载这次迭代的selected_indices和remaining_indices\n",
    "    selected_indices = np.load(f'DIVIDED_DATA/GOwith_{i}%.npy')\n",
    "    \n",
    "    # 根据selected_indices和remaining_indices调整特征\n",
    "    masked_features = features.clone()\n",
    "    masked_features[torch.tensor(selected_indices)] = 0  # 假设features是一个PyTorch tensor\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=200, verbose=True, delta=0.00001)\n",
    "    \n",
    "    if i > 0:\n",
    "        # 从上一个迭代保存的模型中加载参数\n",
    "        model.load_state_dict(torch.load(f'G-G_DATA/G-G_model_WITH{i-1}.pth'))\n",
    "        # 重新初始化优化器和调度器\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=100, verbose=True)\n",
    "    \n",
    "    # 模型训练和评估逻辑\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model_scheduler(model, masked_features, data.y, data.edge_index, optimizer, loss_fn, scheduler, train_mask)\n",
    "        test_f1, test_auc = evaluate_model(model, masked_features, data.y, data.edge_index, test_mask)\n",
    "        #high_f1, high_auc = evaluate_model(model, data.x, data.y, data.edge_index, test_high)\n",
    "        #low_f1, low_auc = evaluate_model(model, data.x, data.y, data.edge_index, test_low)\n",
    "        if epoch % 50 == 0:  # 每10个epoch打印一次信息\n",
    "            print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Macro_F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "        early_stopping(train_loss, model, i)\n",
    "        if early_stopping.early_stop or epoch == num_epochs - 1:\n",
    "            results.append({\n",
    "                            'Train Loss': train_loss,\n",
    "                            'F1': test_f1,\n",
    "                            'AUC_score': test_auc\n",
    "                        })\n",
    "                            #'highinfo F1': high_f1,                                                            \n",
    "                            #'highinfo AUC_score': high_auc,\n",
    "                            #'lowinfo F1': low_f1,\n",
    "                            #'lowinfo AUC_score': low_auc\n",
    "            print(\"acc save\")\n",
    "            rate = 1 - count * (i + 1)\n",
    "            rate = rate * 100\n",
    "            print(f'{rate}% node features transform to 0: F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "            torch.save(model.state_dict(), f'G-G_DATA/G-G_model_WITH{i}.pth')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0464b1d-95ba-4b93-b940-5ee747029211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Train Loss': 0.048501625657081604,\n",
       "  'F1': array(0.9383856, dtype=float32),\n",
       "  'AUC_score': 0.9723565272230397,\n",
       "  'highinfo F1': array(0.9319728, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9695980542754735,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.99375},\n",
       " {'Train Loss': 0.014602147974073887,\n",
       "  'F1': array(0.9383662, dtype=float32),\n",
       "  'AUC_score': 0.9751944502837924,\n",
       "  'highinfo F1': array(0.9359836, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9727342549923194,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.99375},\n",
       " {'Train Loss': 0.0157631766051054,\n",
       "  'F1': array(0.9455943, dtype=float32),\n",
       "  'AUC_score': 0.9753521126760564,\n",
       "  'highinfo F1': array(0.93596315, dtype=float32),\n",
       "  'highinfo AUC_score': 0.972990271377368,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.99375},\n",
       " {'Train Loss': 0.011848380789160728,\n",
       "  'F1': array(0.9455943, dtype=float32),\n",
       "  'AUC_score': 0.9747740172377547,\n",
       "  'highinfo F1': array(0.93596315, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9722862263184844,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.00813149381428957,\n",
       "  'F1': array(0.94198024, dtype=float32),\n",
       "  'AUC_score': 0.973617826361152,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9713261648745519,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.008575012907385826,\n",
       "  'F1': array(0.94198024, dtype=float32),\n",
       "  'AUC_score': 0.973460163968888,\n",
       "  'highinfo F1': array(0.9439964, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9712621607782898,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.0085398955270648,\n",
       "  'F1': array(0.9492327, dtype=float32),\n",
       "  'AUC_score': 0.9730922850536052,\n",
       "  'highinfo F1': array(0.9439964, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9708141321044547,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.0074961306527256966,\n",
       "  'F1': array(0.9455943, dtype=float32),\n",
       "  'AUC_score': 0.9743010300609628,\n",
       "  'highinfo F1': array(0.9359836, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9717101894521248,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.010487317107617855,\n",
       "  'F1': array(0.94198024, dtype=float32),\n",
       "  'AUC_score': 0.9737229346226615,\n",
       "  'highinfo F1': array(0.939999, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9720942140296979,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.01836857758462429,\n",
       "  'F1': array(0.9456172, dtype=float32),\n",
       "  'AUC_score': 0.9722514189615303,\n",
       "  'highinfo F1': array(0.939999, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9702380952380951,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.006203328259289265,\n",
       "  'F1': array(0.9456172, dtype=float32),\n",
       "  'AUC_score': 0.972461635484549,\n",
       "  'highinfo F1': array(0.939999, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9702380952380951,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.008200530894100666,\n",
       "  'F1': array(0.9456172, dtype=float32),\n",
       "  'AUC_score': 0.9725141896153038,\n",
       "  'highinfo F1': array(0.94399995, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9704301075268816,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.008705627173185349,\n",
       "  'F1': array(0.9456172, dtype=float32),\n",
       "  'AUC_score': 0.9730397309228505,\n",
       "  'highinfo F1': array(0.939999, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9705581157194061,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.006344633176922798,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9734076098381333,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.970878136200717,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.006442796438932419,\n",
       "  'F1': array(0.9492327, dtype=float32),\n",
       "  'AUC_score': 0.9729346226613412,\n",
       "  'highinfo F1': array(0.939999, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9699820788530464,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.006564098875969648,\n",
       "  'F1': array(0.9456172, dtype=float32),\n",
       "  'AUC_score': 0.9731973933151145,\n",
       "  'highinfo F1': array(0.94399995, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9706861239119303,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.008569044061005116,\n",
       "  'F1': array(0.9492327, dtype=float32),\n",
       "  'AUC_score': 0.9726718520075678,\n",
       "  'highinfo F1': array(0.939999, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9703661034306196,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.005988868419080973,\n",
       "  'F1': array(0.9455943, dtype=float32),\n",
       "  'AUC_score': 0.973617826361152,\n",
       "  'highinfo F1': array(0.9439964, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9716461853558628,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.00638858787715435,\n",
       "  'F1': array(0.9492327, dtype=float32),\n",
       "  'AUC_score': 0.9724616354845491,\n",
       "  'highinfo F1': array(0.9439964, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9697260624679979,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.008728536777198315,\n",
       "  'F1': array(0.9456172, dtype=float32),\n",
       "  'AUC_score': 0.9715682152617195,\n",
       "  'highinfo F1': array(0.94399995, dtype=float32),\n",
       "  'highinfo AUC_score': 0.969342037890425,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.007896599359810352,\n",
       "  'F1': array(0.9455943, dtype=float32),\n",
       "  'AUC_score': 0.9725141896153038,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9698540706605222,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.007329126354306936,\n",
       "  'F1': array(0.9455943, dtype=float32),\n",
       "  'AUC_score': 0.972146310700021,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9698540706605222,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.006743739824742079,\n",
       "  'F1': array(0.94198024, dtype=float32),\n",
       "  'AUC_score': 0.971988648307757,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9695340501792113,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.00573302898555994,\n",
       "  'F1': array(0.94198024, dtype=float32),\n",
       "  'AUC_score': 0.9716207693924741,\n",
       "  'highinfo F1': array(0.939999, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9692140296979006,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.0042353468015789986,\n",
       "  'F1': array(0.94198024, dtype=float32),\n",
       "  'AUC_score': 0.9714631070002102,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9692780337941628,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.009465789422392845,\n",
       "  'F1': array(0.94198024, dtype=float32),\n",
       "  'AUC_score': 0.9724616354845491,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9697900665642601,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.006090280134230852,\n",
       "  'F1': array(0.94198024, dtype=float32),\n",
       "  'AUC_score': 0.9720412024385117,\n",
       "  'highinfo F1': array(0.939999, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9694700460829493,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.0035202570725232363,\n",
       "  'F1': array(0.9455943, dtype=float32),\n",
       "  'AUC_score': 0.9713579987387009,\n",
       "  'highinfo F1': array(0.939999, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9694060419866871,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.003748140297830105,\n",
       "  'F1': array(0.9455943, dtype=float32),\n",
       "  'AUC_score': 0.9720412024385118,\n",
       "  'highinfo F1': array(0.939999, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9700460829493087,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.006270322483032942,\n",
       "  'F1': array(0.9455943, dtype=float32),\n",
       "  'AUC_score': 0.9721988648307757,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9699820788530465,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.013050815090537071,\n",
       "  'F1': array(0.9456172, dtype=float32),\n",
       "  'AUC_score': 0.9720412024385117,\n",
       "  'highinfo F1': array(0.94399995, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9695340501792115,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.00446980819106102,\n",
       "  'F1': array(0.9455943, dtype=float32),\n",
       "  'AUC_score': 0.9714631070002102,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9691500256016384,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.004669506568461657,\n",
       "  'F1': array(0.9455943, dtype=float32),\n",
       "  'AUC_score': 0.9723565272230398,\n",
       "  'highinfo F1': array(0.939999, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9699820788530465,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.007161194458603859,\n",
       "  'F1': array(0.9455943, dtype=float32),\n",
       "  'AUC_score': 0.9720937565692663,\n",
       "  'highinfo F1': array(0.939999, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9699820788530465,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.0038114977069199085,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.971830985915493,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9696620583717358,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.004356833640486002,\n",
       "  'F1': array(0.9528682, dtype=float32),\n",
       "  'AUC_score': 0.9729346226613412,\n",
       "  'highinfo F1': array(0.9479991, dtype=float32),\n",
       "  'highinfo AUC_score': 0.970942140296979,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.006260572001338005,\n",
       "  'F1': array(0.9455943, dtype=float32),\n",
       "  'AUC_score': 0.972146310700021,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9699180747567844,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.013119111768901348,\n",
       "  'F1': array(0.9456172, dtype=float32),\n",
       "  'AUC_score': 0.971673323523229,\n",
       "  'highinfo F1': array(0.94399995, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9692140296979007,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.004725159611552954,\n",
       "  'F1': array(0.9383662, dtype=float32),\n",
       "  'AUC_score': 0.9714631070002102,\n",
       "  'highinfo F1': array(0.939999, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9690220174091141,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.004228639882057905,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.972146310700021,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9699820788530465,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.004376186989247799,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.972146310700021,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9699180747567845,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.02169245108962059,\n",
       "  'F1': array(0.94198024, dtype=float32),\n",
       "  'AUC_score': 0.971830985915493,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9690860215053764,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.005031086970120668,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9718835400462477,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9693420378904251,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.011648783460259438,\n",
       "  'F1': array(0.9455943, dtype=float32),\n",
       "  'AUC_score': 0.9715156611309649,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9692780337941628,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.008490934036672115,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9720412024385117,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.969406041986687,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.0053621865808963776,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9716733235232289,\n",
       "  'highinfo F1': array(0.939999, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9692140296979006,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.0038164369761943817,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9715682152617195,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.968958013312852,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.004337175749242306,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.971830985915493,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9695340501792115,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.017881013453006744,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.971830985915493,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.96889400921659,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.004404393490403891,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9715682152617197,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9688300051203277,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.006941713858395815,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9716207693924742,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9690860215053764,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.004647756926715374,\n",
       "  'F1': array(0.9455943, dtype=float32),\n",
       "  'AUC_score': 0.9721463107000211,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9698540706605223,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.004871751647442579,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9712528904771917,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9691500256016384,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.0045300861820578575,\n",
       "  'F1': array(0.9455943, dtype=float32),\n",
       "  'AUC_score': 0.9712528904771915,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.968958013312852,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.0035952632315456867,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9716207693924742,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9692140296979006,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.004668398294597864,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9713579987387009,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9688300051203276,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.005077254958450794,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.971830985915493,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.969342037890425,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.0042219809256494045,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9718835400462477,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.969342037890425,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.006068042479455471,\n",
       "  'F1': array(0.9383662, dtype=float32),\n",
       "  'AUC_score': 0.9720937565692664,\n",
       "  'highinfo F1': array(0.939999, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9695340501792113,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.005671659950166941,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9712528904771915,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.968573988735279,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.005777330137789249,\n",
       "  'F1': array(0.9455943, dtype=float32),\n",
       "  'AUC_score': 0.9717258776539835,\n",
       "  'highinfo F1': array(0.939999, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9695980542754736,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.004440102726221085,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9710952280849274,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9688940092165897,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.013019670732319355,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9713579987387009,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9690220174091142,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.005477328784763813,\n",
       "  'F1': array(0.9455943, dtype=float32),\n",
       "  'AUC_score': 0.9706747950388901,\n",
       "  'highinfo F1': array(0.939999, dtype=float32),\n",
       "  'highinfo AUC_score': 0.968189964157706,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.01736404374241829,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9709375656926634,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9685739887352789,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.003489707363769412,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9715682152617195,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9693420378904251,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.0036046328023076057,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9717258776539837,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9696620583717357,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.003308625426143408,\n",
       "  'F1': array(0.9383402, dtype=float32),\n",
       "  'AUC_score': 0.9715156611309649,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9692140296979006,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.007647837046533823,\n",
       "  'F1': array(0.9420016, dtype=float32),\n",
       "  'AUC_score': 0.9715156611309649,\n",
       "  'highinfo F1': array(0.936, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9690860215053763,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.005256914999336004,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9708850115619089,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9687660010240655,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.006204094737768173,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9711477822156823,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9687019969278033,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.0035090232267975807,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9713579987387009,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9690220174091141,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.004916235338896513,\n",
       "  'F1': array(0.9455943, dtype=float32),\n",
       "  'AUC_score': 0.9711477822156822,\n",
       "  'highinfo F1': array(0.939999, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9690860215053763,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.003085617907345295,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.971357998738701,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9693420378904248,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.004249453544616699,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9716207693924742,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9693420378904251,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.005331501364707947,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9715682152617198,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9695340501792115,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.005018316674977541,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9714631070002102,\n",
       "  'highinfo F1': array(0.939999, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9693420378904251,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.004032742232084274,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9709375656926633,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9688940092165899,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.002992543624714017,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9710426739541729,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9687019969278035,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.005004119127988815,\n",
       "  'F1': array(0.9455943, dtype=float32),\n",
       "  'AUC_score': 0.9708850115619087,\n",
       "  'highinfo F1': array(0.9439964, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9688300051203277,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.0058918003924191,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9708324574311542,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9683819764464925,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.004268788732588291,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9708850115619087,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9686379928315412,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.0036591868847608566,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9712003363464369,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9687660010240655,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.011950154788792133,\n",
       "  'F1': array(0.9455943, dtype=float32),\n",
       "  'AUC_score': 0.9710426739541728,\n",
       "  'highinfo F1': array(0.9439964, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9688940092165899,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.0034927718807011843,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.970832457431154,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9687019969278035,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.0051217093132436275,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9712528904771915,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9691500256016384,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.0030389504972845316,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9712003363464369,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.96889400921659,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.0062034847214818,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9714631070002102,\n",
       "  'highinfo F1': array(0.93599594, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9692780337941628,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.00793453585356474,\n",
       "  'F1': array(0.9455943, dtype=float32),\n",
       "  'AUC_score': 0.9713579987387009,\n",
       "  'highinfo F1': array(0.9439964, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9693420378904251,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.0029214632231742144,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9716207693924743,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9694060419866872,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.004626695066690445,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9710952280849274,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.96889400921659,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.003156043356284499,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9709901198234181,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9690220174091141,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.004353412892669439,\n",
       "  'F1': array(0.9455943, dtype=float32),\n",
       "  'AUC_score': 0.9709375656926635,\n",
       "  'highinfo F1': array(0.9439964, dtype=float32),\n",
       "  'highinfo AUC_score': 0.968958013312852,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.005310930777341127,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9714631070002102,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9693420378904251,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9812500000000001},\n",
       " {'Train Loss': 0.004763654433190823,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9710426739541728,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.968958013312852,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.0031544819939881563,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9714631070002103,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9694060419866871,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.0034589495044201612,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9708850115619088,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.968573988735279,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.010659310966730118,\n",
       "  'F1': array(0.9455657, dtype=float32),\n",
       "  'AUC_score': 0.9708850115619088,\n",
       "  'highinfo F1': array(0.9439857, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9687019969278032,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.0039732432924211025,\n",
       "  'F1': array(0.9383662, dtype=float32),\n",
       "  'AUC_score': 0.9707799033003994,\n",
       "  'highinfo F1': array(0.936, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9687660010240655,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875},\n",
       " {'Train Loss': 0.0040395078249275684,\n",
       "  'F1': array(0.94195276, dtype=float32),\n",
       "  'AUC_score': 0.9711477822156821,\n",
       "  'highinfo F1': array(0.93999135, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9691500256016385,\n",
       "  'lowinfo F1': array(0.9585327, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9875}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1018d45a-5396-418c-88cd-e4ed6f3d7c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from numpy import array, float32\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, np.generic):\n",
    "            return obj.item()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "\n",
    "# 保存为 JSON 文件\n",
    "with open('GNN/zeroed_protein.json', 'w') as f:\n",
    "    json.dump(results, f, cls=NumpyEncoder, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f566442-f377-4b52-950e-4158ff83a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from numpy import array, float32\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, np.generic):\n",
    "            return obj.item()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "results = [{'Train Loss': 0.5224899053573608,\n",
    "  'F1': array(0.75060236, dtype=float32),\n",
    "  'AUC_score': 0.8627626050420167,\n",
    "  'highinfo F1': array(0.6002752, dtype=float32),\n",
    "  'highinfo AUC_score': 0.8030367231638418,\n",
    "  'lowinfo F1': array(0.73389363, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.8352272727272727},\n",
    " {'Train Loss': 0.4383935034275055,\n",
    "  'F1': array(0.851213, dtype=float32),\n",
    "  'AUC_score': 0.8709558823529412,\n",
    "  'highinfo F1': array(0.6882392, dtype=float32),\n",
    "  'highinfo AUC_score': 0.8238700564971752,\n",
    "  'lowinfo F1': array(0.73389363, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.8380681818181818},\n",
    " {'Train Loss': 0.40343186259269714,\n",
    "  'F1': array(0.85356534, dtype=float32),\n",
    "  'AUC_score': 0.8606617647058823,\n",
    "  'highinfo F1': array(0.73701656, dtype=float32),\n",
    "  'highinfo AUC_score': 0.846680790960452,\n",
    "  'lowinfo F1': array(0.8683299, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.8352272727272727},\n",
    " {'Train Loss': 0.3463888168334961,\n",
    "  'F1': array(0.8386737, dtype=float32),\n",
    "  'AUC_score': 0.8164915966386553,\n",
    "  'highinfo F1': array(0.79359686, dtype=float32),\n",
    "  'highinfo AUC_score': 0.8819915254237287,\n",
    "  'lowinfo F1': array(0.8683299, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.8636363636363636},\n",
    " {'Train Loss': 0.19426140189170837,\n",
    "  'F1': array(0.8623116, dtype=float32),\n",
    "  'AUC_score': 0.9043592436974791,\n",
    "  'highinfo F1': array(0.80534816, dtype=float32),\n",
    "  'highinfo AUC_score': 0.8737994350282486,\n",
    "  'lowinfo F1': array(0.78711486, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.8664772727272727},\n",
    " {'Train Loss': 0.12317196279764175,\n",
    "  'F1': array(0.87305343, dtype=float32),\n",
    "  'AUC_score': 0.9067752100840335,\n",
    "  'highinfo F1': array(0.8058452, dtype=float32),\n",
    "  'highinfo AUC_score': 0.8775423728813558,\n",
    "  'lowinfo F1': array(0.7548387, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.8522727272727273},\n",
    " {'Train Loss': 0.12100005149841309,\n",
    "  'F1': array(0.87305343, dtype=float32),\n",
    "  'AUC_score': 0.9341911764705881,\n",
    "  'highinfo F1': array(0.85281384, dtype=float32),\n",
    "  'highinfo AUC_score': 0.8845338983050847,\n",
    "  'lowinfo F1': array(0.6933236, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.8693181818181819},\n",
    " {'Train Loss': 0.09250399470329285,\n",
    "  'F1': array(0.8838384, dtype=float32),\n",
    "  'AUC_score': 0.9196953781512606,\n",
    "  'highinfo F1': array(0.84440327, dtype=float32),\n",
    "  'highinfo AUC_score': 0.8834039548022599,\n",
    "  'lowinfo F1': array(0.809319, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.8806818181818182},\n",
    " {'Train Loss': 0.07261261343955994,\n",
    "  'F1': array(0.8874314, dtype=float32),\n",
    "  'AUC_score': 0.9243172268907563,\n",
    "  'highinfo F1': array(0.8440726, dtype=float32),\n",
    "  'highinfo AUC_score': 0.8745056497175141,\n",
    "  'lowinfo F1': array(0.7840909, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9204545454545454},\n",
    " {'Train Loss': 0.06745654344558716,\n",
    "  'F1': array(0.87281275, dtype=float32),\n",
    "  'AUC_score': 0.9169642857142858,\n",
    "  'highinfo F1': array(0.8440726, dtype=float32),\n",
    "  'highinfo AUC_score': 0.8765536723163843,\n",
    "  'lowinfo F1': array(0.809319, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9090909090909092},\n",
    " {'Train Loss': 0.0410689115524292,\n",
    "  'F1': array(0.88403356, dtype=float32),\n",
    "  'AUC_score': 0.9373949579831933,\n",
    "  'highinfo F1': array(0.84023464, dtype=float32),\n",
    "  'highinfo AUC_score': 0.8760593220338981,\n",
    "  'lowinfo F1': array(0.7797102, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9034090909090909},\n",
    " {'Train Loss': 0.036087535321712494,\n",
    "  'F1': array(0.87305343, dtype=float32),\n",
    "  'AUC_score': 0.934453781512605,\n",
    "  'highinfo F1': array(0.82772803, dtype=float32),\n",
    "  'highinfo AUC_score': 0.8771186440677966,\n",
    "  'lowinfo F1': array(0.80484223, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.8977272727272727},\n",
    " {'Train Loss': 0.02374139428138733,\n",
    "  'F1': array(0.8839605, dtype=float32),\n",
    "  'AUC_score': 0.9375,\n",
    "  'highinfo F1': array(0.8191713, dtype=float32),\n",
    "  'highinfo AUC_score': 0.8713983050847458,\n",
    "  'lowinfo F1': array(0.77380955, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.8863636363636364},\n",
    " {'Train Loss': 0.017087901011109352,\n",
    "  'F1': array(0.8912815, dtype=float32),\n",
    "  'AUC_score': 0.9467962184873949,\n",
    "  'highinfo F1': array(0.8150084, dtype=float32),\n",
    "  'highinfo AUC_score': 0.8687853107344633,\n",
    "  'lowinfo F1': array(0.80484223, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.8636363636363636},\n",
    " {'Train Loss': 0.024169573560357094,\n",
    "  'F1': array(0.8912815, dtype=float32),\n",
    "  'AUC_score': 0.9478991596638655,\n",
    "  'highinfo F1': array(0.81065357, dtype=float32),\n",
    "  'highinfo AUC_score': 0.8838276836158192,\n",
    "  'lowinfo F1': array(0.8303571, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.934659090909091},\n",
    " {'Train Loss': 0.015433990396559238,\n",
    "  'F1': array(0.90211093, dtype=float32),\n",
    "  'AUC_score': 0.9574579831932772,\n",
    "  'highinfo F1': array(0.8235294, dtype=float32),\n",
    "  'highinfo AUC_score': 0.8870056497175142,\n",
    "  'lowinfo F1': array(0.8303571, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9261363636363636},\n",
    " {'Train Loss': 0.009460334666073322,\n",
    "  'F1': array(0.8948599, dtype=float32),\n",
    "  'AUC_score': 0.9555672268907562,\n",
    "  'highinfo F1': array(0.81507385, dtype=float32),\n",
    "  'highinfo AUC_score': 0.8829802259887005,\n",
    "  'lowinfo F1': array(0.8303571, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9204545454545455},\n",
    " {'Train Loss': 0.013440035283565521,\n",
    "  'F1': array(0.8984174, dtype=float32),\n",
    "  'AUC_score': 0.9652836134453782,\n",
    "  'highinfo F1': array(0.8193245, dtype=float32),\n",
    "  'highinfo AUC_score': 0.8994350282485876,\n",
    "  'lowinfo F1': array(0.8303571, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9573863636363636},\n",
    " {'Train Loss': 0.015707021579146385,\n",
    "  'F1': array(0.891253, dtype=float32),\n",
    "  'AUC_score': 0.9648109243697479,\n",
    "  'highinfo F1': array(0.82758194, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9034604519774011,\n",
    "  'lowinfo F1': array(0.856387, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9517045454545454},\n",
    " {'Train Loss': 0.009879552759230137,\n",
    "  'F1': array(0.891253, dtype=float32),\n",
    "  'AUC_score': 0.9627100840336135,\n",
    "  'highinfo F1': array(0.8191713, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9083333333333332,\n",
    "  'lowinfo F1': array(0.8869048, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9517045454545454},\n",
    " {'Train Loss': 0.015236727893352509,\n",
    "  'F1': array(0.8839605, dtype=float32),\n",
    "  'AUC_score': 0.9688025210084034,\n",
    "  'highinfo F1': array(0.8230797, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9167372881355932,\n",
    "  'lowinfo F1': array(0.8869048, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9460227272727273},\n",
    " {'Train Loss': 0.006653571501374245,\n",
    "  'F1': array(0.8839605, dtype=float32),\n",
    "  'AUC_score': 0.9716386554621848,\n",
    "  'highinfo F1': array(0.83174264, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9231638418079094,\n",
    "  'lowinfo F1': array(0.8869048, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9232954545454546},\n",
    " {'Train Loss': 0.008208495564758778,\n",
    "  'F1': array(0.891253, dtype=float32),\n",
    "  'AUC_score': 0.9621848739495799,\n",
    "  'highinfo F1': array(0.8613225, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9225282485875705,\n",
    "  'lowinfo F1': array(0.8869048, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9318181818181819},\n",
    " {'Train Loss': 0.014964545145630836,\n",
    "  'F1': array(0.89121294, dtype=float32),\n",
    "  'AUC_score': 0.9676470588235294,\n",
    "  'highinfo F1': array(0.8655367, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9269067796610169,\n",
    "  'lowinfo F1': array(0.8869048, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9346590909090909},\n",
    " {'Train Loss': 0.00846161600202322,\n",
    "  'F1': array(0.89129865, dtype=float32),\n",
    "  'AUC_score': 0.9709558823529412,\n",
    "  'highinfo F1': array(0.86546075, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9355225988700565,\n",
    "  'lowinfo F1': array(0.8869048, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9545454545454546},\n",
    " {'Train Loss': 0.0107728261500597,\n",
    "  'F1': array(0.90940964, dtype=float32),\n",
    "  'AUC_score': 0.9722163865546218,\n",
    "  'highinfo F1': array(0.8739407, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9334039548022598,\n",
    "  'lowinfo F1': array(0.8869048, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9375},\n",
    " {'Train Loss': 0.00939225871115923,\n",
    "  'F1': array(0.8985294, dtype=float32),\n",
    "  'AUC_score': 0.9776785714285715,\n",
    "  'highinfo F1': array(0.8697456, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9329802259887006,\n",
    "  'lowinfo F1': array(0.8869048, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9204545454545454},\n",
    " {'Train Loss': 0.011036264710128307,\n",
    "  'F1': array(0.9383662, dtype=float32),\n",
    "  'AUC_score': 0.9851890756302522,\n",
    "  'highinfo F1': array(0.8863921, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9401129943502825,\n",
    "  'lowinfo F1': array(0.94602275, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9886363636363636},\n",
    " {'Train Loss': 0.005777299404144287,\n",
    "  'F1': array(0.9057921, dtype=float32),\n",
    "  'AUC_score': 0.9804621848739495,\n",
    "  'highinfo F1': array(0.8865526, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9319209039548023,\n",
    "  'lowinfo F1': array(0.8869048, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9545454545454546},\n",
    " {'Train Loss': 0.01656293496489525,\n",
    "  'F1': array(0.91303897, dtype=float32),\n",
    "  'AUC_score': 0.9846113445378152,\n",
    "  'highinfo F1': array(0.8781491, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9305790960451977,\n",
    "  'lowinfo F1': array(0.8869048, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9602272727272727},\n",
    " {'Train Loss': 0.009177271276712418,\n",
    "  'F1': array(0.9164814, dtype=float32),\n",
    "  'AUC_score': 0.9888655462184874,\n",
    "  'highinfo F1': array(0.8823197, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9409604519774011,\n",
    "  'lowinfo F1': array(0.8606016, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.96875},\n",
    " {'Train Loss': 0.005756791681051254,\n",
    "  'F1': array(0.9092189, dtype=float32),\n",
    "  'AUC_score': 0.9821953781512606,\n",
    "  'highinfo F1': array(0.86974555, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9273305084745763,\n",
    "  'lowinfo F1': array(0.8869048, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.96875},\n",
    " {'Train Loss': 0.005266912747174501,\n",
    "  'F1': array(0.92013884, dtype=float32),\n",
    "  'AUC_score': 0.9841911764705882,\n",
    "  'highinfo F1': array(0.8823446, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9396892655367232,\n",
    "  'lowinfo F1': array(0.8606016, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9857954545454546},\n",
    " {'Train Loss': 0.00484166806563735,\n",
    "  'F1': array(0.916613, dtype=float32),\n",
    "  'AUC_score': 0.9815126050420168,\n",
    "  'highinfo F1': array(0.87372667, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9331920903954802,\n",
    "  'lowinfo F1': array(0.856387, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9403409090909091},\n",
    " {'Train Loss': 0.007429955527186394,\n",
    "  'F1': array(0.9420259, dtype=float32),\n",
    "  'AUC_score': 0.979936974789916,\n",
    "  'highinfo F1': array(0.88205314, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9442796610169492,\n",
    "  'lowinfo F1': array(0.856387, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9914772727272727},\n",
    " {'Train Loss': 0.006036914419382811,\n",
    "  'F1': array(0.94198024, dtype=float32),\n",
    "  'AUC_score': 0.9790966386554623,\n",
    "  'highinfo F1': array(0.89063275, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9502824858757062,\n",
    "  'lowinfo F1': array(0.856387, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9886363636363636},\n",
    " {'Train Loss': 0.00476914132013917,\n",
    "  'F1': array(0.94198024, dtype=float32),\n",
    "  'AUC_score': 0.9795693277310925,\n",
    "  'highinfo F1': array(0.8863922, dtype=float32),\n",
    "  'highinfo AUC_score': 0.946186440677966,\n",
    "  'lowinfo F1': array(0.856387, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9573863636363636},\n",
    " {'Train Loss': 0.00859004259109497,\n",
    "  'F1': array(0.92747533, dtype=float32),\n",
    "  'AUC_score': 0.9737394957983194,\n",
    "  'highinfo F1': array(0.8905631, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9392655367231637,\n",
    "  'lowinfo F1': array(0.856387, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9431818181818181},\n",
    " {'Train Loss': 0.007178982254117727,\n",
    "  'F1': array(0.9383662, dtype=float32),\n",
    "  'AUC_score': 0.9773109243697479,\n",
    "  'highinfo F1': array(0.8905631, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9422316384180791,\n",
    "  'lowinfo F1': array(0.856387, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9460227272727273},\n",
    " {'Train Loss': 0.0034060280304402113,\n",
    "  'F1': array(0.92386407, dtype=float32),\n",
    "  'AUC_score': 0.9785189075630252,\n",
    "  'highinfo F1': array(0.88214487, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9354519774011298,\n",
    "  'lowinfo F1': array(0.856387, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9318181818181819},\n",
    " {'Train Loss': 0.005657465197145939,\n",
    "  'F1': array(0.9419527, dtype=float32),\n",
    "  'AUC_score': 0.9758928571428571,\n",
    "  'highinfo F1': array(0.9032229, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9351694915254237,\n",
    "  'lowinfo F1': array(0.916361, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9488636363636364},\n",
    " {'Train Loss': 0.0033094389364123344,\n",
    "  'F1': array(0.9383662, dtype=float32),\n",
    "  'AUC_score': 0.9775210084033613,\n",
    "  'highinfo F1': array(0.8733773, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9499293785310734,\n",
    "  'lowinfo F1': array(0.856387, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9431818181818182},\n",
    " {'Train Loss': 0.006701362784951925,\n",
    "  'F1': array(0.952893, dtype=float32),\n",
    "  'AUC_score': 0.98125,\n",
    "  'highinfo F1': array(0.86891246, dtype=float32),\n",
    "  'highinfo AUC_score': 0.943361581920904,\n",
    "  'lowinfo F1': array(0.856387, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9460227272727273},\n",
    " {'Train Loss': 0.007686189375817776,\n",
    "  'F1': array(0.9492647, dtype=float32),\n",
    "  'AUC_score': 0.9794117647058823,\n",
    "  'highinfo F1': array(0.8775264, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9475988700564972,\n",
    "  'lowinfo F1': array(0.856387, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9431818181818182},\n",
    " {'Train Loss': 0.0054614790715277195,\n",
    "  'F1': array(0.95284843, dtype=float32),\n",
    "  'AUC_score': 0.9777836134453781,\n",
    "  'highinfo F1': array(0.93696475, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9639830508474576,\n",
    "  'lowinfo F1': array(0.9449276, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9517045454545454},\n",
    " {'Train Loss': 0.002809555269777775,\n",
    "  'F1': array(0.9383662, dtype=float32),\n",
    "  'AUC_score': 0.9788340336134453,\n",
    "  'highinfo F1': array(0.9033187, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9609463276836159,\n",
    "  'lowinfo F1': array(0.9449276, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9488636363636364},\n",
    " {'Train Loss': 0.0031079419422894716,\n",
    "  'F1': array(0.9565195, dtype=float32),\n",
    "  'AUC_score': 0.9790966386554621,\n",
    "  'highinfo F1': array(0.8945391, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9519774011299436,\n",
    "  'lowinfo F1': array(0.916361, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9460227272727273},\n",
    " {'Train Loss': 0.0022691311314702034,\n",
    "  'F1': array(0.9565012, dtype=float32),\n",
    "  'AUC_score': 0.9787289915966385,\n",
    "  'highinfo F1': array(0.890377, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9548022598870056,\n",
    "  'lowinfo F1': array(0.8869048, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9403409090909091},\n",
    " {'Train Loss': 0.002626045374199748,\n",
    "  'F1': array(0.9347278, dtype=float32),\n",
    "  'AUC_score': 0.9762605042016806,\n",
    "  'highinfo F1': array(0.9073996, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9652542372881356,\n",
    "  'lowinfo F1': array(0.9449276, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9602272727272727},\n",
    " {'Train Loss': 0.01435729581862688,\n",
    "  'F1': array(0.9347278, dtype=float32),\n",
    "  'AUC_score': 0.9902836134453781,\n",
    "  'highinfo F1': array(0.93276834, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9764124293785311,\n",
    "  'lowinfo F1': array(0.9449276, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9488636363636364},\n",
    " {'Train Loss': 0.002415803261101246,\n",
    "  'F1': array(0.9347278, dtype=float32),\n",
    "  'AUC_score': 0.9797268907563026,\n",
    "  'highinfo F1': array(0.9075042, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9660310734463275,\n",
    "  'lowinfo F1': array(0.9449276, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9488636363636364},\n",
    " {'Train Loss': 0.007185796741396189,\n",
    "  'F1': array(0.9565126, dtype=float32),\n",
    "  'AUC_score': 0.9816176470588236,\n",
    "  'highinfo F1': array(0.9115007, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9676553672316383,\n",
    "  'lowinfo F1': array(0.9449276, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9715909090909091},\n",
    " {'Train Loss': 0.002636982360854745,\n",
    "  'F1': array(0.9492514, dtype=float32),\n",
    "  'AUC_score': 0.9800945378151261,\n",
    "  'highinfo F1': array(0.9074585, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9699858757062149,\n",
    "  'lowinfo F1': array(0.9449276, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9630681818181819},\n",
    " {'Train Loss': 0.006525544915348291,\n",
    "  'F1': array(0.93104994, dtype=float32),\n",
    "  'AUC_score': 0.9751050420168067,\n",
    "  'highinfo F1': array(0.93276834, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9687146892655368,\n",
    "  'lowinfo F1': array(0.9449276, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9943181818181819},\n",
    " {'Train Loss': 0.004266633652150631,\n",
    "  'F1': array(0.9456172, dtype=float32),\n",
    "  'AUC_score': 0.9796743697478991,\n",
    "  'highinfo F1': array(0.9158713, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9700564971751413,\n",
    "  'lowinfo F1': array(0.9449276, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9715909090909091},\n",
    " {'Train Loss': 0.006280777510255575,\n",
    "  'F1': array(0.9383402, dtype=float32),\n",
    "  'AUC_score': 0.9738445378151261,\n",
    "  'highinfo F1': array(0.92434835, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9697033898305085,\n",
    "  'lowinfo F1': array(0.9449276, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.96875},\n",
    " {'Train Loss': 0.006153980270028114,\n",
    "  'F1': array(0.93469685, dtype=float32),\n",
    "  'AUC_score': 0.9714810924369748,\n",
    "  'highinfo F1': array(0.9243644, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9642655367231638,\n",
    "  'lowinfo F1': array(0.9449276, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9744318181818182},\n",
    " {'Train Loss': 0.0018373191123828292,\n",
    "  'F1': array(0.9347278, dtype=float32),\n",
    "  'AUC_score': 0.9767857142857143,\n",
    "  'highinfo F1': array(0.9243644, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9708333333333332,\n",
    "  'lowinfo F1': array(0.9449276, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9659090909090909},\n",
    " {'Train Loss': 0.0019959257915616035,\n",
    "  'F1': array(0.9347278, dtype=float32),\n",
    "  'AUC_score': 0.9727415966386554,\n",
    "  'highinfo F1': array(0.92434835, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9734463276836158,\n",
    "  'lowinfo F1': array(0.9449276, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9545454545454546},\n",
    " {'Train Loss': 0.002284497953951359,\n",
    "  'F1': array(0.9492514, dtype=float32),\n",
    "  'AUC_score': 0.971218487394958,\n",
    "  'highinfo F1': array(0.920099, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9686440677966102,\n",
    "  'lowinfo F1': array(0.916361, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9971590909090909},\n",
    " {'Train Loss': 0.002086743712425232,\n",
    "  'F1': array(0.9347278, dtype=float32),\n",
    "  'AUC_score': 0.9723739495798319,\n",
    "  'highinfo F1': array(0.9243644, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9646892655367232,\n",
    "  'lowinfo F1': array(0.9449276, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9914772727272727},\n",
    " {'Train Loss': 0.003558192867785692,\n",
    "  'F1': array(0.9383662, dtype=float32),\n",
    "  'AUC_score': 0.9651260504201681,\n",
    "  'highinfo F1': array(0.91168827, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9682203389830508,\n",
    "  'lowinfo F1': array(0.916361, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9971590909090909},\n",
    " {'Train Loss': 0.0023635737597942352,\n",
    "  'F1': array(0.94198024, dtype=float32),\n",
    "  'AUC_score': 0.9722163865546218,\n",
    "  'highinfo F1': array(0.92013276, dtype=float32),\n",
    "  'highinfo AUC_score': 0.971186440677966,\n",
    "  'lowinfo F1': array(0.9449276, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9886363636363636},\n",
    " {'Train Loss': 0.002227094955742359,\n",
    "  'F1': array(0.9456172, dtype=float32),\n",
    "  'AUC_score': 0.9698529411764706,\n",
    "  'highinfo F1': array(0.92013276, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9704096045197741,\n",
    "  'lowinfo F1': array(0.9449276, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9971590909090909},\n",
    " {'Train Loss': 0.0035450805444270372,\n",
    "  'F1': array(0.94198024, dtype=float32),\n",
    "  'AUC_score': 0.9727941176470588,\n",
    "  'highinfo F1': array(0.91594267, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9681497175141244,\n",
    "  'lowinfo F1': array(0.9182795, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9886363636363636},\n",
    " {'Train Loss': 0.0019835997372865677,\n",
    "  'F1': array(0.9492647, dtype=float32),\n",
    "  'AUC_score': 0.9726890756302521,\n",
    "  'highinfo F1': array(0.9116384, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9661723163841809,\n",
    "  'lowinfo F1': array(0.8869048, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9488636363636364},\n",
    " {'Train Loss': 0.001824672450311482,\n",
    "  'F1': array(0.94198024, dtype=float32),\n",
    "  'AUC_score': 0.9688550420168068,\n",
    "  'highinfo F1': array(0.920099, dtype=float32),\n",
    "  'highinfo AUC_score': 0.971045197740113,\n",
    "  'lowinfo F1': array(0.9449276, dtype=float32),\n",
    "  'lowinfo AUC_score': 0.9715909090909091},\n",
    " {'Train Loss': 0.008919095620512962,\n",
    "  'F1': array(0.9456343, dtype=float32),\n",
    "  'AUC_score': 0.9608718487394957,\n",
    "  'highinfo F1': array(0.93275416, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9655367231638418,\n",
    "  'lowinfo F1': array(0.97275984, dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.004075885750353336,\n",
    "  'F1': array(0.9492647, dtype=float32),\n",
    "  'AUC_score': 0.9745273109243697,\n",
    "  'highinfo F1': array(0.94115984, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9711864406779661,\n",
    "  'lowinfo F1': array(1., dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.0016640423564240336,\n",
    "  'F1': array(0.9456343, dtype=float32),\n",
    "  'AUC_score': 0.9769957983193277,\n",
    "  'highinfo F1': array(0.9411723, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9728813559322034,\n",
    "  'lowinfo F1': array(0.97275984, dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.00264760828576982,\n",
    "  'F1': array(0.9420016, dtype=float32),\n",
    "  'AUC_score': 0.9785714285714286,\n",
    "  'highinfo F1': array(0.94957626, dtype=float32),\n",
    "  'highinfo AUC_score': 0.973728813559322,\n",
    "  'lowinfo F1': array(0.97275984, dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.0019195948261767626,\n",
    "  'F1': array(0.9492647, dtype=float32),\n",
    "  'AUC_score': 0.9751050420168067,\n",
    "  'highinfo F1': array(0.94115984, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9714689265536723,\n",
    "  'lowinfo F1': array(0.97275984, dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.0022223656997084618,\n",
    "  'F1': array(0.9456343, dtype=float32),\n",
    "  'AUC_score': 0.9748949579831933,\n",
    "  'highinfo F1': array(0.9453541, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9725988700564971,\n",
    "  'lowinfo F1': array(1., dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.0020320324692875147,\n",
    "  'F1': array(0.9383856, dtype=float32),\n",
    "  'AUC_score': 0.9754201680672269,\n",
    "  'highinfo F1': array(0.94956553, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9714689265536722,\n",
    "  'lowinfo F1': array(0.97275984, dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.002575908089056611,\n",
    "  'F1': array(0.952893, dtype=float32),\n",
    "  'AUC_score': 0.9748949579831933,\n",
    "  'highinfo F1': array(0.9453772, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9719632768361581,\n",
    "  'lowinfo F1': array(0.97275984, dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.0027930259238928556,\n",
    "  'F1': array(0.9456343, dtype=float32),\n",
    "  'AUC_score': 0.9752626050420168,\n",
    "  'highinfo F1': array(0.94957626, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9747175141242939,\n",
    "  'lowinfo F1': array(0.97275984, dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.005477116443216801,\n",
    "  'F1': array(0.9456343, dtype=float32),\n",
    "  'AUC_score': 0.9756827731092438,\n",
    "  'highinfo F1': array(0.9453772, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9721045197740112,\n",
    "  'lowinfo F1': array(0.97275984, dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.00650023715570569,\n",
    "  'F1': array(0.9456343, dtype=float32),\n",
    "  'AUC_score': 0.9773109243697479,\n",
    "  'highinfo F1': array(0.94956553, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9746468926553673,\n",
    "  'lowinfo F1': array(1., dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.0036637780722230673,\n",
    "  'F1': array(0.9456172, dtype=float32),\n",
    "  'AUC_score': 0.9757878151260505,\n",
    "  'highinfo F1': array(0.9579802, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9741525423728813,\n",
    "  'lowinfo F1': array(1., dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.0021731334272772074,\n",
    "  'F1': array(0.9420016, dtype=float32),\n",
    "  'AUC_score': 0.9729516806722689,\n",
    "  'highinfo F1': array(0.936947, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9671610169491526,\n",
    "  'lowinfo F1': array(0.97275984, dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.002427050843834877,\n",
    "  'F1': array(0.9456343, dtype=float32),\n",
    "  'AUC_score': 0.9744747899159664,\n",
    "  'highinfo F1': array(0.9411765, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9725988700564971,\n",
    "  'lowinfo F1': array(0.97275984, dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.005376866552978754,\n",
    "  'F1': array(0.9456343, dtype=float32),\n",
    "  'AUC_score': 0.9733718487394958,\n",
    "  'highinfo F1': array(0.94956553, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9716807909604519,\n",
    "  'lowinfo F1': array(1., dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.0025574269238859415,\n",
    "  'F1': array(0.952893, dtype=float32),\n",
    "  'AUC_score': 0.975,\n",
    "  'highinfo F1': array(0.9369737, dtype=float32),\n",
    "  'highinfo AUC_score': 0.972457627118644,\n",
    "  'lowinfo F1': array(1., dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.0030438543763011694,\n",
    "  'F1': array(0.9456343, dtype=float32),\n",
    "  'AUC_score': 0.9730042016806723,\n",
    "  'highinfo F1': array(0.94954777, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9720338983050847,\n",
    "  'lowinfo F1': array(1., dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.0019586498383432627,\n",
    "  'F1': array(0.9456343, dtype=float32),\n",
    "  'AUC_score': 0.9771008403361344,\n",
    "  'highinfo F1': array(0.94526124, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9751412429378531,\n",
    "  'lowinfo F1': array(1., dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.00300946575589478,\n",
    "  'F1': array(0.9420016, dtype=float32),\n",
    "  'AUC_score': 0.9764705882352941,\n",
    "  'highinfo F1': array(0.9368845, dtype=float32),\n",
    "  'highinfo AUC_score': 0.973093220338983,\n",
    "  'lowinfo F1': array(1., dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.0017435108311474323,\n",
    "  'F1': array(0.9601402, dtype=float32),\n",
    "  'AUC_score': 0.9782563025210084,\n",
    "  'highinfo F1': array(0.9621521, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9713983050847458,\n",
    "  'lowinfo F1': array(1., dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.001799501944333315,\n",
    "  'F1': array(0.9565195, dtype=float32),\n",
    "  'AUC_score': 0.9759453781512605,\n",
    "  'highinfo F1': array(0.9453308, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9729519774011299,\n",
    "  'lowinfo F1': array(1., dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.0017800971399992704,\n",
    "  'F1': array(0.9528831, dtype=float32),\n",
    "  'AUC_score': 0.9749474789915966,\n",
    "  'highinfo F1': array(0.9368398, dtype=float32),\n",
    "  'highinfo AUC_score': 0.971822033898305,\n",
    "  'lowinfo F1': array(1., dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.0015957766445353627,\n",
    "  'F1': array(0.9528831, dtype=float32),\n",
    "  'AUC_score': 0.974422268907563,\n",
    "  'highinfo F1': array(0.94526124, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9718220338983051,\n",
    "  'lowinfo F1': array(1., dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.0028251083567738533,\n",
    "  'F1': array(0.9601402, dtype=float32),\n",
    "  'AUC_score': 0.9764180672268907,\n",
    "  'highinfo F1': array(0.94949067, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9716101694915255,\n",
    "  'lowinfo F1': array(1., dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.0020065056160092354,\n",
    "  'F1': array(0.9565126, dtype=float32),\n",
    "  'AUC_score': 0.9729516806722689,\n",
    "  'highinfo F1': array(0.94526124, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9704802259887007,\n",
    "  'lowinfo F1': array(1., dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.0017294194549322128,\n",
    "  'F1': array(0.9565126, dtype=float32),\n",
    "  'AUC_score': 0.973844537815126,\n",
    "  'highinfo F1': array(0.9453, dtype=float32),\n",
    "  'highinfo AUC_score': 0.967725988700565,\n",
    "  'lowinfo F1': array(1., dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.0017452733591198921,\n",
    "  'F1': array(0.9492514, dtype=float32),\n",
    "  'AUC_score': 0.9723214285714286,\n",
    "  'highinfo F1': array(0.94107246, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9687853107344634,\n",
    "  'lowinfo F1': array(1., dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.0018426559399813414,\n",
    "  'F1': array(0.9565126, dtype=float32),\n",
    "  'AUC_score': 0.9684873949579832,\n",
    "  'highinfo F1': array(0.94526124, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9625,\n",
    "  'lowinfo F1': array(1., dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.001574847032316029,\n",
    "  'F1': array(0.9456343, dtype=float32),\n",
    "  'AUC_score': 0.9685399159663866,\n",
    "  'highinfo F1': array(0.9326018, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9622881355932205,\n",
    "  'lowinfo F1': array(1., dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.0016381937311962247,\n",
    "  'F1': array(0.9492647, dtype=float32),\n",
    "  'AUC_score': 0.9698004201680672,\n",
    "  'highinfo F1': array(0.94107246, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9625706214689266,\n",
    "  'lowinfo F1': array(1., dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.002581252483651042,\n",
    "  'F1': array(0.952893, dtype=float32),\n",
    "  'AUC_score': 0.967594537815126,\n",
    "  'highinfo F1': array(0.94107246, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9606638418079095,\n",
    "  'lowinfo F1': array(1., dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.0020618997514247894,\n",
    "  'F1': array(0.9420016, dtype=float32),\n",
    "  'AUC_score': 0.9638655462184874,\n",
    "  'highinfo F1': array(0.9326018, dtype=float32),\n",
    "  'highinfo AUC_score': 0.956497175141243,\n",
    "  'lowinfo F1': array(1., dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0},\n",
    " {'Train Loss': 0.0016754006501287222,\n",
    "  'F1': array(0.9383402, dtype=float32),\n",
    "  'AUC_score': 0.9761029411764707,\n",
    "  'highinfo F1': array(0.9282866, dtype=float32),\n",
    "  'highinfo AUC_score': 0.9712570621468927,\n",
    "  'lowinfo F1': array(1., dtype=float32),\n",
    "  'lowinfo AUC_score': 1.0}]\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, np.generic):\n",
    "            return obj.item()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "\n",
    "# 保存为 JSON 文件\n",
    "with open('GNN/results_GP_GAT_with.json', 'w') as f:\n",
    "    json.dump(results, f, cls=NumpyEncoder, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36b14e2-2f1b-426c-8915-4e6e630d339a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 0.0507, Macro_F1: 0.4472, AUC_score: 0.8614\n",
      "Validation loss decreased (0.050728 --> 0.050728).\n",
      "Epoch 50: Train Loss: 0.0675, Macro_F1: 0.8331, AUC_score: 0.8552\n",
      "Validation loss decreased (0.045188 --> 0.045188).\n",
      "Validation loss decreased (0.040963 --> 0.040963).\n",
      "Epoch 100: Train Loss: 0.0413, Macro_F1: 0.8255, AUC_score: 0.8545\n",
      "Validation loss decreased (0.039264 --> 0.039264).\n",
      "Epoch 150: Train Loss: 0.0578, Macro_F1: 0.8367, AUC_score: 0.8533\n",
      "Validation loss decreased (0.037043 --> 0.037043).\n",
      "Epoch 200: Train Loss: 0.0422, Macro_F1: 0.8295, AUC_score: 0.8563\n",
      "Epoch 250: Train Loss: 0.0488, Macro_F1: 0.8333, AUC_score: 0.8626\n",
      "Epoch 00263: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.030417 --> 0.030417).\n",
      "Epoch 300: Train Loss: 0.0512, Macro_F1: 0.8329, AUC_score: 0.8539\n",
      "Epoch 350: Train Loss: 0.0486, Macro_F1: 0.8293, AUC_score: 0.8541\n",
      "Epoch 00392: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0469, Macro_F1: 0.8294, AUC_score: 0.8575\n",
      "Epoch 450: Train Loss: 0.0464, Macro_F1: 0.8256, AUC_score: 0.8553\n",
      "Early stopping triggered\n",
      "acc save\n",
      "79.0% node features transform to 0: F1: 0.8256, AUC_score: 0.8553\n",
      "Epoch 0: Train Loss: 0.0587, Macro_F1: 0.8080, AUC_score: 0.8376\n",
      "Validation loss decreased (0.058686 --> 0.058686).\n",
      "Epoch 50: Train Loss: 0.0925, Macro_F1: 0.8366, AUC_score: 0.6975\n",
      "Validation loss decreased (0.055689 --> 0.055689).\n",
      "Validation loss decreased (0.055671 --> 0.055671).\n",
      "Validation loss decreased (0.055575 --> 0.055575).\n",
      "Validation loss decreased (0.050436 --> 0.050436).\n",
      "Validation loss decreased (0.050297 --> 0.050297).\n",
      "Validation loss decreased (0.045988 --> 0.045988).\n",
      "Epoch 100: Train Loss: 0.0556, Macro_F1: 0.8405, AUC_score: 0.8684\n",
      "Validation loss decreased (0.045779 --> 0.045779).\n",
      "Validation loss decreased (0.043379 --> 0.043379).\n",
      "Validation loss decreased (0.039127 --> 0.039127).\n",
      "Epoch 150: Train Loss: 0.0525, Macro_F1: 0.8475, AUC_score: 0.8706\n",
      "Validation loss decreased (0.039079 --> 0.039079).\n",
      "Validation loss decreased (0.038785 --> 0.038785).\n",
      "Validation loss decreased (0.035106 --> 0.035106).\n",
      "Epoch 200: Train Loss: 0.0532, Macro_F1: 0.8404, AUC_score: 0.8676\n",
      "Epoch 250: Train Loss: 0.0394, Macro_F1: 0.8440, AUC_score: 0.8682\n",
      "Epoch 00293: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0523, Macro_F1: 0.8439, AUC_score: 0.8683\n",
      "Validation loss decreased (0.034947 --> 0.034947).\n",
      "Epoch 350: Train Loss: 0.0505, Macro_F1: 0.8332, AUC_score: 0.8702\n",
      "Epoch 400: Train Loss: 0.0496, Macro_F1: 0.8368, AUC_score: 0.8692\n",
      "Epoch 00417: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.032733 --> 0.032733).\n",
      "Epoch 450: Train Loss: 0.0309, Macro_F1: 0.8368, AUC_score: 0.8664\n",
      "Validation loss decreased (0.030876 --> 0.030876).\n",
      "Validation loss decreased (0.029742 --> 0.029742).\n",
      "Epoch 500: Train Loss: 0.0456, Macro_F1: 0.8368, AUC_score: 0.8676\n"
     ]
    }
   ],
   "source": [
    "for i in range(80):\n",
    "    i = i + 20\n",
    "    # 加载这次迭代的selected_indices和remaining_indices\n",
    "    selected_indices = np.load(f'DIVIDED_DATA/GOwith_{i}%.npy')\n",
    "    \n",
    "    # 根据selected_indices和remaining_indices调整特征\n",
    "    masked_features = features.clone()\n",
    "    masked_features[torch.tensor(selected_indices)] = 0  # 假设features是一个PyTorch tensor\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=200, verbose=True, delta=0.00001)\n",
    "    \n",
    "    if i > 0:\n",
    "        # 从上一个迭代保存的模型中加载参数\n",
    "        model.load_state_dict(torch.load(f'G-G_DATA/G-G_model_WITH{i-1}.pth'))\n",
    "        # 重新初始化优化器和调度器\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=100, verbose=True)\n",
    "    \n",
    "    # 模型训练和评估逻辑\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model_scheduler(model, masked_features, data.y, data.edge_index, optimizer, loss_fn, scheduler, train_mask)\n",
    "        test_f1, test_auc = evaluate_model(model, masked_features, data.y, data.edge_index, test_mask)\n",
    "        high_f1, high_auc = evaluate_model(model, data.x, data.y, data.edge_index, test_high)\n",
    "        low_f1, low_auc = evaluate_model(model, data.x, data.y, data.edge_index, test_low)\n",
    "        if epoch % 50 == 0:  # 每10个epoch打印一次信息\n",
    "            print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Macro_F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "        early_stopping(train_loss, model, i)\n",
    "        if early_stopping.early_stop or epoch == num_epochs - 1:\n",
    "            results.append({\n",
    "                            'Train Loss': train_loss,\n",
    "                            'F1': test_f1,\n",
    "                            'AUC_score': test_auc,\n",
    "                            'highinfo F1': high_f1,\n",
    "                            'highinfo AUC_score': high_auc,\n",
    "                            'lowinfo F1': low_f1,\n",
    "                            'lowinfo AUC_score': low_auc\n",
    "                        })\n",
    "            print(\"acc save\")\n",
    "            rate = 1 - count * (i + 1)\n",
    "            rate = rate * 100\n",
    "            print(f'{rate}% node features transform to 0: F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "            torch.save(model.state_dict(), f'G-G_DATA/G-G_model_WITH{i}.pth')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72c8c34-764a-4463-8e23-4519bc253bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
