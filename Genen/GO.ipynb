{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c44fb475-00d8-4d3a-a35b-bbfb19ae9b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1197098/2362945497.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import obonet\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd650206-066c-4ea8-b09f-f97b737b8fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Source      Target\n",
      "0  GO:0000001  GO:0048308\n",
      "1  GO:0000001  GO:0048311\n",
      "2  GO:0000002  GO:0007005\n",
      "3  GO:0000003  GO:0008150\n",
      "4  GO:0000006  GO:0005385\n",
      "           GO  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
      "0  GO:0000001 -1.168093 -0.355214  0.265877 -0.710051  0.515028 -0.525165   \n",
      "1  GO:0000002 -1.185879 -0.098765  0.388240 -0.295556  0.327296 -0.119842   \n",
      "2  GO:0000003  0.063323 -0.199995  0.151511 -0.942141  0.109313  0.015316   \n",
      "3  GO:0000005  0.163135  0.301527  0.219680  0.094342 -0.129769  0.225696   \n",
      "4  GO:0000006 -0.641113 -0.541363  0.413941  0.699345  0.461507 -0.497388   \n",
      "\n",
      "   feature7  feature8  feature9  ...  feature759  feature760  feature761  \\\n",
      "0 -0.186588 -0.161192  0.186984  ...   -1.350874   -0.991801   -0.648123   \n",
      "1  0.399882 -0.035890  0.853417  ...   -1.086927   -0.842870   -0.385764   \n",
      "2  0.633298  0.507875  0.665548  ...    0.174185    0.351648    0.138497   \n",
      "3  0.357577  0.819992  0.852388  ...   -0.084025   -0.291103   -0.003621   \n",
      "4 -0.044589 -0.655766 -0.596647  ...   -0.561434    0.246475   -0.029871   \n",
      "\n",
      "   feature762  feature763  feature764  feature765  feature766  feature767  \\\n",
      "0   -0.361629   -0.914965   -0.506993    0.389760    0.207266    0.070705   \n",
      "1    0.175797   -1.223772   -0.999628    0.101473   -0.051212    0.048775   \n",
      "2    0.119273   -0.295167   -0.331179    0.102570   -0.524301   -0.139264   \n",
      "3    0.245929   -0.443244    0.229245   -0.685159   -0.725621    0.285964   \n",
      "4   -0.212828   -0.985273    0.677472    0.582681    0.299317   -0.131577   \n",
      "\n",
      "   feature768  \n",
      "0    0.938593  \n",
      "1    0.780470  \n",
      "2    0.761573  \n",
      "3    0.313211  \n",
      "4    0.739702  \n",
      "\n",
      "[5 rows x 769 columns]\n"
     ]
    }
   ],
   "source": [
    "GO_graph = obonet.read_obo(\"GNN/go-basic.obo\")\n",
    "\n",
    "go_edges = []\n",
    "for u, v, data in GO_graph.edges(data=True):\n",
    "    go_edges.append([u, v])\n",
    "go_edges_df = pd.DataFrame(go_edges, columns=['Source', 'Target']).dropna()\n",
    "print(go_edges_df.head())\n",
    "col_name = ['GO']\n",
    "for i in range(1,769):\n",
    "  col_name.append('feature'+str(i))\n",
    "go_features_df = pd.read_csv(\"GNN/go_terms_embeddings.csv\", skiprows=1, names=col_name).dropna()\n",
    "print(go_features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b99ecc4-455d-422c-b433-5229a42f4abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  protein  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
      "0     FES  0.339602 -0.030744 -0.901381  0.100888  0.886443  0.383596   \n",
      "1  HADHA  -0.131799 -0.025745 -0.677301 -0.053545  0.971046  0.180315   \n",
      "2  SLC7A7  0.385693 -0.070692 -0.847796 -0.022054  0.959772  0.085487   \n",
      "3    LCK   0.650428  0.014479 -0.866163  0.053508  0.951529  0.269402   \n",
      "4   HSPA2  0.322262  0.017484 -0.849302  0.046401  0.920429  0.463832   \n",
      "\n",
      "   feature7  feature8  feature9  ...  feature759  feature760  feature761  \\\n",
      "0 -0.192082 -0.032063 -0.154869  ...   -0.549204   -0.856123    0.714672   \n",
      "1 -0.028189 -0.077389 -0.095152  ...    0.927885   -0.817812    0.809631   \n",
      "2  0.076455 -0.003006 -0.032268  ...    0.941094   -0.912443    0.789828   \n",
      "3 -0.214788  0.045179 -0.506429  ...   -0.576739   -0.969558    0.916549   \n",
      "4 -0.050414 -0.033398  0.387791  ...    0.387301   -0.860696    0.678607   \n",
      "\n",
      "   feature762  feature763  feature764  feature765  feature766  feature767  \\\n",
      "0   -0.046649   -0.894424   -0.001815    0.739485    0.015581   -0.023863   \n",
      "1   -0.005827   -0.848839    0.024516    0.526404   -0.039926   -0.102787   \n",
      "2    0.046979   -0.715636    0.085842    0.150494    0.025392   -0.066035   \n",
      "3   -0.080332   -0.927649   -0.047398    0.741663   -0.000096   -0.096318   \n",
      "4   -0.060695   -0.945793    0.040472    0.831079   -0.001711   -0.079842   \n",
      "\n",
      "   feature768  \n",
      "0   -0.022002  \n",
      "1   -0.026980  \n",
      "2   -0.028283  \n",
      "3   -0.056501  \n",
      "4   -0.011189  \n",
      "\n",
      "[5 rows x 769 columns]\n",
      "    Target      Source\n",
      "0    MT-TF  GO:0030533\n",
      "1    MT-TF  GO:0006412\n",
      "4  MT-RNR2  GO:0003735\n",
      "5  MT-RNR2  GO:0005840\n",
      "6   MT-TL1  GO:0030533\n"
     ]
    }
   ],
   "source": [
    "col_name = ['protein']\n",
    "for i in range(1,769):\n",
    "  col_name.append('feature'+str(i))\n",
    "gene_features_df = pd.read_csv('GNN/gene_embedding_GeneLLM_2.csv', header=None, names=col_name).dropna()\n",
    "#gene_features_df = gene_features_df.drop(gene_features_df.columns[0], axis=1)\n",
    "print(gene_features_df.head())\n",
    "\n",
    "col_name = ['Target', 'Source']\n",
    "go_protein_df = pd.read_csv(\n",
    "    \"GNN/mart_export.txt\", \n",
    "    skiprows=1, \n",
    "    names=col_name, \n",
    "    usecols=[1, 2] \n",
    ").dropna()\n",
    "print(go_protein_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43ddf506-05e2-411e-9f58-5537a341a432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47595\n"
     ]
    }
   ],
   "source": [
    "print(len(go_features_df))\n",
    "go_features_df.rename(columns={'GO': 'protein'}, inplace=True)\n",
    "combined_features = pd.concat([gene_features_df, go_features_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2032e27-b4cd-42da-96dc-5a392b484ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature759</th>\n",
       "      <th>feature760</th>\n",
       "      <th>feature761</th>\n",
       "      <th>feature762</th>\n",
       "      <th>feature763</th>\n",
       "      <th>feature764</th>\n",
       "      <th>feature765</th>\n",
       "      <th>feature766</th>\n",
       "      <th>feature767</th>\n",
       "      <th>feature768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FES</td>\n",
       "      <td>0.339602</td>\n",
       "      <td>-0.030744</td>\n",
       "      <td>-0.901381</td>\n",
       "      <td>0.100888</td>\n",
       "      <td>0.886443</td>\n",
       "      <td>0.383596</td>\n",
       "      <td>-0.192082</td>\n",
       "      <td>-0.032063</td>\n",
       "      <td>-0.154869</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.549204</td>\n",
       "      <td>-0.856123</td>\n",
       "      <td>0.714672</td>\n",
       "      <td>-0.046649</td>\n",
       "      <td>-0.894424</td>\n",
       "      <td>-0.001815</td>\n",
       "      <td>0.739485</td>\n",
       "      <td>0.015581</td>\n",
       "      <td>-0.023863</td>\n",
       "      <td>-0.022002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HADHA</td>\n",
       "      <td>-0.131799</td>\n",
       "      <td>-0.025745</td>\n",
       "      <td>-0.677301</td>\n",
       "      <td>-0.053545</td>\n",
       "      <td>0.971046</td>\n",
       "      <td>0.180315</td>\n",
       "      <td>-0.028189</td>\n",
       "      <td>-0.077389</td>\n",
       "      <td>-0.095152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.927885</td>\n",
       "      <td>-0.817812</td>\n",
       "      <td>0.809631</td>\n",
       "      <td>-0.005827</td>\n",
       "      <td>-0.848839</td>\n",
       "      <td>0.024516</td>\n",
       "      <td>0.526404</td>\n",
       "      <td>-0.039926</td>\n",
       "      <td>-0.102787</td>\n",
       "      <td>-0.026980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLC7A7</td>\n",
       "      <td>0.385693</td>\n",
       "      <td>-0.070692</td>\n",
       "      <td>-0.847796</td>\n",
       "      <td>-0.022054</td>\n",
       "      <td>0.959772</td>\n",
       "      <td>0.085487</td>\n",
       "      <td>0.076455</td>\n",
       "      <td>-0.003006</td>\n",
       "      <td>-0.032268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.941094</td>\n",
       "      <td>-0.912443</td>\n",
       "      <td>0.789828</td>\n",
       "      <td>0.046979</td>\n",
       "      <td>-0.715636</td>\n",
       "      <td>0.085842</td>\n",
       "      <td>0.150494</td>\n",
       "      <td>0.025392</td>\n",
       "      <td>-0.066035</td>\n",
       "      <td>-0.028283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LCK</td>\n",
       "      <td>0.650428</td>\n",
       "      <td>0.014479</td>\n",
       "      <td>-0.866163</td>\n",
       "      <td>0.053508</td>\n",
       "      <td>0.951529</td>\n",
       "      <td>0.269402</td>\n",
       "      <td>-0.214788</td>\n",
       "      <td>0.045179</td>\n",
       "      <td>-0.506429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.576739</td>\n",
       "      <td>-0.969558</td>\n",
       "      <td>0.916549</td>\n",
       "      <td>-0.080332</td>\n",
       "      <td>-0.927649</td>\n",
       "      <td>-0.047398</td>\n",
       "      <td>0.741663</td>\n",
       "      <td>-0.000096</td>\n",
       "      <td>-0.096318</td>\n",
       "      <td>-0.056501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HSPA2</td>\n",
       "      <td>0.322262</td>\n",
       "      <td>0.017484</td>\n",
       "      <td>-0.849302</td>\n",
       "      <td>0.046401</td>\n",
       "      <td>0.920429</td>\n",
       "      <td>0.463832</td>\n",
       "      <td>-0.050414</td>\n",
       "      <td>-0.033398</td>\n",
       "      <td>0.387791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387301</td>\n",
       "      <td>-0.860696</td>\n",
       "      <td>0.678607</td>\n",
       "      <td>-0.060695</td>\n",
       "      <td>-0.945793</td>\n",
       "      <td>0.040472</td>\n",
       "      <td>0.831079</td>\n",
       "      <td>-0.001711</td>\n",
       "      <td>-0.079842</td>\n",
       "      <td>-0.011189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47590</th>\n",
       "      <td>GO:2001313</td>\n",
       "      <td>0.174428</td>\n",
       "      <td>0.194728</td>\n",
       "      <td>-0.284376</td>\n",
       "      <td>0.282102</td>\n",
       "      <td>-0.713190</td>\n",
       "      <td>-0.272055</td>\n",
       "      <td>0.121190</td>\n",
       "      <td>0.129901</td>\n",
       "      <td>-0.983496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500545</td>\n",
       "      <td>0.429651</td>\n",
       "      <td>-0.292929</td>\n",
       "      <td>-0.464941</td>\n",
       "      <td>-0.740187</td>\n",
       "      <td>0.179149</td>\n",
       "      <td>-0.960807</td>\n",
       "      <td>-0.746958</td>\n",
       "      <td>1.069112</td>\n",
       "      <td>-0.848182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47591</th>\n",
       "      <td>GO:2001314</td>\n",
       "      <td>0.025886</td>\n",
       "      <td>0.306214</td>\n",
       "      <td>-0.254303</td>\n",
       "      <td>0.253673</td>\n",
       "      <td>-0.533680</td>\n",
       "      <td>-0.269355</td>\n",
       "      <td>0.150939</td>\n",
       "      <td>-0.229323</td>\n",
       "      <td>-1.078991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042979</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>-0.356661</td>\n",
       "      <td>-0.381828</td>\n",
       "      <td>-0.638338</td>\n",
       "      <td>0.077176</td>\n",
       "      <td>-0.788312</td>\n",
       "      <td>-0.683442</td>\n",
       "      <td>1.087031</td>\n",
       "      <td>-0.593092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47592</th>\n",
       "      <td>GO:2001315</td>\n",
       "      <td>0.027134</td>\n",
       "      <td>0.241391</td>\n",
       "      <td>-0.227353</td>\n",
       "      <td>0.317366</td>\n",
       "      <td>-0.726657</td>\n",
       "      <td>-0.197968</td>\n",
       "      <td>0.045653</td>\n",
       "      <td>0.038912</td>\n",
       "      <td>-0.954113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349853</td>\n",
       "      <td>0.370059</td>\n",
       "      <td>-0.144606</td>\n",
       "      <td>-0.493184</td>\n",
       "      <td>-0.655063</td>\n",
       "      <td>0.217335</td>\n",
       "      <td>-0.841272</td>\n",
       "      <td>-0.821077</td>\n",
       "      <td>1.036363</td>\n",
       "      <td>-0.836614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47593</th>\n",
       "      <td>GO:2001316</td>\n",
       "      <td>0.139543</td>\n",
       "      <td>0.028883</td>\n",
       "      <td>0.899480</td>\n",
       "      <td>0.152932</td>\n",
       "      <td>0.576852</td>\n",
       "      <td>0.330342</td>\n",
       "      <td>0.916943</td>\n",
       "      <td>0.012306</td>\n",
       "      <td>-0.020316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.354748</td>\n",
       "      <td>-0.083168</td>\n",
       "      <td>0.043640</td>\n",
       "      <td>-0.663565</td>\n",
       "      <td>0.543016</td>\n",
       "      <td>-0.652230</td>\n",
       "      <td>-1.427882</td>\n",
       "      <td>-0.985257</td>\n",
       "      <td>1.673561</td>\n",
       "      <td>0.109659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47594</th>\n",
       "      <td>GO:2001317</td>\n",
       "      <td>0.083064</td>\n",
       "      <td>0.090899</td>\n",
       "      <td>0.888541</td>\n",
       "      <td>0.309920</td>\n",
       "      <td>0.403966</td>\n",
       "      <td>0.202783</td>\n",
       "      <td>0.706517</td>\n",
       "      <td>-0.017584</td>\n",
       "      <td>-0.171057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.544680</td>\n",
       "      <td>-0.046654</td>\n",
       "      <td>0.262865</td>\n",
       "      <td>-0.767305</td>\n",
       "      <td>0.753788</td>\n",
       "      <td>-0.577503</td>\n",
       "      <td>-1.194910</td>\n",
       "      <td>-0.799556</td>\n",
       "      <td>1.519368</td>\n",
       "      <td>0.263210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62045 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          protein  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
       "0             FES  0.339602 -0.030744 -0.901381  0.100888  0.886443  0.383596   \n",
       "1          HADHA  -0.131799 -0.025745 -0.677301 -0.053545  0.971046  0.180315   \n",
       "2          SLC7A7  0.385693 -0.070692 -0.847796 -0.022054  0.959772  0.085487   \n",
       "3            LCK   0.650428  0.014479 -0.866163  0.053508  0.951529  0.269402   \n",
       "4           HSPA2  0.322262  0.017484 -0.849302  0.046401  0.920429  0.463832   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "47590  GO:2001313  0.174428  0.194728 -0.284376  0.282102 -0.713190 -0.272055   \n",
       "47591  GO:2001314  0.025886  0.306214 -0.254303  0.253673 -0.533680 -0.269355   \n",
       "47592  GO:2001315  0.027134  0.241391 -0.227353  0.317366 -0.726657 -0.197968   \n",
       "47593  GO:2001316  0.139543  0.028883  0.899480  0.152932  0.576852  0.330342   \n",
       "47594  GO:2001317  0.083064  0.090899  0.888541  0.309920  0.403966  0.202783   \n",
       "\n",
       "       feature7  feature8  feature9  ...  feature759  feature760  feature761  \\\n",
       "0     -0.192082 -0.032063 -0.154869  ...   -0.549204   -0.856123    0.714672   \n",
       "1     -0.028189 -0.077389 -0.095152  ...    0.927885   -0.817812    0.809631   \n",
       "2      0.076455 -0.003006 -0.032268  ...    0.941094   -0.912443    0.789828   \n",
       "3     -0.214788  0.045179 -0.506429  ...   -0.576739   -0.969558    0.916549   \n",
       "4     -0.050414 -0.033398  0.387791  ...    0.387301   -0.860696    0.678607   \n",
       "...         ...       ...       ...  ...         ...         ...         ...   \n",
       "47590  0.121190  0.129901 -0.983496  ...    0.500545    0.429651   -0.292929   \n",
       "47591  0.150939 -0.229323 -1.078991  ...    0.042979    0.134560   -0.356661   \n",
       "47592  0.045653  0.038912 -0.954113  ...    0.349853    0.370059   -0.144606   \n",
       "47593  0.916943  0.012306 -0.020316  ...   -0.354748   -0.083168    0.043640   \n",
       "47594  0.706517 -0.017584 -0.171057  ...   -0.544680   -0.046654    0.262865   \n",
       "\n",
       "       feature762  feature763  feature764  feature765  feature766  feature767  \\\n",
       "0       -0.046649   -0.894424   -0.001815    0.739485    0.015581   -0.023863   \n",
       "1       -0.005827   -0.848839    0.024516    0.526404   -0.039926   -0.102787   \n",
       "2        0.046979   -0.715636    0.085842    0.150494    0.025392   -0.066035   \n",
       "3       -0.080332   -0.927649   -0.047398    0.741663   -0.000096   -0.096318   \n",
       "4       -0.060695   -0.945793    0.040472    0.831079   -0.001711   -0.079842   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "47590   -0.464941   -0.740187    0.179149   -0.960807   -0.746958    1.069112   \n",
       "47591   -0.381828   -0.638338    0.077176   -0.788312   -0.683442    1.087031   \n",
       "47592   -0.493184   -0.655063    0.217335   -0.841272   -0.821077    1.036363   \n",
       "47593   -0.663565    0.543016   -0.652230   -1.427882   -0.985257    1.673561   \n",
       "47594   -0.767305    0.753788   -0.577503   -1.194910   -0.799556    1.519368   \n",
       "\n",
       "       feature768  \n",
       "0       -0.022002  \n",
       "1       -0.026980  \n",
       "2       -0.028283  \n",
       "3       -0.056501  \n",
       "4       -0.011189  \n",
       "...           ...  \n",
       "47590   -0.848182  \n",
       "47591   -0.593092  \n",
       "47592   -0.836614  \n",
       "47593    0.109659  \n",
       "47594    0.263210  \n",
       "\n",
       "[62045 rows x 769 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ced49da9-544d-4549-92a5-83f166a60d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature759</th>\n",
       "      <th>feature760</th>\n",
       "      <th>feature761</th>\n",
       "      <th>feature762</th>\n",
       "      <th>feature763</th>\n",
       "      <th>feature764</th>\n",
       "      <th>feature765</th>\n",
       "      <th>feature766</th>\n",
       "      <th>feature767</th>\n",
       "      <th>feature768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GO:0102030</td>\n",
       "      <td>0.245079</td>\n",
       "      <td>0.060955</td>\n",
       "      <td>0.177880</td>\n",
       "      <td>0.350591</td>\n",
       "      <td>-0.269516</td>\n",
       "      <td>0.093303</td>\n",
       "      <td>0.166691</td>\n",
       "      <td>-0.526311</td>\n",
       "      <td>-0.489648</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.506199</td>\n",
       "      <td>0.249561</td>\n",
       "      <td>-0.208474</td>\n",
       "      <td>-0.410255</td>\n",
       "      <td>-0.444679</td>\n",
       "      <td>0.305353</td>\n",
       "      <td>-0.565994</td>\n",
       "      <td>-0.232176</td>\n",
       "      <td>0.268691</td>\n",
       "      <td>-0.316552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GO:0019222</td>\n",
       "      <td>0.343653</td>\n",
       "      <td>-0.020588</td>\n",
       "      <td>0.291593</td>\n",
       "      <td>-0.609495</td>\n",
       "      <td>-0.374338</td>\n",
       "      <td>0.275575</td>\n",
       "      <td>0.113448</td>\n",
       "      <td>0.142848</td>\n",
       "      <td>1.171809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.044567</td>\n",
       "      <td>-0.371669</td>\n",
       "      <td>0.325822</td>\n",
       "      <td>-0.259854</td>\n",
       "      <td>-0.027968</td>\n",
       "      <td>-0.134610</td>\n",
       "      <td>-0.576043</td>\n",
       "      <td>0.584789</td>\n",
       "      <td>0.513540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GO:0043651</td>\n",
       "      <td>-0.117396</td>\n",
       "      <td>0.459912</td>\n",
       "      <td>0.029887</td>\n",
       "      <td>0.131992</td>\n",
       "      <td>0.258211</td>\n",
       "      <td>0.491710</td>\n",
       "      <td>0.228968</td>\n",
       "      <td>-0.441609</td>\n",
       "      <td>0.125457</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.801257</td>\n",
       "      <td>0.228723</td>\n",
       "      <td>-0.626664</td>\n",
       "      <td>-0.156087</td>\n",
       "      <td>-0.470884</td>\n",
       "      <td>-0.783440</td>\n",
       "      <td>-0.082460</td>\n",
       "      <td>0.570474</td>\n",
       "      <td>1.346214</td>\n",
       "      <td>0.938884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GO:0047715</td>\n",
       "      <td>-0.526277</td>\n",
       "      <td>0.565159</td>\n",
       "      <td>0.570244</td>\n",
       "      <td>-0.480896</td>\n",
       "      <td>-0.259288</td>\n",
       "      <td>0.733499</td>\n",
       "      <td>-0.427194</td>\n",
       "      <td>-0.378893</td>\n",
       "      <td>0.899117</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.496169</td>\n",
       "      <td>0.358538</td>\n",
       "      <td>-0.724079</td>\n",
       "      <td>-0.695102</td>\n",
       "      <td>0.765133</td>\n",
       "      <td>-0.527925</td>\n",
       "      <td>-0.333353</td>\n",
       "      <td>0.358480</td>\n",
       "      <td>1.133885</td>\n",
       "      <td>0.674472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GO:0045228</td>\n",
       "      <td>1.044302</td>\n",
       "      <td>0.296786</td>\n",
       "      <td>0.807624</td>\n",
       "      <td>-0.958006</td>\n",
       "      <td>-1.118146</td>\n",
       "      <td>-0.197549</td>\n",
       "      <td>0.055317</td>\n",
       "      <td>-0.054696</td>\n",
       "      <td>-0.102629</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070419</td>\n",
       "      <td>0.611316</td>\n",
       "      <td>-0.454071</td>\n",
       "      <td>-0.311416</td>\n",
       "      <td>-0.298850</td>\n",
       "      <td>0.454362</td>\n",
       "      <td>-0.907055</td>\n",
       "      <td>-0.584782</td>\n",
       "      <td>-0.082862</td>\n",
       "      <td>0.334650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62040</th>\n",
       "      <td>GO:0060225</td>\n",
       "      <td>0.168129</td>\n",
       "      <td>-0.268879</td>\n",
       "      <td>-0.208608</td>\n",
       "      <td>-0.353042</td>\n",
       "      <td>0.272458</td>\n",
       "      <td>-0.142894</td>\n",
       "      <td>0.751686</td>\n",
       "      <td>-0.469460</td>\n",
       "      <td>-0.328117</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.141342</td>\n",
       "      <td>-0.410156</td>\n",
       "      <td>-0.555614</td>\n",
       "      <td>0.102004</td>\n",
       "      <td>0.093699</td>\n",
       "      <td>-1.112738</td>\n",
       "      <td>0.084535</td>\n",
       "      <td>-0.450197</td>\n",
       "      <td>-0.774177</td>\n",
       "      <td>-0.703010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62041</th>\n",
       "      <td>GO:0103077</td>\n",
       "      <td>-0.123654</td>\n",
       "      <td>0.493901</td>\n",
       "      <td>-0.475551</td>\n",
       "      <td>0.535989</td>\n",
       "      <td>0.046150</td>\n",
       "      <td>0.055551</td>\n",
       "      <td>0.558193</td>\n",
       "      <td>-0.602708</td>\n",
       "      <td>-0.754179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.854525</td>\n",
       "      <td>0.204208</td>\n",
       "      <td>-0.182769</td>\n",
       "      <td>-1.125280</td>\n",
       "      <td>-0.247155</td>\n",
       "      <td>0.140447</td>\n",
       "      <td>-0.604479</td>\n",
       "      <td>-0.359852</td>\n",
       "      <td>0.039873</td>\n",
       "      <td>-0.030531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62042</th>\n",
       "      <td>KY</td>\n",
       "      <td>-0.270294</td>\n",
       "      <td>0.023677</td>\n",
       "      <td>-0.968183</td>\n",
       "      <td>-0.069810</td>\n",
       "      <td>0.976778</td>\n",
       "      <td>0.609132</td>\n",
       "      <td>-0.083191</td>\n",
       "      <td>-0.003226</td>\n",
       "      <td>-0.753098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.895261</td>\n",
       "      <td>-0.888501</td>\n",
       "      <td>0.710627</td>\n",
       "      <td>-0.081105</td>\n",
       "      <td>-0.822674</td>\n",
       "      <td>0.019582</td>\n",
       "      <td>-0.087569</td>\n",
       "      <td>-0.125763</td>\n",
       "      <td>-0.045489</td>\n",
       "      <td>-0.025476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62043</th>\n",
       "      <td>TLE4</td>\n",
       "      <td>0.495169</td>\n",
       "      <td>-0.060087</td>\n",
       "      <td>-0.838078</td>\n",
       "      <td>0.018835</td>\n",
       "      <td>0.901561</td>\n",
       "      <td>0.086731</td>\n",
       "      <td>-0.140366</td>\n",
       "      <td>0.013444</td>\n",
       "      <td>0.305359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447866</td>\n",
       "      <td>-0.753992</td>\n",
       "      <td>0.674311</td>\n",
       "      <td>-0.064127</td>\n",
       "      <td>-0.874367</td>\n",
       "      <td>-0.014441</td>\n",
       "      <td>0.829651</td>\n",
       "      <td>0.066681</td>\n",
       "      <td>-0.166960</td>\n",
       "      <td>-0.009769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62044</th>\n",
       "      <td>GO:0044715</td>\n",
       "      <td>-0.548555</td>\n",
       "      <td>-0.149527</td>\n",
       "      <td>-0.332260</td>\n",
       "      <td>-0.464838</td>\n",
       "      <td>-0.165426</td>\n",
       "      <td>0.509768</td>\n",
       "      <td>0.451933</td>\n",
       "      <td>-0.354805</td>\n",
       "      <td>1.090326</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.627024</td>\n",
       "      <td>-0.280556</td>\n",
       "      <td>0.187644</td>\n",
       "      <td>0.339320</td>\n",
       "      <td>0.509350</td>\n",
       "      <td>-0.664863</td>\n",
       "      <td>-0.569919</td>\n",
       "      <td>0.222357</td>\n",
       "      <td>0.716119</td>\n",
       "      <td>-0.309255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62045 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          protein  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
       "0      GO:0102030  0.245079  0.060955  0.177880  0.350591 -0.269516  0.093303   \n",
       "1      GO:0019222  0.343653 -0.020588  0.291593 -0.609495 -0.374338  0.275575   \n",
       "2      GO:0043651 -0.117396  0.459912  0.029887  0.131992  0.258211  0.491710   \n",
       "3      GO:0047715 -0.526277  0.565159  0.570244 -0.480896 -0.259288  0.733499   \n",
       "4      GO:0045228  1.044302  0.296786  0.807624 -0.958006 -1.118146 -0.197549   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "62040  GO:0060225  0.168129 -0.268879 -0.208608 -0.353042  0.272458 -0.142894   \n",
       "62041  GO:0103077 -0.123654  0.493901 -0.475551  0.535989  0.046150  0.055551   \n",
       "62042          KY -0.270294  0.023677 -0.968183 -0.069810  0.976778  0.609132   \n",
       "62043        TLE4  0.495169 -0.060087 -0.838078  0.018835  0.901561  0.086731   \n",
       "62044  GO:0044715 -0.548555 -0.149527 -0.332260 -0.464838 -0.165426  0.509768   \n",
       "\n",
       "       feature7  feature8  feature9  ...  feature759  feature760  feature761  \\\n",
       "0      0.166691 -0.526311 -0.489648  ...   -0.506199    0.249561   -0.208474   \n",
       "1      0.113448  0.142848  1.171809  ...    0.000831    0.044567   -0.371669   \n",
       "2      0.228968 -0.441609  0.125457  ...   -0.801257    0.228723   -0.626664   \n",
       "3     -0.427194 -0.378893  0.899117  ...   -0.496169    0.358538   -0.724079   \n",
       "4      0.055317 -0.054696 -0.102629  ...    0.070419    0.611316   -0.454071   \n",
       "...         ...       ...       ...  ...         ...         ...         ...   \n",
       "62040  0.751686 -0.469460 -0.328117  ...   -0.141342   -0.410156   -0.555614   \n",
       "62041  0.558193 -0.602708 -0.754179  ...    0.854525    0.204208   -0.182769   \n",
       "62042 -0.083191 -0.003226 -0.753098  ...    0.895261   -0.888501    0.710627   \n",
       "62043 -0.140366  0.013444  0.305359  ...    0.447866   -0.753992    0.674311   \n",
       "62044  0.451933 -0.354805  1.090326  ...   -1.627024   -0.280556    0.187644   \n",
       "\n",
       "       feature762  feature763  feature764  feature765  feature766  feature767  \\\n",
       "0       -0.410255   -0.444679    0.305353   -0.565994   -0.232176    0.268691   \n",
       "1        0.325822   -0.259854   -0.027968   -0.134610   -0.576043    0.584789   \n",
       "2       -0.156087   -0.470884   -0.783440   -0.082460    0.570474    1.346214   \n",
       "3       -0.695102    0.765133   -0.527925   -0.333353    0.358480    1.133885   \n",
       "4       -0.311416   -0.298850    0.454362   -0.907055   -0.584782   -0.082862   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "62040    0.102004    0.093699   -1.112738    0.084535   -0.450197   -0.774177   \n",
       "62041   -1.125280   -0.247155    0.140447   -0.604479   -0.359852    0.039873   \n",
       "62042   -0.081105   -0.822674    0.019582   -0.087569   -0.125763   -0.045489   \n",
       "62043   -0.064127   -0.874367   -0.014441    0.829651    0.066681   -0.166960   \n",
       "62044    0.339320    0.509350   -0.664863   -0.569919    0.222357    0.716119   \n",
       "\n",
       "       feature768  \n",
       "0       -0.316552  \n",
       "1        0.513540  \n",
       "2        0.938884  \n",
       "3        0.674472  \n",
       "4        0.334650  \n",
       "...           ...  \n",
       "62040   -0.703010  \n",
       "62041   -0.030531  \n",
       "62042   -0.025476  \n",
       "62043   -0.009769  \n",
       "62044   -0.309255  \n",
       "\n",
       "[62045 rows x 769 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_shuffled = combined_features.sample(frac=1).reset_index(drop=True)\n",
    "features_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17952635-b7ad-4376-81ba-a6eb44440923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GO:0030533</td>\n",
       "      <td>MT-TF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GO:0006412</td>\n",
       "      <td>MT-TF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GO:0003735</td>\n",
       "      <td>MT-RNR2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GO:0005840</td>\n",
       "      <td>MT-RNR2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GO:0030533</td>\n",
       "      <td>MT-TL1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83792</th>\n",
       "      <td>GO:2001317</td>\n",
       "      <td>GO:0034309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83793</th>\n",
       "      <td>GO:2001317</td>\n",
       "      <td>GO:0042181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83794</th>\n",
       "      <td>GO:2001317</td>\n",
       "      <td>GO:0120255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83795</th>\n",
       "      <td>GO:2001317</td>\n",
       "      <td>GO:1901362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83796</th>\n",
       "      <td>GO:2001317</td>\n",
       "      <td>GO:2001316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>477028 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Source      Target\n",
       "0      GO:0030533       MT-TF\n",
       "1      GO:0006412       MT-TF\n",
       "4      GO:0003735     MT-RNR2\n",
       "5      GO:0005840     MT-RNR2\n",
       "6      GO:0030533      MT-TL1\n",
       "...           ...         ...\n",
       "83792  GO:2001317  GO:0034309\n",
       "83793  GO:2001317  GO:0042181\n",
       "83794  GO:2001317  GO:0120255\n",
       "83795  GO:2001317  GO:1901362\n",
       "83796  GO:2001317  GO:2001316\n",
       "\n",
       "[477028 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_edges = pd.concat([go_protein_df, go_edges_df])\n",
    "combined_edges = combined_edges[['Source', 'Target']]\n",
    "combined_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3712be36-38eb-4200-81fb-8707ca2f8216",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_in_features = set(combined_features['protein'])\n",
    "\n",
    "filtered_edges_df = combined_edges[\n",
    "    combined_edges['Source'].isin(nodes_in_features) & combined_edges['Target'].isin(nodes_in_features)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48406992-6059-4ae7-8e49-c5ca3cfb00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_id_to_index = {node_id: i for i, node_id in enumerate(combined_features['protein'])}\n",
    "# 确保edge_index是按照这个新的索引顺序排列的\n",
    "source_indices = [node_id_to_index[node_id] for node_id in filtered_edges_df['Source']]\n",
    "target_indices = [node_id_to_index[node_id] for node_id in filtered_edges_df['Target']]\n",
    "edge_index = torch.tensor([source_indices, target_indices], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8338c9a-6618-4fae-8312-bbc98b152c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24323, 17460, 27132,  ..., 62044, 62044, 62044],\n",
       "        [ 2077,  2077,  2077,  ..., 52909, 55447, 62043]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "759bb039-9561-4002-bea9-be2f95ad8342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>protein</th>\n",
       "      <th>Solubility</th>\n",
       "      <th>Label</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>Count_Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ERAP2</td>\n",
       "      <td>Membrane</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ADAMTSL5</td>\n",
       "      <td>Soluble</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>TBC1D30</td>\n",
       "      <td>Membrane</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>KCNK18</td>\n",
       "      <td>Membrane</td>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NDNF</td>\n",
       "      <td>Soluble</td>\n",
       "      <td>1</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>1374</td>\n",
       "      <td>TRABD2B</td>\n",
       "      <td>Membrane</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>1375</td>\n",
       "      <td>RPS9</td>\n",
       "      <td>Soluble</td>\n",
       "      <td>1</td>\n",
       "      <td>205</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>1376</td>\n",
       "      <td>SLC22A16</td>\n",
       "      <td>Membrane</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>1377</td>\n",
       "      <td>FBN3</td>\n",
       "      <td>Soluble</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378</th>\n",
       "      <td>1378</td>\n",
       "      <td>BDH2</td>\n",
       "      <td>Soluble</td>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1379 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0   protein Solubility  Label  Word_Count  Count_Category\n",
       "0              0     ERAP2   Membrane      0         117               0\n",
       "1              1  ADAMTSL5    Soluble      1          28               1\n",
       "2              2   TBC1D30   Membrane      0          55               0\n",
       "3              3    KCNK18   Membrane      0         184               0\n",
       "4              4      NDNF    Soluble      1         129               0\n",
       "...          ...       ...        ...    ...         ...             ...\n",
       "1374        1374   TRABD2B   Membrane      0          96               0\n",
       "1375        1375      RPS9    Soluble      1         205               0\n",
       "1376        1376  SLC22A16   Membrane      0          93               0\n",
       "1377        1377      FBN3    Soluble      1          90               0\n",
       "1378        1378      BDH2    Soluble      1         102               0\n",
       "\n",
       "[1379 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df = pd.read_csv('GNN/new_labels.csv')\n",
    "labels_df.rename(columns={'Gene name': 'protein'}, inplace=True)\n",
    "labels_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38af3965-917c-453b-b080-d1cbfecd7862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67, 81, 93, 94, 100, 116, 129, 142, 148, 149, 157, 167, 170, 202, 228, 236, 241, 243, 255, 258, 276, 281, 287, 294, 305, 319, 321, 334, 340, 369, 373, 375, 383, 384, 399, 402, 418, 447, 455, 474, 483, 484, 490, 498, 511, 518, 526, 558, 569, 585, 586, 587, 593, 594, 595, 596, 599, 615, 641, 653, 654, 664, 682, 736, 754, 762, 764, 773, 777, 781, 833, 852, 894, 895, 899, 915, 917, 938, 944, 957, 982, 983, 997, 1003, 1019, 1040, 1047, 1053, 1062, 1064, 1065, 1067, 1074, 1075, 1078, 1079, 1105, 1108, 1115, 1116, 1120, 1124, 1131, 1136, 1137, 1143, 1157, 1170, 1173, 1183, 1195, 1196, 1197, 1215, 1235, 1236, 1237, 1243, 1256, 1261, 1262, 1273, 1274, 1277, 1311, 1315, 1327, 1333, 1347, 1363, 1371, 1377, 1378, 1383, 1392, 1397, 1426, 1427, 1431, 1435, 1444, 1445, 1469, 1470, 1475, 1484, 1489, 1492, 1493, 1521, 1527, 1540, 1548, 1559, 1575, 1593, 1606, 1609, 1614, 1617, 1619, 1637, 1638, 1645, 1674, 1681, 1710, 1720, 1733, 1736, 1737, 1759, 1776, 1795, 1807, 1823, 1833, 1834, 1844, 1848, 1849, 1852, 1854, 1882, 1893, 1895, 1898, 1914, 1917, 1923, 1927, 1960, 1970, 1972, 1978, 1986, 1995, 2015, 2016, 2022, 2031, 2046, 2048, 2071, 2096, 2097, 2099, 2101, 2104, 2109, 2112, 2115, 2117, 2119, 2140, 2151, 2154, 2166, 2188, 2196, 2209, 2214, 2227, 2231, 2238, 2250, 2256, 2265, 2273, 2274, 2277, 2281, 2283, 2284, 2286, 2292, 2293, 2302, 2308, 2316, 2326, 2337, 2353, 2354, 2358, 2362, 2364, 2365, 2370, 2372, 2400, 2417, 2419, 2421, 2422, 2424, 2427, 2455, 2465, 2476, 2481, 2489, 2496, 2505, 2532, 2533, 2535, 2540, 2551, 2556, 2589, 2620, 2625, 2630, 2634, 2660, 2672, 2676, 2686, 2701, 2705, 2708, 2716, 2730, 2738, 2767, 2770, 2785, 2788, 2802, 2804, 2819, 2823, 2827, 2836, 2842, 2862, 2871, 2879, 2888, 2896, 2903, 2907, 2915, 2927, 2928, 2929, 2937, 2950, 2957, 2958, 2960, 2982, 2986, 2995, 3000, 3013, 3014, 3021, 3024, 3027, 3037, 3043, 3044, 3054, 3057, 3064, 3084, 3092, 3099, 3117, 3123, 3147, 3150, 3159, 3180, 3186, 3217, 3240, 3244, 3255, 3268, 3272, 3285, 3291, 3305, 3319, 3325, 3326, 3327, 3330, 3353, 3356, 3382, 3389, 3393, 3408, 3410, 3417, 3433, 3454, 3456, 3465, 3486, 3502, 3511, 3519, 3530, 3535, 3536, 3538, 3543, 3584, 3615, 3619, 3620, 3630, 3647, 3663, 3680, 3684, 3692, 3706, 3707, 3715, 3727, 3747, 3751, 3752, 3754, 3774, 3779, 3781, 3783, 3785, 3786, 3792, 3797, 3802, 3810, 3829, 3833, 3839, 3850, 3882, 3884, 3886, 3902, 3917, 3921, 3923, 3929, 3943, 3949, 3959, 3960, 3968, 3969, 3977, 3985, 3990, 3991, 3992, 4019, 4025, 4034, 4040, 4048, 4049, 4071, 4102, 4120, 4133, 4159, 4165, 4166, 4169, 4178, 4184, 4185, 4197, 4209, 4225, 4232, 4239, 4249, 4251, 4258, 4288, 4323, 4328, 4332, 4333, 4343, 4351, 4360, 4361, 4365, 4375, 4376, 4387, 4417, 4431, 4433, 4438, 4439, 4440, 4464, 4465, 4484, 4485, 4495, 4500, 4507, 4513, 4520, 4524, 4526, 4539, 4545, 4558, 4560, 4565, 4572, 4607, 4616, 4630, 4637, 4653, 4656, 4668, 4676, 4711, 4712, 4716, 4721, 4751, 4760, 4776, 4780, 4783, 4792, 4795, 4796, 4800, 4804, 4806, 4815, 4833, 4836, 4840, 4853, 4882, 4888, 4897, 4913, 4915, 4926, 4930, 4935, 4961, 4963, 4995, 4999, 5006, 5013, 5020, 5041, 5061, 5072, 5077, 5079, 5083, 5085, 5124, 5139, 5144, 5146, 5151, 5154, 5167, 5170, 5173, 5178, 5192, 5209, 5216, 5218, 5226, 5228, 5233, 5242, 5243, 5253, 5264, 5271, 5278, 5280, 5298, 5306, 5312, 5327, 5351, 5379, 5383, 5396, 5399, 5405, 5410, 5418, 5437, 5486, 5488, 5506, 5510, 5512, 5513, 5522, 5555, 5556, 5566, 5569, 5570, 5575, 5582, 5586, 5589, 5593, 5628, 5634, 5635, 5644, 5670, 5674, 5687, 5693, 5702, 5703, 5710, 5713, 5721, 5736, 5738, 5784, 5785, 5787, 5792, 5793, 5820, 5823, 5866, 5884, 5890, 5898, 5899, 5900, 5902, 5913, 5943, 5954, 5958, 5963, 5985, 5991, 5992, 6006, 6011, 6038, 6043, 6057, 6058, 6064, 6066, 6078, 6081, 6094, 6099, 6101, 6112, 6119, 6120, 6127, 6128, 6131, 6134, 6136, 6168, 6177, 6179, 6196, 6203, 6211, 6223, 6226, 6227, 6239, 6246, 6278, 6301, 6302, 6314, 6319, 6328, 6343, 6377, 6389, 6416, 6426, 6427, 6428, 6436, 6438, 6448, 6460, 6494, 6514, 6516, 6517, 6518, 6527, 6528, 6549, 6556, 6559, 6562, 6567, 6569, 6579, 6583, 6593, 6595, 6606, 6607, 6612, 6623, 6628, 6630, 6639, 6667, 6679, 6681, 6699, 6702, 6703, 6714, 6717, 6728, 6744, 6772, 6774, 6782, 6785, 6792, 6804, 6815, 6819, 6823, 6832, 6844, 6850, 6859, 6865, 6871, 6884, 6885, 6886, 6943, 6947, 6963, 6995, 7000, 7017, 7018, 7019, 7026, 7040, 7047, 7049, 7065, 7067, 7073, 7074, 7081, 7083, 7089, 7093, 7108, 7116, 7125, 7130, 7139, 7151, 7152, 7153, 7157, 7170, 7185, 7189, 7192, 7201, 7202, 7203, 7221, 7252, 7266, 7278, 7280, 7281, 7286, 7296, 7300, 7312, 7313, 7341, 7349, 7357, 7365, 7375, 7377, 7387, 7389, 7390, 7412, 7415, 7420, 7427, 7433, 7440, 7449, 7456, 7465, 7469, 7481, 7510, 7520, 7527, 7528, 7549, 7561, 7563, 7564, 7573, 7590, 7591, 7599, 7609, 7610, 7631, 7634, 7640, 7645, 7653, 7654, 7665, 7688, 7692, 7705, 7715, 7721, 7755, 7760, 7769, 7782, 7783, 7793, 7839, 7865, 7889, 7891, 7892, 7921, 7930, 7956, 7958, 8012, 8013, 8031, 8041, 8046, 8060, 8066, 8073, 8075, 8077, 8078, 8124, 8139, 8156, 8175, 8189, 8196, 8204, 8212, 8215, 8225, 8226, 8228, 8229, 8231, 8234, 8243, 8250, 8262, 8266, 8272, 8285, 8290, 8303, 8305, 8323, 8327, 8329, 8339, 8343, 8348, 8349, 8350, 8351, 8356, 8357, 8360, 8366, 8383, 8386, 8389, 8405, 8413, 8419, 8441, 8447, 8451, 8454, 8467, 8480, 8491, 8492, 8497, 8517, 8519, 8535, 8554, 8557, 8560, 8567, 8568, 8576, 8592, 8595, 8600, 8608, 8631, 8632, 8633, 8641, 8645, 8647, 8653, 8672, 8683, 8691, 8716, 8720, 8727, 8733, 8741, 8746, 8748, 8749, 8786, 8808, 8814, 8815, 8818, 8820, 8826, 8828, 8865, 8903, 8927, 8931, 8942, 8946, 8991, 8998, 9013, 9026, 9027, 9056, 9066, 9085, 9097, 9098, 9115, 9126, 9134, 9135, 9144, 9148, 9165, 9189, 9190, 9225, 9231, 9246, 9263, 9293, 9301, 9302, 9323, 9325, 9364, 9385, 9410, 9416, 9423, 9431, 9435, 9437, 9457, 9500, 9512, 9514, 9517, 9535, 9547, 9552, 9561, 9573, 9589, 9607, 9627, 9638, 9639, 9659, 9682, 9708, 9757, 9772, 9776, 9784, 9800, 9809, 9822, 9823, 9828, 9836, 9852, 9853, 9862, 9875, 9885, 9900, 9908, 9910, 9926, 9937, 9978, 10000, 10010, 10023, 10031, 10038, 10045, 10046, 10053, 10060, 10063, 10075, 10090, 10094, 10119, 10125, 10138, 10146, 10147, 10151, 10158, 10159, 10168, 10172, 10183, 10201, 10217, 10246, 10298, 10300, 10309, 10321, 10323, 10325, 10327, 10364, 10372, 10379, 10390, 10409, 10410, 10411, 10418, 10424, 10432, 10435, 10436, 10443, 10452, 10459, 10479, 10483, 10485, 10500, 10502, 10527, 10534, 10536, 10542, 10552, 10561, 10580, 10581, 10607, 10619, 10622, 10623, 10629, 10655, 10666, 10685, 10686, 10695, 10697, 10699, 10700, 10701, 10705, 10708, 10730, 10732, 10751, 10754, 10764, 10765, 10776, 10779, 10789, 10794, 10825, 10828, 10862, 10877, 10887, 10890, 10905, 10917, 10926, 10934, 10951, 10988, 11000, 11002, 11022, 11028, 11033, 11035, 11040, 11050, 11089, 11108, 11109, 11125, 11140, 11142, 11150, 11164, 11175, 11178, 11184, 11187, 11192, 11210, 11222, 11247, 11248, 11273, 11277, 11278, 11286, 11289, 11296, 11307, 11321, 11324, 11340, 11343, 11345, 11362, 11406, 11407, 11410, 11418, 11428, 11458, 11482, 11492, 11497, 11523, 11533, 11538, 11542, 11548, 11549, 11562, 11572, 11583, 11601, 11609, 11634, 11638, 11655, 11657, 11658, 11676, 11688, 11707, 11711, 11734, 11746, 11749, 11752, 11758, 11774, 11803, 11805, 11810, 11820, 11844, 11887, 11928, 11938, 11951, 11958, 11967, 11984, 11987, 12071, 12080, 12102, 12103, 12107, 12111, 12116, 12124, 12130, 12142, 12151, 12154, 12169, 12174, 12193, 12194, 12199, 12205, 12262, 12271, 12272, 12284, 12289, 12296, 12302, 12304, 12323, 12345, 12348, 12358, 12373, 12386, 12429, 12432, 12457, 12470, 12487, 12488, 12511, 12517, 12578, 12579, 12587, 12593, 12666, 12667, 12684, 12705, 12706, 12760, 12776, 12784, 12805, 12819, 12821, 12848, 12864, 12873, 12889, 12918, 12954, 12955, 12956, 12958, 12959, 12974, 13001, 13003, 13017, 13022, 13042, 13045, 13049, 13078, 13083, 13084, 13088, 13090, 13120, 13130, 13138, 13164, 13165, 13181, 13186, 13192, 13206, 13214, 13243, 13251, 13266, 13292, 13295, 13305, 13314, 13321, 13334, 13394, 13409, 13422, 13443, 13447, 13469, 13502, 13512, 13536, 13559, 13566, 13567, 13571, 13580, 13582, 13586, 13594, 13657, 13662, 13666, 13672, 13673, 13685, 13724, 13727, 13741, 13760, 13779, 13826, 13831, 13838, 13847, 13855, 13858, 13862, 13871, 13872, 13874, 13882, 13892, 13898, 13901, 13906, 13924, 13931, 13936, 13943, 13951, 13965, 13978, 13984, 14012, 14015, 14024, 14036, 14052, 14090, 14117, 14146, 14151, 14162, 14214, 14234, 14240, 14241, 14263, 14268, 14279, 14281, 14288, 14293, 14315, 14317, 14318, 14324, 14325, 14398, 14407, 14426]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1197098/2534729209.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_tensor = torch.tensor(labels, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "label_indices = [node_id_to_index[node_id] for node_id in labels_df['protein']]\n",
    "print(label_indices)\n",
    "num_nodes = len(combined_features)\n",
    "labels = torch.full((num_nodes,), -1, dtype=torch.long)\n",
    "for i, index in enumerate(labels_df['Label']):\n",
    "    labels[label_indices[i]] = index\n",
    "\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8b53871-c51a-4ff4-a409-978efc3cad2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33960226, -0.03074448, -0.90138096, ...,  0.01558092,\n",
       "        -0.02386307, -0.02200161],\n",
       "       [-0.13179901, -0.02574519, -0.67730105, ..., -0.03992649,\n",
       "        -0.10278717, -0.02697964],\n",
       "       [ 0.38569278, -0.07069244, -0.8477959 , ...,  0.0253919 ,\n",
       "        -0.06603534, -0.02828273],\n",
       "       ...,\n",
       "       [ 0.02713387,  0.24139147, -0.22735251, ..., -0.82107705,\n",
       "         1.036363  , -0.83661443],\n",
       "       [ 0.13954346,  0.02888298,  0.89947975, ..., -0.9852566 ,\n",
       "         1.6735605 ,  0.10965873],\n",
       "       [ 0.08306409,  0.09089889,  0.8885408 , ..., -0.79955566,\n",
       "         1.5193683 ,  0.2632099 ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = combined_features.iloc[:, 1:].values\n",
    "features_tensor = torch.tensor(features, dtype=torch.float)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c588b1f-57c1-49db-8907-c4d842b89f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([62045, 768]) torch.float32\n",
      "edge_index: torch.Size([2, 411251]) torch.int64\n",
      "labels: torch.Size([62045]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "data = Data(x=features_tensor, edge_index=edge_index, y=labels_tensor)\n",
    "\n",
    "print(\"x:\", data.x.shape, data.x.dtype)\n",
    "print(\"edge_index:\", data.edge_index.shape, data.edge_index.dtype)\n",
    "print(\"labels:\", data.y.shape, data.y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08752e73-7f36-4a72-b187-4d716d32529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, num_layers, in_dim, num_hidden, num_classes, heads, activation, dropout, negative_slope, residual):\n",
    "        super(GAT, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        self.activation = activation\n",
    "\n",
    "        # Input projection (no residual)\n",
    "        self.gat_layers.append(GATConv(\n",
    "            in_dim, num_hidden, heads=heads[0],\n",
    "            dropout=dropout, negative_slope=negative_slope, concat=True, add_self_loops=True))\n",
    "\n",
    "        # Hidden layers\n",
    "        for l in range(1, num_layers):\n",
    "            # Due to multi-head, the in_dim = num_hidden * num_heads\n",
    "            self.gat_layers.append(GATConv(\n",
    "                num_hidden * heads[l-1], num_hidden, heads=heads[l],\n",
    "                dropout=dropout, negative_slope=negative_slope, concat=True, add_self_loops=True))\n",
    "\n",
    "        # Output projection\n",
    "        self.gat_layers.append(GATConv(\n",
    "            num_hidden * heads[-2], num_classes, heads=heads[-1],\n",
    "            dropout=dropout, negative_slope=negative_slope, concat=False, add_self_loops=True))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = x\n",
    "        for l, layer in enumerate(self.gat_layers[:-1]):\n",
    "            h = layer(h, edge_index)\n",
    "            if self.activation:\n",
    "                h = self.activation(h)\n",
    "            if l < self.num_layers - 1:\n",
    "                h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Output projection\n",
    "        logits = self.gat_layers[-1](h, edge_index)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e00dda86-c25c-4c10-9097-c239ef1488d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear, ModuleList, Dropout\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, num_classes, num_layers, activation, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.convs = ModuleList([GCNConv(hidden_dim, hidden_dim) for _ in range(num_layers - 2)])\n",
    "        self.conv_last = GCNConv(hidden_dim, num_classes)\n",
    "        self.activation = activation\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # 输入层\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 隐藏层\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # 输出层\n",
    "        x = self.conv_last(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06d2cf0e-cff5-4034-9e97-8091a13ae69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def accuracy(pred, target):\n",
    "    r\"\"\"Computes the accuracy of correct predictions.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    return (pred == target).sum().item() / target.numel()\n",
    "\n",
    "\n",
    "\n",
    "def true_positive(pred, target, num_classes):\n",
    "    r\"\"\"Computes the number of true positive predictions.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`LongTensor`\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred == i) & (target == i)).sum())\n",
    "\n",
    "    return torch.tensor(out)\n",
    "\n",
    "\n",
    "\n",
    "def true_negative(pred, target, num_classes):\n",
    "    r\"\"\"Computes the number of true negative predictions.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`LongTensor`\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred != i) & (target != i)).sum())\n",
    "\n",
    "    return torch.tensor(out)\n",
    "\n",
    "\n",
    "\n",
    "def false_positive(pred, target, num_classes):\n",
    "    r\"\"\"Computes the number of false positive predictions.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`LongTensor`\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred == i) & (target != i)).sum())\n",
    "\n",
    "    return torch.tensor(out)\n",
    "\n",
    "\n",
    "\n",
    "def false_negative(pred, target, num_classes):\n",
    "    r\"\"\"Computes the number of false negative predictions.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`LongTensor`\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred != i) & (target == i)).sum())\n",
    "\n",
    "    return torch.tensor(out)\n",
    "\n",
    "\n",
    "\n",
    "def precision(pred, target, num_classes):\n",
    "    r\"\"\"Computes the precision:\n",
    "    :math:`\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}}`.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`Tensor`\n",
    "    \"\"\"\n",
    "    tp = true_positive(pred, target, num_classes).to(torch.float)\n",
    "    fp = false_positive(pred, target, num_classes).to(torch.float)\n",
    "\n",
    "    out = tp / (tp + fp)\n",
    "    out[torch.isnan(out)] = 0\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def recall(pred, target, num_classes):\n",
    "    r\"\"\"Computes the recall:\n",
    "    :math:`\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}`.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`Tensor`\n",
    "    \"\"\"\n",
    "    tp = true_positive(pred, target, num_classes).to(torch.float)\n",
    "    fn = false_negative(pred, target, num_classes).to(torch.float)\n",
    "\n",
    "    out = tp / (tp + fn)\n",
    "    out[torch.isnan(out)] = 0\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def f1_score(pred, target, num_classes):\n",
    "    r\"\"\"Computes the :math:`F_1` score:\n",
    "    :math:`2 \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}\n",
    "    {\\mathrm{precision}+\\mathrm{recall}}`.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`Tensor`\n",
    "    \"\"\"\n",
    "    prec = precision(pred, target, num_classes)\n",
    "    rec = recall(pred, target, num_classes)\n",
    "\n",
    "    score = 2 * (prec * rec) / (prec + rec)\n",
    "    score[torch.isnan(score)] = 0\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d14c67dc-90bf-4404-ae67-d3e8005d8aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "def train_model_scheduler(model, masked_features, labels, edge_index, optimizer, criterion, scheduler, train_mask):\n",
    "    model.train()  # 设置模型为训练模\n",
    "    optimizer.zero_grad()  # 清空梯度\n",
    "    out = model(masked_features, edge_index)  # 获取模型输出\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])  # 计算损失值，只针对训练集的节点\n",
    "    loss.backward()  # 反向传播\n",
    "    optimizer.step()  # 更新模型参数\n",
    "    scheduler.step(loss)\n",
    "    return loss.item()\n",
    "\n",
    "def train_model(model, masked_features, labels, edge_index, optimizer, criterion, train_mask):\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    optimizer.zero_grad()  # 清空梯度\n",
    "    out = model(masked_features, edge_index) # 获取模型输出\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])  # 计算损失值，只针对训练集的节点\n",
    "    loss.backward()  # 反向传播\n",
    "    optimizer.step()  # 更新模型参数\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate_model(model, features, labels, edge_index, mask):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    with torch.no_grad():  # 关闭梯度计算\n",
    "        # 获取模型输出，这里假设输出已经是经过sigmoid的概率\n",
    "        probabilities = model(features, edge_index)\n",
    "        if probabilities.shape[1] == 2:  # 假设有两个输出（每个类一个概率）\n",
    "            positive_probs = probabilities[mask, 1]  # 选择正类概率\n",
    "        else:\n",
    "            positive_probs = probabilities[mask]  # 如果只有一个输出，假设已经是正类概率\n",
    "        val_f1 = torch.mean(f1_score(torch.argmax(probabilities[mask],dim=1), labels[mask], num_classes=2)).cpu().numpy()\n",
    "        auc_score = roc_auc_score(labels[mask].cpu().numpy(), positive_probs.cpu().numpy())\n",
    "\n",
    "    return val_f1, auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "515d22ae-5121-44a9-bb7e-c8b2393d0d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276\n",
      "tensor([False, False, False,  ..., False, False, False])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 实例化模型\n",
    "device = torch.device('cuda:1')\n",
    "data = data.to(device)\n",
    "model = GCN(num_features=data.x.shape[1], hidden_dim=64, num_classes=2, num_layers=2, activation=F.relu, dropout=0.5)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=100, verbose=True)\n",
    "\n",
    "\n",
    "labeled_indices = label_indices\n",
    "random.shuffle(labeled_indices)\n",
    "num_labeled = len(labeled_indices)\n",
    "num_train = int(num_labeled * 0.8)\n",
    "num_test = num_labeled - num_train\n",
    "print(num_test)\n",
    "\n",
    "# 创建训练和测试掩码\n",
    "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "\n",
    "train_mask[labeled_indices[:num_train]] = True\n",
    "test_mask[labeled_indices[num_train:num_train+num_test]] = True\n",
    "print(test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d7012da-8568-4498-9d27-62aaeb8e77a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4166,\n",
       " 3992,\n",
       " 6804,\n",
       " 9682,\n",
       " 8075,\n",
       " 7839,\n",
       " 6377,\n",
       " 498,\n",
       " 8600,\n",
       " 7278,\n",
       " 11109,\n",
       " 7083,\n",
       " 11562,\n",
       " 3839,\n",
       " 5233,\n",
       " 13314,\n",
       " 10053,\n",
       " 5298,\n",
       " 142,\n",
       " 1995,\n",
       " 654,\n",
       " 9800,\n",
       " 10764,\n",
       " 7266,\n",
       " 13779,\n",
       " 1426,\n",
       " 599,\n",
       " 11407,\n",
       " 13214,\n",
       " 13243,\n",
       " 4676,\n",
       " 2896,\n",
       " 2819,\n",
       " 7573,\n",
       " 11002,\n",
       " 6681,\n",
       " 7783,\n",
       " 7252,\n",
       " 4935,\n",
       " 6101,\n",
       " 938,\n",
       " 13001,\n",
       " 9364,\n",
       " 11000,\n",
       " 2785,\n",
       " 7640,\n",
       " 915,\n",
       " 2419,\n",
       " 1898,\n",
       " 11345,\n",
       " 7892,\n",
       " 5506,\n",
       " 11286,\n",
       " 13965,\n",
       " 9910,\n",
       " 243,\n",
       " 10201,\n",
       " 8031,\n",
       " 14214,\n",
       " 1493,\n",
       " 11458,\n",
       " 5242,\n",
       " 6136,\n",
       " 8733,\n",
       " 7528,\n",
       " 8272,\n",
       " 6885,\n",
       " 12304,\n",
       " 373,\n",
       " 1431,\n",
       " 14117,\n",
       " 13512,\n",
       " 3159,\n",
       " 2929,\n",
       " 9416,\n",
       " 10119,\n",
       " 7375,\n",
       " 6078,\n",
       " 1136,\n",
       " 7645,\n",
       " 1143,\n",
       " 10699,\n",
       " 10700,\n",
       " 14024,\n",
       " 4232,\n",
       " 944,\n",
       " 4721,\n",
       " 4025,\n",
       " 1261,\n",
       " 4780,\n",
       " 852,\n",
       " 2119,\n",
       " 9908,\n",
       " 6782,\n",
       " 1614,\n",
       " 1003,\n",
       " 9325,\n",
       " 13120,\n",
       " 11125,\n",
       " 13580,\n",
       " 4323,\n",
       " 8285,\n",
       " 10890,\n",
       " 11307,\n",
       " 2283,\n",
       " 12918,\n",
       " 5253,\n",
       " 6223,\n",
       " 9639,\n",
       " 6559,\n",
       " 558,\n",
       " 11210,\n",
       " 7192,\n",
       " 4760,\n",
       " 10483,\n",
       " 483,\n",
       " 5898,\n",
       " 14317,\n",
       " 5586,\n",
       " 5954,\n",
       " 1235,\n",
       " 5243,\n",
       " 10172,\n",
       " 2166,\n",
       " 1736,\n",
       " 5383,\n",
       " 8012,\n",
       " 5124,\n",
       " 13931,\n",
       " 3882,\n",
       " 8748,\n",
       " 129,\n",
       " 13741,\n",
       " 7930,\n",
       " 6717,\n",
       " 13936,\n",
       " 5077,\n",
       " 10159,\n",
       " 6301,\n",
       " 67,\n",
       " 13559,\n",
       " 7958,\n",
       " 5079,\n",
       " 9225,\n",
       " 3985,\n",
       " 11523,\n",
       " 1645,\n",
       " 5228,\n",
       " 8560,\n",
       " 4249,\n",
       " 6819,\n",
       " 13321,\n",
       " 7130,\n",
       " 305,\n",
       " 9823,\n",
       " 11321,\n",
       " 2031,\n",
       " 13334,\n",
       " 2738,\n",
       " 13305,\n",
       " 2701,\n",
       " 3902,\n",
       " 5703,\n",
       " 5958,\n",
       " 9500,\n",
       " 3217,\n",
       " 10776,\n",
       " 13502,\n",
       " 8041,\n",
       " 4360,\n",
       " 12488,\n",
       " 4520,\n",
       " 9517,\n",
       " 5570,\n",
       " 2958,\n",
       " 3408,\n",
       " 7040,\n",
       " 6995,\n",
       " 12470,\n",
       " 1733,\n",
       " 4607,\n",
       " 2265,\n",
       " 12169,\n",
       " 4387,\n",
       " 9659,\n",
       " 3584,\n",
       " 228,\n",
       " 5943,\n",
       " 5020,\n",
       " 1333,\n",
       " 7956,\n",
       " 1273,\n",
       " 10443,\n",
       " 2532,\n",
       " 6792,\n",
       " 6832,\n",
       " 8339,\n",
       " 14241,\n",
       " 10905,\n",
       " 2950,\n",
       " 13872,\n",
       " 4783,\n",
       " 11688,\n",
       " 8234,\n",
       " 5173,\n",
       " 1274,\n",
       " 7081,\n",
       " 1215,\n",
       " 10534,\n",
       " 2705,\n",
       " 13672,\n",
       " 3027,\n",
       " 5628,\n",
       " 8647,\n",
       " 13657,\n",
       " 10046,\n",
       " 11187,\n",
       " 10424,\n",
       " 4376,\n",
       " 6168,\n",
       " 1776,\n",
       " 4926,\n",
       " 1548,\n",
       " 3797,\n",
       " 11958,\n",
       " 5154,\n",
       " 1435,\n",
       " 10862,\n",
       " 10217,\n",
       " 10988,\n",
       " 8927,\n",
       " 5820,\n",
       " 13855,\n",
       " 4365,\n",
       " 2982,\n",
       " 8517,\n",
       " 2281,\n",
       " 2957,\n",
       " 2209,\n",
       " 402,\n",
       " 2589,\n",
       " 1243,\n",
       " 13186,\n",
       " 11887,\n",
       " 9115,\n",
       " 2151,\n",
       " 5327,\n",
       " 7073,\n",
       " 8228,\n",
       " 2708,\n",
       " 12289,\n",
       " 9126,\n",
       " 5738,\n",
       " 1371,\n",
       " 615,\n",
       " 13422,\n",
       " 6886,\n",
       " 8229,\n",
       " 10666,\n",
       " 2372,\n",
       " 2274,\n",
       " 6567,\n",
       " 9589,\n",
       " 10732,\n",
       " 9085,\n",
       " 8746,\n",
       " 11248,\n",
       " 9776,\n",
       " 13083,\n",
       " 3536,\n",
       " 11482,\n",
       " 11175,\n",
       " 6314,\n",
       " 3486,\n",
       " 12974,\n",
       " 4495,\n",
       " 11222,\n",
       " 7549,\n",
       " 14315,\n",
       " 4806,\n",
       " 9978,\n",
       " 3969,\n",
       " 7760,\n",
       " 4716,\n",
       " 12666,\n",
       " 7152,\n",
       " 8946,\n",
       " 8343,\n",
       " 3943,\n",
       " 4913,\n",
       " 1606,\n",
       " 10309,\n",
       " 4500,\n",
       " 10459,\n",
       " 8139,\n",
       " 5556,\n",
       " 3092,\n",
       " 6203,\n",
       " 3043,\n",
       " 13469,\n",
       " 3382,\n",
       " 997,\n",
       " 2556,\n",
       " 3715,\n",
       " 1475,\n",
       " 13685,\n",
       " 6517,\n",
       " 375,\n",
       " 3057,\n",
       " 2326,\n",
       " 6549,\n",
       " 11938,\n",
       " 1852,\n",
       " 1492,\n",
       " 9431,\n",
       " 13594,\n",
       " 8329,\n",
       " 3786,\n",
       " 6815,\n",
       " 7440,\n",
       " 11844,\n",
       " 1927,\n",
       " 2214,\n",
       " 10246,\n",
       " 202,\n",
       " 7427,\n",
       " 4751,\n",
       " 8535,\n",
       " 7889,\n",
       " 170,\n",
       " 7151,\n",
       " 258,\n",
       " 4333,\n",
       " 3454,\n",
       " 664,\n",
       " 9165,\n",
       " 8557,\n",
       " 6416,\n",
       " 2928,\n",
       " 2422,\n",
       " 7865,\n",
       " 10877,\n",
       " 5566,\n",
       " 8567,\n",
       " 4565,\n",
       " 983,\n",
       " 1593,\n",
       " 8672,\n",
       " 1521,\n",
       " 12194,\n",
       " 8454,\n",
       " 8633,\n",
       " 3785,\n",
       " 94,\n",
       " 5702,\n",
       " 7449,\n",
       " 8351,\n",
       " 287,\n",
       " 11810,\n",
       " 4572,\n",
       " 8818,\n",
       " 6785,\n",
       " 7365,\n",
       " 10031,\n",
       " 2630,\n",
       " 3117,\n",
       " 1807,\n",
       " 13088,\n",
       " 10926,\n",
       " 11028,\n",
       " 6947,\n",
       " 10561,\n",
       " 5351,\n",
       " 9772,\n",
       " 3929,\n",
       " 4178,\n",
       " 10622,\n",
       " 6128,\n",
       " 3706,\n",
       " 6081,\n",
       " 6528,\n",
       " 2907,\n",
       " 754,\n",
       " 5013,\n",
       " 2022,\n",
       " 2250,\n",
       " 518,\n",
       " 1115,\n",
       " 236,\n",
       " 1173,\n",
       " 14090,\n",
       " 490,\n",
       " 3990,\n",
       " 10581,\n",
       " 2986,\n",
       " 4616,\n",
       " 6064,\n",
       " 9301,\n",
       " 6579,\n",
       " 9385,\n",
       " 11142,\n",
       " 1540,\n",
       " 6699,\n",
       " 1131,\n",
       " 2117,\n",
       " 7074,\n",
       " 10060,\n",
       " 13874,\n",
       " 6884,\n",
       " 12107,\n",
       " 11164,\n",
       " 1914,\n",
       " 369,\n",
       " 11638,\n",
       " 4431,\n",
       " 8356,\n",
       " 4184,\n",
       " 10418,\n",
       " 9263,\n",
       " 8991,\n",
       " 7520,\n",
       " 3968,\n",
       " 4071,\n",
       " 8492,\n",
       " 12864,\n",
       " 1078,\n",
       " 11362,\n",
       " 8727,\n",
       " 1196,\n",
       " 4507,\n",
       " 5218,\n",
       " 2823,\n",
       " 11609,\n",
       " 11178,\n",
       " 1427,\n",
       " 10327,\n",
       " 13898,\n",
       " 13164,\n",
       " 1445,\n",
       " 12873,\n",
       " 1759,\n",
       " 12373,\n",
       " 3393,\n",
       " 11089,\n",
       " 12193,\n",
       " 5418,\n",
       " 4040,\n",
       " 5793,\n",
       " 10789,\n",
       " 7049,\n",
       " 8389,\n",
       " 1720,\n",
       " 11711,\n",
       " 13586,\n",
       " 13181,\n",
       " 2937,\n",
       " 8931,\n",
       " 1986,\n",
       " 6436,\n",
       " 4239,\n",
       " 9828,\n",
       " 1347,\n",
       " 2915,\n",
       " 2533,\n",
       " 8554,\n",
       " 11428,\n",
       " 4328,\n",
       " 4464,\n",
       " 11805,\n",
       " 9437,\n",
       " 2353,\n",
       " 10552,\n",
       " 7692,\n",
       " 736,\n",
       " 13192,\n",
       " 11583,\n",
       " 5913,\n",
       " 4656,\n",
       " 2231,\n",
       " 4048,\n",
       " 2424,\n",
       " 8073,\n",
       " 4209,\n",
       " 7891,\n",
       " 6744,\n",
       " 6043,\n",
       " 6595,\n",
       " 12457,\n",
       " 5884,\n",
       " 9627,\n",
       " 13049,\n",
       " 12124,\n",
       " 12111,\n",
       " 3960,\n",
       " 8327,\n",
       " 1137,\n",
       " 6593,\n",
       " 5902,\n",
       " 10705,\n",
       " 1681,\n",
       " 593,\n",
       " 5139,\n",
       " 10697,\n",
       " 3502,\n",
       " 5512,\n",
       " 6679,\n",
       " 1116,\n",
       " 116,\n",
       " 11601,\n",
       " 3326,\n",
       " 4804,\n",
       " 7415,\n",
       " 4288,\n",
       " 8592,\n",
       " 9097,\n",
       " 10023,\n",
       " 13536,\n",
       " 3884,\n",
       " 13901,\n",
       " 1617,\n",
       " 1383,\n",
       " 7921,\n",
       " 8386,\n",
       " 9013,\n",
       " 4712,\n",
       " 2421,\n",
       " 8215,\n",
       " 12706,\n",
       " 7510,\n",
       " 5963,\n",
       " 4560,\n",
       " 10075,\n",
       " 2016,\n",
       " 4796,\n",
       " 10045,\n",
       " 14288,\n",
       " 2804,\n",
       " 9708,\n",
       " 3923,\n",
       " 2071,\n",
       " 5399,\n",
       " 7469,\n",
       " 9423,\n",
       " 3802,\n",
       " 9937,\n",
       " 9231,\n",
       " 10765,\n",
       " 13943,\n",
       " 11273,\n",
       " 2358,\n",
       " 8903,\n",
       " 2316,\n",
       " 3186,\n",
       " 1978,\n",
       " 13165,\n",
       " 6630,\n",
       " 10607,\n",
       " 6438,\n",
       " 4484,\n",
       " 81,\n",
       " 5041,\n",
       " 14012,\n",
       " 10685,\n",
       " 1833,\n",
       " 7202,\n",
       " 10951,\n",
       " 6319,\n",
       " 14234,\n",
       " 4882,\n",
       " 9293,\n",
       " 11758,\n",
       " 1378,\n",
       " 2302,\n",
       " 1075,\n",
       " 8243,\n",
       " 2802,\n",
       " 6038,\n",
       " 13022,\n",
       " 1053,\n",
       " 3014,\n",
       " 8632,\n",
       " 13862,\n",
       " 14279,\n",
       " 3147,\n",
       " 4375,\n",
       " 11928,\n",
       " 3684,\n",
       " 2046,\n",
       " 11658,\n",
       " 6120,\n",
       " 7465,\n",
       " 4711,\n",
       " 167,\n",
       " 2101,\n",
       " 982,\n",
       " 4888,\n",
       " 7280,\n",
       " 13447,\n",
       " 2370,\n",
       " 11820,\n",
       " 10158,\n",
       " 8645,\n",
       " 2489,\n",
       " 6583,\n",
       " 3353,\n",
       " 5787,\n",
       " 1262,\n",
       " 3305,\n",
       " 10125,\n",
       " 14036,\n",
       " 12889,\n",
       " 6112,\n",
       " 5410,\n",
       " 3647,\n",
       " 6865,\n",
       " 12272,\n",
       " 2227,\n",
       " 3781,\n",
       " 6518,\n",
       " 8683,\n",
       " 5379,\n",
       " 1065,\n",
       " 8595,\n",
       " 10623,\n",
       " 2154,\n",
       " 6516,\n",
       " 6527,\n",
       " 5405,\n",
       " 6343,\n",
       " 10730,\n",
       " 10168,\n",
       " 1197,\n",
       " 11184,\n",
       " 9148,\n",
       " 14151,\n",
       " 2256,\n",
       " 9190,\n",
       " 2903,\n",
       " 5670,\n",
       " 1470,\n",
       " 11548,\n",
       " 10094,\n",
       " 8826,\n",
       " 12821,\n",
       " 13826,\n",
       " 6623,\n",
       " 10323,\n",
       " 2364,\n",
       " 8349,\n",
       " 6134,\n",
       " 2827,\n",
       " 9852,\n",
       " 13871,\n",
       " 9302,\n",
       " 2505,\n",
       " 13978,\n",
       " 8066,\n",
       " 14324,\n",
       " 3752,\n",
       " 5083,\n",
       " 11803,\n",
       " 11707,\n",
       " 6871,\n",
       " 8303,\n",
       " 4485,\n",
       " 12358,\n",
       " 2048,\n",
       " 2862,\n",
       " 11324,\n",
       " 8156,\n",
       " 3511,\n",
       " 4417,\n",
       " 10502,\n",
       " 2188,\n",
       " 1960,\n",
       " 4833,\n",
       " 2400,\n",
       " 12959,\n",
       " 5899,\n",
       " 11634,\n",
       " 569,\n",
       " 2104,\n",
       " 3330,\n",
       " 12262,\n",
       " 9134,\n",
       " 9862,\n",
       " 11035,\n",
       " 12593,\n",
       " 4120,\n",
       " 1882,\n",
       " 1972,\n",
       " 2112,\n",
       " 587,\n",
       " 7201,\n",
       " 8189,\n",
       " 3456,\n",
       " 11984,\n",
       " 7433,\n",
       " 10686,\n",
       " 1923,\n",
       " 3850,\n",
       " 9836,\n",
       " 1256,\n",
       " 1917,\n",
       " 3833,\n",
       " 12174,\n",
       " 4776,\n",
       " 12684,\n",
       " 2140,\n",
       " 12819,\n",
       " 8078,\n",
       " 1047,\n",
       " 3747,\n",
       " 321,\n",
       " 13292,\n",
       " 9607,\n",
       " 5513,\n",
       " 13662,\n",
       " 7170,\n",
       " 10655,\n",
       " 3615,\n",
       " 3519,\n",
       " 7221,\n",
       " 2634,\n",
       " 2879,\n",
       " 1674,\n",
       " 9410,\n",
       " 762,\n",
       " 5085,\n",
       " 12805,\n",
       " 6426,\n",
       " 4637,\n",
       " 5306,\n",
       " 3180,\n",
       " 12429,\n",
       " 2115,\n",
       " 12579,\n",
       " 3619,\n",
       " 14162,\n",
       " 7688,\n",
       " 8305,\n",
       " 281,\n",
       " 7189,\n",
       " 14146,\n",
       " 13251,\n",
       " 9561,\n",
       " 1236,\n",
       " 5555,\n",
       " 7654,\n",
       " 9573,\n",
       " 8360,\n",
       " 12784,\n",
       " 9144,\n",
       " 9098,\n",
       " 4795,\n",
       " 7412,\n",
       " 4049,\n",
       " 3024,\n",
       " 2836,\n",
       " 10479,\n",
       " 9547,\n",
       " 12760,\n",
       " 10934,\n",
       " 11774,\n",
       " 447,\n",
       " 2888,\n",
       " 11497,\n",
       " 3727,\n",
       " 5209,\n",
       " 8447,\n",
       " 1067,\n",
       " 8413,\n",
       " 13838,\n",
       " 1040,\n",
       " 6569,\n",
       " 3099,\n",
       " 9822,\n",
       " 9926,\n",
       " 7527,\n",
       " 10751,\n",
       " 1895,\n",
       " 5634,\n",
       " 1854,\n",
       " 4197,\n",
       " 8519,\n",
       " 3977,\n",
       " 11549,\n",
       " 4440,\n",
       " 8124,\n",
       " 13673,\n",
       " 3000,\n",
       " 682,\n",
       " 7296,\n",
       " 7067,\n",
       " 8419,\n",
       " 11108,\n",
       " 3829,\n",
       " 8226,\n",
       " 5582,\n",
       " 4999,\n",
       " 11050,\n",
       " 7108,\n",
       " 8568,\n",
       " 4836,\n",
       " 7312,\n",
       " 8350,\n",
       " 2277,\n",
       " 7609,\n",
       " 7705,\n",
       " 2672,\n",
       " 8323,\n",
       " 3630,\n",
       " 3021,\n",
       " 9514,\n",
       " 3917,\n",
       " 8865,\n",
       " 1277,\n",
       " 12848,\n",
       " 6850,\n",
       " 3792,\n",
       " 3535,\n",
       " 9056,\n",
       " 2308,\n",
       " 276,\n",
       " 10090,\n",
       " 2465,\n",
       " 3123,\n",
       " 6196,\n",
       " 2365,\n",
       " 4102,\n",
       " 12667,\n",
       " 7139,\n",
       " 11277,\n",
       " 6628,\n",
       " 4961,\n",
       " 9853,\n",
       " 6066,\n",
       " 9885,\n",
       " 6428,\n",
       " 8225,\n",
       " 6556,\n",
       " 3959,\n",
       " 12116,\n",
       " 14426,\n",
       " 11657,\n",
       " 3285,\n",
       " 7456,\n",
       " 11022,\n",
       " 5486,\n",
       " 594,\n",
       " 1315,\n",
       " 4251,\n",
       " 10010,\n",
       " 11278,\n",
       " 585,\n",
       " 13443,\n",
       " 7561,\n",
       " 7590,\n",
       " 13266,\n",
       " 12511,\n",
       " 14318,\n",
       " 7000,\n",
       " 13724,\n",
       " 7047,\n",
       " 1074,\n",
       " 7610,\n",
       " 10695,\n",
       " 5736,\n",
       " 2788,\n",
       " 6606,\n",
       " 12776,\n",
       " 4159,\n",
       " 8641,\n",
       " 596,\n",
       " 9135,\n",
       " 12142,\n",
       " 10379,\n",
       " 7591,\n",
       " 12323,\n",
       " 6389,\n",
       " 10063,\n",
       " 10887,\n",
       " 1737,\n",
       " 3319,\n",
       " 13571,\n",
       " 2337,\n",
       " 9552,\n",
       " 2292,\n",
       " 6246,\n",
       " 12199,\n",
       " 8942,\n",
       " 11492,\n",
       " 10828,\n",
       " 5192,\n",
       " 1079,\n",
       " 3268,\n",
       " 7313,\n",
       " 8250,\n",
       " 12386,\n",
       " 7755,\n",
       " 5784,\n",
       " 5061,\n",
       " 4433,\n",
       " 2099,\n",
       " 7203,\n",
       " 11296,\n",
       " 10038,\n",
       " 1124,\n",
       " 6612,\n",
       " 294,\n",
       " 2676,\n",
       " 455,\n",
       " 6963,\n",
       " 10542,\n",
       " 3037,\n",
       " 586,\n",
       " 10409,\n",
       " 9189,\n",
       " 5792,\n",
       " 13567,\n",
       " 10452,\n",
       " 14263,\n",
       " 3327,\n",
       " 10138,\n",
       " 1484,\n",
       " 3272,\n",
       " 7017,\n",
       " 13045,\n",
       " 5823,\n",
       " 10435,\n",
       " 2686,\n",
       " 3013,\n",
       " 13892,\n",
       " 4800,\n",
       " 8231,\n",
       " 8716,\n",
       " 11406,\n",
       " 10500,\n",
       " 10147,\n",
       " 5593,\n",
       " 13906,\n",
       " 7019,\n",
       " 12578,\n",
       " 11289,\n",
       " 2995,\n",
       " 7089,\n",
       " 13831,\n",
       " 13138,\n",
       " 3150,\n",
       " 13003,\n",
       " 8405,\n",
       " 7769,\n",
       " 8998,\n",
       " 6639,\n",
       " 6278,\n",
       " 1575,\n",
       " 10321,\n",
       " 148,\n",
       " 8820,\n",
       " 3389,\n",
       " 10701,\n",
       " 7341,\n",
       " 11967,\n",
       " 4840,\n",
       " 5146,\n",
       " 9026,\n",
       " 5985,\n",
       " 6514,\n",
       " 1619,\n",
       " 13858,\n",
       " 13090,\n",
       " 5280,\n",
       " 319,\n",
       " 7377,\n",
       " 1844,\n",
       " 8814,\n",
       " 13130,\n",
       " 13206,\n",
       " 149,\n",
       " 13017,\n",
       " 5151,\n",
       " 8497,\n",
       " 8467,\n",
       " 1311,\n",
       " 10151,\n",
       " 3356,\n",
       " 255,\n",
       " 5312,\n",
       " 8786,\n",
       " 4792,\n",
       " 13078,\n",
       " 7631,\n",
       " 340,\n",
       " 641,\n",
       " 2097,\n",
       " 3707,\n",
       " 11542,\n",
       " 484,\n",
       " 8383,\n",
       " 4558,\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_indices = label_indices\n",
    "labeled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "084c8912-b5db-4150-9d55-1afc7b87ff92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4166, 3992, 6804, 9682, 8075, 7839, 6377, 498, 8600, 7278, 11109, 7083, 11562, 3839, 5233, 13314, 10053, 5298, 142, 1995, 654, 9800, 10764, 7266, 13779, 1426, 599, 11407, 13214, 13243, 4676, 2896, 2819, 7573, 11002, 6681, 7783, 7252, 4935, 6101, 938, 13001, 9364, 11000, 2785, 7640, 915, 2419, 1898, 11345, 7892, 5506, 11286, 13965, 9910, 243, 10201, 8031, 14214, 1493, 11458, 5242, 6136, 8733, 7528, 8272, 6885, 12304, 373, 1431, 14117, 13512, 3159, 2929, 9416, 10119, 7375, 6078, 1136, 7645, 1143, 10699, 10700, 14024, 4232, 944, 4721, 4025, 1261, 4780, 852, 2119, 9908, 6782, 1614, 1003, 9325, 13120, 11125, 13580, 4323, 8285, 10890, 11307, 2283, 12918, 5253, 6223, 9639, 6559, 558, 11210, 7192, 4760, 10483, 483, 5898, 14317, 5586, 5954, 1235, 5243, 10172, 2166, 1736, 5383, 8012, 5124, 13931, 3882, 8748, 129, 13741, 7930, 6717, 13936, 5077, 10159, 6301, 67, 13559, 7958, 5079, 9225, 3985, 11523, 1645, 5228, 8560, 4249, 6819, 13321, 7130, 305, 9823, 11321, 2031, 13334, 2738, 13305, 2701, 3902, 5703, 5958, 9500, 3217, 10776, 13502, 8041, 4360, 12488, 4520, 9517, 5570, 2958, 3408, 7040, 6995, 12470, 1733, 4607, 2265, 12169, 4387, 9659, 3584, 228, 5943, 5020, 1333, 7956, 1273, 10443, 2532, 6792, 6832, 8339, 14241, 10905, 2950, 13872, 4783, 11688, 8234, 5173, 1274, 7081, 1215, 10534, 2705, 13672, 3027, 5628, 8647, 13657, 10046, 11187, 10424, 4376, 6168, 1776, 4926, 1548, 3797, 11958, 5154, 1435, 10862, 10217, 10988, 8927, 5820, 13855, 4365, 2982, 8517, 2281, 2957, 2209, 402, 2589, 1243, 13186, 11887, 9115, 2151, 5327, 7073, 8228, 2708, 12289, 9126, 5738, 1371, 615, 13422, 6886, 8229, 10666, 2372, 2274, 6567, 9589, 10732, 9085, 8746, 11248, 9776, 13083, 3536, 11482, 11175, 6314, 3486, 12974, 4495, 11222, 7549, 14315, 4806, 9978, 3969, 7760, 4716, 12666, 7152, 8946, 8343, 3943, 4913, 1606, 10309, 4500, 10459, 8139, 5556, 3092, 6203, 3043, 13469, 3382, 997, 2556, 3715, 1475, 13685, 6517, 375, 3057, 2326, 6549, 11938, 1852, 1492, 9431, 13594, 8329, 3786, 6815, 7440, 11844, 1927, 2214, 10246, 202, 7427, 4751, 8535, 7889, 170, 7151, 258, 4333, 3454, 664, 9165, 8557, 6416, 2928, 2422, 7865, 10877, 5566, 8567, 4565, 983, 1593, 8672, 1521, 12194, 8454, 8633, 3785, 94, 5702, 7449, 8351, 287, 11810, 4572, 8818, 6785, 7365, 10031, 2630, 3117, 1807, 13088, 10926, 11028, 6947, 10561, 5351, 9772, 3929, 4178, 10622, 6128, 3706, 6081, 6528, 2907, 754, 5013, 2022, 2250, 518, 1115, 236, 1173, 14090, 490, 3990, 10581, 2986, 4616, 6064, 9301, 6579, 9385, 11142, 1540, 6699, 1131, 2117, 7074, 10060, 13874, 6884, 12107, 11164, 1914, 369, 11638, 4431, 8356, 4184, 10418, 9263, 8991, 7520, 3968, 4071, 8492, 12864, 1078, 11362, 8727, 1196, 4507, 5218, 2823, 11609, 11178, 1427, 10327, 13898, 13164, 1445, 12873, 1759, 12373, 3393, 11089, 12193, 5418, 4040, 5793, 10789, 7049, 8389, 1720, 11711, 13586, 13181, 2937, 8931, 1986, 6436, 4239, 9828, 1347, 2915, 2533, 8554, 11428, 4328, 4464, 11805, 9437, 2353, 10552, 7692, 736, 13192, 11583, 5913, 4656, 2231, 4048, 2424, 8073, 4209, 7891, 6744, 6043, 6595, 12457, 5884, 9627, 13049, 12124, 12111, 3960, 8327, 1137, 6593, 5902, 10705, 1681, 593, 5139, 10697, 3502, 5512, 6679, 1116, 116, 11601, 3326, 4804, 7415, 4288, 8592, 9097, 10023, 13536, 3884, 13901, 1617, 1383, 7921, 8386, 9013, 4712, 2421, 8215, 12706, 7510, 5963, 4560, 10075, 2016, 4796, 10045, 14288, 2804, 9708, 3923, 2071, 5399, 7469, 9423, 3802, 9937, 9231, 10765, 13943, 11273, 2358, 8903, 2316, 3186, 1978, 13165, 6630, 10607, 6438, 4484, 81, 5041, 14012, 10685, 1833, 7202, 10951, 6319, 14234, 4882, 9293, 11758, 1378, 2302, 1075, 8243, 2802, 6038, 13022, 1053, 3014, 8632, 13862, 14279, 3147, 4375, 11928, 3684, 2046, 11658, 6120, 7465, 4711, 167, 2101, 982, 4888, 7280, 13447, 2370, 11820, 10158, 8645, 2489, 6583, 3353, 5787, 1262, 3305, 10125, 14036, 12889, 6112, 5410, 3647, 6865, 12272, 2227, 3781, 6518, 8683, 5379, 1065, 8595, 10623, 2154, 6516, 6527, 5405, 6343, 10730, 10168, 1197, 11184, 9148, 14151, 2256, 9190, 2903, 5670, 1470, 11548, 10094, 8826, 12821, 13826, 6623, 10323, 2364, 8349, 6134, 2827, 9852, 13871, 9302, 2505, 13978, 8066, 14324, 3752, 5083, 11803, 11707, 6871, 8303, 4485, 12358, 2048, 2862, 11324, 8156, 3511, 4417, 10502, 2188, 1960, 4833, 2400, 12959, 5899, 11634, 569, 2104, 3330, 12262, 9134, 9862, 11035, 12593, 4120, 1882, 1972, 2112, 587, 7201, 8189, 3456, 11984, 7433, 10686, 1923, 3850, 9836, 1256, 1917, 3833, 12174, 4776, 12684, 2140, 12819, 8078, 1047, 3747, 321, 13292, 9607, 5513, 13662, 7170, 10655, 3615, 3519, 7221, 2634, 2879, 1674, 9410, 762, 5085, 12805, 6426, 4637, 5306, 3180, 12429, 2115, 12579, 3619, 14162, 7688, 8305, 281, 7189, 14146, 13251, 9561, 1236, 5555, 7654, 9573, 8360, 12784, 9144, 9098, 4795, 7412, 4049, 3024, 2836, 10479, 9547, 12760, 10934, 11774, 447, 2888, 11497, 3727, 5209, 8447, 1067, 8413, 13838, 1040, 6569, 3099, 9822, 9926, 7527, 10751, 1895, 5634, 1854, 4197, 8519, 3977, 11549, 4440, 8124, 13673, 3000, 682, 7296, 7067, 8419, 11108, 3829, 8226, 5582, 4999, 11050, 7108, 8568, 4836, 7312, 8350, 2277, 7609, 7705, 2672, 8323, 3630, 3021, 9514, 3917, 8865, 1277, 12848, 6850, 3792, 3535, 9056, 2308, 276, 10090, 2465, 3123, 6196, 2365, 4102, 12667, 7139, 11277, 6628, 4961, 9853, 6066, 9885, 6428, 8225, 6556, 3959, 12116, 14426, 11657, 3285, 7456, 11022, 5486, 594, 1315, 4251, 10010, 11278, 585, 13443, 7561, 7590, 13266, 12511, 14318, 7000, 13724, 7047, 1074, 7610, 10695, 5736, 2788, 6606, 12776, 4159, 8641, 596, 9135, 12142, 10379, 7591, 12323, 6389, 10063, 10887, 1737, 3319, 13571, 2337, 9552, 2292, 6246, 12199, 8942, 11492, 10828, 5192, 1079, 3268, 7313, 8250, 12386, 7755, 5784, 5061, 4433, 2099, 7203, 11296, 10038, 1124, 6612, 294, 2676, 455, 6963, 10542, 3037, 586, 10409, 9189, 5792, 13567, 10452, 14263, 3327, 10138, 1484, 3272, 7017, 13045, 5823, 10435, 2686, 3013, 13892, 4800, 8231, 8716, 11406, 10500, 10147, 5593, 13906, 7019, 12578, 11289, 2995, 7089, 13831, 13138, 3150, 13003, 8405, 7769, 8998, 6639, 6278, 1575, 10321, 148, 8820, 3389, 10701, 7341, 11967, 4840, 5146, 9026, 5985, 6514, 1619, 13858, 13090, 5280, 319, 7377, 1844, 8814, 13130, 13206, 149, 13017, 5151, 8497, 8467, 1311, 10151, 3356, 255, 5312, 8786, 4792, 13078, 7631, 340, 641, 2097, 3707, 11542, 484, 8383, 4558, 5226, 7093, 2476, 7563, 3783, 773, 5216, 8060, 13084, 6448, 9512, 4897, 2535, 10629, 10580, 13760, 10825, 12958, 6728, 5178, 4915, 11033, 11987, 10917, 11533, 12080, 11340, 8491, 5713, 4465, 7026, 12130, 1120, 1848, 13666, 5721, 10298, 334, 3465, 8262, 11410, 100, 6427, 13409, 8196, 7185, 11951, 5437, 4133, 7793, 13566, 13727, 9435, 4439, 6127, 8631, 9757, 3663, 12154, 5510, 3680, 1170, 8828, 10325, 7300, 5693, 4351, 2284, 8741, 8175, 13984, 14240, 12151, 9900, 3064, 10300, 899, 5072, 9535, 7357, 14268, 4668, 10183, 1795, 2660, 6823, 4539, 5144, 12487, 13042, 1392, 3538, 2273, 4930, 10779, 3240, 13847, 8046, 7564, 3754, 4526, 12302, 3084]\n",
      "[5488, 3620, 3054, 5589, 6011, 14325, 9809, 2455, 7653, 8266, 12955, 4995, 3255, 2286, 4332, 5674, 3810, 5167, 12071, 764, 9246, 7157, 6131, 3410, 7286, 2109, 8348, 12296, 6703, 12102, 8357, 1195, 7153, 2015, 383, 5278, 6494, 3244, 7715, 5264, 8576, 2871, 6328, 14052, 2927, 10411, 7116, 2770, 8366, 14293, 2551, 11343, 11655, 7721, 5991, 8808, 894, 384, 4630, 1444, 7481, 1064, 4545, 12205, 11192, 4513, 833, 12587, 2716, 4963, 9323, 10000, 11140, 1157, 10527, 10364, 5710, 3949, 14015, 6006, 5271, 1327, 6702, 4653, 5522, 1105, 1710, 4524, 7349, 13882, 7387, 13295, 7665, 5785, 13951, 5396, 12432, 9027, 5569, 895, 6607, 9784, 6179, 6239, 1834, 5170, 2362, 1019, 1823, 12103, 4361, 7065, 2196, 8653, 6667, 6943, 2496, 157, 7389, 8441, 11418, 13924, 3991, 10536, 3543, 5992, 10436, 6057, 8290, 2427, 1559, 10372, 2625, 7782, 917, 8451, 1638, 777, 2842, 10410, 14407, 11676, 2293, 12705, 12956, 6714, 93, 6302, 13582, 2481, 1849, 11752, 1062, 5866, 6211, 6460, 1183, 6099, 12345, 11734, 11572, 5575, 6058, 526, 3325, 1397, 10432, 6094, 8013, 3921, 5006, 3530, 12271, 3417, 12517, 6844, 3751, 11040, 10485, 8691, 399, 3779, 474, 4853, 1237, 1527, 1609, 4225, 13394, 2096, 6226, 7018, 7420, 10619, 10794, 9875, 8749, 7634, 8212, 5890, 5687, 2730, 6774, 7390, 6772, 8077, 2238, 241, 4169, 11150, 3692, 7281, 1637, 11247, 1363, 8608, 6227, 10754, 653, 9066, 11746, 10708, 10390, 2354, 1108, 12954, 8480, 418, 8720, 2417, 3433, 1469, 3886, 4165, 9457, 4034, 14398, 1377, 6859, 4185, 595, 4815, 5900, 7599, 2960, 11749, 5635, 1489, 2620, 4438, 4343, 4258, 9638, 14281, 3044, 1970, 6119, 10146, 781, 2767, 3291, 6562, 8204, 7125, 5644, 12348, 511, 6177, 8815, 12284, 11538, 3774, 2540, 957, 4019, 1893]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "train_indices = labeled_indices[:num_train]\n",
    "test_indices = labeled_indices[num_train:num_train+num_test]\n",
    "print(train_indices)\n",
    "print(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3dd4e28c-5160-4bdb-b71f-9a99072810e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61769\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 100\n",
    "count = 1 / num_iterations\n",
    "\n",
    "# 总节点数\n",
    "num_nodes = data.x.shape[0]\n",
    "# 所有节点的索引\n",
    "all_indices = np.arange(num_nodes)\n",
    "mask_out = torch.ones(num_nodes, dtype=torch.bool)\n",
    "# 将测试集索引处的掩码设为False\n",
    "#mask_out[test_indices] = False\n",
    "end = len(combined_features) - 47595\n",
    "mask_out[-47595:] = False\n",
    "mask_out[0:end] = False\n",
    "# 使用掩码获取剩余的索引\n",
    "remaining_indices = all_indices[mask_out]\n",
    "complement_mask = ~mask_out\n",
    "complement_mask[test_indices] = False\n",
    "\n",
    "# 得到既不在 remaining_indices 也不在 test_indices 中的索引\n",
    "complement_indices = all_indices[complement_mask]\n",
    "print(len(complement_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f88947fd-0c6b-4bff-8d66-8adb48c502a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: 61769\n",
      "s: 0\n",
      "c: 61152\n",
      "s: 617\n",
      "c: 60535\n",
      "s: 617\n",
      "c: 59918\n",
      "s: 617\n",
      "c: 59301\n",
      "s: 617\n",
      "c: 58684\n",
      "s: 617\n",
      "c: 58067\n",
      "s: 617\n",
      "c: 57450\n",
      "s: 617\n",
      "c: 56833\n",
      "s: 617\n",
      "c: 56216\n",
      "s: 617\n",
      "c: 55599\n",
      "s: 617\n",
      "c: 54982\n",
      "s: 617\n",
      "c: 54365\n",
      "s: 617\n",
      "c: 53748\n",
      "s: 617\n",
      "c: 53131\n",
      "s: 617\n",
      "c: 52514\n",
      "s: 617\n",
      "c: 51897\n",
      "s: 617\n",
      "c: 51280\n",
      "s: 617\n",
      "c: 50663\n",
      "s: 617\n",
      "c: 50046\n",
      "s: 617\n",
      "c: 49429\n",
      "s: 617\n",
      "c: 48812\n",
      "s: 617\n",
      "c: 48195\n",
      "s: 617\n",
      "c: 47578\n",
      "s: 617\n",
      "c: 46961\n",
      "s: 617\n",
      "c: 46344\n",
      "s: 617\n",
      "c: 45727\n",
      "s: 617\n",
      "c: 45110\n",
      "s: 617\n",
      "c: 44493\n",
      "s: 617\n",
      "c: 43876\n",
      "s: 617\n",
      "c: 43259\n",
      "s: 617\n",
      "c: 42642\n",
      "s: 617\n",
      "c: 42025\n",
      "s: 617\n",
      "c: 41408\n",
      "s: 617\n",
      "c: 40791\n",
      "s: 617\n",
      "c: 40174\n",
      "s: 617\n",
      "c: 39557\n",
      "s: 617\n",
      "c: 38940\n",
      "s: 617\n",
      "c: 38323\n",
      "s: 617\n",
      "c: 37706\n",
      "s: 617\n",
      "c: 37089\n",
      "s: 617\n",
      "c: 36472\n",
      "s: 617\n",
      "c: 35855\n",
      "s: 617\n",
      "c: 35238\n",
      "s: 617\n",
      "c: 34621\n",
      "s: 617\n",
      "c: 34004\n",
      "s: 617\n",
      "c: 33387\n",
      "s: 617\n",
      "c: 32770\n",
      "s: 617\n",
      "c: 32153\n",
      "s: 617\n",
      "c: 31536\n",
      "s: 617\n",
      "c: 30919\n",
      "s: 617\n",
      "c: 30302\n",
      "s: 617\n",
      "c: 29685\n",
      "s: 617\n",
      "c: 29068\n",
      "s: 617\n",
      "c: 28451\n",
      "s: 617\n",
      "c: 27834\n",
      "s: 617\n",
      "c: 27217\n",
      "s: 617\n",
      "c: 26600\n",
      "s: 617\n",
      "c: 25983\n",
      "s: 617\n",
      "c: 25366\n",
      "s: 617\n",
      "c: 24749\n",
      "s: 617\n",
      "c: 24132\n",
      "s: 617\n",
      "c: 23515\n",
      "s: 617\n",
      "c: 22898\n",
      "s: 617\n",
      "c: 22281\n",
      "s: 617\n",
      "c: 21664\n",
      "s: 617\n",
      "c: 21047\n",
      "s: 617\n",
      "c: 20430\n",
      "s: 617\n",
      "c: 19813\n",
      "s: 617\n",
      "c: 19196\n",
      "s: 617\n",
      "c: 18579\n",
      "s: 617\n",
      "c: 17962\n",
      "s: 617\n",
      "c: 17345\n",
      "s: 617\n",
      "c: 16728\n",
      "s: 617\n",
      "c: 16111\n",
      "s: 617\n",
      "c: 15494\n",
      "s: 617\n",
      "c: 14877\n",
      "s: 617\n",
      "c: 14260\n",
      "s: 617\n",
      "c: 13643\n",
      "s: 617\n",
      "c: 13026\n",
      "s: 617\n",
      "c: 12409\n",
      "s: 617\n",
      "c: 11792\n",
      "s: 617\n",
      "c: 11175\n",
      "s: 617\n",
      "c: 10558\n",
      "s: 617\n",
      "c: 9941\n",
      "s: 617\n",
      "c: 9324\n",
      "s: 617\n",
      "c: 8707\n",
      "s: 617\n",
      "c: 8090\n",
      "s: 617\n",
      "c: 7473\n",
      "s: 617\n",
      "c: 6856\n",
      "s: 617\n",
      "c: 6239\n",
      "s: 617\n",
      "c: 5622\n",
      "s: 617\n",
      "c: 5005\n",
      "s: 617\n",
      "c: 4388\n",
      "s: 617\n",
      "c: 3771\n",
      "s: 617\n",
      "c: 3154\n",
      "s: 617\n",
      "c: 2537\n",
      "s: 617\n",
      "c: 1920\n",
      "s: 617\n",
      "c: 1303\n",
      "s: 617\n",
      "c: 686\n",
      "s: 617\n",
      "c: 69\n",
      "s: 617\n",
      "所有迭代的selected_indices和remaining_indices已保存到文件。\n"
     ]
    }
   ],
   "source": [
    "num_nodes_out = len(complement_indices)\n",
    "num_to_select = int(num_nodes_out * count)\n",
    "for i in range(num_iterations+1):\n",
    "    if i > 0:\n",
    "    # 随机选择节点\n",
    "        selected_indices = np.random.choice(complement_indices, num_to_select, replace=False)\n",
    "        # 更新剩余节点列表\n",
    "        complement_indices = np.setdiff1d(complement_indices, selected_indices)\n",
    "    else:\n",
    "        selected_indices = np.random.choice(complement_indices, 0, replace=False)\n",
    "        complement_indices = np.setdiff1d(complement_indices, selected_indices)\n",
    "    print(\"c:\",len(complement_indices))\n",
    "    # 保存到文件\n",
    "    print(\"s:\",len(selected_indices))\n",
    "    np.save(f'DIVIDED_DATA/without_{i}%.npy', complement_indices)  # 修改保存路径\n",
    "\n",
    "print(\"所有迭代的selected_indices和remaining_indices已保存到文件。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d33b919-677d-4c37-82b9-04441d1b7c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"早停机制，用于在验证损失停止改善时终止训练。\"\"\"\n",
    "    def __init__(self, patience=200, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            patience (int): 损失没有改善的迭代次数，在这之后训练将会被停止。\n",
    "            verbose (bool): 如果为True，则打印一条消息表明早停被触发。\n",
    "            delta (float): 损失的最小改变，被认为是改善。\n",
    "            path (str): 最佳模型保存路径。\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    print(\"Early stopping triggered\")\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "    \n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        '''保存模型当验证损失减少时'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a793bc7d-4ba9-4b41-9b49-3956e9e996fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33960226, -0.03074448, -0.90138096, ...,  0.01558092,\n",
       "        -0.02386307, -0.02200161],\n",
       "       [-0.13179901, -0.02574519, -0.67730105, ..., -0.03992649,\n",
       "        -0.10278717, -0.02697964],\n",
       "       [ 0.38569278, -0.07069244, -0.8477959 , ...,  0.0253919 ,\n",
       "        -0.06603534, -0.02828273],\n",
       "       ...,\n",
       "       [ 0.02713387,  0.24139147, -0.22735251, ..., -0.82107705,\n",
       "         1.036363  , -0.83661443],\n",
       "       [ 0.13954346,  0.02888298,  0.89947975, ..., -0.9852566 ,\n",
       "         1.6735605 ,  0.10965873],\n",
       "       [ 0.08306409,  0.09089889,  0.8885408 , ..., -0.79955566,\n",
       "         1.5193683 ,  0.2632099 ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_features = features\n",
    "masked_features[torch.tensor(complement_indices)] = 0\n",
    "masked_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51b960d9-8359-420c-a7b1-bca685ed97af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 0.6931, Macro_F1: 0.3566, AUC_score: 0.5907\n",
      "Validation loss decreased (0.693147 --> 0.693147).\n",
      "Validation loss decreased (0.693119 --> 0.693119).\n",
      "Epoch 50: Train Loss: 0.6931, Macro_F1: 0.3566, AUC_score: 0.5766\n",
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 100: Train Loss: 0.6931, Macro_F1: 0.3566, AUC_score: 0.4154\n",
      "Epoch 00103: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.6931, Macro_F1: 0.3566, AUC_score: 0.4839\n",
      "Epoch 00154: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 200: Train Loss: 0.6931, Macro_F1: 0.3566, AUC_score: 0.4878\n",
      "Early stopping triggered\n",
      "acc save\n",
      "99.0% node features transform to 0: F1: 0.3566, AUC_score: 0.4917\n",
      "Epoch 0: Train Loss: 0.6931, Macro_F1: 0.3566, AUC_score: 0.6031\n",
      "Validation loss decreased (0.693114 --> 0.693114).\n",
      "Validation loss decreased (0.693083 --> 0.693083).\n",
      "Validation loss decreased (0.693065 --> 0.693065).\n",
      "Validation loss decreased (0.693034 --> 0.693034).\n",
      "Validation loss decreased (0.693013 --> 0.693013).\n",
      "Validation loss decreased (0.692953 --> 0.692953).\n",
      "Validation loss decreased (0.692860 --> 0.692860).\n",
      "Validation loss decreased (0.692768 --> 0.692768).\n",
      "Validation loss decreased (0.692661 --> 0.692661).\n",
      "Validation loss decreased (0.692465 --> 0.692465).\n",
      "Validation loss decreased (0.692408 --> 0.692408).\n",
      "Validation loss decreased (0.692165 --> 0.692165).\n",
      "Validation loss decreased (0.691892 --> 0.691892).\n",
      "Validation loss decreased (0.691261 --> 0.691261).\n",
      "Validation loss decreased (0.690873 --> 0.690873).\n",
      "Validation loss decreased (0.690321 --> 0.690321).\n",
      "Validation loss decreased (0.689923 --> 0.689923).\n",
      "Validation loss decreased (0.688960 --> 0.688960).\n",
      "Validation loss decreased (0.688664 --> 0.688664).\n",
      "Validation loss decreased (0.687810 --> 0.687810).\n",
      "Validation loss decreased (0.685720 --> 0.685720).\n",
      "Validation loss decreased (0.685111 --> 0.685111).\n",
      "Validation loss decreased (0.683650 --> 0.683650).\n",
      "Validation loss decreased (0.681578 --> 0.681578).\n",
      "Validation loss decreased (0.681160 --> 0.681160).\n",
      "Validation loss decreased (0.678041 --> 0.678041).\n",
      "Validation loss decreased (0.675924 --> 0.675924).\n",
      "Validation loss decreased (0.675775 --> 0.675775).\n",
      "Validation loss decreased (0.672740 --> 0.672740).\n",
      "Validation loss decreased (0.672370 --> 0.672370).\n",
      "Validation loss decreased (0.668533 --> 0.668533).\n",
      "Validation loss decreased (0.663688 --> 0.663688).\n",
      "Validation loss decreased (0.661388 --> 0.661388).\n",
      "Validation loss decreased (0.657575 --> 0.657575).\n",
      "Validation loss decreased (0.654974 --> 0.654974).\n",
      "Validation loss decreased (0.654071 --> 0.654071).\n",
      "Validation loss decreased (0.648837 --> 0.648837).\n",
      "Validation loss decreased (0.646171 --> 0.646171).\n",
      "Validation loss decreased (0.642283 --> 0.642283).\n",
      "Validation loss decreased (0.638271 --> 0.638271).\n",
      "Validation loss decreased (0.629234 --> 0.629234).\n",
      "Validation loss decreased (0.626554 --> 0.626554).\n",
      "Validation loss decreased (0.620625 --> 0.620625).\n",
      "Validation loss decreased (0.615486 --> 0.615486).\n",
      "Validation loss decreased (0.612016 --> 0.612016).\n",
      "Validation loss decreased (0.609951 --> 0.609951).\n",
      "Epoch 50: Train Loss: 0.6035, Macro_F1: 0.7396, AUC_score: 0.8667\n",
      "Validation loss decreased (0.603454 --> 0.603454).\n",
      "Validation loss decreased (0.597869 --> 0.597869).\n",
      "Validation loss decreased (0.593474 --> 0.593474).\n",
      "Validation loss decreased (0.584316 --> 0.584316).\n",
      "Validation loss decreased (0.577160 --> 0.577160).\n",
      "Validation loss decreased (0.565304 --> 0.565304).\n",
      "Validation loss decreased (0.564189 --> 0.564189).\n",
      "Validation loss decreased (0.562910 --> 0.562910).\n",
      "Validation loss decreased (0.545927 --> 0.545927).\n",
      "Validation loss decreased (0.543584 --> 0.543584).\n",
      "Validation loss decreased (0.537470 --> 0.537470).\n",
      "Validation loss decreased (0.532416 --> 0.532416).\n",
      "Validation loss decreased (0.516473 --> 0.516473).\n",
      "Validation loss decreased (0.512337 --> 0.512337).\n",
      "Validation loss decreased (0.505940 --> 0.505940).\n",
      "Validation loss decreased (0.488130 --> 0.488130).\n",
      "Validation loss decreased (0.482865 --> 0.482865).\n",
      "Validation loss decreased (0.468482 --> 0.468482).\n",
      "Validation loss decreased (0.463292 --> 0.463292).\n",
      "Validation loss decreased (0.449026 --> 0.449026).\n",
      "Validation loss decreased (0.439959 --> 0.439959).\n",
      "Epoch 100: Train Loss: 0.4588, Macro_F1: 0.7854, AUC_score: 0.8929\n",
      "Validation loss decreased (0.433385 --> 0.433385).\n",
      "Validation loss decreased (0.418436 --> 0.418436).\n",
      "Validation loss decreased (0.417385 --> 0.417385).\n",
      "Validation loss decreased (0.412772 --> 0.412772).\n",
      "Validation loss decreased (0.412716 --> 0.412716).\n",
      "Validation loss decreased (0.412626 --> 0.412626).\n",
      "Validation loss decreased (0.393574 --> 0.393574).\n",
      "Validation loss decreased (0.388431 --> 0.388431).\n",
      "Validation loss decreased (0.381842 --> 0.381842).\n",
      "Epoch 150: Train Loss: 0.4219, Macro_F1: 0.7843, AUC_score: 0.9097\n",
      "Validation loss decreased (0.377586 --> 0.377586).\n",
      "Validation loss decreased (0.377002 --> 0.377002).\n",
      "Validation loss decreased (0.372918 --> 0.372918).\n",
      "Validation loss decreased (0.369130 --> 0.369130).\n",
      "Validation loss decreased (0.363246 --> 0.363246).\n",
      "Epoch 200: Train Loss: 0.3671, Macro_F1: 0.8031, AUC_score: 0.9172\n",
      "Validation loss decreased (0.358431 --> 0.358431).\n",
      "Validation loss decreased (0.354253 --> 0.354253).\n",
      "Validation loss decreased (0.347689 --> 0.347689).\n",
      "Validation loss decreased (0.344774 --> 0.344774).\n",
      "Epoch 250: Train Loss: 0.3694, Macro_F1: 0.8184, AUC_score: 0.9173\n",
      "Validation loss decreased (0.344147 --> 0.344147).\n",
      "Validation loss decreased (0.343166 --> 0.343166).\n",
      "Validation loss decreased (0.339226 --> 0.339226).\n",
      "Epoch 300: Train Loss: 0.3441, Macro_F1: 0.8185, AUC_score: 0.9177\n",
      "Validation loss decreased (0.338456 --> 0.338456).\n",
      "Validation loss decreased (0.335714 --> 0.335714).\n",
      "Validation loss decreased (0.330190 --> 0.330190).\n",
      "Validation loss decreased (0.320584 --> 0.320584).\n",
      "Epoch 350: Train Loss: 0.3481, Macro_F1: 0.8214, AUC_score: 0.9195\n",
      "Validation loss decreased (0.315840 --> 0.315840).\n",
      "Epoch 400: Train Loss: 0.3496, Macro_F1: 0.8222, AUC_score: 0.9223\n",
      "Epoch 450: Train Loss: 0.3251, Macro_F1: 0.8296, AUC_score: 0.9217\n",
      "Epoch 00499: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 500: Train Loss: 0.3414, Macro_F1: 0.8332, AUC_score: 0.9211\n",
      "Validation loss decreased (0.312859 --> 0.312859).\n",
      "Validation loss decreased (0.298995 --> 0.298995).\n",
      "Epoch 550: Train Loss: 0.3626, Macro_F1: 0.8292, AUC_score: 0.9212\n",
      "Epoch 600: Train Loss: 0.3333, Macro_F1: 0.8259, AUC_score: 0.9224\n",
      "Epoch 00641: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 650: Train Loss: 0.3450, Macro_F1: 0.8366, AUC_score: 0.9219\n",
      "Validation loss decreased (0.298608 --> 0.298608).\n",
      "Epoch 700: Train Loss: 0.3293, Macro_F1: 0.8366, AUC_score: 0.9218\n",
      "Epoch 750: Train Loss: 0.3125, Macro_F1: 0.8259, AUC_score: 0.9214\n",
      "Epoch 00766: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 800: Train Loss: 0.3283, Macro_F1: 0.8259, AUC_score: 0.9215\n",
      "Epoch 850: Train Loss: 0.3265, Macro_F1: 0.8259, AUC_score: 0.9217\n",
      "Early stopping triggered\n",
      "acc save\n",
      "98.0% node features transform to 0: F1: 0.8259, AUC_score: 0.9216\n",
      "Epoch 0: Train Loss: 0.6085, Macro_F1: 0.7923, AUC_score: 0.8795\n",
      "Validation loss decreased (0.608517 --> 0.608517).\n",
      "Validation loss decreased (0.550856 --> 0.550856).\n",
      "Validation loss decreased (0.480084 --> 0.480084).\n",
      "Validation loss decreased (0.463719 --> 0.463719).\n",
      "Validation loss decreased (0.420267 --> 0.420267).\n",
      "Validation loss decreased (0.402581 --> 0.402581).\n",
      "Validation loss decreased (0.387105 --> 0.387105).\n",
      "Validation loss decreased (0.373354 --> 0.373354).\n",
      "Validation loss decreased (0.355021 --> 0.355021).\n",
      "Validation loss decreased (0.350915 --> 0.350915).\n",
      "Validation loss decreased (0.349365 --> 0.349365).\n",
      "Validation loss decreased (0.339578 --> 0.339578).\n",
      "Validation loss decreased (0.315691 --> 0.315691).\n",
      "Validation loss decreased (0.312180 --> 0.312180).\n",
      "Epoch 50: Train Loss: 0.3341, Macro_F1: 0.8433, AUC_score: 0.9297\n",
      "Validation loss decreased (0.305274 --> 0.305274).\n",
      "Validation loss decreased (0.297234 --> 0.297234).\n",
      "Validation loss decreased (0.296848 --> 0.296848).\n",
      "Validation loss decreased (0.294504 --> 0.294504).\n",
      "Validation loss decreased (0.291690 --> 0.291690).\n",
      "Validation loss decreased (0.278981 --> 0.278981).\n",
      "Epoch 100: Train Loss: 0.2838, Macro_F1: 0.8718, AUC_score: 0.9321\n",
      "Validation loss decreased (0.277503 --> 0.277503).\n",
      "Validation loss decreased (0.266572 --> 0.266572).\n",
      "Validation loss decreased (0.262387 --> 0.262387).\n",
      "Validation loss decreased (0.258478 --> 0.258478).\n",
      "Epoch 150: Train Loss: 0.2638, Macro_F1: 0.8614, AUC_score: 0.9363\n",
      "Validation loss decreased (0.249877 --> 0.249877).\n",
      "Epoch 200: Train Loss: 0.2691, Macro_F1: 0.8648, AUC_score: 0.9374\n",
      "Validation loss decreased (0.247108 --> 0.247108).\n",
      "Epoch 250: Train Loss: 0.2700, Macro_F1: 0.8680, AUC_score: 0.9384\n",
      "Validation loss decreased (0.242525 --> 0.242525).\n",
      "Validation loss decreased (0.242506 --> 0.242506).\n",
      "Validation loss decreased (0.238270 --> 0.238270).\n",
      "Validation loss decreased (0.234048 --> 0.234048).\n",
      "Epoch 300: Train Loss: 0.2508, Macro_F1: 0.8791, AUC_score: 0.9371\n",
      "Validation loss decreased (0.231295 --> 0.231295).\n",
      "Epoch 350: Train Loss: 0.2547, Macro_F1: 0.8755, AUC_score: 0.9370\n",
      "Validation loss decreased (0.228232 --> 0.228232).\n",
      "Epoch 400: Train Loss: 0.2361, Macro_F1: 0.8791, AUC_score: 0.9359\n",
      "Validation loss decreased (0.225896 --> 0.225896).\n",
      "Epoch 450: Train Loss: 0.2388, Macro_F1: 0.8724, AUC_score: 0.9342\n",
      "Validation loss decreased (0.225190 --> 0.225190).\n",
      "Validation loss decreased (0.222729 --> 0.222729).\n",
      "Validation loss decreased (0.221104 --> 0.221104).\n",
      "Validation loss decreased (0.219385 --> 0.219385).\n",
      "Validation loss decreased (0.218397 --> 0.218397).\n",
      "Epoch 500: Train Loss: 0.2540, Macro_F1: 0.8862, AUC_score: 0.9336\n",
      "Validation loss decreased (0.215670 --> 0.215670).\n",
      "Validation loss decreased (0.212217 --> 0.212217).\n",
      "Epoch 550: Train Loss: 0.2441, Macro_F1: 0.8727, AUC_score: 0.9309\n",
      "Epoch 600: Train Loss: 0.2665, Macro_F1: 0.8827, AUC_score: 0.9327\n",
      "Epoch 00622: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.210727 --> 0.210727).\n",
      "Epoch 650: Train Loss: 0.2200, Macro_F1: 0.8686, AUC_score: 0.9333\n",
      "Epoch 700: Train Loss: 0.2274, Macro_F1: 0.8721, AUC_score: 0.9322\n",
      "Validation loss decreased (0.210063 --> 0.210063).\n",
      "Epoch 750: Train Loss: 0.2191, Macro_F1: 0.8687, AUC_score: 0.9327\n",
      "Validation loss decreased (0.206600 --> 0.206600).\n",
      "Epoch 800: Train Loss: 0.2200, Macro_F1: 0.8793, AUC_score: 0.9324\n",
      "Epoch 850: Train Loss: 0.2131, Macro_F1: 0.8799, AUC_score: 0.9325\n",
      "Epoch 00855: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 900: Train Loss: 0.2386, Macro_F1: 0.8760, AUC_score: 0.9326\n",
      "Epoch 950: Train Loss: 0.2386, Macro_F1: 0.8762, AUC_score: 0.9322\n",
      "Early stopping triggered\n",
      "acc save\n",
      "97.0% node features transform to 0: F1: 0.8762, AUC_score: 0.9322\n",
      "Epoch 0: Train Loss: 0.4704, Macro_F1: 0.8566, AUC_score: 0.9221\n",
      "Validation loss decreased (0.470431 --> 0.470431).\n",
      "Validation loss decreased (0.424605 --> 0.424605).\n",
      "Validation loss decreased (0.396079 --> 0.396079).\n",
      "Validation loss decreased (0.348526 --> 0.348526).\n",
      "Validation loss decreased (0.334854 --> 0.334854).\n",
      "Validation loss decreased (0.326374 --> 0.326374).\n",
      "Validation loss decreased (0.322736 --> 0.322736).\n",
      "Validation loss decreased (0.299000 --> 0.299000).\n",
      "Validation loss decreased (0.296496 --> 0.296496).\n",
      "Validation loss decreased (0.282269 --> 0.282269).\n",
      "Validation loss decreased (0.274608 --> 0.274608).\n",
      "Validation loss decreased (0.261337 --> 0.261337).\n",
      "Validation loss decreased (0.252766 --> 0.252766).\n",
      "Validation loss decreased (0.250095 --> 0.250095).\n",
      "Validation loss decreased (0.245195 --> 0.245195).\n",
      "Validation loss decreased (0.240866 --> 0.240866).\n",
      "Validation loss decreased (0.230741 --> 0.230741).\n",
      "Validation loss decreased (0.229766 --> 0.229766).\n",
      "Validation loss decreased (0.227779 --> 0.227779).\n",
      "Validation loss decreased (0.225795 --> 0.225795).\n",
      "Validation loss decreased (0.219373 --> 0.219373).\n",
      "Validation loss decreased (0.216195 --> 0.216195).\n",
      "Validation loss decreased (0.210717 --> 0.210717).\n",
      "Epoch 50: Train Loss: 0.2155, Macro_F1: 0.8687, AUC_score: 0.9258\n",
      "Validation loss decreased (0.197261 --> 0.197261).\n",
      "Validation loss decreased (0.194960 --> 0.194960).\n",
      "Validation loss decreased (0.192005 --> 0.192005).\n",
      "Epoch 100: Train Loss: 0.2437, Macro_F1: 0.8724, AUC_score: 0.9251\n",
      "Validation loss decreased (0.190995 --> 0.190995).\n",
      "Validation loss decreased (0.182789 --> 0.182789).\n",
      "Epoch 150: Train Loss: 0.2025, Macro_F1: 0.8866, AUC_score: 0.9243\n",
      "Validation loss decreased (0.179796 --> 0.179796).\n",
      "Validation loss decreased (0.173662 --> 0.173662).\n",
      "Epoch 200: Train Loss: 0.1903, Macro_F1: 0.8791, AUC_score: 0.9216\n",
      "Validation loss decreased (0.173049 --> 0.173049).\n",
      "Epoch 250: Train Loss: 0.1883, Macro_F1: 0.8723, AUC_score: 0.9211\n",
      "Validation loss decreased (0.172469 --> 0.172469).\n",
      "Validation loss decreased (0.169746 --> 0.169746).\n",
      "Validation loss decreased (0.163543 --> 0.163543).\n",
      "Epoch 300: Train Loss: 0.1772, Macro_F1: 0.8753, AUC_score: 0.9203\n",
      "Validation loss decreased (0.159840 --> 0.159840).\n",
      "Epoch 350: Train Loss: 0.2018, Macro_F1: 0.8684, AUC_score: 0.9232\n",
      "Validation loss decreased (0.157552 --> 0.157552).\n",
      "Epoch 400: Train Loss: 0.1663, Macro_F1: 0.8827, AUC_score: 0.9226\n",
      "Epoch 450: Train Loss: 0.1735, Macro_F1: 0.8791, AUC_score: 0.9195\n",
      "Epoch 00460: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.156570 --> 0.156570).\n",
      "Epoch 500: Train Loss: 0.2049, Macro_F1: 0.8684, AUC_score: 0.9205\n",
      "Validation loss decreased (0.152330 --> 0.152330).\n",
      "Epoch 550: Train Loss: 0.1583, Macro_F1: 0.8753, AUC_score: 0.9190\n",
      "Validation loss decreased (0.149512 --> 0.149512).\n",
      "Epoch 600: Train Loss: 0.1643, Macro_F1: 0.8755, AUC_score: 0.9195\n",
      "Epoch 650: Train Loss: 0.1694, Macro_F1: 0.8687, AUC_score: 0.9195\n",
      "Epoch 00653: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.148107 --> 0.148107).\n",
      "Epoch 700: Train Loss: 0.1638, Macro_F1: 0.8684, AUC_score: 0.9202\n",
      "Epoch 750: Train Loss: 0.1689, Macro_F1: 0.8720, AUC_score: 0.9201\n",
      "Epoch 00761: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 800: Train Loss: 0.1675, Macro_F1: 0.8684, AUC_score: 0.9200\n",
      "Epoch 850: Train Loss: 0.1735, Macro_F1: 0.8684, AUC_score: 0.9200\n",
      "Early stopping triggered\n",
      "acc save\n",
      "96.0% node features transform to 0: F1: 0.8684, AUC_score: 0.9200\n",
      "Epoch 0: Train Loss: 0.2497, Macro_F1: 0.8616, AUC_score: 0.9265\n",
      "Validation loss decreased (0.249683 --> 0.249683).\n",
      "Validation loss decreased (0.222879 --> 0.222879).\n",
      "Validation loss decreased (0.217286 --> 0.217286).\n",
      "Validation loss decreased (0.200335 --> 0.200335).\n",
      "Validation loss decreased (0.195870 --> 0.195870).\n",
      "Validation loss decreased (0.181482 --> 0.181482).\n",
      "Validation loss decreased (0.166570 --> 0.166570).\n",
      "Validation loss decreased (0.162653 --> 0.162653).\n",
      "Epoch 50: Train Loss: 0.1797, Macro_F1: 0.8716, AUC_score: 0.9214\n",
      "Validation loss decreased (0.162030 --> 0.162030).\n",
      "Validation loss decreased (0.153895 --> 0.153895).\n",
      "Epoch 100: Train Loss: 0.1801, Macro_F1: 0.8686, AUC_score: 0.9216\n",
      "Validation loss decreased (0.150034 --> 0.150034).\n",
      "Validation loss decreased (0.149687 --> 0.149687).\n",
      "Epoch 150: Train Loss: 0.1753, Macro_F1: 0.8616, AUC_score: 0.9222\n",
      "Validation loss decreased (0.146315 --> 0.146315).\n",
      "Validation loss decreased (0.144317 --> 0.144317).\n",
      "Epoch 200: Train Loss: 0.1605, Macro_F1: 0.8827, AUC_score: 0.9223\n",
      "Validation loss decreased (0.144056 --> 0.144056).\n",
      "Validation loss decreased (0.141676 --> 0.141676).\n",
      "Epoch 250: Train Loss: 0.1456, Macro_F1: 0.8791, AUC_score: 0.9245\n",
      "Validation loss decreased (0.141508 --> 0.141508).\n",
      "Validation loss decreased (0.141218 --> 0.141218).\n",
      "Validation loss decreased (0.138900 --> 0.138900).\n",
      "Epoch 300: Train Loss: 0.1550, Macro_F1: 0.8829, AUC_score: 0.9259\n",
      "Validation loss decreased (0.136564 --> 0.136564).\n",
      "Validation loss decreased (0.135584 --> 0.135584).\n",
      "Epoch 350: Train Loss: 0.1420, Macro_F1: 0.8902, AUC_score: 0.9244\n",
      "Validation loss decreased (0.133338 --> 0.133338).\n",
      "Epoch 400: Train Loss: 0.1582, Macro_F1: 0.8723, AUC_score: 0.9263\n",
      "Validation loss decreased (0.130644 --> 0.130644).\n",
      "Validation loss decreased (0.130252 --> 0.130252).\n",
      "Epoch 450: Train Loss: 0.1526, Macro_F1: 0.8789, AUC_score: 0.9247\n",
      "Validation loss decreased (0.128563 --> 0.128563).\n",
      "Epoch 500: Train Loss: 0.1369, Macro_F1: 0.8832, AUC_score: 0.9269\n",
      "Validation loss decreased (0.125809 --> 0.125809).\n",
      "Epoch 550: Train Loss: 0.1369, Macro_F1: 0.8652, AUC_score: 0.9240\n",
      "Epoch 600: Train Loss: 0.1368, Macro_F1: 0.8829, AUC_score: 0.9226\n",
      "Epoch 00631: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.124445 --> 0.124445).\n",
      "Epoch 650: Train Loss: 0.1268, Macro_F1: 0.8827, AUC_score: 0.9283\n",
      "Validation loss decreased (0.122053 --> 0.122053).\n",
      "Epoch 700: Train Loss: 0.1306, Macro_F1: 0.8827, AUC_score: 0.9279\n",
      "Validation loss decreased (0.116159 --> 0.116159).\n",
      "Epoch 750: Train Loss: 0.1228, Macro_F1: 0.8866, AUC_score: 0.9265\n",
      "Epoch 800: Train Loss: 0.1490, Macro_F1: 0.8830, AUC_score: 0.9257\n",
      "Epoch 00838: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 850: Train Loss: 0.1324, Macro_F1: 0.8827, AUC_score: 0.9277\n",
      "Epoch 900: Train Loss: 0.1442, Macro_F1: 0.8827, AUC_score: 0.9279\n",
      "Early stopping triggered\n",
      "acc save\n",
      "95.0% node features transform to 0: F1: 0.8830, AUC_score: 0.9271\n",
      "Epoch 0: Train Loss: 0.2519, Macro_F1: 0.8690, AUC_score: 0.9246\n",
      "Validation loss decreased (0.251887 --> 0.251887).\n",
      "Validation loss decreased (0.228442 --> 0.228442).\n",
      "Validation loss decreased (0.199806 --> 0.199806).\n",
      "Validation loss decreased (0.199188 --> 0.199188).\n",
      "Validation loss decreased (0.191233 --> 0.191233).\n",
      "Validation loss decreased (0.162662 --> 0.162662).\n",
      "Validation loss decreased (0.156508 --> 0.156508).\n",
      "Validation loss decreased (0.150225 --> 0.150225).\n",
      "Validation loss decreased (0.146776 --> 0.146776).\n",
      "Validation loss decreased (0.143907 --> 0.143907).\n",
      "Validation loss decreased (0.129509 --> 0.129509).\n",
      "Validation loss decreased (0.127849 --> 0.127849).\n",
      "Validation loss decreased (0.122946 --> 0.122946).\n",
      "Validation loss decreased (0.120873 --> 0.120873).\n",
      "Validation loss decreased (0.118061 --> 0.118061).\n",
      "Validation loss decreased (0.114650 --> 0.114650).\n",
      "Validation loss decreased (0.111881 --> 0.111881).\n",
      "Validation loss decreased (0.106629 --> 0.106629).\n",
      "Epoch 50: Train Loss: 0.1205, Macro_F1: 0.8791, AUC_score: 0.9325\n",
      "Validation loss decreased (0.103507 --> 0.103507).\n",
      "Validation loss decreased (0.098583 --> 0.098583).\n",
      "Epoch 100: Train Loss: 0.1139, Macro_F1: 0.8825, AUC_score: 0.9303\n",
      "Validation loss decreased (0.097933 --> 0.097933).\n",
      "Validation loss decreased (0.095161 --> 0.095161).\n",
      "Validation loss decreased (0.094073 --> 0.094073).\n",
      "Validation loss decreased (0.093642 --> 0.093642).\n",
      "Epoch 150: Train Loss: 0.1277, Macro_F1: 0.8858, AUC_score: 0.9339\n",
      "Validation loss decreased (0.090790 --> 0.090790).\n",
      "Epoch 200: Train Loss: 0.1369, Macro_F1: 0.8821, AUC_score: 0.9298\n",
      "Validation loss decreased (0.089672 --> 0.089672).\n",
      "Validation loss decreased (0.088398 --> 0.088398).\n",
      "Epoch 250: Train Loss: 0.0897, Macro_F1: 0.8861, AUC_score: 0.9318\n",
      "Validation loss decreased (0.087932 --> 0.087932).\n",
      "Validation loss decreased (0.084145 --> 0.084145).\n",
      "Validation loss decreased (0.082200 --> 0.082200).\n",
      "Epoch 300: Train Loss: 0.0912, Macro_F1: 0.8894, AUC_score: 0.9386\n",
      "Validation loss decreased (0.079565 --> 0.079565).\n",
      "Epoch 350: Train Loss: 0.1071, Macro_F1: 0.8787, AUC_score: 0.9318\n",
      "Epoch 400: Train Loss: 0.1183, Macro_F1: 0.8858, AUC_score: 0.9408\n",
      "Validation loss decreased (0.077096 --> 0.077096).\n",
      "Epoch 450: Train Loss: 0.0855, Macro_F1: 0.8898, AUC_score: 0.9386\n",
      "Epoch 500: Train Loss: 0.0868, Macro_F1: 0.8757, AUC_score: 0.9398\n",
      "Epoch 00517: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 550: Train Loss: 0.0745, Macro_F1: 0.8751, AUC_score: 0.9419\n",
      "Validation loss decreased (0.074494 --> 0.074494).\n",
      "Validation loss decreased (0.072929 --> 0.072929).\n",
      "Validation loss decreased (0.072045 --> 0.072045).\n",
      "Epoch 600: Train Loss: 0.0800, Macro_F1: 0.8791, AUC_score: 0.9419\n",
      "Epoch 650: Train Loss: 0.0821, Macro_F1: 0.8823, AUC_score: 0.9431\n",
      "Validation loss decreased (0.071523 --> 0.071523).\n",
      "Validation loss decreased (0.069674 --> 0.069674).\n",
      "Epoch 700: Train Loss: 0.0867, Macro_F1: 0.8858, AUC_score: 0.9425\n",
      "Epoch 750: Train Loss: 0.0768, Macro_F1: 0.8791, AUC_score: 0.9399\n",
      "Epoch 00800: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 800: Train Loss: 0.0872, Macro_F1: 0.8898, AUC_score: 0.9409\n",
      "Epoch 850: Train Loss: 0.0720, Macro_F1: 0.8898, AUC_score: 0.9402\n",
      "Early stopping triggered\n",
      "acc save\n",
      "94.0% node features transform to 0: F1: 0.8898, AUC_score: 0.9396\n",
      "Epoch 0: Train Loss: 0.2803, Macro_F1: 0.8755, AUC_score: 0.9257\n",
      "Validation loss decreased (0.280331 --> 0.280331).\n",
      "Validation loss decreased (0.202363 --> 0.202363).\n",
      "Validation loss decreased (0.176217 --> 0.176217).\n",
      "Validation loss decreased (0.160691 --> 0.160691).\n",
      "Validation loss decreased (0.131385 --> 0.131385).\n",
      "Validation loss decreased (0.119860 --> 0.119860).\n",
      "Validation loss decreased (0.101730 --> 0.101730).\n",
      "Validation loss decreased (0.101668 --> 0.101668).\n",
      "Validation loss decreased (0.094572 --> 0.094572).\n",
      "Validation loss decreased (0.092151 --> 0.092151).\n",
      "Validation loss decreased (0.084667 --> 0.084667).\n",
      "Validation loss decreased (0.084172 --> 0.084172).\n",
      "Validation loss decreased (0.082941 --> 0.082941).\n",
      "Validation loss decreased (0.078012 --> 0.078012).\n",
      "Validation loss decreased (0.074895 --> 0.074895).\n",
      "Epoch 50: Train Loss: 0.0884, Macro_F1: 0.8789, AUC_score: 0.9259\n",
      "Validation loss decreased (0.071648 --> 0.071648).\n",
      "Epoch 100: Train Loss: 0.0909, Macro_F1: 0.8926, AUC_score: 0.9237\n",
      "Epoch 150: Train Loss: 0.0831, Macro_F1: 0.8789, AUC_score: 0.9235\n",
      "Validation loss decreased (0.067967 --> 0.067967).\n",
      "Validation loss decreased (0.067810 --> 0.067810).\n",
      "Validation loss decreased (0.062309 --> 0.062309).\n",
      "Epoch 200: Train Loss: 0.0878, Macro_F1: 0.8751, AUC_score: 0.9249\n",
      "Epoch 250: Train Loss: 0.0785, Macro_F1: 0.8861, AUC_score: 0.9282\n",
      "Epoch 00301: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0771, Macro_F1: 0.8928, AUC_score: 0.9295\n",
      "Validation loss decreased (0.060787 --> 0.060787).\n",
      "Epoch 350: Train Loss: 0.0759, Macro_F1: 0.8894, AUC_score: 0.9249\n",
      "Epoch 400: Train Loss: 0.0710, Macro_F1: 0.8827, AUC_score: 0.9259\n",
      "Epoch 00443: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.058162 --> 0.058162).\n",
      "Validation loss decreased (0.055875 --> 0.055875).\n",
      "Epoch 450: Train Loss: 0.0722, Macro_F1: 0.8827, AUC_score: 0.9262\n",
      "Epoch 500: Train Loss: 0.0669, Macro_F1: 0.8823, AUC_score: 0.9269\n",
      "Epoch 00548: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0848, Macro_F1: 0.8861, AUC_score: 0.9252\n",
      "Epoch 600: Train Loss: 0.0736, Macro_F1: 0.8861, AUC_score: 0.9253\n",
      "Early stopping triggered\n",
      "acc save\n",
      "93.0% node features transform to 0: F1: 0.8823, AUC_score: 0.9253\n",
      "Epoch 0: Train Loss: 0.2362, Macro_F1: 0.8760, AUC_score: 0.9191\n",
      "Validation loss decreased (0.236206 --> 0.236206).\n",
      "Validation loss decreased (0.134389 --> 0.134389).\n",
      "Validation loss decreased (0.113882 --> 0.113882).\n",
      "Validation loss decreased (0.107279 --> 0.107279).\n",
      "Validation loss decreased (0.098162 --> 0.098162).\n",
      "Validation loss decreased (0.095194 --> 0.095194).\n",
      "Validation loss decreased (0.080848 --> 0.080848).\n",
      "Validation loss decreased (0.075689 --> 0.075689).\n",
      "Validation loss decreased (0.066998 --> 0.066998).\n",
      "Validation loss decreased (0.062172 --> 0.062172).\n",
      "Epoch 50: Train Loss: 0.0831, Macro_F1: 0.8932, AUC_score: 0.9164\n",
      "Validation loss decreased (0.060180 --> 0.060180).\n",
      "Validation loss decreased (0.059811 --> 0.059811).\n",
      "Validation loss decreased (0.058122 --> 0.058122).\n",
      "Epoch 100: Train Loss: 0.0770, Macro_F1: 0.8830, AUC_score: 0.9154\n",
      "Validation loss decreased (0.057329 --> 0.057329).\n",
      "Epoch 150: Train Loss: 0.1213, Macro_F1: 0.8887, AUC_score: 0.9201\n",
      "Epoch 200: Train Loss: 0.0653, Macro_F1: 0.8821, AUC_score: 0.9130\n",
      "Epoch 00220: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.054929 --> 0.054929).\n",
      "Epoch 250: Train Loss: 0.0655, Macro_F1: 0.8757, AUC_score: 0.9140\n",
      "Validation loss decreased (0.053988 --> 0.053988).\n",
      "Epoch 300: Train Loss: 0.0702, Macro_F1: 0.8720, AUC_score: 0.9146\n",
      "Validation loss decreased (0.053321 --> 0.053321).\n",
      "Validation loss decreased (0.052236 --> 0.052236).\n",
      "Epoch 350: Train Loss: 0.0613, Macro_F1: 0.8720, AUC_score: 0.9126\n",
      "Epoch 400: Train Loss: 0.0584, Macro_F1: 0.8720, AUC_score: 0.9119\n",
      "Epoch 00438: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.0605, Macro_F1: 0.8791, AUC_score: 0.9135\n",
      "Epoch 500: Train Loss: 0.0656, Macro_F1: 0.8791, AUC_score: 0.9137\n",
      "Early stopping triggered\n",
      "acc save\n",
      "92.0% node features transform to 0: F1: 0.8720, AUC_score: 0.9129\n",
      "Epoch 0: Train Loss: 0.1988, Macro_F1: 0.8882, AUC_score: 0.9195\n",
      "Validation loss decreased (0.198829 --> 0.198829).\n",
      "Validation loss decreased (0.167424 --> 0.167424).\n",
      "Validation loss decreased (0.129633 --> 0.129633).\n",
      "Validation loss decreased (0.094512 --> 0.094512).\n",
      "Validation loss decreased (0.073111 --> 0.073111).\n",
      "Validation loss decreased (0.067690 --> 0.067690).\n",
      "Validation loss decreased (0.062549 --> 0.062549).\n",
      "Validation loss decreased (0.061952 --> 0.061952).\n",
      "Validation loss decreased (0.053355 --> 0.053355).\n",
      "Validation loss decreased (0.051606 --> 0.051606).\n",
      "Epoch 50: Train Loss: 0.0727, Macro_F1: 0.8825, AUC_score: 0.9212\n",
      "Validation loss decreased (0.049701 --> 0.049701).\n",
      "Validation loss decreased (0.045884 --> 0.045884).\n",
      "Epoch 100: Train Loss: 0.0494, Macro_F1: 0.8898, AUC_score: 0.9276\n",
      "Validation loss decreased (0.043061 --> 0.043061).\n",
      "Validation loss decreased (0.040811 --> 0.040811).\n",
      "Epoch 150: Train Loss: 0.0447, Macro_F1: 0.8900, AUC_score: 0.9324\n",
      "Epoch 200: Train Loss: 0.0453, Macro_F1: 0.8900, AUC_score: 0.9316\n",
      "Epoch 00237: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.040718 --> 0.040718).\n",
      "Validation loss decreased (0.039899 --> 0.039899).\n",
      "Epoch 250: Train Loss: 0.0412, Macro_F1: 0.8898, AUC_score: 0.9312\n",
      "Validation loss decreased (0.037993 --> 0.037993).\n",
      "Validation loss decreased (0.037780 --> 0.037780).\n",
      "Epoch 300: Train Loss: 0.0482, Macro_F1: 0.8936, AUC_score: 0.9328\n",
      "Validation loss decreased (0.037041 --> 0.037041).\n",
      "Epoch 350: Train Loss: 0.0412, Macro_F1: 0.8900, AUC_score: 0.9327\n",
      "Validation loss decreased (0.036808 --> 0.036808).\n",
      "Epoch 400: Train Loss: 0.0451, Macro_F1: 0.8900, AUC_score: 0.9328\n",
      "Validation loss decreased (0.036262 --> 0.036262).\n",
      "Epoch 450: Train Loss: 0.0469, Macro_F1: 0.8829, AUC_score: 0.9318\n",
      "Validation loss decreased (0.034913 --> 0.034913).\n",
      "Epoch 500: Train Loss: 0.0430, Macro_F1: 0.8900, AUC_score: 0.9335\n",
      "Epoch 550: Train Loss: 0.0483, Macro_F1: 0.8934, AUC_score: 0.9333\n",
      "Validation loss decreased (0.033787 --> 0.033787).\n",
      "Epoch 600: Train Loss: 0.0416, Macro_F1: 0.8932, AUC_score: 0.9330\n",
      "Epoch 650: Train Loss: 0.0394, Macro_F1: 0.8793, AUC_score: 0.9329\n",
      "Epoch 00658: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 700: Train Loss: 0.0400, Macro_F1: 0.8864, AUC_score: 0.9334\n",
      "Epoch 750: Train Loss: 0.0413, Macro_F1: 0.8972, AUC_score: 0.9335\n",
      "Early stopping triggered\n",
      "acc save\n",
      "91.0% node features transform to 0: F1: 0.8936, AUC_score: 0.9333\n",
      "Epoch 0: Train Loss: 0.0668, Macro_F1: 0.8849, AUC_score: 0.9372\n",
      "Validation loss decreased (0.066784 --> 0.066784).\n",
      "Validation loss decreased (0.059692 --> 0.059692).\n",
      "Validation loss decreased (0.053929 --> 0.053929).\n",
      "Validation loss decreased (0.047095 --> 0.047095).\n",
      "Validation loss decreased (0.041405 --> 0.041405).\n",
      "Validation loss decreased (0.038231 --> 0.038231).\n",
      "Validation loss decreased (0.036241 --> 0.036241).\n",
      "Validation loss decreased (0.033505 --> 0.033505).\n",
      "Epoch 50: Train Loss: 0.0401, Macro_F1: 0.9011, AUC_score: 0.9401\n",
      "Validation loss decreased (0.033452 --> 0.033452).\n",
      "Validation loss decreased (0.033112 --> 0.033112).\n",
      "Validation loss decreased (0.030002 --> 0.030002).\n",
      "Epoch 100: Train Loss: 0.0500, Macro_F1: 0.8896, AUC_score: 0.9335\n",
      "Epoch 150: Train Loss: 0.0483, Macro_F1: 0.8928, AUC_score: 0.9382\n",
      "Epoch 00197: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0355, Macro_F1: 0.8973, AUC_score: 0.9375\n",
      "Validation loss decreased (0.029435 --> 0.029435).\n",
      "Validation loss decreased (0.029021 --> 0.029021).\n",
      "Validation loss decreased (0.027815 --> 0.027815).\n",
      "Epoch 250: Train Loss: 0.0308, Macro_F1: 0.8939, AUC_score: 0.9327\n",
      "Validation loss decreased (0.027182 --> 0.027182).\n",
      "Epoch 300: Train Loss: 0.0329, Macro_F1: 0.9006, AUC_score: 0.9330\n",
      "Validation loss decreased (0.027101 --> 0.027101).\n",
      "Epoch 350: Train Loss: 0.0511, Macro_F1: 0.8970, AUC_score: 0.9334\n",
      "Validation loss decreased (0.026453 --> 0.026453).\n",
      "Validation loss decreased (0.026213 --> 0.026213).\n",
      "Validation loss decreased (0.025974 --> 0.025974).\n",
      "Epoch 400: Train Loss: 0.0397, Macro_F1: 0.8939, AUC_score: 0.9346\n",
      "Epoch 450: Train Loss: 0.0377, Macro_F1: 0.9009, AUC_score: 0.9341\n",
      "Epoch 00495: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.0350, Macro_F1: 0.9008, AUC_score: 0.9339\n",
      "Epoch 550: Train Loss: 0.0331, Macro_F1: 0.8973, AUC_score: 0.9337\n",
      "Validation loss decreased (0.024832 --> 0.024832).\n",
      "Validation loss decreased (0.024264 --> 0.024264).\n",
      "Epoch 600: Train Loss: 0.0354, Macro_F1: 0.8972, AUC_score: 0.9331\n",
      "Epoch 650: Train Loss: 0.0279, Macro_F1: 0.9009, AUC_score: 0.9329\n",
      "Validation loss decreased (0.024034 --> 0.024034).\n",
      "Epoch 700: Train Loss: 0.0277, Macro_F1: 0.8973, AUC_score: 0.9334\n",
      "Epoch 750: Train Loss: 0.0339, Macro_F1: 0.8973, AUC_score: 0.9341\n",
      "Epoch 00768: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 800: Train Loss: 0.0315, Macro_F1: 0.8973, AUC_score: 0.9338\n",
      "Validation loss decreased (0.023845 --> 0.023845).\n",
      "Epoch 850: Train Loss: 0.0264, Macro_F1: 0.8973, AUC_score: 0.9338\n",
      "Epoch 900: Train Loss: 0.0316, Macro_F1: 0.9045, AUC_score: 0.9338\n",
      "Epoch 00945: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 950: Train Loss: 0.0273, Macro_F1: 0.8973, AUC_score: 0.9337\n",
      "acc save\n",
      "90.0% node features transform to 0: F1: 0.8973, AUC_score: 0.9337\n",
      "Epoch 0: Train Loss: 0.0534, Macro_F1: 0.8794, AUC_score: 0.9309\n",
      "Validation loss decreased (0.053408 --> 0.053408).\n",
      "Validation loss decreased (0.049692 --> 0.049692).\n",
      "Validation loss decreased (0.044113 --> 0.044113).\n",
      "Validation loss decreased (0.039006 --> 0.039006).\n",
      "Validation loss decreased (0.038528 --> 0.038528).\n",
      "Validation loss decreased (0.037789 --> 0.037789).\n",
      "Validation loss decreased (0.036516 --> 0.036516).\n",
      "Validation loss decreased (0.035434 --> 0.035434).\n",
      "Validation loss decreased (0.035006 --> 0.035006).\n",
      "Validation loss decreased (0.034955 --> 0.034955).\n",
      "Validation loss decreased (0.033565 --> 0.033565).\n",
      "Validation loss decreased (0.029825 --> 0.029825).\n",
      "Validation loss decreased (0.027094 --> 0.027094).\n",
      "Epoch 50: Train Loss: 0.0324, Macro_F1: 0.8903, AUC_score: 0.9424\n",
      "Validation loss decreased (0.025265 --> 0.025265).\n",
      "Validation loss decreased (0.022895 --> 0.022895).\n",
      "Epoch 100: Train Loss: 0.0293, Macro_F1: 0.8902, AUC_score: 0.9345\n",
      "Epoch 150: Train Loss: 0.0322, Macro_F1: 0.8936, AUC_score: 0.9351\n",
      "Epoch 00182: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0267, Macro_F1: 0.8900, AUC_score: 0.9342\n",
      "Validation loss decreased (0.020655 --> 0.020655).\n",
      "Epoch 250: Train Loss: 0.0235, Macro_F1: 0.8900, AUC_score: 0.9338\n",
      "Validation loss decreased (0.020211 --> 0.020211).\n",
      "Epoch 300: Train Loss: 0.0399, Macro_F1: 0.8900, AUC_score: 0.9323\n",
      "Epoch 350: Train Loss: 0.0332, Macro_F1: 0.8936, AUC_score: 0.9322\n",
      "Epoch 00361: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0367, Macro_F1: 0.8972, AUC_score: 0.9324\n",
      "Epoch 450: Train Loss: 0.0311, Macro_F1: 0.8936, AUC_score: 0.9327\n",
      "Validation loss decreased (0.019667 --> 0.019667).\n",
      "Epoch 500: Train Loss: 0.0274, Macro_F1: 0.8936, AUC_score: 0.9327\n",
      "Epoch 550: Train Loss: 0.0311, Macro_F1: 0.8936, AUC_score: 0.9326\n",
      "Epoch 00553: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 600: Train Loss: 0.0271, Macro_F1: 0.8936, AUC_score: 0.9326\n",
      "Epoch 650: Train Loss: 0.0299, Macro_F1: 0.8936, AUC_score: 0.9329\n",
      "Early stopping triggered\n",
      "acc save\n",
      "89.0% node features transform to 0: F1: 0.8936, AUC_score: 0.9329\n",
      "Epoch 0: Train Loss: 0.0749, Macro_F1: 0.9072, AUC_score: 0.9314\n",
      "Validation loss decreased (0.074949 --> 0.074949).\n",
      "Validation loss decreased (0.064175 --> 0.064175).\n",
      "Validation loss decreased (0.030806 --> 0.030806).\n",
      "Validation loss decreased (0.021635 --> 0.021635).\n",
      "Validation loss decreased (0.018950 --> 0.018950).\n",
      "Validation loss decreased (0.017368 --> 0.017368).\n",
      "Validation loss decreased (0.015728 --> 0.015728).\n",
      "Epoch 50: Train Loss: 0.0212, Macro_F1: 0.8868, AUC_score: 0.9465\n",
      "Validation loss decreased (0.014113 --> 0.014113).\n",
      "Epoch 100: Train Loss: 0.0159, Macro_F1: 0.8866, AUC_score: 0.9432\n",
      "Validation loss decreased (0.014098 --> 0.014098).\n",
      "Epoch 150: Train Loss: 0.1005, Macro_F1: 0.8874, AUC_score: 0.9421\n",
      "Epoch 200: Train Loss: 0.0181, Macro_F1: 0.8939, AUC_score: 0.9476\n",
      "Epoch 00206: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.013123 --> 0.013123).\n",
      "Epoch 250: Train Loss: 0.0161, Macro_F1: 0.8866, AUC_score: 0.9468\n",
      "Validation loss decreased (0.010104 --> 0.010104).\n",
      "Epoch 300: Train Loss: 0.0133, Macro_F1: 0.8938, AUC_score: 0.9450\n",
      "Epoch 350: Train Loss: 0.0193, Macro_F1: 0.8794, AUC_score: 0.9465\n",
      "Epoch 00384: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0176, Macro_F1: 0.8866, AUC_score: 0.9444\n",
      "Epoch 450: Train Loss: 0.0107, Macro_F1: 0.8938, AUC_score: 0.9441\n",
      "Early stopping triggered\n",
      "acc save\n",
      "88.0% node features transform to 0: F1: 0.8938, AUC_score: 0.9436\n",
      "Epoch 0: Train Loss: 0.0248, Macro_F1: 0.8766, AUC_score: 0.9417\n",
      "Validation loss decreased (0.024755 --> 0.024755).\n",
      "Validation loss decreased (0.019102 --> 0.019102).\n",
      "Validation loss decreased (0.016428 --> 0.016428).\n",
      "Validation loss decreased (0.015753 --> 0.015753).\n",
      "Validation loss decreased (0.014522 --> 0.014522).\n",
      "Validation loss decreased (0.012972 --> 0.012972).\n",
      "Epoch 50: Train Loss: 0.0161, Macro_F1: 0.9117, AUC_score: 0.9499\n",
      "Validation loss decreased (0.012597 --> 0.012597).\n",
      "Epoch 100: Train Loss: 0.0235, Macro_F1: 0.9151, AUC_score: 0.9480\n",
      "Epoch 150: Train Loss: 0.0163, Macro_F1: 0.9054, AUC_score: 0.9535\n",
      "Validation loss decreased (0.012481 --> 0.012481).\n",
      "Validation loss decreased (0.012465 --> 0.012465).\n",
      "Validation loss decreased (0.010393 --> 0.010393).\n",
      "Epoch 200: Train Loss: 0.0127, Macro_F1: 0.9043, AUC_score: 0.9497\n",
      "Epoch 250: Train Loss: 0.0245, Macro_F1: 0.8981, AUC_score: 0.9538\n",
      "Validation loss decreased (0.009766 --> 0.009766).\n",
      "Epoch 300: Train Loss: 0.0139, Macro_F1: 0.9045, AUC_score: 0.9516\n",
      "Epoch 350: Train Loss: 0.0101, Macro_F1: 0.9115, AUC_score: 0.9494\n",
      "Validation loss decreased (0.009704 --> 0.009704).\n",
      "Validation loss decreased (0.008201 --> 0.008201).\n",
      "Epoch 400: Train Loss: 0.0118, Macro_F1: 0.8902, AUC_score: 0.9470\n",
      "Epoch 450: Train Loss: 0.0147, Macro_F1: 0.9119, AUC_score: 0.9549\n",
      "Epoch 00492: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 500: Train Loss: 0.0114, Macro_F1: 0.9119, AUC_score: 0.9494\n",
      "Epoch 550: Train Loss: 0.0090, Macro_F1: 0.9008, AUC_score: 0.9472\n",
      "Early stopping triggered\n",
      "acc save\n",
      "87.0% node features transform to 0: F1: 0.9043, AUC_score: 0.9480\n",
      "Epoch 0: Train Loss: 0.0301, Macro_F1: 0.9142, AUC_score: 0.9430\n",
      "Validation loss decreased (0.030115 --> 0.030115).\n",
      "Validation loss decreased (0.023595 --> 0.023595).\n",
      "Validation loss decreased (0.018487 --> 0.018487).\n",
      "Validation loss decreased (0.017212 --> 0.017212).\n",
      "Validation loss decreased (0.016043 --> 0.016043).\n",
      "Validation loss decreased (0.015275 --> 0.015275).\n",
      "Validation loss decreased (0.011854 --> 0.011854).\n",
      "Validation loss decreased (0.011759 --> 0.011759).\n",
      "Validation loss decreased (0.011150 --> 0.011150).\n",
      "Validation loss decreased (0.011071 --> 0.011071).\n",
      "Epoch 50: Train Loss: 0.0201, Macro_F1: 0.9004, AUC_score: 0.9465\n",
      "Validation loss decreased (0.010217 --> 0.010217).\n",
      "Epoch 100: Train Loss: 0.0202, Macro_F1: 0.9009, AUC_score: 0.9522\n",
      "Validation loss decreased (0.009917 --> 0.009917).\n",
      "Epoch 150: Train Loss: 0.0165, Macro_F1: 0.9081, AUC_score: 0.9520\n",
      "Epoch 200: Train Loss: 0.0125, Macro_F1: 0.8966, AUC_score: 0.9534\n",
      "Epoch 00210: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0091, Macro_F1: 0.9083, AUC_score: 0.9524\n",
      "Validation loss decreased (0.009142 --> 0.009142).\n",
      "Validation loss decreased (0.009001 --> 0.009001).\n",
      "Validation loss decreased (0.008787 --> 0.008787).\n",
      "Epoch 300: Train Loss: 0.0192, Macro_F1: 0.8932, AUC_score: 0.9503\n",
      "Validation loss decreased (0.008314 --> 0.008314).\n",
      "Epoch 350: Train Loss: 0.0169, Macro_F1: 0.9009, AUC_score: 0.9515\n",
      "Validation loss decreased (0.008239 --> 0.008239).\n",
      "Validation loss decreased (0.008186 --> 0.008186).\n",
      "Epoch 400: Train Loss: 0.0080, Macro_F1: 0.9045, AUC_score: 0.9505\n",
      "Validation loss decreased (0.007991 --> 0.007991).\n",
      "Validation loss decreased (0.007460 --> 0.007460).\n",
      "Epoch 450: Train Loss: 0.0104, Macro_F1: 0.8970, AUC_score: 0.9503\n",
      "Epoch 500: Train Loss: 0.0084, Macro_F1: 0.8970, AUC_score: 0.9511\n",
      "Epoch 00537: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 550: Train Loss: 0.0110, Macro_F1: 0.8968, AUC_score: 0.9507\n",
      "Epoch 600: Train Loss: 0.0126, Macro_F1: 0.9009, AUC_score: 0.9501\n",
      "Early stopping triggered\n",
      "acc save\n",
      "86.0% node features transform to 0: F1: 0.8970, AUC_score: 0.9502\n",
      "Epoch 0: Train Loss: 0.0127, Macro_F1: 0.8837, AUC_score: 0.9485\n",
      "Validation loss decreased (0.012705 --> 0.012705).\n",
      "Validation loss decreased (0.008998 --> 0.008998).\n",
      "Epoch 50: Train Loss: 0.0270, Macro_F1: 0.9043, AUC_score: 0.9487\n",
      "Validation loss decreased (0.008350 --> 0.008350).\n",
      "Epoch 100: Train Loss: 0.0131, Macro_F1: 0.9045, AUC_score: 0.9481\n",
      "Validation loss decreased (0.008202 --> 0.008202).\n",
      "Validation loss decreased (0.007086 --> 0.007086).\n",
      "Epoch 150: Train Loss: 0.0129, Macro_F1: 0.9043, AUC_score: 0.9525\n",
      "Epoch 200: Train Loss: 0.0187, Macro_F1: 0.9009, AUC_score: 0.9459\n",
      "Epoch 00238: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0110, Macro_F1: 0.9006, AUC_score: 0.9504\n",
      "Epoch 300: Train Loss: 0.0095, Macro_F1: 0.9084, AUC_score: 0.9504\n",
      "Early stopping triggered\n",
      "acc save\n",
      "85.0% node features transform to 0: F1: 0.9084, AUC_score: 0.9479\n",
      "Epoch 0: Train Loss: 0.0198, Macro_F1: 0.9110, AUC_score: 0.9410\n",
      "Validation loss decreased (0.019823 --> 0.019823).\n",
      "Validation loss decreased (0.016403 --> 0.016403).\n",
      "Validation loss decreased (0.015755 --> 0.015755).\n",
      "Validation loss decreased (0.011070 --> 0.011070).\n",
      "Validation loss decreased (0.011027 --> 0.011027).\n",
      "Validation loss decreased (0.009976 --> 0.009976).\n",
      "Epoch 50: Train Loss: 0.0123, Macro_F1: 0.9120, AUC_score: 0.9473\n",
      "Validation loss decreased (0.009656 --> 0.009656).\n",
      "Validation loss decreased (0.008354 --> 0.008354).\n",
      "Validation loss decreased (0.008291 --> 0.008291).\n",
      "Epoch 100: Train Loss: 0.0175, Macro_F1: 0.9157, AUC_score: 0.9499\n",
      "Validation loss decreased (0.007400 --> 0.007400).\n",
      "Epoch 150: Train Loss: 0.0316, Macro_F1: 0.9047, AUC_score: 0.9512\n",
      "Epoch 200: Train Loss: 0.0178, Macro_F1: 0.9042, AUC_score: 0.9470\n",
      "Epoch 00223: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.006620 --> 0.006620).\n",
      "Epoch 250: Train Loss: 0.0304, Macro_F1: 0.9084, AUC_score: 0.9450\n",
      "Validation loss decreased (0.006534 --> 0.006534).\n",
      "Validation loss decreased (0.006359 --> 0.006359).\n",
      "Epoch 300: Train Loss: 0.0091, Macro_F1: 0.9081, AUC_score: 0.9454\n",
      "Epoch 350: Train Loss: 0.0179, Macro_F1: 0.9009, AUC_score: 0.9448\n",
      "Epoch 00391: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0138, Macro_F1: 0.9047, AUC_score: 0.9461\n",
      "Epoch 450: Train Loss: 0.0094, Macro_F1: 0.9009, AUC_score: 0.9445\n",
      "Early stopping triggered\n",
      "acc save\n",
      "84.0% node features transform to 0: F1: 0.9009, AUC_score: 0.9446\n",
      "Epoch 0: Train Loss: 0.0551, Macro_F1: 0.8946, AUC_score: 0.9486\n",
      "Validation loss decreased (0.055119 --> 0.055119).\n",
      "Validation loss decreased (0.024765 --> 0.024765).\n",
      "Validation loss decreased (0.023711 --> 0.023711).\n",
      "Validation loss decreased (0.015954 --> 0.015954).\n",
      "Validation loss decreased (0.013222 --> 0.013222).\n",
      "Validation loss decreased (0.008718 --> 0.008718).\n",
      "Epoch 50: Train Loss: 0.0131, Macro_F1: 0.9079, AUC_score: 0.9471\n",
      "Validation loss decreased (0.008689 --> 0.008689).\n",
      "Validation loss decreased (0.008547 --> 0.008547).\n",
      "Validation loss decreased (0.008316 --> 0.008316).\n",
      "Validation loss decreased (0.008189 --> 0.008189).\n",
      "Validation loss decreased (0.007647 --> 0.007647).\n",
      "Validation loss decreased (0.007567 --> 0.007567).\n",
      "Epoch 100: Train Loss: 0.0102, Macro_F1: 0.9047, AUC_score: 0.9510\n",
      "Validation loss decreased (0.007439 --> 0.007439).\n",
      "Validation loss decreased (0.007101 --> 0.007101).\n",
      "Epoch 150: Train Loss: 0.0408, Macro_F1: 0.9009, AUC_score: 0.9501\n",
      "Validation loss decreased (0.006297 --> 0.006297).\n",
      "Epoch 200: Train Loss: 0.0176, Macro_F1: 0.9009, AUC_score: 0.9489\n",
      "Epoch 250: Train Loss: 0.0121, Macro_F1: 0.8900, AUC_score: 0.9522\n",
      "Epoch 00272: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0096, Macro_F1: 0.9045, AUC_score: 0.9503\n",
      "Validation loss decreased (0.005897 --> 0.005897).\n",
      "Validation loss decreased (0.005522 --> 0.005522).\n",
      "Epoch 350: Train Loss: 0.0089, Macro_F1: 0.8975, AUC_score: 0.9499\n",
      "Validation loss decreased (0.005467 --> 0.005467).\n",
      "Validation loss decreased (0.005372 --> 0.005372).\n",
      "Epoch 400: Train Loss: 0.0095, Macro_F1: 0.9011, AUC_score: 0.9506\n",
      "Epoch 450: Train Loss: 0.0175, Macro_F1: 0.8938, AUC_score: 0.9508\n",
      "Epoch 00496: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.0072, Macro_F1: 0.8938, AUC_score: 0.9507\n",
      "Epoch 550: Train Loss: 0.0285, Macro_F1: 0.8938, AUC_score: 0.9490\n",
      "Early stopping triggered\n",
      "acc save\n",
      "83.0% node features transform to 0: F1: 0.8900, AUC_score: 0.9494\n",
      "Epoch 0: Train Loss: 0.0194, Macro_F1: 0.8982, AUC_score: 0.9541\n",
      "Validation loss decreased (0.019391 --> 0.019391).\n",
      "Validation loss decreased (0.010240 --> 0.010240).\n",
      "Validation loss decreased (0.009035 --> 0.009035).\n",
      "Validation loss decreased (0.008628 --> 0.008628).\n",
      "Validation loss decreased (0.008559 --> 0.008559).\n",
      "Validation loss decreased (0.008170 --> 0.008170).\n",
      "Validation loss decreased (0.008066 --> 0.008066).\n",
      "Epoch 50: Train Loss: 0.0124, Macro_F1: 0.9234, AUC_score: 0.9515\n",
      "Validation loss decreased (0.007890 --> 0.007890).\n",
      "Validation loss decreased (0.006663 --> 0.006663).\n",
      "Epoch 100: Train Loss: 0.0156, Macro_F1: 0.9047, AUC_score: 0.9507\n",
      "Epoch 150: Train Loss: 0.0289, Macro_F1: 0.9042, AUC_score: 0.9477\n",
      "Epoch 00156: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.006296 --> 0.006296).\n",
      "Validation loss decreased (0.005566 --> 0.005566).\n",
      "Epoch 200: Train Loss: 0.0178, Macro_F1: 0.9012, AUC_score: 0.9481\n",
      "Epoch 250: Train Loss: 0.0145, Macro_F1: 0.8973, AUC_score: 0.9480\n",
      "Epoch 00301: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 300: Train Loss: 0.0102, Macro_F1: 0.8972, AUC_score: 0.9481\n",
      "Epoch 350: Train Loss: 0.0062, Macro_F1: 0.8936, AUC_score: 0.9480\n",
      "Validation loss decreased (0.005107 --> 0.005107).\n",
      "Epoch 400: Train Loss: 0.0068, Macro_F1: 0.8936, AUC_score: 0.9474\n",
      "Epoch 450: Train Loss: 0.0094, Macro_F1: 0.8936, AUC_score: 0.9476\n",
      "Validation loss decreased (0.004835 --> 0.004835).\n",
      "Epoch 500: Train Loss: 0.0076, Macro_F1: 0.8936, AUC_score: 0.9487\n",
      "Epoch 550: Train Loss: 0.0094, Macro_F1: 0.8936, AUC_score: 0.9477\n",
      "Epoch 00558: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 600: Train Loss: 0.0079, Macro_F1: 0.8936, AUC_score: 0.9477\n",
      "Epoch 650: Train Loss: 0.0090, Macro_F1: 0.8936, AUC_score: 0.9478\n",
      "Early stopping triggered\n",
      "acc save\n",
      "82.0% node features transform to 0: F1: 0.8936, AUC_score: 0.9478\n",
      "Epoch 0: Train Loss: 0.0111, Macro_F1: 0.9011, AUC_score: 0.9429\n",
      "Validation loss decreased (0.011088 --> 0.011088).\n",
      "Validation loss decreased (0.009467 --> 0.009467).\n",
      "Validation loss decreased (0.008309 --> 0.008309).\n",
      "Validation loss decreased (0.006499 --> 0.006499).\n",
      "Epoch 50: Train Loss: 0.0082, Macro_F1: 0.9125, AUC_score: 0.9522\n",
      "Epoch 100: Train Loss: 0.0141, Macro_F1: 0.9048, AUC_score: 0.9454\n",
      "Epoch 00132: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.0071, Macro_F1: 0.9047, AUC_score: 0.9489\n",
      "Validation loss decreased (0.006455 --> 0.006455).\n",
      "Validation loss decreased (0.005293 --> 0.005293).\n",
      "Epoch 200: Train Loss: 0.0096, Macro_F1: 0.9047, AUC_score: 0.9456\n",
      "Validation loss decreased (0.005194 --> 0.005194).\n",
      "Validation loss decreased (0.005106 --> 0.005106).\n",
      "Epoch 250: Train Loss: 0.0129, Macro_F1: 0.8972, AUC_score: 0.9474\n",
      "Epoch 300: Train Loss: 0.0091, Macro_F1: 0.9009, AUC_score: 0.9470\n",
      "Epoch 00318: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.0070, Macro_F1: 0.9047, AUC_score: 0.9478\n",
      "Validation loss decreased (0.004677 --> 0.004677).\n",
      "Epoch 400: Train Loss: 0.0059, Macro_F1: 0.9011, AUC_score: 0.9477\n",
      "Epoch 450: Train Loss: 0.0073, Macro_F1: 0.9009, AUC_score: 0.9476\n",
      "Epoch 00468: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 500: Train Loss: 0.0110, Macro_F1: 0.9009, AUC_score: 0.9472\n",
      "Validation loss decreased (0.004034 --> 0.004034).\n",
      "Epoch 550: Train Loss: 0.0090, Macro_F1: 0.9011, AUC_score: 0.9474\n",
      "Epoch 600: Train Loss: 0.0067, Macro_F1: 0.8973, AUC_score: 0.9469\n",
      "Epoch 00609: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 650: Train Loss: 0.0069, Macro_F1: 0.8973, AUC_score: 0.9470\n",
      "Epoch 700: Train Loss: 0.0176, Macro_F1: 0.8973, AUC_score: 0.9470\n",
      "Early stopping triggered\n",
      "acc save\n",
      "81.0% node features transform to 0: F1: 0.8973, AUC_score: 0.9470\n",
      "Epoch 0: Train Loss: 0.0425, Macro_F1: 0.9011, AUC_score: 0.9464\n",
      "Validation loss decreased (0.042453 --> 0.042453).\n",
      "Validation loss decreased (0.019649 --> 0.019649).\n",
      "Validation loss decreased (0.016847 --> 0.016847).\n",
      "Validation loss decreased (0.013090 --> 0.013090).\n",
      "Validation loss decreased (0.011098 --> 0.011098).\n",
      "Validation loss decreased (0.011032 --> 0.011032).\n",
      "Validation loss decreased (0.010852 --> 0.010852).\n",
      "Validation loss decreased (0.007910 --> 0.007910).\n",
      "Validation loss decreased (0.006400 --> 0.006400).\n",
      "Epoch 50: Train Loss: 0.0170, Macro_F1: 0.9083, AUC_score: 0.9516\n",
      "Validation loss decreased (0.005988 --> 0.005988).\n",
      "Validation loss decreased (0.005756 --> 0.005756).\n",
      "Validation loss decreased (0.004902 --> 0.004902).\n",
      "Epoch 100: Train Loss: 0.0087, Macro_F1: 0.9047, AUC_score: 0.9472\n",
      "Epoch 150: Train Loss: 0.0081, Macro_F1: 0.9084, AUC_score: 0.9513\n",
      "Validation loss decreased (0.004271 --> 0.004271).\n",
      "Epoch 200: Train Loss: 0.0109, Macro_F1: 0.9153, AUC_score: 0.9573\n",
      "Epoch 250: Train Loss: 0.0124, Macro_F1: 0.9197, AUC_score: 0.9471\n",
      "Epoch 00253: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0085, Macro_F1: 0.9121, AUC_score: 0.9447\n",
      "Epoch 350: Train Loss: 0.0076, Macro_F1: 0.9157, AUC_score: 0.9453\n",
      "Early stopping triggered\n",
      "acc save\n",
      "80.0% node features transform to 0: F1: 0.9121, AUC_score: 0.9452\n",
      "Epoch 0: Train Loss: 0.0128, Macro_F1: 0.8946, AUC_score: 0.9470\n",
      "Validation loss decreased (0.012775 --> 0.012775).\n",
      "Validation loss decreased (0.007920 --> 0.007920).\n",
      "Validation loss decreased (0.005244 --> 0.005244).\n",
      "Validation loss decreased (0.004826 --> 0.004826).\n",
      "Epoch 50: Train Loss: 0.0071, Macro_F1: 0.9120, AUC_score: 0.9482\n",
      "Epoch 100: Train Loss: 0.0090, Macro_F1: 0.9197, AUC_score: 0.9486\n",
      "Validation loss decreased (0.004260 --> 0.004260).\n",
      "Epoch 150: Train Loss: 0.0069, Macro_F1: 0.9115, AUC_score: 0.9467\n",
      "Validation loss decreased (0.003922 --> 0.003922).\n",
      "Epoch 200: Train Loss: 0.0093, Macro_F1: 0.9159, AUC_score: 0.9552\n",
      "Epoch 250: Train Loss: 0.0074, Macro_F1: 0.9153, AUC_score: 0.9507\n",
      "Epoch 00256: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0109, Macro_F1: 0.9159, AUC_score: 0.9514\n",
      "Validation loss decreased (0.003571 --> 0.003571).\n",
      "Epoch 350: Train Loss: 0.0044, Macro_F1: 0.9159, AUC_score: 0.9513\n",
      "Validation loss decreased (0.003491 --> 0.003491).\n",
      "Epoch 400: Train Loss: 0.0075, Macro_F1: 0.9159, AUC_score: 0.9508\n",
      "Epoch 450: Train Loss: 0.0081, Macro_F1: 0.9121, AUC_score: 0.9494\n",
      "Epoch 00485: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.0032, Macro_F1: 0.9121, AUC_score: 0.9502\n",
      "Validation loss decreased (0.003219 --> 0.003219).\n",
      "Epoch 550: Train Loss: 0.0050, Macro_F1: 0.9121, AUC_score: 0.9514\n",
      "Epoch 600: Train Loss: 0.0055, Macro_F1: 0.9159, AUC_score: 0.9518\n",
      "Epoch 00602: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Validation loss decreased (0.003054 --> 0.003054).\n",
      "Epoch 650: Train Loss: 0.0061, Macro_F1: 0.9159, AUC_score: 0.9513\n",
      "Epoch 700: Train Loss: 0.0045, Macro_F1: 0.9159, AUC_score: 0.9517\n",
      "Epoch 00723: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 750: Train Loss: 0.0068, Macro_F1: 0.9159, AUC_score: 0.9513\n",
      "Epoch 800: Train Loss: 0.0059, Macro_F1: 0.9159, AUC_score: 0.9512\n",
      "Early stopping triggered\n",
      "acc save\n",
      "79.0% node features transform to 0: F1: 0.9159, AUC_score: 0.9513\n",
      "Epoch 0: Train Loss: 0.0125, Macro_F1: 0.9148, AUC_score: 0.9524\n",
      "Validation loss decreased (0.012452 --> 0.012452).\n",
      "Validation loss decreased (0.009462 --> 0.009462).\n",
      "Validation loss decreased (0.009095 --> 0.009095).\n",
      "Validation loss decreased (0.008147 --> 0.008147).\n",
      "Validation loss decreased (0.007545 --> 0.007545).\n",
      "Validation loss decreased (0.006662 --> 0.006662).\n",
      "Validation loss decreased (0.006478 --> 0.006478).\n",
      "Validation loss decreased (0.006049 --> 0.006049).\n",
      "Epoch 50: Train Loss: 0.0102, Macro_F1: 0.9269, AUC_score: 0.9485\n",
      "Validation loss decreased (0.005486 --> 0.005486).\n",
      "Validation loss decreased (0.005248 --> 0.005248).\n",
      "Validation loss decreased (0.003921 --> 0.003921).\n",
      "Epoch 100: Train Loss: 0.0133, Macro_F1: 0.9195, AUC_score: 0.9496\n",
      "Epoch 150: Train Loss: 0.0255, Macro_F1: 0.9084, AUC_score: 0.9506\n",
      "Epoch 00169: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0112, Macro_F1: 0.9157, AUC_score: 0.9472\n",
      "Epoch 250: Train Loss: 0.0042, Macro_F1: 0.9234, AUC_score: 0.9502\n",
      "Early stopping triggered\n",
      "acc save\n",
      "78.0% node features transform to 0: F1: 0.9159, AUC_score: 0.9487\n",
      "Epoch 0: Train Loss: 0.0728, Macro_F1: 0.8443, AUC_score: 0.9444\n",
      "Validation loss decreased (0.072833 --> 0.072833).\n",
      "Validation loss decreased (0.069842 --> 0.069842).\n",
      "Validation loss decreased (0.018972 --> 0.018972).\n",
      "Validation loss decreased (0.018485 --> 0.018485).\n",
      "Validation loss decreased (0.018006 --> 0.018006).\n",
      "Validation loss decreased (0.009901 --> 0.009901).\n",
      "Validation loss decreased (0.008447 --> 0.008447).\n",
      "Validation loss decreased (0.006570 --> 0.006570).\n",
      "Epoch 50: Train Loss: 0.0179, Macro_F1: 0.9087, AUC_score: 0.9536\n",
      "Validation loss decreased (0.006474 --> 0.006474).\n",
      "Validation loss decreased (0.006035 --> 0.006035).\n",
      "Validation loss decreased (0.004761 --> 0.004761).\n",
      "Validation loss decreased (0.004546 --> 0.004546).\n",
      "Epoch 100: Train Loss: 0.0064, Macro_F1: 0.9117, AUC_score: 0.9500\n",
      "Epoch 150: Train Loss: 0.0172, Macro_F1: 0.9195, AUC_score: 0.9521\n",
      "Epoch 00188: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0065, Macro_F1: 0.9117, AUC_score: 0.9522\n",
      "Validation loss decreased (0.004400 --> 0.004400).\n",
      "Validation loss decreased (0.003864 --> 0.003864).\n",
      "Epoch 250: Train Loss: 0.0048, Macro_F1: 0.9161, AUC_score: 0.9535\n",
      "Epoch 300: Train Loss: 0.0038, Macro_F1: 0.9119, AUC_score: 0.9529\n",
      "Validation loss decreased (0.003823 --> 0.003823).\n",
      "Epoch 350: Train Loss: 0.0052, Macro_F1: 0.9123, AUC_score: 0.9531\n",
      "Epoch 400: Train Loss: 0.0053, Macro_F1: 0.9161, AUC_score: 0.9535\n",
      "Epoch 00402: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.003633 --> 0.003633).\n",
      "Epoch 450: Train Loss: 0.0073, Macro_F1: 0.9123, AUC_score: 0.9528\n",
      "Epoch 500: Train Loss: 0.0057, Macro_F1: 0.9123, AUC_score: 0.9534\n",
      "Validation loss decreased (0.003475 --> 0.003475).\n",
      "Validation loss decreased (0.002930 --> 0.002930).\n",
      "Epoch 550: Train Loss: 0.0072, Macro_F1: 0.9159, AUC_score: 0.9535\n",
      "Epoch 600: Train Loss: 0.0043, Macro_F1: 0.9161, AUC_score: 0.9533\n",
      "Epoch 00621: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 650: Train Loss: 0.0077, Macro_F1: 0.9087, AUC_score: 0.9535\n",
      "Epoch 700: Train Loss: 0.0049, Macro_F1: 0.9087, AUC_score: 0.9533\n",
      "Early stopping triggered\n",
      "acc save\n",
      "77.0% node features transform to 0: F1: 0.9087, AUC_score: 0.9533\n",
      "Epoch 0: Train Loss: 0.0062, Macro_F1: 0.8683, AUC_score: 0.9478\n",
      "Validation loss decreased (0.006171 --> 0.006171).\n",
      "Validation loss decreased (0.005994 --> 0.005994).\n",
      "Validation loss decreased (0.005300 --> 0.005300).\n",
      "Epoch 50: Train Loss: 0.0099, Macro_F1: 0.9231, AUC_score: 0.9505\n",
      "Validation loss decreased (0.005225 --> 0.005225).\n",
      "Validation loss decreased (0.004543 --> 0.004543).\n",
      "Validation loss decreased (0.004521 --> 0.004521).\n",
      "Validation loss decreased (0.003673 --> 0.003673).\n",
      "Epoch 100: Train Loss: 0.0075, Macro_F1: 0.9195, AUC_score: 0.9522\n",
      "Validation loss decreased (0.003110 --> 0.003110).\n",
      "Epoch 150: Train Loss: 0.0051, Macro_F1: 0.9195, AUC_score: 0.9561\n",
      "Epoch 200: Train Loss: 0.0077, Macro_F1: 0.9161, AUC_score: 0.9541\n",
      "Epoch 00217: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0090, Macro_F1: 0.9125, AUC_score: 0.9541\n",
      "Epoch 300: Train Loss: 0.0047, Macro_F1: 0.9124, AUC_score: 0.9537\n",
      "Early stopping triggered\n",
      "acc save\n",
      "76.0% node features transform to 0: F1: 0.9160, AUC_score: 0.9543\n",
      "Epoch 0: Train Loss: 0.0055, Macro_F1: 0.8623, AUC_score: 0.9517\n",
      "Validation loss decreased (0.005538 --> 0.005538).\n",
      "Validation loss decreased (0.005422 --> 0.005422).\n",
      "Validation loss decreased (0.005319 --> 0.005319).\n",
      "Epoch 50: Train Loss: 0.0064, Macro_F1: 0.9339, AUC_score: 0.9563\n",
      "Validation loss decreased (0.004788 --> 0.004788).\n",
      "Validation loss decreased (0.004211 --> 0.004211).\n",
      "Validation loss decreased (0.004051 --> 0.004051).\n",
      "Epoch 100: Train Loss: 0.0062, Macro_F1: 0.9231, AUC_score: 0.9535\n",
      "Validation loss decreased (0.003821 --> 0.003821).\n",
      "Validation loss decreased (0.003468 --> 0.003468).\n",
      "Validation loss decreased (0.003096 --> 0.003096).\n",
      "Epoch 150: Train Loss: 0.0034, Macro_F1: 0.9160, AUC_score: 0.9573\n",
      "Validation loss decreased (0.002770 --> 0.002770).\n",
      "Epoch 200: Train Loss: 0.0048, Macro_F1: 0.9162, AUC_score: 0.9536\n",
      "Epoch 250: Train Loss: 0.0048, Macro_F1: 0.9191, AUC_score: 0.9554\n",
      "Epoch 00271: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0074, Macro_F1: 0.9160, AUC_score: 0.9565\n",
      "Epoch 350: Train Loss: 0.0033, Macro_F1: 0.9160, AUC_score: 0.9572\n",
      "Validation loss decreased (0.002626 --> 0.002626).\n",
      "Epoch 400: Train Loss: 0.0046, Macro_F1: 0.9193, AUC_score: 0.9550\n",
      "Epoch 450: Train Loss: 0.0034, Macro_F1: 0.9232, AUC_score: 0.9574\n",
      "Epoch 00461: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.0047, Macro_F1: 0.9160, AUC_score: 0.9562\n",
      "Epoch 550: Train Loss: 0.0136, Macro_F1: 0.9160, AUC_score: 0.9561\n",
      "Early stopping triggered\n",
      "acc save\n",
      "75.0% node features transform to 0: F1: 0.9124, AUC_score: 0.9562\n",
      "Epoch 0: Train Loss: 0.0266, Macro_F1: 0.8849, AUC_score: 0.9491\n",
      "Validation loss decreased (0.026575 --> 0.026575).\n",
      "Validation loss decreased (0.015931 --> 0.015931).\n",
      "Validation loss decreased (0.015388 --> 0.015388).\n",
      "Validation loss decreased (0.010025 --> 0.010025).\n",
      "Validation loss decreased (0.009408 --> 0.009408).\n",
      "Validation loss decreased (0.008223 --> 0.008223).\n",
      "Validation loss decreased (0.007722 --> 0.007722).\n",
      "Validation loss decreased (0.006803 --> 0.006803).\n",
      "Validation loss decreased (0.006358 --> 0.006358).\n",
      "Validation loss decreased (0.006027 --> 0.006027).\n",
      "Validation loss decreased (0.005547 --> 0.005547).\n",
      "Epoch 50: Train Loss: 0.0059, Macro_F1: 0.9123, AUC_score: 0.9534\n",
      "Validation loss decreased (0.004681 --> 0.004681).\n",
      "Validation loss decreased (0.004334 --> 0.004334).\n",
      "Validation loss decreased (0.004236 --> 0.004236).\n",
      "Validation loss decreased (0.003593 --> 0.003593).\n",
      "Epoch 100: Train Loss: 0.0051, Macro_F1: 0.9228, AUC_score: 0.9539\n",
      "Epoch 150: Train Loss: 0.0078, Macro_F1: 0.9227, AUC_score: 0.9544\n",
      "Epoch 00166: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0089, Macro_F1: 0.9159, AUC_score: 0.9569\n",
      "Validation loss decreased (0.003426 --> 0.003426).\n",
      "Epoch 250: Train Loss: 0.0043, Macro_F1: 0.9267, AUC_score: 0.9547\n",
      "Epoch 300: Train Loss: 0.0049, Macro_F1: 0.9123, AUC_score: 0.9556\n",
      "Epoch 00316: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.002975 --> 0.002975).\n",
      "Epoch 350: Train Loss: 0.0057, Macro_F1: 0.9123, AUC_score: 0.9552\n",
      "Validation loss decreased (0.002694 --> 0.002694).\n",
      "Epoch 400: Train Loss: 0.0056, Macro_F1: 0.9159, AUC_score: 0.9548\n",
      "Epoch 450: Train Loss: 0.0050, Macro_F1: 0.9123, AUC_score: 0.9540\n",
      "Epoch 00481: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Validation loss decreased (0.002633 --> 0.002633).\n",
      "Validation loss decreased (0.002506 --> 0.002506).\n",
      "Epoch 500: Train Loss: 0.0044, Macro_F1: 0.9123, AUC_score: 0.9541\n",
      "Epoch 550: Train Loss: 0.0045, Macro_F1: 0.9123, AUC_score: 0.9540\n",
      "Epoch 00595: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 600: Train Loss: 0.0046, Macro_F1: 0.9123, AUC_score: 0.9538\n",
      "Epoch 650: Train Loss: 0.0041, Macro_F1: 0.9123, AUC_score: 0.9539\n",
      "Early stopping triggered\n",
      "acc save\n",
      "74.0% node features transform to 0: F1: 0.9123, AUC_score: 0.9539\n",
      "Epoch 0: Train Loss: 0.0108, Macro_F1: 0.8876, AUC_score: 0.9522\n",
      "Validation loss decreased (0.010755 --> 0.010755).\n",
      "Validation loss decreased (0.005100 --> 0.005100).\n",
      "Validation loss decreased (0.005062 --> 0.005062).\n",
      "Validation loss decreased (0.004797 --> 0.004797).\n",
      "Validation loss decreased (0.003223 --> 0.003223).\n",
      "Epoch 50: Train Loss: 0.0052, Macro_F1: 0.9195, AUC_score: 0.9603\n",
      "Epoch 100: Train Loss: 0.0169, Macro_F1: 0.9087, AUC_score: 0.9614\n",
      "Validation loss decreased (0.003126 --> 0.003126).\n",
      "Epoch 150: Train Loss: 0.0039, Macro_F1: 0.9123, AUC_score: 0.9545\n",
      "Epoch 200: Train Loss: 0.0058, Macro_F1: 0.9123, AUC_score: 0.9575\n",
      "Epoch 00217: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.002518 --> 0.002518).\n",
      "Epoch 250: Train Loss: 0.0050, Macro_F1: 0.9123, AUC_score: 0.9541\n",
      "Epoch 300: Train Loss: 0.0059, Macro_F1: 0.9123, AUC_score: 0.9548\n",
      "Epoch 00336: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.0039, Macro_F1: 0.9123, AUC_score: 0.9548\n",
      "Epoch 400: Train Loss: 0.0036, Macro_F1: 0.9123, AUC_score: 0.9548\n",
      "Early stopping triggered\n",
      "acc save\n",
      "73.0% node features transform to 0: F1: 0.9123, AUC_score: 0.9549\n",
      "Epoch 0: Train Loss: 0.0107, Macro_F1: 0.8843, AUC_score: 0.9471\n",
      "Validation loss decreased (0.010743 --> 0.010743).\n",
      "Validation loss decreased (0.010281 --> 0.010281).\n",
      "Validation loss decreased (0.009843 --> 0.009843).\n",
      "Validation loss decreased (0.009279 --> 0.009279).\n",
      "Validation loss decreased (0.006774 --> 0.006774).\n",
      "Validation loss decreased (0.006176 --> 0.006176).\n",
      "Validation loss decreased (0.005483 --> 0.005483).\n",
      "Validation loss decreased (0.005178 --> 0.005178).\n",
      "Epoch 50: Train Loss: 0.0067, Macro_F1: 0.9264, AUC_score: 0.9597\n",
      "Validation loss decreased (0.005127 --> 0.005127).\n",
      "Validation loss decreased (0.004543 --> 0.004543).\n",
      "Validation loss decreased (0.004021 --> 0.004021).\n",
      "Epoch 100: Train Loss: 0.0047, Macro_F1: 0.9124, AUC_score: 0.9598\n",
      "Validation loss decreased (0.003640 --> 0.003640).\n",
      "Validation loss decreased (0.002875 --> 0.002875).\n",
      "Epoch 150: Train Loss: 0.0047, Macro_F1: 0.9155, AUC_score: 0.9569\n",
      "Epoch 200: Train Loss: 0.0060, Macro_F1: 0.9123, AUC_score: 0.9605\n",
      "Epoch 00222: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0048, Macro_F1: 0.9229, AUC_score: 0.9576\n",
      "Epoch 300: Train Loss: 0.0094, Macro_F1: 0.9195, AUC_score: 0.9586\n",
      "Early stopping triggered\n",
      "acc save\n",
      "72.0% node features transform to 0: F1: 0.9155, AUC_score: 0.9577\n",
      "Epoch 0: Train Loss: 0.0140, Macro_F1: 0.8839, AUC_score: 0.9605\n",
      "Validation loss decreased (0.014002 --> 0.014002).\n",
      "Validation loss decreased (0.007520 --> 0.007520).\n",
      "Validation loss decreased (0.007110 --> 0.007110).\n",
      "Validation loss decreased (0.005455 --> 0.005455).\n",
      "Validation loss decreased (0.004983 --> 0.004983).\n",
      "Validation loss decreased (0.004745 --> 0.004745).\n",
      "Validation loss decreased (0.004090 --> 0.004090).\n",
      "Epoch 50: Train Loss: 0.0049, Macro_F1: 0.9195, AUC_score: 0.9597\n",
      "Validation loss decreased (0.003658 --> 0.003658).\n",
      "Validation loss decreased (0.003017 --> 0.003017).\n",
      "Epoch 100: Train Loss: 0.0053, Macro_F1: 0.9161, AUC_score: 0.9583\n",
      "Validation loss decreased (0.002998 --> 0.002998).\n",
      "Validation loss decreased (0.002872 --> 0.002872).\n",
      "Validation loss decreased (0.002811 --> 0.002811).\n",
      "Epoch 150: Train Loss: 0.0027, Macro_F1: 0.9229, AUC_score: 0.9587\n",
      "Validation loss decreased (0.002736 --> 0.002736).\n",
      "Validation loss decreased (0.002140 --> 0.002140).\n",
      "Epoch 200: Train Loss: 0.0068, Macro_F1: 0.9153, AUC_score: 0.9582\n",
      "Epoch 250: Train Loss: 0.0079, Macro_F1: 0.9196, AUC_score: 0.9575\n",
      "Epoch 00255: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0038, Macro_F1: 0.9193, AUC_score: 0.9588\n",
      "Epoch 350: Train Loss: 0.0056, Macro_F1: 0.9160, AUC_score: 0.9590\n",
      "Early stopping triggered\n",
      "acc save\n",
      "71.0% node features transform to 0: F1: 0.9160, AUC_score: 0.9589\n",
      "Epoch 0: Train Loss: 0.0056, Macro_F1: 0.9090, AUC_score: 0.9557\n",
      "Validation loss decreased (0.005642 --> 0.005642).\n",
      "Validation loss decreased (0.005519 --> 0.005519).\n",
      "Validation loss decreased (0.004554 --> 0.004554).\n",
      "Validation loss decreased (0.004474 --> 0.004474).\n",
      "Validation loss decreased (0.004459 --> 0.004459).\n",
      "Validation loss decreased (0.004198 --> 0.004198).\n",
      "Epoch 50: Train Loss: 0.0062, Macro_F1: 0.9229, AUC_score: 0.9605\n",
      "Validation loss decreased (0.003793 --> 0.003793).\n",
      "Epoch 100: Train Loss: 0.0053, Macro_F1: 0.9199, AUC_score: 0.9617\n",
      "Epoch 150: Train Loss: 0.0046, Macro_F1: 0.9340, AUC_score: 0.9621\n",
      "Validation loss decreased (0.003460 --> 0.003460).\n",
      "Epoch 200: Train Loss: 0.0054, Macro_F1: 0.9270, AUC_score: 0.9622\n",
      "Validation loss decreased (0.003416 --> 0.003416).\n",
      "Validation loss decreased (0.003386 --> 0.003386).\n",
      "Epoch 250: Train Loss: 0.0078, Macro_F1: 0.9115, AUC_score: 0.9574\n",
      "Validation loss decreased (0.003280 --> 0.003280).\n",
      "Epoch 300: Train Loss: 0.0101, Macro_F1: 0.9191, AUC_score: 0.9552\n",
      "Validation loss decreased (0.003133 --> 0.003133).\n",
      "Validation loss decreased (0.002780 --> 0.002780).\n",
      "Epoch 350: Train Loss: 0.0051, Macro_F1: 0.9232, AUC_score: 0.9597\n",
      "Epoch 400: Train Loss: 0.0090, Macro_F1: 0.9115, AUC_score: 0.9614\n",
      "Epoch 00421: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 450: Train Loss: 0.0043, Macro_F1: 0.9196, AUC_score: 0.9599\n",
      "Validation loss decreased (0.002750 --> 0.002750).\n",
      "Epoch 500: Train Loss: 0.0050, Macro_F1: 0.9231, AUC_score: 0.9593\n",
      "Epoch 550: Train Loss: 0.0036, Macro_F1: 0.9124, AUC_score: 0.9590\n",
      "Epoch 600: Train Loss: 0.0059, Macro_F1: 0.9119, AUC_score: 0.9587\n",
      "Validation loss decreased (0.002569 --> 0.002569).\n",
      "Validation loss decreased (0.002395 --> 0.002395).\n",
      "Epoch 650: Train Loss: 0.0040, Macro_F1: 0.9157, AUC_score: 0.9593\n",
      "Epoch 700: Train Loss: 0.0034, Macro_F1: 0.9120, AUC_score: 0.9570\n",
      "Epoch 00747: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 750: Train Loss: 0.0042, Macro_F1: 0.9156, AUC_score: 0.9585\n",
      "Epoch 800: Train Loss: 0.0039, Macro_F1: 0.9123, AUC_score: 0.9586\n",
      "Validation loss decreased (0.002348 --> 0.002348).\n",
      "Epoch 850: Train Loss: 0.0055, Macro_F1: 0.9157, AUC_score: 0.9580\n",
      "Epoch 900: Train Loss: 0.0030, Macro_F1: 0.9120, AUC_score: 0.9573\n",
      "Epoch 00922: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 950: Train Loss: 0.0035, Macro_F1: 0.9157, AUC_score: 0.9581\n",
      "acc save\n",
      "70.0% node features transform to 0: F1: 0.9157, AUC_score: 0.9581\n",
      "Epoch 0: Train Loss: 0.0038, Macro_F1: 0.9019, AUC_score: 0.9610\n",
      "Validation loss decreased (0.003841 --> 0.003841).\n",
      "Epoch 50: Train Loss: 0.0088, Macro_F1: 0.9089, AUC_score: 0.9608\n",
      "Validation loss decreased (0.003791 --> 0.003791).\n",
      "Validation loss decreased (0.003724 --> 0.003724).\n",
      "Validation loss decreased (0.002832 --> 0.002832).\n",
      "Validation loss decreased (0.002763 --> 0.002763).\n",
      "Epoch 100: Train Loss: 0.0087, Macro_F1: 0.9157, AUC_score: 0.9592\n",
      "Validation loss decreased (0.002725 --> 0.002725).\n",
      "Epoch 150: Train Loss: 0.0036, Macro_F1: 0.9192, AUC_score: 0.9569\n",
      "Epoch 200: Train Loss: 0.0075, Macro_F1: 0.9189, AUC_score: 0.9604\n",
      "Epoch 00230: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0058, Macro_F1: 0.9304, AUC_score: 0.9603\n",
      "Validation loss decreased (0.002664 --> 0.002664).\n",
      "Epoch 300: Train Loss: 0.0107, Macro_F1: 0.9231, AUC_score: 0.9616\n",
      "Validation loss decreased (0.002534 --> 0.002534).\n",
      "Epoch 350: Train Loss: 0.0072, Macro_F1: 0.9157, AUC_score: 0.9609\n",
      "Validation loss decreased (0.002422 --> 0.002422).\n",
      "Validation loss decreased (0.002406 --> 0.002406).\n",
      "Epoch 400: Train Loss: 0.0032, Macro_F1: 0.9195, AUC_score: 0.9617\n",
      "Validation loss decreased (0.001906 --> 0.001906).\n",
      "Epoch 450: Train Loss: 0.0039, Macro_F1: 0.9195, AUC_score: 0.9620\n",
      "Epoch 500: Train Loss: 0.0052, Macro_F1: 0.9157, AUC_score: 0.9607\n",
      "Epoch 00531: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 550: Train Loss: 0.0048, Macro_F1: 0.9157, AUC_score: 0.9605\n",
      "Epoch 600: Train Loss: 0.0048, Macro_F1: 0.9232, AUC_score: 0.9604\n",
      "Early stopping triggered\n",
      "acc save\n",
      "69.0% node features transform to 0: F1: 0.9157, AUC_score: 0.9601\n",
      "Epoch 0: Train Loss: 0.0105, Macro_F1: 0.8915, AUC_score: 0.9566\n",
      "Validation loss decreased (0.010471 --> 0.010471).\n",
      "Validation loss decreased (0.010443 --> 0.010443).\n",
      "Validation loss decreased (0.008872 --> 0.008872).\n",
      "Validation loss decreased (0.006612 --> 0.006612).\n",
      "Validation loss decreased (0.006326 --> 0.006326).\n",
      "Validation loss decreased (0.004491 --> 0.004491).\n",
      "Validation loss decreased (0.004400 --> 0.004400).\n",
      "Epoch 50: Train Loss: 0.0062, Macro_F1: 0.9232, AUC_score: 0.9614\n",
      "Validation loss decreased (0.004247 --> 0.004247).\n",
      "Validation loss decreased (0.003528 --> 0.003528).\n",
      "Validation loss decreased (0.003279 --> 0.003279).\n",
      "Validation loss decreased (0.002680 --> 0.002680).\n",
      "Validation loss decreased (0.002663 --> 0.002663).\n",
      "Epoch 100: Train Loss: 0.0035, Macro_F1: 0.9196, AUC_score: 0.9620\n",
      "Epoch 150: Train Loss: 0.0098, Macro_F1: 0.9087, AUC_score: 0.9629\n",
      "Epoch 00198: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0192, Macro_F1: 0.9053, AUC_score: 0.9613\n",
      "Epoch 250: Train Loss: 0.0038, Macro_F1: 0.9193, AUC_score: 0.9625\n",
      "Early stopping triggered\n",
      "acc save\n",
      "68.0% node features transform to 0: F1: 0.9159, AUC_score: 0.9614\n",
      "Epoch 0: Train Loss: 0.0070, Macro_F1: 0.8755, AUC_score: 0.9555\n",
      "Validation loss decreased (0.006983 --> 0.006983).\n",
      "Validation loss decreased (0.004456 --> 0.004456).\n",
      "Validation loss decreased (0.003857 --> 0.003857).\n",
      "Validation loss decreased (0.003657 --> 0.003657).\n",
      "Epoch 50: Train Loss: 0.0145, Macro_F1: 0.9265, AUC_score: 0.9636\n",
      "Validation loss decreased (0.003353 --> 0.003353).\n",
      "Validation loss decreased (0.003207 --> 0.003207).\n",
      "Validation loss decreased (0.003018 --> 0.003018).\n",
      "Validation loss decreased (0.002940 --> 0.002940).\n",
      "Epoch 100: Train Loss: 0.0057, Macro_F1: 0.9159, AUC_score: 0.9646\n",
      "Validation loss decreased (0.002717 --> 0.002717).\n",
      "Validation loss decreased (0.002196 --> 0.002196).\n",
      "Epoch 150: Train Loss: 0.0098, Macro_F1: 0.9157, AUC_score: 0.9628\n",
      "Epoch 200: Train Loss: 0.0066, Macro_F1: 0.9196, AUC_score: 0.9656\n",
      "Epoch 00248: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0032, Macro_F1: 0.9123, AUC_score: 0.9667\n",
      "Epoch 300: Train Loss: 0.0098, Macro_F1: 0.9192, AUC_score: 0.9628\n",
      "Early stopping triggered\n",
      "acc save\n",
      "67.0% node features transform to 0: F1: 0.9228, AUC_score: 0.9637\n",
      "Epoch 0: Train Loss: 0.0109, Macro_F1: 0.8731, AUC_score: 0.9635\n",
      "Validation loss decreased (0.010918 --> 0.010918).\n",
      "Validation loss decreased (0.007413 --> 0.007413).\n",
      "Validation loss decreased (0.005422 --> 0.005422).\n",
      "Validation loss decreased (0.003830 --> 0.003830).\n",
      "Validation loss decreased (0.003482 --> 0.003482).\n",
      "Validation loss decreased (0.003373 --> 0.003373).\n",
      "Epoch 50: Train Loss: 0.0074, Macro_F1: 0.9196, AUC_score: 0.9628\n",
      "Validation loss decreased (0.003020 --> 0.003020).\n",
      "Validation loss decreased (0.002882 --> 0.002882).\n",
      "Validation loss decreased (0.002780 --> 0.002780).\n",
      "Validation loss decreased (0.002534 --> 0.002534).\n",
      "Epoch 100: Train Loss: 0.0038, Macro_F1: 0.9087, AUC_score: 0.9618\n",
      "Epoch 150: Train Loss: 0.0033, Macro_F1: 0.9084, AUC_score: 0.9624\n",
      "Validation loss decreased (0.002191 --> 0.002191).\n",
      "Epoch 200: Train Loss: 0.0035, Macro_F1: 0.9088, AUC_score: 0.9638\n",
      "Epoch 250: Train Loss: 0.0087, Macro_F1: 0.9048, AUC_score: 0.9596\n",
      "Epoch 00286: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0036, Macro_F1: 0.9302, AUC_score: 0.9633\n",
      "Epoch 350: Train Loss: 0.0062, Macro_F1: 0.9050, AUC_score: 0.9617\n",
      "Early stopping triggered\n",
      "acc save\n",
      "65.99999999999999% node features transform to 0: F1: 0.9050, AUC_score: 0.9612\n",
      "Epoch 0: Train Loss: 0.0044, Macro_F1: 0.9157, AUC_score: 0.9644\n",
      "Validation loss decreased (0.004368 --> 0.004368).\n",
      "Validation loss decreased (0.004354 --> 0.004354).\n",
      "Validation loss decreased (0.003071 --> 0.003071).\n",
      "Validation loss decreased (0.003020 --> 0.003020).\n",
      "Validation loss decreased (0.002455 --> 0.002455).\n",
      "Epoch 50: Train Loss: 0.0043, Macro_F1: 0.9121, AUC_score: 0.9632\n",
      "Validation loss decreased (0.002426 --> 0.002426).\n",
      "Validation loss decreased (0.002117 --> 0.002117).\n",
      "Epoch 100: Train Loss: 0.0059, Macro_F1: 0.9124, AUC_score: 0.9646\n",
      "Epoch 150: Train Loss: 0.0089, Macro_F1: 0.9036, AUC_score: 0.9614\n",
      "Epoch 00191: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0097, Macro_F1: 0.9157, AUC_score: 0.9613\n",
      "Epoch 250: Train Loss: 0.0124, Macro_F1: 0.9123, AUC_score: 0.9641\n",
      "Validation loss decreased (0.001985 --> 0.001985).\n",
      "Epoch 300: Train Loss: 0.0044, Macro_F1: 0.9052, AUC_score: 0.9633\n",
      "Epoch 350: Train Loss: 0.0028, Macro_F1: 0.9123, AUC_score: 0.9640\n",
      "Validation loss decreased (0.001835 --> 0.001835).\n",
      "Epoch 400: Train Loss: 0.0029, Macro_F1: 0.9085, AUC_score: 0.9639\n",
      "Validation loss decreased (0.001521 --> 0.001521).\n",
      "Epoch 450: Train Loss: 0.0038, Macro_F1: 0.9156, AUC_score: 0.9615\n",
      "Epoch 500: Train Loss: 0.0094, Macro_F1: 0.9119, AUC_score: 0.9618\n",
      "Epoch 00538: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 550: Train Loss: 0.0058, Macro_F1: 0.9051, AUC_score: 0.9620\n",
      "Epoch 600: Train Loss: 0.0033, Macro_F1: 0.9084, AUC_score: 0.9631\n",
      "Early stopping triggered\n",
      "acc save\n",
      "64.99999999999999% node features transform to 0: F1: 0.9048, AUC_score: 0.9631\n",
      "Epoch 0: Train Loss: 0.0367, Macro_F1: 0.8830, AUC_score: 0.9546\n",
      "Validation loss decreased (0.036696 --> 0.036696).\n",
      "Validation loss decreased (0.016308 --> 0.016308).\n",
      "Validation loss decreased (0.014220 --> 0.014220).\n",
      "Validation loss decreased (0.008305 --> 0.008305).\n",
      "Validation loss decreased (0.007375 --> 0.007375).\n",
      "Validation loss decreased (0.005930 --> 0.005930).\n",
      "Validation loss decreased (0.004665 --> 0.004665).\n",
      "Validation loss decreased (0.004604 --> 0.004604).\n",
      "Validation loss decreased (0.004433 --> 0.004433).\n",
      "Validation loss decreased (0.003941 --> 0.003941).\n",
      "Epoch 50: Train Loss: 0.0064, Macro_F1: 0.9192, AUC_score: 0.9612\n",
      "Validation loss decreased (0.003623 --> 0.003623).\n",
      "Validation loss decreased (0.002552 --> 0.002552).\n",
      "Epoch 100: Train Loss: 0.0068, Macro_F1: 0.9124, AUC_score: 0.9628\n",
      "Epoch 150: Train Loss: 0.0048, Macro_F1: 0.9162, AUC_score: 0.9637\n",
      "Validation loss decreased (0.001649 --> 0.001649).\n",
      "Epoch 200: Train Loss: 0.0071, Macro_F1: 0.9124, AUC_score: 0.9625\n",
      "Epoch 250: Train Loss: 0.0062, Macro_F1: 0.9191, AUC_score: 0.9582\n",
      "Epoch 00263: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0037, Macro_F1: 0.9121, AUC_score: 0.9601\n",
      "Epoch 350: Train Loss: 0.0031, Macro_F1: 0.9124, AUC_score: 0.9609\n",
      "Early stopping triggered\n",
      "acc save\n",
      "64.0% node features transform to 0: F1: 0.9156, AUC_score: 0.9606\n",
      "Epoch 0: Train Loss: 0.0105, Macro_F1: 0.8838, AUC_score: 0.9620\n",
      "Validation loss decreased (0.010508 --> 0.010508).\n",
      "Validation loss decreased (0.004616 --> 0.004616).\n",
      "Validation loss decreased (0.004167 --> 0.004167).\n",
      "Validation loss decreased (0.004019 --> 0.004019).\n",
      "Validation loss decreased (0.003676 --> 0.003676).\n",
      "Epoch 50: Train Loss: 0.0059, Macro_F1: 0.9193, AUC_score: 0.9559\n",
      "Validation loss decreased (0.002946 --> 0.002946).\n",
      "Validation loss decreased (0.002925 --> 0.002925).\n",
      "Epoch 100: Train Loss: 0.0062, Macro_F1: 0.9088, AUC_score: 0.9591\n",
      "Validation loss decreased (0.002030 --> 0.002030).\n",
      "Epoch 150: Train Loss: 0.0052, Macro_F1: 0.9229, AUC_score: 0.9612\n",
      "Epoch 200: Train Loss: 0.0069, Macro_F1: 0.9193, AUC_score: 0.9646\n",
      "Epoch 00210: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0025, Macro_F1: 0.9231, AUC_score: 0.9628\n",
      "Validation loss decreased (0.001712 --> 0.001712).\n",
      "Epoch 300: Train Loss: 0.0025, Macro_F1: 0.9159, AUC_score: 0.9621\n",
      "Validation loss decreased (0.001685 --> 0.001685).\n",
      "Epoch 350: Train Loss: 0.0032, Macro_F1: 0.9195, AUC_score: 0.9628\n",
      "Epoch 400: Train Loss: 0.0021, Macro_F1: 0.9157, AUC_score: 0.9614\n",
      "Validation loss decreased (0.001565 --> 0.001565).\n",
      "Epoch 450: Train Loss: 0.0164, Macro_F1: 0.9192, AUC_score: 0.9616\n",
      "Epoch 500: Train Loss: 0.0035, Macro_F1: 0.9120, AUC_score: 0.9610\n",
      "Epoch 00505: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 550: Train Loss: 0.0029, Macro_F1: 0.9196, AUC_score: 0.9623\n",
      "Epoch 600: Train Loss: 0.0025, Macro_F1: 0.9121, AUC_score: 0.9620\n",
      "Early stopping triggered\n",
      "acc save\n",
      "63.0% node features transform to 0: F1: 0.9121, AUC_score: 0.9618\n",
      "Epoch 0: Train Loss: 0.0038, Macro_F1: 0.8367, AUC_score: 0.9564\n",
      "Validation loss decreased (0.003842 --> 0.003842).\n",
      "Epoch 50: Train Loss: 0.0052, Macro_F1: 0.9342, AUC_score: 0.9659\n",
      "Validation loss decreased (0.002871 --> 0.002871).\n",
      "Epoch 100: Train Loss: 0.0026, Macro_F1: 0.9157, AUC_score: 0.9633\n",
      "Validation loss decreased (0.002628 --> 0.002628).\n",
      "Validation loss decreased (0.002543 --> 0.002543).\n",
      "Validation loss decreased (0.002355 --> 0.002355).\n",
      "Validation loss decreased (0.002315 --> 0.002315).\n",
      "Validation loss decreased (0.002074 --> 0.002074).\n",
      "Validation loss decreased (0.001889 --> 0.001889).\n",
      "Epoch 150: Train Loss: 0.0040, Macro_F1: 0.9084, AUC_score: 0.9624\n",
      "Validation loss decreased (0.001876 --> 0.001876).\n",
      "Epoch 200: Train Loss: 0.0035, Macro_F1: 0.9087, AUC_score: 0.9604\n",
      "Validation loss decreased (0.001777 --> 0.001777).\n",
      "Epoch 250: Train Loss: 0.0031, Macro_F1: 0.9193, AUC_score: 0.9638\n",
      "Validation loss decreased (0.001449 --> 0.001449).\n",
      "Epoch 300: Train Loss: 0.0036, Macro_F1: 0.9265, AUC_score: 0.9611\n",
      "Epoch 350: Train Loss: 0.0034, Macro_F1: 0.9123, AUC_score: 0.9612\n",
      "Epoch 00371: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 400: Train Loss: 0.0058, Macro_F1: 0.9087, AUC_score: 0.9623\n",
      "Epoch 450: Train Loss: 0.0063, Macro_F1: 0.9156, AUC_score: 0.9616\n",
      "Early stopping triggered\n",
      "acc save\n",
      "62.0% node features transform to 0: F1: 0.9087, AUC_score: 0.9624\n",
      "Epoch 0: Train Loss: 0.0058, Macro_F1: 0.9029, AUC_score: 0.9599\n",
      "Validation loss decreased (0.005816 --> 0.005816).\n",
      "Validation loss decreased (0.003787 --> 0.003787).\n",
      "Validation loss decreased (0.003644 --> 0.003644).\n",
      "Validation loss decreased (0.002936 --> 0.002936).\n",
      "Validation loss decreased (0.002697 --> 0.002697).\n",
      "Validation loss decreased (0.002492 --> 0.002492).\n",
      "Epoch 50: Train Loss: 0.0093, Macro_F1: 0.9157, AUC_score: 0.9655\n",
      "Epoch 100: Train Loss: 0.0035, Macro_F1: 0.9197, AUC_score: 0.9645\n",
      "Validation loss decreased (0.002363 --> 0.002363).\n",
      "Validation loss decreased (0.002149 --> 0.002149).\n",
      "Epoch 150: Train Loss: 0.0027, Macro_F1: 0.9162, AUC_score: 0.9621\n",
      "Validation loss decreased (0.002114 --> 0.002114).\n",
      "Epoch 200: Train Loss: 0.0112, Macro_F1: 0.9232, AUC_score: 0.9630\n",
      "Epoch 250: Train Loss: 0.0058, Macro_F1: 0.9338, AUC_score: 0.9637\n",
      "Epoch 00299: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0190, Macro_F1: 0.9112, AUC_score: 0.9601\n",
      "Validation loss decreased (0.001995 --> 0.001995).\n",
      "Epoch 350: Train Loss: 0.0036, Macro_F1: 0.9160, AUC_score: 0.9643\n",
      "Epoch 400: Train Loss: 0.0077, Macro_F1: 0.9267, AUC_score: 0.9632\n",
      "Validation loss decreased (0.001500 --> 0.001500).\n",
      "Epoch 450: Train Loss: 0.0060, Macro_F1: 0.9124, AUC_score: 0.9635\n",
      "Validation loss decreased (0.001422 --> 0.001422).\n",
      "Epoch 500: Train Loss: 0.0019, Macro_F1: 0.9123, AUC_score: 0.9638\n",
      "Validation loss decreased (0.001276 --> 0.001276).\n",
      "Epoch 550: Train Loss: 0.0023, Macro_F1: 0.9159, AUC_score: 0.9612\n",
      "Epoch 600: Train Loss: 0.0021, Macro_F1: 0.9123, AUC_score: 0.9631\n",
      "Epoch 00608: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 650: Train Loss: 0.0040, Macro_F1: 0.9159, AUC_score: 0.9621\n",
      "Epoch 700: Train Loss: 0.0027, Macro_F1: 0.9157, AUC_score: 0.9622\n",
      "Early stopping triggered\n",
      "acc save\n",
      "61.0% node features transform to 0: F1: 0.9159, AUC_score: 0.9623\n",
      "Epoch 0: Train Loss: 0.0033, Macro_F1: 0.8550, AUC_score: 0.9510\n",
      "Validation loss decreased (0.003290 --> 0.003290).\n",
      "Validation loss decreased (0.003236 --> 0.003236).\n",
      "Epoch 50: Train Loss: 0.0043, Macro_F1: 0.9267, AUC_score: 0.9634\n",
      "Validation loss decreased (0.002791 --> 0.002791).\n",
      "Validation loss decreased (0.002773 --> 0.002773).\n",
      "Validation loss decreased (0.002540 --> 0.002540).\n",
      "Validation loss decreased (0.002525 --> 0.002525).\n",
      "Validation loss decreased (0.002003 --> 0.002003).\n",
      "Epoch 100: Train Loss: 0.0038, Macro_F1: 0.9192, AUC_score: 0.9607\n",
      "Validation loss decreased (0.001902 --> 0.001902).\n",
      "Epoch 150: Train Loss: 0.0040, Macro_F1: 0.9123, AUC_score: 0.9619\n",
      "Validation loss decreased (0.001497 --> 0.001497).\n",
      "Epoch 200: Train Loss: 0.0027, Macro_F1: 0.9231, AUC_score: 0.9616\n",
      "Epoch 250: Train Loss: 0.0018, Macro_F1: 0.9228, AUC_score: 0.9631\n",
      "Epoch 00273: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0037, Macro_F1: 0.9267, AUC_score: 0.9639\n",
      "Epoch 350: Train Loss: 0.0042, Macro_F1: 0.9303, AUC_score: 0.9631\n",
      "Early stopping triggered\n",
      "acc save\n",
      "60.0% node features transform to 0: F1: 0.9228, AUC_score: 0.9622\n",
      "Epoch 0: Train Loss: 0.0042, Macro_F1: 0.9055, AUC_score: 0.9652\n",
      "Validation loss decreased (0.004246 --> 0.004246).\n",
      "Validation loss decreased (0.002891 --> 0.002891).\n",
      "Validation loss decreased (0.002713 --> 0.002713).\n",
      "Validation loss decreased (0.002560 --> 0.002560).\n",
      "Validation loss decreased (0.002230 --> 0.002230).\n",
      "Epoch 50: Train Loss: 0.0048, Macro_F1: 0.9088, AUC_score: 0.9614\n",
      "Validation loss decreased (0.002158 --> 0.002158).\n",
      "Epoch 100: Train Loss: 0.0099, Macro_F1: 0.9156, AUC_score: 0.9584\n",
      "Validation loss decreased (0.001921 --> 0.001921).\n",
      "Epoch 150: Train Loss: 0.0031, Macro_F1: 0.9268, AUC_score: 0.9636\n",
      "Epoch 200: Train Loss: 0.0082, Macro_F1: 0.9128, AUC_score: 0.9647\n",
      "Epoch 00226: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.001890 --> 0.001890).\n",
      "Epoch 250: Train Loss: 0.0036, Macro_F1: 0.9124, AUC_score: 0.9629\n",
      "Validation loss decreased (0.001439 --> 0.001439).\n",
      "Epoch 300: Train Loss: 0.0033, Macro_F1: 0.9232, AUC_score: 0.9639\n",
      "Epoch 350: Train Loss: 0.0022, Macro_F1: 0.9191, AUC_score: 0.9638\n",
      "Epoch 00361: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0032, Macro_F1: 0.9196, AUC_score: 0.9640\n",
      "Epoch 450: Train Loss: 0.0046, Macro_F1: 0.9265, AUC_score: 0.9645\n",
      "Early stopping triggered\n",
      "acc save\n",
      "59.0% node features transform to 0: F1: 0.9268, AUC_score: 0.9646\n",
      "Epoch 0: Train Loss: 0.0110, Macro_F1: 0.9263, AUC_score: 0.9685\n",
      "Validation loss decreased (0.010988 --> 0.010988).\n",
      "Validation loss decreased (0.007852 --> 0.007852).\n",
      "Validation loss decreased (0.002521 --> 0.002521).\n",
      "Validation loss decreased (0.002410 --> 0.002410).\n",
      "Epoch 50: Train Loss: 0.0101, Macro_F1: 0.9160, AUC_score: 0.9654\n",
      "Validation loss decreased (0.002384 --> 0.002384).\n",
      "Validation loss decreased (0.002156 --> 0.002156).\n",
      "Validation loss decreased (0.001940 --> 0.001940).\n",
      "Validation loss decreased (0.001766 --> 0.001766).\n",
      "Epoch 100: Train Loss: 0.0078, Macro_F1: 0.9150, AUC_score: 0.9617\n",
      "Epoch 150: Train Loss: 0.0051, Macro_F1: 0.9162, AUC_score: 0.9664\n",
      "Validation loss decreased (0.001741 --> 0.001741).\n",
      "Epoch 200: Train Loss: 0.0032, Macro_F1: 0.9231, AUC_score: 0.9657\n",
      "Epoch 250: Train Loss: 0.0032, Macro_F1: 0.9156, AUC_score: 0.9669\n",
      "Epoch 00291: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0038, Macro_F1: 0.9232, AUC_score: 0.9645\n",
      "Epoch 350: Train Loss: 0.0031, Macro_F1: 0.9196, AUC_score: 0.9667\n",
      "Early stopping triggered\n",
      "acc save\n",
      "58.00000000000001% node features transform to 0: F1: 0.9232, AUC_score: 0.9650\n",
      "Epoch 0: Train Loss: 0.0038, Macro_F1: 0.8477, AUC_score: 0.9616\n",
      "Validation loss decreased (0.003835 --> 0.003835).\n",
      "Validation loss decreased (0.003454 --> 0.003454).\n",
      "Epoch 50: Train Loss: 0.0023, Macro_F1: 0.9302, AUC_score: 0.9687\n",
      "Validation loss decreased (0.002278 --> 0.002278).\n",
      "Validation loss decreased (0.002167 --> 0.002167).\n",
      "Validation loss decreased (0.002142 --> 0.002142).\n",
      "Epoch 100: Train Loss: 0.0030, Macro_F1: 0.9339, AUC_score: 0.9688\n",
      "Validation loss decreased (0.002009 --> 0.002009).\n",
      "Validation loss decreased (0.001868 --> 0.001868).\n",
      "Epoch 150: Train Loss: 0.0014, Macro_F1: 0.9302, AUC_score: 0.9680\n",
      "Validation loss decreased (0.001448 --> 0.001448).\n",
      "Validation loss decreased (0.001208 --> 0.001208).\n",
      "Epoch 200: Train Loss: 0.0027, Macro_F1: 0.9303, AUC_score: 0.9673\n",
      "Epoch 250: Train Loss: 0.0057, Macro_F1: 0.9269, AUC_score: 0.9679\n",
      "Epoch 00263: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0067, Macro_F1: 0.9267, AUC_score: 0.9674\n",
      "Epoch 350: Train Loss: 0.0026, Macro_F1: 0.9265, AUC_score: 0.9663\n",
      "Early stopping triggered\n",
      "acc save\n",
      "57.00000000000001% node features transform to 0: F1: 0.9229, AUC_score: 0.9671\n",
      "Epoch 0: Train Loss: 0.0034, Macro_F1: 0.8827, AUC_score: 0.9583\n",
      "Validation loss decreased (0.003435 --> 0.003435).\n",
      "Validation loss decreased (0.003072 --> 0.003072).\n",
      "Validation loss decreased (0.002408 --> 0.002408).\n",
      "Epoch 50: Train Loss: 0.0032, Macro_F1: 0.9302, AUC_score: 0.9672\n",
      "Validation loss decreased (0.002133 --> 0.002133).\n",
      "Validation loss decreased (0.001969 --> 0.001969).\n",
      "Epoch 100: Train Loss: 0.0022, Macro_F1: 0.9338, AUC_score: 0.9662\n",
      "Validation loss decreased (0.001788 --> 0.001788).\n",
      "Epoch 150: Train Loss: 0.0039, Macro_F1: 0.9263, AUC_score: 0.9657\n",
      "Epoch 200: Train Loss: 0.0081, Macro_F1: 0.9267, AUC_score: 0.9635\n",
      "Epoch 00237: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0043, Macro_F1: 0.9338, AUC_score: 0.9659\n",
      "Validation loss decreased (0.001711 --> 0.001711).\n",
      "Epoch 300: Train Loss: 0.0053, Macro_F1: 0.9302, AUC_score: 0.9668\n",
      "Epoch 350: Train Loss: 0.0066, Macro_F1: 0.9265, AUC_score: 0.9666\n",
      "Epoch 00383: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.001540 --> 0.001540).\n",
      "Epoch 400: Train Loss: 0.0021, Macro_F1: 0.9302, AUC_score: 0.9665\n",
      "Epoch 450: Train Loss: 0.0031, Macro_F1: 0.9229, AUC_score: 0.9673\n",
      "Epoch 00485: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 500: Train Loss: 0.0034, Macro_F1: 0.9302, AUC_score: 0.9661\n",
      "Validation loss decreased (0.001413 --> 0.001413).\n",
      "Epoch 550: Train Loss: 0.0033, Macro_F1: 0.9302, AUC_score: 0.9667\n",
      "Epoch 600: Train Loss: 0.0013, Macro_F1: 0.9302, AUC_score: 0.9665\n",
      "Validation loss decreased (0.001263 --> 0.001263).\n",
      "Epoch 650: Train Loss: 0.0034, Macro_F1: 0.9302, AUC_score: 0.9667\n",
      "Epoch 700: Train Loss: 0.0061, Macro_F1: 0.9302, AUC_score: 0.9668\n",
      "Epoch 00702: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 750: Train Loss: 0.0030, Macro_F1: 0.9302, AUC_score: 0.9668\n",
      "Epoch 800: Train Loss: 0.0038, Macro_F1: 0.9302, AUC_score: 0.9668\n",
      "Early stopping triggered\n",
      "acc save\n",
      "56.00000000000001% node features transform to 0: F1: 0.9302, AUC_score: 0.9668\n",
      "Epoch 0: Train Loss: 0.0106, Macro_F1: 0.9091, AUC_score: 0.9705\n",
      "Validation loss decreased (0.010623 --> 0.010623).\n",
      "Validation loss decreased (0.010087 --> 0.010087).\n",
      "Validation loss decreased (0.006779 --> 0.006779).\n",
      "Validation loss decreased (0.004867 --> 0.004867).\n",
      "Validation loss decreased (0.004256 --> 0.004256).\n",
      "Validation loss decreased (0.003451 --> 0.003451).\n",
      "Epoch 50: Train Loss: 0.0176, Macro_F1: 0.9233, AUC_score: 0.9666\n",
      "Validation loss decreased (0.002879 --> 0.002879).\n",
      "Validation loss decreased (0.002141 --> 0.002141).\n",
      "Validation loss decreased (0.002072 --> 0.002072).\n",
      "Validation loss decreased (0.001749 --> 0.001749).\n",
      "Epoch 100: Train Loss: 0.0072, Macro_F1: 0.9229, AUC_score: 0.9664\n",
      "Epoch 150: Train Loss: 0.0016, Macro_F1: 0.9229, AUC_score: 0.9667\n",
      "Validation loss decreased (0.001605 --> 0.001605).\n",
      "Epoch 200: Train Loss: 0.0064, Macro_F1: 0.9127, AUC_score: 0.9675\n",
      "Validation loss decreased (0.001540 --> 0.001540).\n",
      "Epoch 250: Train Loss: 0.0106, Macro_F1: 0.9302, AUC_score: 0.9657\n",
      "Validation loss decreased (0.001398 --> 0.001398).\n",
      "Epoch 300: Train Loss: 0.0023, Macro_F1: 0.9302, AUC_score: 0.9679\n",
      "Epoch 350: Train Loss: 0.0048, Macro_F1: 0.9195, AUC_score: 0.9662\n",
      "Epoch 00376: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 400: Train Loss: 0.0033, Macro_F1: 0.9265, AUC_score: 0.9655\n",
      "Epoch 450: Train Loss: 0.0025, Macro_F1: 0.9302, AUC_score: 0.9668\n",
      "Early stopping triggered\n",
      "acc save\n",
      "55.00000000000001% node features transform to 0: F1: 0.9229, AUC_score: 0.9668\n",
      "Epoch 0: Train Loss: 0.0099, Macro_F1: 0.8468, AUC_score: 0.9584\n",
      "Validation loss decreased (0.009894 --> 0.009894).\n",
      "Validation loss decreased (0.005818 --> 0.005818).\n",
      "Validation loss decreased (0.004313 --> 0.004313).\n",
      "Validation loss decreased (0.003048 --> 0.003048).\n",
      "Epoch 50: Train Loss: 0.0040, Macro_F1: 0.9341, AUC_score: 0.9704\n",
      "Validation loss decreased (0.002962 --> 0.002962).\n",
      "Validation loss decreased (0.002549 --> 0.002549).\n",
      "Validation loss decreased (0.002163 --> 0.002163).\n",
      "Validation loss decreased (0.001910 --> 0.001910).\n",
      "Epoch 100: Train Loss: 0.0043, Macro_F1: 0.9232, AUC_score: 0.9699\n",
      "Epoch 150: Train Loss: 0.0030, Macro_F1: 0.9234, AUC_score: 0.9665\n",
      "Validation loss decreased (0.001745 --> 0.001745).\n",
      "Validation loss decreased (0.001725 --> 0.001725).\n",
      "Epoch 200: Train Loss: 0.0047, Macro_F1: 0.9267, AUC_score: 0.9676\n",
      "Validation loss decreased (0.001682 --> 0.001682).\n",
      "Epoch 250: Train Loss: 0.0076, Macro_F1: 0.9375, AUC_score: 0.9682\n",
      "Validation loss decreased (0.001475 --> 0.001475).\n",
      "Epoch 300: Train Loss: 0.0044, Macro_F1: 0.9375, AUC_score: 0.9674\n",
      "Epoch 350: Train Loss: 0.0036, Macro_F1: 0.9231, AUC_score: 0.9677\n",
      "Epoch 00394: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 400: Train Loss: 0.0037, Macro_F1: 0.9305, AUC_score: 0.9690\n",
      "Epoch 450: Train Loss: 0.0037, Macro_F1: 0.9303, AUC_score: 0.9696\n",
      "Early stopping triggered\n",
      "acc save\n",
      "54.0% node features transform to 0: F1: 0.9268, AUC_score: 0.9685\n",
      "Epoch 0: Train Loss: 0.0035, Macro_F1: 0.8386, AUC_score: 0.9553\n",
      "Validation loss decreased (0.003494 --> 0.003494).\n",
      "Validation loss decreased (0.003188 --> 0.003188).\n",
      "Validation loss decreased (0.002259 --> 0.002259).\n",
      "Epoch 50: Train Loss: 0.0052, Macro_F1: 0.9306, AUC_score: 0.9725\n",
      "Epoch 100: Train Loss: 0.0030, Macro_F1: 0.9304, AUC_score: 0.9719\n",
      "Validation loss decreased (0.002185 --> 0.002185).\n",
      "Validation loss decreased (0.002086 --> 0.002086).\n",
      "Validation loss decreased (0.001443 --> 0.001443).\n",
      "Epoch 150: Train Loss: 0.0050, Macro_F1: 0.9304, AUC_score: 0.9711\n",
      "Epoch 200: Train Loss: 0.0041, Macro_F1: 0.9304, AUC_score: 0.9696\n",
      "Epoch 00246: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0031, Macro_F1: 0.9233, AUC_score: 0.9685\n",
      "Validation loss decreased (0.001303 --> 0.001303).\n",
      "Epoch 300: Train Loss: 0.0018, Macro_F1: 0.9340, AUC_score: 0.9682\n",
      "Epoch 350: Train Loss: 0.0045, Macro_F1: 0.9340, AUC_score: 0.9696\n",
      "Epoch 00396: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0012, Macro_F1: 0.9340, AUC_score: 0.9694\n",
      "Validation loss decreased (0.001235 --> 0.001235).\n",
      "Epoch 450: Train Loss: 0.0035, Macro_F1: 0.9304, AUC_score: 0.9697\n",
      "Epoch 500: Train Loss: 0.0051, Macro_F1: 0.9340, AUC_score: 0.9695\n",
      "Epoch 00502: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0017, Macro_F1: 0.9340, AUC_score: 0.9696\n",
      "Epoch 600: Train Loss: 0.0043, Macro_F1: 0.9340, AUC_score: 0.9697\n",
      "Early stopping triggered\n",
      "acc save\n",
      "53.0% node features transform to 0: F1: 0.9340, AUC_score: 0.9697\n",
      "Epoch 0: Train Loss: 0.0132, Macro_F1: 0.8468, AUC_score: 0.9572\n",
      "Validation loss decreased (0.013223 --> 0.013223).\n",
      "Validation loss decreased (0.003139 --> 0.003139).\n",
      "Validation loss decreased (0.002738 --> 0.002738).\n",
      "Epoch 50: Train Loss: 0.0166, Macro_F1: 0.9302, AUC_score: 0.9686\n",
      "Validation loss decreased (0.002532 --> 0.002532).\n",
      "Validation loss decreased (0.002191 --> 0.002191).\n",
      "Epoch 100: Train Loss: 0.0102, Macro_F1: 0.9412, AUC_score: 0.9683\n",
      "Validation loss decreased (0.002105 --> 0.002105).\n",
      "Validation loss decreased (0.001978 --> 0.001978).\n",
      "Validation loss decreased (0.001960 --> 0.001960).\n",
      "Epoch 150: Train Loss: 0.0029, Macro_F1: 0.9340, AUC_score: 0.9663\n",
      "Validation loss decreased (0.001475 --> 0.001475).\n",
      "Epoch 200: Train Loss: 0.0035, Macro_F1: 0.9125, AUC_score: 0.9715\n",
      "Validation loss decreased (0.001372 --> 0.001372).\n",
      "Epoch 250: Train Loss: 0.0073, Macro_F1: 0.9304, AUC_score: 0.9683\n",
      "Epoch 300: Train Loss: 0.0030, Macro_F1: 0.9307, AUC_score: 0.9727\n",
      "Epoch 00346: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 350: Train Loss: 0.0029, Macro_F1: 0.9268, AUC_score: 0.9684\n",
      "Epoch 400: Train Loss: 0.0091, Macro_F1: 0.9268, AUC_score: 0.9697\n",
      "Validation loss decreased (0.001349 --> 0.001349).\n",
      "Epoch 450: Train Loss: 0.0061, Macro_F1: 0.9304, AUC_score: 0.9690\n",
      "Epoch 500: Train Loss: 0.0016, Macro_F1: 0.9304, AUC_score: 0.9701\n",
      "Epoch 00541: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 550: Train Loss: 0.0026, Macro_F1: 0.9340, AUC_score: 0.9696\n",
      "Validation loss decreased (0.001281 --> 0.001281).\n",
      "Validation loss decreased (0.001226 --> 0.001226).\n",
      "Epoch 600: Train Loss: 0.0040, Macro_F1: 0.9340, AUC_score: 0.9694\n",
      "Epoch 650: Train Loss: 0.0053, Macro_F1: 0.9340, AUC_score: 0.9696\n",
      "Epoch 00701: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 700: Train Loss: 0.0020, Macro_F1: 0.9304, AUC_score: 0.9700\n",
      "Epoch 750: Train Loss: 0.0018, Macro_F1: 0.9304, AUC_score: 0.9698\n",
      "Early stopping triggered\n",
      "acc save\n",
      "52.0% node features transform to 0: F1: 0.9304, AUC_score: 0.9697\n",
      "Epoch 0: Train Loss: 0.0025, Macro_F1: 0.8219, AUC_score: 0.9652\n",
      "Validation loss decreased (0.002463 --> 0.002463).\n",
      "Epoch 50: Train Loss: 0.0087, Macro_F1: 0.9268, AUC_score: 0.9714\n",
      "Validation loss decreased (0.002430 --> 0.002430).\n",
      "Epoch 100: Train Loss: 0.0038, Macro_F1: 0.9305, AUC_score: 0.9696\n",
      "Validation loss decreased (0.002193 --> 0.002193).\n",
      "Validation loss decreased (0.002105 --> 0.002105).\n",
      "Validation loss decreased (0.001748 --> 0.001748).\n",
      "Validation loss decreased (0.001737 --> 0.001737).\n",
      "Epoch 150: Train Loss: 0.0024, Macro_F1: 0.9305, AUC_score: 0.9706\n",
      "Validation loss decreased (0.001531 --> 0.001531).\n",
      "Epoch 200: Train Loss: 0.0018, Macro_F1: 0.9304, AUC_score: 0.9701\n",
      "Validation loss decreased (0.001402 --> 0.001402).\n",
      "Epoch 250: Train Loss: 0.0020, Macro_F1: 0.9305, AUC_score: 0.9685\n",
      "Epoch 300: Train Loss: 0.0029, Macro_F1: 0.9374, AUC_score: 0.9693\n",
      "Epoch 00314: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 350: Train Loss: 0.0023, Macro_F1: 0.9268, AUC_score: 0.9714\n",
      "Epoch 400: Train Loss: 0.0036, Macro_F1: 0.9268, AUC_score: 0.9695\n",
      "Early stopping triggered\n",
      "acc save\n",
      "51.0% node features transform to 0: F1: 0.9268, AUC_score: 0.9702\n",
      "Epoch 0: Train Loss: 0.0031, Macro_F1: 0.9161, AUC_score: 0.9661\n",
      "Validation loss decreased (0.003133 --> 0.003133).\n",
      "Validation loss decreased (0.002656 --> 0.002656).\n",
      "Epoch 50: Train Loss: 0.0026, Macro_F1: 0.9126, AUC_score: 0.9702\n",
      "Validation loss decreased (0.002644 --> 0.002644).\n",
      "Validation loss decreased (0.002245 --> 0.002245).\n",
      "Validation loss decreased (0.002141 --> 0.002141).\n",
      "Validation loss decreased (0.002112 --> 0.002112).\n",
      "Validation loss decreased (0.001645 --> 0.001645).\n",
      "Validation loss decreased (0.001532 --> 0.001532).\n",
      "Epoch 100: Train Loss: 0.0044, Macro_F1: 0.9340, AUC_score: 0.9699\n",
      "Epoch 150: Train Loss: 0.0045, Macro_F1: 0.9227, AUC_score: 0.9708\n",
      "Epoch 00199: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0054, Macro_F1: 0.9160, AUC_score: 0.9666\n",
      "Epoch 250: Train Loss: 0.0052, Macro_F1: 0.9232, AUC_score: 0.9707\n",
      "Validation loss decreased (0.001281 --> 0.001281).\n",
      "Epoch 300: Train Loss: 0.0027, Macro_F1: 0.9304, AUC_score: 0.9692\n",
      "Epoch 350: Train Loss: 0.0033, Macro_F1: 0.9268, AUC_score: 0.9699\n",
      "Epoch 00356: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0032, Macro_F1: 0.9340, AUC_score: 0.9701\n",
      "Epoch 450: Train Loss: 0.0021, Macro_F1: 0.9268, AUC_score: 0.9701\n",
      "Early stopping triggered\n",
      "acc save\n",
      "50.0% node features transform to 0: F1: 0.9268, AUC_score: 0.9701\n",
      "Epoch 0: Train Loss: 0.0028, Macro_F1: 0.8551, AUC_score: 0.9649\n",
      "Validation loss decreased (0.002774 --> 0.002774).\n",
      "Epoch 50: Train Loss: 0.0034, Macro_F1: 0.9305, AUC_score: 0.9745\n",
      "Validation loss decreased (0.002447 --> 0.002447).\n",
      "Validation loss decreased (0.001759 --> 0.001759).\n",
      "Epoch 100: Train Loss: 0.0037, Macro_F1: 0.9305, AUC_score: 0.9717\n",
      "Epoch 150: Train Loss: 0.0110, Macro_F1: 0.9233, AUC_score: 0.9714\n",
      "Epoch 00182: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.001678 --> 0.001678).\n",
      "Validation loss decreased (0.001395 --> 0.001395).\n",
      "Epoch 200: Train Loss: 0.0017, Macro_F1: 0.9268, AUC_score: 0.9723\n",
      "Epoch 250: Train Loss: 0.0097, Macro_F1: 0.9193, AUC_score: 0.9719\n",
      "Epoch 00290: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 300: Train Loss: 0.0085, Macro_F1: 0.9193, AUC_score: 0.9715\n",
      "Epoch 350: Train Loss: 0.0025, Macro_F1: 0.9268, AUC_score: 0.9719\n",
      "Validation loss decreased (0.001355 --> 0.001355).\n",
      "Epoch 400: Train Loss: 0.0056, Macro_F1: 0.9231, AUC_score: 0.9719\n",
      "Epoch 450: Train Loss: 0.0071, Macro_F1: 0.9231, AUC_score: 0.9716\n",
      "Epoch 00477: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 500: Train Loss: 0.0024, Macro_F1: 0.9268, AUC_score: 0.9721\n",
      "Epoch 550: Train Loss: 0.0029, Macro_F1: 0.9231, AUC_score: 0.9720\n",
      "Early stopping triggered\n",
      "acc save\n",
      "49.0% node features transform to 0: F1: 0.9231, AUC_score: 0.9720\n",
      "Epoch 0: Train Loss: 0.0041, Macro_F1: 0.9302, AUC_score: 0.9720\n",
      "Validation loss decreased (0.004087 --> 0.004087).\n",
      "Validation loss decreased (0.004004 --> 0.004004).\n",
      "Validation loss decreased (0.003581 --> 0.003581).\n",
      "Validation loss decreased (0.002349 --> 0.002349).\n",
      "Epoch 50: Train Loss: 0.0064, Macro_F1: 0.9341, AUC_score: 0.9709\n",
      "Validation loss decreased (0.002183 --> 0.002183).\n",
      "Validation loss decreased (0.002101 --> 0.002101).\n",
      "Epoch 100: Train Loss: 0.0058, Macro_F1: 0.9198, AUC_score: 0.9713\n",
      "Epoch 150: Train Loss: 0.0022, Macro_F1: 0.9265, AUC_score: 0.9693\n",
      "Validation loss decreased (0.001362 --> 0.001362).\n",
      "Epoch 200: Train Loss: 0.0023, Macro_F1: 0.9234, AUC_score: 0.9708\n",
      "Epoch 250: Train Loss: 0.0038, Macro_F1: 0.9193, AUC_score: 0.9700\n",
      "Epoch 00253: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.001188 --> 0.001188).\n",
      "Epoch 300: Train Loss: 0.0026, Macro_F1: 0.9231, AUC_score: 0.9709\n",
      "Epoch 350: Train Loss: 0.0033, Macro_F1: 0.9268, AUC_score: 0.9708\n",
      "Epoch 00382: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0025, Macro_F1: 0.9231, AUC_score: 0.9698\n",
      "Epoch 450: Train Loss: 0.0018, Macro_F1: 0.9231, AUC_score: 0.9693\n",
      "Early stopping triggered\n",
      "acc save\n",
      "48.0% node features transform to 0: F1: 0.9231, AUC_score: 0.9693\n",
      "Epoch 0: Train Loss: 0.0047, Macro_F1: 0.8386, AUC_score: 0.9597\n",
      "Validation loss decreased (0.004710 --> 0.004710).\n",
      "Validation loss decreased (0.004620 --> 0.004620).\n",
      "Validation loss decreased (0.003400 --> 0.003400).\n",
      "Epoch 50: Train Loss: 0.0024, Macro_F1: 0.9269, AUC_score: 0.9683\n",
      "Validation loss decreased (0.002401 --> 0.002401).\n",
      "Validation loss decreased (0.002306 --> 0.002306).\n",
      "Validation loss decreased (0.002062 --> 0.002062).\n",
      "Validation loss decreased (0.001975 --> 0.001975).\n",
      "Validation loss decreased (0.001873 --> 0.001873).\n",
      "Epoch 100: Train Loss: 0.0113, Macro_F1: 0.9196, AUC_score: 0.9666\n",
      "Validation loss decreased (0.001850 --> 0.001850).\n",
      "Validation loss decreased (0.001711 --> 0.001711).\n",
      "Epoch 150: Train Loss: 0.0034, Macro_F1: 0.9232, AUC_score: 0.9670\n",
      "Validation loss decreased (0.001664 --> 0.001664).\n",
      "Validation loss decreased (0.001468 --> 0.001468).\n",
      "Validation loss decreased (0.001450 --> 0.001450).\n",
      "Epoch 200: Train Loss: 0.0031, Macro_F1: 0.9231, AUC_score: 0.9640\n",
      "Validation loss decreased (0.001428 --> 0.001428).\n",
      "Validation loss decreased (0.001325 --> 0.001325).\n",
      "Epoch 250: Train Loss: 0.0023, Macro_F1: 0.9268, AUC_score: 0.9650\n",
      "Epoch 300: Train Loss: 0.0048, Macro_F1: 0.9268, AUC_score: 0.9674\n",
      "Epoch 00342: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 350: Train Loss: 0.0022, Macro_F1: 0.9267, AUC_score: 0.9662\n",
      "Epoch 400: Train Loss: 0.0015, Macro_F1: 0.9268, AUC_score: 0.9670\n",
      "Early stopping triggered\n",
      "acc save\n",
      "47.0% node features transform to 0: F1: 0.9268, AUC_score: 0.9676\n",
      "Epoch 0: Train Loss: 0.0077, Macro_F1: 0.8303, AUC_score: 0.9549\n",
      "Validation loss decreased (0.007663 --> 0.007663).\n",
      "Validation loss decreased (0.005008 --> 0.005008).\n",
      "Validation loss decreased (0.004874 --> 0.004874).\n",
      "Validation loss decreased (0.004231 --> 0.004231).\n",
      "Validation loss decreased (0.003783 --> 0.003783).\n",
      "Validation loss decreased (0.003088 --> 0.003088).\n",
      "Validation loss decreased (0.002437 --> 0.002437).\n",
      "Validation loss decreased (0.002256 --> 0.002256).\n",
      "Epoch 50: Train Loss: 0.0023, Macro_F1: 0.9159, AUC_score: 0.9679\n",
      "Validation loss decreased (0.002143 --> 0.002143).\n",
      "Validation loss decreased (0.002026 --> 0.002026).\n",
      "Epoch 100: Train Loss: 0.0021, Macro_F1: 0.9232, AUC_score: 0.9675\n",
      "Validation loss decreased (0.001395 --> 0.001395).\n",
      "Epoch 150: Train Loss: 0.0023, Macro_F1: 0.9267, AUC_score: 0.9665\n",
      "Epoch 200: Train Loss: 0.0019, Macro_F1: 0.9229, AUC_score: 0.9676\n",
      "Epoch 00214: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0019, Macro_F1: 0.9232, AUC_score: 0.9676\n",
      "Validation loss decreased (0.001341 --> 0.001341).\n",
      "Epoch 300: Train Loss: 0.0023, Macro_F1: 0.9267, AUC_score: 0.9675\n",
      "Validation loss decreased (0.001291 --> 0.001291).\n",
      "Validation loss decreased (0.001266 --> 0.001266).\n",
      "Epoch 350: Train Loss: 0.0055, Macro_F1: 0.9232, AUC_score: 0.9675\n",
      "Epoch 400: Train Loss: 0.0018, Macro_F1: 0.9231, AUC_score: 0.9679\n",
      "Epoch 00414: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.001226 --> 0.001226).\n",
      "Epoch 450: Train Loss: 0.0017, Macro_F1: 0.9268, AUC_score: 0.9676\n",
      "Validation loss decreased (0.001122 --> 0.001122).\n",
      "Epoch 500: Train Loss: 0.0083, Macro_F1: 0.9268, AUC_score: 0.9677\n",
      "Epoch 550: Train Loss: 0.0040, Macro_F1: 0.9268, AUC_score: 0.9680\n",
      "Epoch 00555: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 600: Train Loss: 0.0058, Macro_F1: 0.9232, AUC_score: 0.9680\n",
      "Epoch 650: Train Loss: 0.0066, Macro_F1: 0.9232, AUC_score: 0.9679\n",
      "Early stopping triggered\n",
      "acc save\n",
      "46.0% node features transform to 0: F1: 0.9232, AUC_score: 0.9679\n",
      "Epoch 0: Train Loss: 0.0037, Macro_F1: 0.8587, AUC_score: 0.9662\n",
      "Validation loss decreased (0.003673 --> 0.003673).\n",
      "Validation loss decreased (0.003345 --> 0.003345).\n",
      "Validation loss decreased (0.003114 --> 0.003114).\n",
      "Validation loss decreased (0.003003 --> 0.003003).\n",
      "Validation loss decreased (0.002767 --> 0.002767).\n",
      "Epoch 50: Train Loss: 0.0035, Macro_F1: 0.9304, AUC_score: 0.9708\n",
      "Validation loss decreased (0.002711 --> 0.002711).\n",
      "Validation loss decreased (0.001944 --> 0.001944).\n",
      "Epoch 100: Train Loss: 0.0028, Macro_F1: 0.9232, AUC_score: 0.9684\n",
      "Validation loss decreased (0.001510 --> 0.001510).\n",
      "Epoch 150: Train Loss: 0.0028, Macro_F1: 0.9270, AUC_score: 0.9681\n",
      "Epoch 200: Train Loss: 0.0037, Macro_F1: 0.9267, AUC_score: 0.9682\n",
      "Epoch 00229: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.001365 --> 0.001365).\n",
      "Epoch 250: Train Loss: 0.0042, Macro_F1: 0.9232, AUC_score: 0.9677\n",
      "Epoch 300: Train Loss: 0.0078, Macro_F1: 0.9268, AUC_score: 0.9677\n",
      "Validation loss decreased (0.001291 --> 0.001291).\n",
      "Epoch 350: Train Loss: 0.0025, Macro_F1: 0.9232, AUC_score: 0.9689\n",
      "Epoch 400: Train Loss: 0.0041, Macro_F1: 0.9232, AUC_score: 0.9685\n",
      "Validation loss decreased (0.000872 --> 0.000872).\n",
      "Epoch 450: Train Loss: 0.0021, Macro_F1: 0.9268, AUC_score: 0.9685\n",
      "Epoch 500: Train Loss: 0.0024, Macro_F1: 0.9268, AUC_score: 0.9685\n",
      "Epoch 00519: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 550: Train Loss: 0.0028, Macro_F1: 0.9268, AUC_score: 0.9680\n",
      "Epoch 600: Train Loss: 0.0055, Macro_F1: 0.9232, AUC_score: 0.9686\n",
      "Early stopping triggered\n",
      "acc save\n",
      "44.99999999999999% node features transform to 0: F1: 0.9268, AUC_score: 0.9682\n",
      "Epoch 0: Train Loss: 0.0036, Macro_F1: 0.8254, AUC_score: 0.9593\n",
      "Validation loss decreased (0.003612 --> 0.003612).\n",
      "Validation loss decreased (0.002922 --> 0.002922).\n",
      "Validation loss decreased (0.002422 --> 0.002422).\n",
      "Epoch 50: Train Loss: 0.0054, Macro_F1: 0.9341, AUC_score: 0.9686\n",
      "Validation loss decreased (0.002293 --> 0.002293).\n",
      "Validation loss decreased (0.002018 --> 0.002018).\n",
      "Validation loss decreased (0.001890 --> 0.001890).\n",
      "Epoch 100: Train Loss: 0.0036, Macro_F1: 0.9300, AUC_score: 0.9683\n",
      "Validation loss decreased (0.001619 --> 0.001619).\n",
      "Validation loss decreased (0.001446 --> 0.001446).\n",
      "Epoch 150: Train Loss: 0.0087, Macro_F1: 0.9191, AUC_score: 0.9671\n",
      "Epoch 200: Train Loss: 0.0035, Macro_F1: 0.9305, AUC_score: 0.9689\n",
      "Epoch 00240: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0055, Macro_F1: 0.9233, AUC_score: 0.9674\n",
      "Epoch 300: Train Loss: 0.0041, Macro_F1: 0.9233, AUC_score: 0.9694\n",
      "Early stopping triggered\n",
      "acc save\n",
      "43.99999999999999% node features transform to 0: F1: 0.9232, AUC_score: 0.9685\n",
      "Epoch 0: Train Loss: 0.0031, Macro_F1: 0.9260, AUC_score: 0.9693\n",
      "Validation loss decreased (0.003150 --> 0.003150).\n",
      "Validation loss decreased (0.002509 --> 0.002509).\n",
      "Validation loss decreased (0.001562 --> 0.001562).\n",
      "Epoch 50: Train Loss: 0.0072, Macro_F1: 0.9267, AUC_score: 0.9711\n",
      "Epoch 100: Train Loss: 0.0063, Macro_F1: 0.9267, AUC_score: 0.9710\n",
      "Epoch 00143: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.0028, Macro_F1: 0.9340, AUC_score: 0.9679\n",
      "Epoch 200: Train Loss: 0.0021, Macro_F1: 0.9269, AUC_score: 0.9690\n",
      "Validation loss decreased (0.001487 --> 0.001487).\n",
      "Epoch 250: Train Loss: 0.0020, Macro_F1: 0.9231, AUC_score: 0.9679\n",
      "Validation loss decreased (0.001383 --> 0.001383).\n",
      "Validation loss decreased (0.001194 --> 0.001194).\n",
      "Epoch 300: Train Loss: 0.0028, Macro_F1: 0.9269, AUC_score: 0.9691\n",
      "Epoch 350: Train Loss: 0.0018, Macro_F1: 0.9268, AUC_score: 0.9691\n",
      "Epoch 00374: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.001060 --> 0.001060).\n",
      "Epoch 400: Train Loss: 0.0030, Macro_F1: 0.9268, AUC_score: 0.9689\n",
      "Epoch 450: Train Loss: 0.0024, Macro_F1: 0.9231, AUC_score: 0.9688\n",
      "Epoch 00479: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 500: Train Loss: 0.0094, Macro_F1: 0.9268, AUC_score: 0.9697\n",
      "Epoch 550: Train Loss: 0.0027, Macro_F1: 0.9268, AUC_score: 0.9692\n",
      "Early stopping triggered\n",
      "acc save\n",
      "42.99999999999999% node features transform to 0: F1: 0.9268, AUC_score: 0.9690\n",
      "Epoch 0: Train Loss: 0.0020, Macro_F1: 0.8441, AUC_score: 0.9704\n",
      "Validation loss decreased (0.002048 --> 0.002048).\n",
      "Epoch 50: Train Loss: 0.0034, Macro_F1: 0.9268, AUC_score: 0.9694\n",
      "Validation loss decreased (0.001871 --> 0.001871).\n",
      "Epoch 100: Train Loss: 0.0040, Macro_F1: 0.9304, AUC_score: 0.9705\n",
      "Validation loss decreased (0.001721 --> 0.001721).\n",
      "Epoch 150: Train Loss: 0.0024, Macro_F1: 0.9231, AUC_score: 0.9693\n",
      "Validation loss decreased (0.001582 --> 0.001582).\n",
      "Validation loss decreased (0.001247 --> 0.001247).\n",
      "Epoch 200: Train Loss: 0.0021, Macro_F1: 0.9233, AUC_score: 0.9715\n",
      "Validation loss decreased (0.001188 --> 0.001188).\n",
      "Validation loss decreased (0.001176 --> 0.001176).\n",
      "Epoch 250: Train Loss: 0.0012, Macro_F1: 0.9195, AUC_score: 0.9703\n",
      "Epoch 300: Train Loss: 0.0102, Macro_F1: 0.9268, AUC_score: 0.9715\n",
      "Epoch 350: Train Loss: 0.0066, Macro_F1: 0.9305, AUC_score: 0.9711\n",
      "Epoch 00352: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 400: Train Loss: 0.0029, Macro_F1: 0.9268, AUC_score: 0.9714\n",
      "Validation loss decreased (0.001125 --> 0.001125).\n",
      "Epoch 450: Train Loss: 0.0055, Macro_F1: 0.9267, AUC_score: 0.9705\n",
      "Epoch 500: Train Loss: 0.0108, Macro_F1: 0.9267, AUC_score: 0.9716\n",
      "Epoch 00525: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 550: Train Loss: 0.0018, Macro_F1: 0.9304, AUC_score: 0.9714\n",
      "Epoch 600: Train Loss: 0.0019, Macro_F1: 0.9267, AUC_score: 0.9706\n",
      "Early stopping triggered\n",
      "acc save\n",
      "42.00000000000001% node features transform to 0: F1: 0.9196, AUC_score: 0.9714\n",
      "Epoch 0: Train Loss: 0.0072, Macro_F1: 0.8380, AUC_score: 0.9588\n",
      "Validation loss decreased (0.007192 --> 0.007192).\n",
      "Validation loss decreased (0.003451 --> 0.003451).\n",
      "Validation loss decreased (0.002665 --> 0.002665).\n",
      "Epoch 50: Train Loss: 0.0045, Macro_F1: 0.9232, AUC_score: 0.9720\n",
      "Validation loss decreased (0.002343 --> 0.002343).\n",
      "Validation loss decreased (0.002260 --> 0.002260).\n",
      "Epoch 100: Train Loss: 0.0060, Macro_F1: 0.9267, AUC_score: 0.9710\n",
      "Validation loss decreased (0.002047 --> 0.002047).\n",
      "Validation loss decreased (0.002000 --> 0.002000).\n",
      "Epoch 150: Train Loss: 0.0026, Macro_F1: 0.9232, AUC_score: 0.9709\n",
      "Validation loss decreased (0.001789 --> 0.001789).\n",
      "Validation loss decreased (0.001448 --> 0.001448).\n",
      "Epoch 200: Train Loss: 0.0040, Macro_F1: 0.9268, AUC_score: 0.9725\n",
      "Validation loss decreased (0.001177 --> 0.001177).\n",
      "Epoch 250: Train Loss: 0.0023, Macro_F1: 0.9233, AUC_score: 0.9696\n",
      "Epoch 300: Train Loss: 0.0045, Macro_F1: 0.9265, AUC_score: 0.9716\n",
      "Epoch 00314: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 350: Train Loss: 0.0017, Macro_F1: 0.9305, AUC_score: 0.9706\n",
      "Validation loss decreased (0.001154 --> 0.001154).\n",
      "Epoch 400: Train Loss: 0.0025, Macro_F1: 0.9267, AUC_score: 0.9712\n",
      "Epoch 450: Train Loss: 0.0023, Macro_F1: 0.9268, AUC_score: 0.9710\n",
      "Epoch 00495: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.0023, Macro_F1: 0.9264, AUC_score: 0.9706\n",
      "Epoch 550: Train Loss: 0.0034, Macro_F1: 0.9268, AUC_score: 0.9709\n",
      "Early stopping triggered\n",
      "acc save\n",
      "41.0% node features transform to 0: F1: 0.9303, AUC_score: 0.9707\n",
      "Epoch 0: Train Loss: 0.0020, Macro_F1: 0.9224, AUC_score: 0.9724\n",
      "Validation loss decreased (0.002019 --> 0.002019).\n",
      "Epoch 50: Train Loss: 0.0084, Macro_F1: 0.9303, AUC_score: 0.9712\n",
      "Validation loss decreased (0.002007 --> 0.002007).\n",
      "Validation loss decreased (0.001512 --> 0.001512).\n",
      "Epoch 100: Train Loss: 0.0036, Macro_F1: 0.9227, AUC_score: 0.9665\n",
      "Epoch 150: Train Loss: 0.0136, Macro_F1: 0.9233, AUC_score: 0.9668\n",
      "Epoch 00191: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0065, Macro_F1: 0.9265, AUC_score: 0.9697\n",
      "Epoch 250: Train Loss: 0.0025, Macro_F1: 0.9267, AUC_score: 0.9716\n",
      "Early stopping triggered\n",
      "acc save\n",
      "40.0% node features transform to 0: F1: 0.9304, AUC_score: 0.9733\n",
      "Epoch 0: Train Loss: 0.0055, Macro_F1: 0.8344, AUC_score: 0.9656\n",
      "Validation loss decreased (0.005460 --> 0.005460).\n",
      "Validation loss decreased (0.005014 --> 0.005014).\n",
      "Validation loss decreased (0.003936 --> 0.003936).\n",
      "Validation loss decreased (0.003413 --> 0.003413).\n",
      "Validation loss decreased (0.002900 --> 0.002900).\n",
      "Validation loss decreased (0.002525 --> 0.002525).\n",
      "Epoch 50: Train Loss: 0.0024, Macro_F1: 0.9269, AUC_score: 0.9709\n",
      "Validation loss decreased (0.002441 --> 0.002441).\n",
      "Validation loss decreased (0.002118 --> 0.002118).\n",
      "Validation loss decreased (0.001649 --> 0.001649).\n",
      "Epoch 100: Train Loss: 0.0036, Macro_F1: 0.9305, AUC_score: 0.9721\n",
      "Validation loss decreased (0.001575 --> 0.001575).\n",
      "Epoch 150: Train Loss: 0.0035, Macro_F1: 0.9376, AUC_score: 0.9708\n",
      "Validation loss decreased (0.001358 --> 0.001358).\n",
      "Epoch 200: Train Loss: 0.0023, Macro_F1: 0.9377, AUC_score: 0.9702\n",
      "Validation loss decreased (0.001262 --> 0.001262).\n",
      "Epoch 250: Train Loss: 0.0063, Macro_F1: 0.9270, AUC_score: 0.9725\n",
      "Validation loss decreased (0.001245 --> 0.001245).\n",
      "Validation loss decreased (0.001149 --> 0.001149).\n",
      "Epoch 300: Train Loss: 0.0051, Macro_F1: 0.9265, AUC_score: 0.9710\n",
      "Epoch 350: Train Loss: 0.0030, Macro_F1: 0.9264, AUC_score: 0.9701\n",
      "Epoch 00385: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 400: Train Loss: 0.0030, Macro_F1: 0.9268, AUC_score: 0.9720\n",
      "Epoch 450: Train Loss: 0.0061, Macro_F1: 0.9232, AUC_score: 0.9709\n",
      "Early stopping triggered\n",
      "acc save\n",
      "39.0% node features transform to 0: F1: 0.9232, AUC_score: 0.9711\n",
      "Epoch 0: Train Loss: 0.0065, Macro_F1: 0.8747, AUC_score: 0.9626\n",
      "Validation loss decreased (0.006493 --> 0.006493).\n",
      "Validation loss decreased (0.005504 --> 0.005504).\n",
      "Validation loss decreased (0.004163 --> 0.004163).\n",
      "Validation loss decreased (0.003942 --> 0.003942).\n",
      "Validation loss decreased (0.003566 --> 0.003566).\n",
      "Epoch 50: Train Loss: 0.0031, Macro_F1: 0.9269, AUC_score: 0.9716\n",
      "Validation loss decreased (0.003069 --> 0.003069).\n",
      "Validation loss decreased (0.002279 --> 0.002279).\n",
      "Validation loss decreased (0.002226 --> 0.002226).\n",
      "Epoch 100: Train Loss: 0.0040, Macro_F1: 0.9270, AUC_score: 0.9707\n",
      "Validation loss decreased (0.002056 --> 0.002056).\n",
      "Validation loss decreased (0.001755 --> 0.001755).\n",
      "Validation loss decreased (0.001356 --> 0.001356).\n",
      "Epoch 150: Train Loss: 0.0049, Macro_F1: 0.9269, AUC_score: 0.9704\n",
      "Validation loss decreased (0.001206 --> 0.001206).\n",
      "Epoch 200: Train Loss: 0.0022, Macro_F1: 0.9338, AUC_score: 0.9700\n",
      "Epoch 250: Train Loss: 0.0035, Macro_F1: 0.9303, AUC_score: 0.9692\n",
      "Epoch 00281: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0026, Macro_F1: 0.9269, AUC_score: 0.9713\n",
      "Validation loss decreased (0.001158 --> 0.001158).\n",
      "Epoch 350: Train Loss: 0.0019, Macro_F1: 0.9269, AUC_score: 0.9716\n",
      "Validation loss decreased (0.001072 --> 0.001072).\n",
      "Epoch 400: Train Loss: 0.0022, Macro_F1: 0.9231, AUC_score: 0.9703\n",
      "Validation loss decreased (0.001018 --> 0.001018).\n",
      "Epoch 450: Train Loss: 0.0022, Macro_F1: 0.9269, AUC_score: 0.9721\n",
      "Validation loss decreased (0.000902 --> 0.000902).\n",
      "Epoch 500: Train Loss: 0.0035, Macro_F1: 0.9339, AUC_score: 0.9711\n",
      "Epoch 550: Train Loss: 0.0021, Macro_F1: 0.9193, AUC_score: 0.9707\n",
      "Epoch 00597: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 600: Train Loss: 0.0019, Macro_F1: 0.9231, AUC_score: 0.9707\n",
      "Epoch 650: Train Loss: 0.0016, Macro_F1: 0.9268, AUC_score: 0.9708\n",
      "Early stopping triggered\n",
      "acc save\n",
      "38.0% node features transform to 0: F1: 0.9269, AUC_score: 0.9710\n",
      "Epoch 0: Train Loss: 0.0040, Macro_F1: 0.8176, AUC_score: 0.9615\n",
      "Validation loss decreased (0.004000 --> 0.004000).\n",
      "Validation loss decreased (0.002140 --> 0.002140).\n",
      "Epoch 50: Train Loss: 0.0041, Macro_F1: 0.9268, AUC_score: 0.9687\n",
      "Validation loss decreased (0.002097 --> 0.002097).\n",
      "Validation loss decreased (0.001446 --> 0.001446).\n",
      "Epoch 100: Train Loss: 0.0021, Macro_F1: 0.9305, AUC_score: 0.9729\n",
      "Epoch 150: Train Loss: 0.0028, Macro_F1: 0.9269, AUC_score: 0.9718\n",
      "Epoch 00182: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0025, Macro_F1: 0.9269, AUC_score: 0.9735\n",
      "Validation loss decreased (0.001314 --> 0.001314).\n",
      "Epoch 250: Train Loss: 0.0020, Macro_F1: 0.9269, AUC_score: 0.9722\n",
      "Validation loss decreased (0.001257 --> 0.001257).\n",
      "Epoch 300: Train Loss: 0.0040, Macro_F1: 0.9195, AUC_score: 0.9718\n",
      "Epoch 350: Train Loss: 0.0014, Macro_F1: 0.9339, AUC_score: 0.9716\n",
      "Validation loss decreased (0.001149 --> 0.001149).\n",
      "Epoch 400: Train Loss: 0.0024, Macro_F1: 0.9269, AUC_score: 0.9716\n",
      "Epoch 450: Train Loss: 0.0016, Macro_F1: 0.9195, AUC_score: 0.9712\n",
      "Epoch 00466: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.0016, Macro_F1: 0.9269, AUC_score: 0.9711\n",
      "Epoch 550: Train Loss: 0.0014, Macro_F1: 0.9269, AUC_score: 0.9710\n",
      "Early stopping triggered\n",
      "acc save\n",
      "37.0% node features transform to 0: F1: 0.9269, AUC_score: 0.9710\n",
      "Epoch 0: Train Loss: 0.0017, Macro_F1: 0.9092, AUC_score: 0.9753\n",
      "Validation loss decreased (0.001660 --> 0.001660).\n",
      "Epoch 50: Train Loss: 0.0028, Macro_F1: 0.9376, AUC_score: 0.9709\n",
      "Epoch 100: Train Loss: 0.0060, Macro_F1: 0.9269, AUC_score: 0.9754\n",
      "Epoch 00102: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.001553 --> 0.001553).\n",
      "Epoch 150: Train Loss: 0.0026, Macro_F1: 0.9412, AUC_score: 0.9736\n",
      "Validation loss decreased (0.001473 --> 0.001473).\n",
      "Epoch 200: Train Loss: 0.0019, Macro_F1: 0.9232, AUC_score: 0.9741\n",
      "Epoch 250: Train Loss: 0.0025, Macro_F1: 0.9376, AUC_score: 0.9738\n",
      "Validation loss decreased (0.001364 --> 0.001364).\n",
      "Validation loss decreased (0.001330 --> 0.001330).\n",
      "Epoch 300: Train Loss: 0.0018, Macro_F1: 0.9268, AUC_score: 0.9737\n",
      "Validation loss decreased (0.001023 --> 0.001023).\n",
      "Validation loss decreased (0.001001 --> 0.001001).\n",
      "Epoch 350: Train Loss: 0.0018, Macro_F1: 0.9304, AUC_score: 0.9744\n",
      "Epoch 400: Train Loss: 0.0036, Macro_F1: 0.9306, AUC_score: 0.9733\n",
      "Epoch 00442: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.0035, Macro_F1: 0.9269, AUC_score: 0.9726\n",
      "Epoch 500: Train Loss: 0.0042, Macro_F1: 0.9269, AUC_score: 0.9725\n",
      "Validation loss decreased (0.000863 --> 0.000863).\n",
      "Epoch 550: Train Loss: 0.0025, Macro_F1: 0.9304, AUC_score: 0.9725\n",
      "Epoch 600: Train Loss: 0.0025, Macro_F1: 0.9376, AUC_score: 0.9724\n",
      "Epoch 00639: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 650: Train Loss: 0.0032, Macro_F1: 0.9269, AUC_score: 0.9733\n",
      "Epoch 700: Train Loss: 0.0018, Macro_F1: 0.9305, AUC_score: 0.9731\n",
      "Early stopping triggered\n",
      "acc save\n",
      "36.0% node features transform to 0: F1: 0.9268, AUC_score: 0.9731\n",
      "Epoch 0: Train Loss: 0.0011, Macro_F1: 0.9108, AUC_score: 0.9640\n",
      "Validation loss decreased (0.001130 --> 0.001130).\n",
      "Epoch 50: Train Loss: 0.0060, Macro_F1: 0.9376, AUC_score: 0.9722\n",
      "Epoch 100: Train Loss: 0.0043, Macro_F1: 0.9340, AUC_score: 0.9707\n",
      "Epoch 00102: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.0018, Macro_F1: 0.9375, AUC_score: 0.9722\n",
      "Validation loss decreased (0.001020 --> 0.001020).\n",
      "Epoch 200: Train Loss: 0.0015, Macro_F1: 0.9340, AUC_score: 0.9725\n",
      "Epoch 250: Train Loss: 0.0020, Macro_F1: 0.9231, AUC_score: 0.9715\n",
      "Epoch 00296: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 300: Train Loss: 0.0017, Macro_F1: 0.9376, AUC_score: 0.9726\n",
      "Epoch 350: Train Loss: 0.0020, Macro_F1: 0.9304, AUC_score: 0.9714\n",
      "Early stopping triggered\n",
      "acc save\n",
      "35.0% node features transform to 0: F1: 0.9270, AUC_score: 0.9719\n",
      "Epoch 0: Train Loss: 0.0017, Macro_F1: 0.8427, AUC_score: 0.9614\n",
      "Validation loss decreased (0.001691 --> 0.001691).\n",
      "Epoch 50: Train Loss: 0.0057, Macro_F1: 0.9305, AUC_score: 0.9723\n",
      "Epoch 100: Train Loss: 0.0026, Macro_F1: 0.9232, AUC_score: 0.9723\n",
      "Epoch 00102: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.001646 --> 0.001646).\n",
      "Epoch 150: Train Loss: 0.0026, Macro_F1: 0.9340, AUC_score: 0.9726\n",
      "Validation loss decreased (0.001623 --> 0.001623).\n",
      "Epoch 200: Train Loss: 0.0065, Macro_F1: 0.9340, AUC_score: 0.9726\n",
      "Validation loss decreased (0.001323 --> 0.001323).\n",
      "Epoch 250: Train Loss: 0.0020, Macro_F1: 0.9340, AUC_score: 0.9717\n",
      "Epoch 300: Train Loss: 0.0030, Macro_F1: 0.9340, AUC_score: 0.9717\n",
      "Epoch 00311: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.0025, Macro_F1: 0.9340, AUC_score: 0.9719\n",
      "Validation loss decreased (0.001105 --> 0.001105).\n",
      "Epoch 400: Train Loss: 0.0026, Macro_F1: 0.9340, AUC_score: 0.9718\n",
      "Epoch 450: Train Loss: 0.0061, Macro_F1: 0.9340, AUC_score: 0.9716\n",
      "Epoch 00456: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 500: Train Loss: 0.0079, Macro_F1: 0.9340, AUC_score: 0.9718\n",
      "Epoch 550: Train Loss: 0.0025, Macro_F1: 0.9340, AUC_score: 0.9719\n",
      "Early stopping triggered\n",
      "acc save\n",
      "34.0% node features transform to 0: F1: 0.9340, AUC_score: 0.9719\n",
      "Epoch 0: Train Loss: 0.0083, Macro_F1: 0.8587, AUC_score: 0.9734\n",
      "Validation loss decreased (0.008322 --> 0.008322).\n",
      "Validation loss decreased (0.003724 --> 0.003724).\n",
      "Validation loss decreased (0.003657 --> 0.003657).\n",
      "Validation loss decreased (0.002823 --> 0.002823).\n",
      "Epoch 50: Train Loss: 0.0031, Macro_F1: 0.9377, AUC_score: 0.9740\n",
      "Validation loss decreased (0.002447 --> 0.002447).\n",
      "Validation loss decreased (0.002404 --> 0.002404).\n",
      "Validation loss decreased (0.001969 --> 0.001969).\n",
      "Validation loss decreased (0.001766 --> 0.001766).\n",
      "Validation loss decreased (0.001484 --> 0.001484).\n",
      "Epoch 100: Train Loss: 0.0275, Macro_F1: 0.9304, AUC_score: 0.9707\n",
      "Validation loss decreased (0.001287 --> 0.001287).\n",
      "Epoch 150: Train Loss: 0.0030, Macro_F1: 0.9268, AUC_score: 0.9708\n",
      "Validation loss decreased (0.001202 --> 0.001202).\n",
      "Validation loss decreased (0.001174 --> 0.001174).\n",
      "Epoch 200: Train Loss: 0.0014, Macro_F1: 0.9269, AUC_score: 0.9703\n",
      "Epoch 250: Train Loss: 0.0021, Macro_F1: 0.9232, AUC_score: 0.9729\n",
      "Epoch 00265: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.001146 --> 0.001146).\n",
      "Validation loss decreased (0.000843 --> 0.000843).\n",
      "Epoch 300: Train Loss: 0.0047, Macro_F1: 0.9305, AUC_score: 0.9733\n",
      "Epoch 350: Train Loss: 0.0038, Macro_F1: 0.9340, AUC_score: 0.9711\n",
      "Epoch 00388: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0045, Macro_F1: 0.9305, AUC_score: 0.9731\n",
      "Epoch 450: Train Loss: 0.0013, Macro_F1: 0.9304, AUC_score: 0.9725\n",
      "Early stopping triggered\n",
      "acc save\n",
      "32.99999999999999% node features transform to 0: F1: 0.9340, AUC_score: 0.9725\n",
      "Epoch 0: Train Loss: 0.0021, Macro_F1: 0.9125, AUC_score: 0.9640\n",
      "Validation loss decreased (0.002106 --> 0.002106).\n",
      "Epoch 50: Train Loss: 0.0126, Macro_F1: 0.9199, AUC_score: 0.9676\n",
      "Validation loss decreased (0.001883 --> 0.001883).\n",
      "Epoch 100: Train Loss: 0.0023, Macro_F1: 0.9144, AUC_score: 0.9631\n",
      "Validation loss decreased (0.001566 --> 0.001566).\n",
      "Epoch 150: Train Loss: 0.0034, Macro_F1: 0.9304, AUC_score: 0.9733\n",
      "Validation loss decreased (0.001512 --> 0.001512).\n",
      "Epoch 200: Train Loss: 0.0025, Macro_F1: 0.9305, AUC_score: 0.9675\n",
      "Validation loss decreased (0.001452 --> 0.001452).\n",
      "Validation loss decreased (0.001352 --> 0.001352).\n",
      "Validation loss decreased (0.000968 --> 0.000968).\n",
      "Epoch 250: Train Loss: 0.0033, Macro_F1: 0.9234, AUC_score: 0.9705\n",
      "Epoch 300: Train Loss: 0.0016, Macro_F1: 0.9449, AUC_score: 0.9743\n",
      "Epoch 00329: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 350: Train Loss: 0.0016, Macro_F1: 0.9269, AUC_score: 0.9709\n",
      "Epoch 400: Train Loss: 0.0015, Macro_F1: 0.9341, AUC_score: 0.9702\n",
      "Validation loss decreased (0.000870 --> 0.000870).\n",
      "Epoch 450: Train Loss: 0.0016, Macro_F1: 0.9269, AUC_score: 0.9703\n",
      "Epoch 500: Train Loss: 0.0061, Macro_F1: 0.9340, AUC_score: 0.9687\n",
      "Epoch 00526: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 550: Train Loss: 0.0044, Macro_F1: 0.9269, AUC_score: 0.9701\n",
      "Validation loss decreased (0.000832 --> 0.000832).\n",
      "Epoch 600: Train Loss: 0.0030, Macro_F1: 0.9341, AUC_score: 0.9701\n",
      "Epoch 650: Train Loss: 0.0021, Macro_F1: 0.9341, AUC_score: 0.9702\n",
      "Validation loss decreased (0.000762 --> 0.000762).\n",
      "Epoch 700: Train Loss: 0.0014, Macro_F1: 0.9341, AUC_score: 0.9708\n",
      "Epoch 750: Train Loss: 0.0019, Macro_F1: 0.9341, AUC_score: 0.9709\n",
      "Validation loss decreased (0.000578 --> 0.000578).\n",
      "Epoch 800: Train Loss: 0.0011, Macro_F1: 0.9377, AUC_score: 0.9707\n",
      "Epoch 850: Train Loss: 0.0044, Macro_F1: 0.9341, AUC_score: 0.9710\n",
      "Epoch 00868: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 900: Train Loss: 0.0014, Macro_F1: 0.9377, AUC_score: 0.9708\n",
      "Epoch 950: Train Loss: 0.0013, Macro_F1: 0.9340, AUC_score: 0.9706\n",
      "Early stopping triggered\n",
      "acc save\n",
      "31.999999999999996% node features transform to 0: F1: 0.9377, AUC_score: 0.9706\n",
      "Epoch 0: Train Loss: 0.0012, Macro_F1: 0.8711, AUC_score: 0.9537\n",
      "Validation loss decreased (0.001175 --> 0.001175).\n",
      "Epoch 50: Train Loss: 0.0037, Macro_F1: 0.9341, AUC_score: 0.9729\n",
      "Validation loss decreased (0.001100 --> 0.001100).\n",
      "Epoch 100: Train Loss: 0.0018, Macro_F1: 0.9340, AUC_score: 0.9708\n",
      "Validation loss decreased (0.000993 --> 0.000993).\n",
      "Validation loss decreased (0.000766 --> 0.000766).\n",
      "Epoch 150: Train Loss: 0.0018, Macro_F1: 0.9305, AUC_score: 0.9710\n",
      "Epoch 200: Train Loss: 0.0010, Macro_F1: 0.9340, AUC_score: 0.9707\n",
      "Epoch 00244: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0022, Macro_F1: 0.9413, AUC_score: 0.9707\n",
      "Epoch 300: Train Loss: 0.0023, Macro_F1: 0.9340, AUC_score: 0.9713\n",
      "Early stopping triggered\n",
      "acc save\n",
      "30.999999999999993% node features transform to 0: F1: 0.9270, AUC_score: 0.9717\n",
      "Epoch 0: Train Loss: 0.0054, Macro_F1: 0.9029, AUC_score: 0.9662\n",
      "Validation loss decreased (0.005354 --> 0.005354).\n",
      "Validation loss decreased (0.002658 --> 0.002658).\n",
      "Validation loss decreased (0.002324 --> 0.002324).\n",
      "Validation loss decreased (0.002127 --> 0.002127).\n",
      "Validation loss decreased (0.002008 --> 0.002008).\n",
      "Epoch 50: Train Loss: 0.0018, Macro_F1: 0.9268, AUC_score: 0.9722\n",
      "Validation loss decreased (0.001752 --> 0.001752).\n",
      "Validation loss decreased (0.001612 --> 0.001612).\n",
      "Validation loss decreased (0.001557 --> 0.001557).\n",
      "Validation loss decreased (0.001412 --> 0.001412).\n",
      "Validation loss decreased (0.001158 --> 0.001158).\n",
      "Epoch 100: Train Loss: 0.0031, Macro_F1: 0.9341, AUC_score: 0.9707\n",
      "Validation loss decreased (0.001111 --> 0.001111).\n",
      "Epoch 150: Train Loss: 0.0015, Macro_F1: 0.9198, AUC_score: 0.9704\n",
      "Validation loss decreased (0.001087 --> 0.001087).\n",
      "Validation loss decreased (0.001030 --> 0.001030).\n",
      "Epoch 200: Train Loss: 0.0088, Macro_F1: 0.9376, AUC_score: 0.9650\n",
      "Epoch 250: Train Loss: 0.0019, Macro_F1: 0.9269, AUC_score: 0.9707\n",
      "Epoch 00277: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.000905 --> 0.000905).\n",
      "Epoch 300: Train Loss: 0.0031, Macro_F1: 0.9340, AUC_score: 0.9702\n",
      "Validation loss decreased (0.000829 --> 0.000829).\n",
      "Epoch 350: Train Loss: 0.0035, Macro_F1: 0.9340, AUC_score: 0.9698\n",
      "Epoch 400: Train Loss: 0.0014, Macro_F1: 0.9305, AUC_score: 0.9706\n",
      "Epoch 00411: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.000771 --> 0.000771).\n",
      "Epoch 450: Train Loss: 0.0016, Macro_F1: 0.9340, AUC_score: 0.9703\n",
      "Epoch 500: Train Loss: 0.0010, Macro_F1: 0.9377, AUC_score: 0.9710\n",
      "Epoch 00528: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0061, Macro_F1: 0.9340, AUC_score: 0.9709\n",
      "Epoch 600: Train Loss: 0.0010, Macro_F1: 0.9340, AUC_score: 0.9710\n",
      "Early stopping triggered\n",
      "acc save\n",
      "29.999999999999993% node features transform to 0: F1: 0.9340, AUC_score: 0.9711\n",
      "Epoch 0: Train Loss: 0.0026, Macro_F1: 0.8261, AUC_score: 0.9639\n",
      "Validation loss decreased (0.002625 --> 0.002625).\n",
      "Validation loss decreased (0.002095 --> 0.002095).\n",
      "Epoch 50: Train Loss: 0.0035, Macro_F1: 0.9341, AUC_score: 0.9718\n",
      "Validation loss decreased (0.002005 --> 0.002005).\n",
      "Validation loss decreased (0.001746 --> 0.001746).\n",
      "Validation loss decreased (0.001542 --> 0.001542).\n",
      "Validation loss decreased (0.001528 --> 0.001528).\n",
      "Epoch 100: Train Loss: 0.0016, Macro_F1: 0.9341, AUC_score: 0.9729\n",
      "Validation loss decreased (0.000950 --> 0.000950).\n",
      "Epoch 150: Train Loss: 0.0023, Macro_F1: 0.9268, AUC_score: 0.9711\n",
      "Validation loss decreased (0.000832 --> 0.000832).\n",
      "Epoch 200: Train Loss: 0.0018, Macro_F1: 0.9265, AUC_score: 0.9702\n",
      "Epoch 250: Train Loss: 0.0020, Macro_F1: 0.9341, AUC_score: 0.9723\n",
      "Validation loss decreased (0.000774 --> 0.000774).\n",
      "Epoch 300: Train Loss: 0.0011, Macro_F1: 0.9338, AUC_score: 0.9735\n",
      "Epoch 350: Train Loss: 0.0020, Macro_F1: 0.9233, AUC_score: 0.9703\n",
      "Epoch 00353: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 400: Train Loss: 0.0013, Macro_F1: 0.9305, AUC_score: 0.9731\n",
      "Epoch 450: Train Loss: 0.0014, Macro_F1: 0.9268, AUC_score: 0.9728\n",
      "Early stopping triggered\n",
      "acc save\n",
      "29.000000000000004% node features transform to 0: F1: 0.9305, AUC_score: 0.9728\n",
      "Epoch 0: Train Loss: 0.0019, Macro_F1: 0.8254, AUC_score: 0.9625\n",
      "Validation loss decreased (0.001883 --> 0.001883).\n",
      "Epoch 50: Train Loss: 0.0038, Macro_F1: 0.9341, AUC_score: 0.9724\n",
      "Validation loss decreased (0.001833 --> 0.001833).\n",
      "Validation loss decreased (0.001336 --> 0.001336).\n",
      "Epoch 100: Train Loss: 0.0026, Macro_F1: 0.9269, AUC_score: 0.9728\n",
      "Validation loss decreased (0.001114 --> 0.001114).\n",
      "Validation loss decreased (0.000985 --> 0.000985).\n",
      "Epoch 150: Train Loss: 0.0013, Macro_F1: 0.9268, AUC_score: 0.9717\n",
      "Epoch 200: Train Loss: 0.0023, Macro_F1: 0.9267, AUC_score: 0.9728\n",
      "Validation loss decreased (0.000955 --> 0.000955).\n",
      "Epoch 250: Train Loss: 0.0016, Macro_F1: 0.9269, AUC_score: 0.9721\n",
      "Validation loss decreased (0.000826 --> 0.000826).\n",
      "Epoch 300: Train Loss: 0.0033, Macro_F1: 0.9376, AUC_score: 0.9737\n",
      "Epoch 350: Train Loss: 0.0015, Macro_F1: 0.9233, AUC_score: 0.9730\n",
      "Epoch 00378: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 400: Train Loss: 0.0017, Macro_F1: 0.9269, AUC_score: 0.9734\n",
      "Validation loss decreased (0.000791 --> 0.000791).\n",
      "Epoch 450: Train Loss: 0.0040, Macro_F1: 0.9303, AUC_score: 0.9736\n",
      "Epoch 500: Train Loss: 0.0029, Macro_F1: 0.9305, AUC_score: 0.9734\n",
      "Epoch 00512: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 550: Train Loss: 0.0022, Macro_F1: 0.9304, AUC_score: 0.9735\n",
      "Epoch 600: Train Loss: 0.0018, Macro_F1: 0.9267, AUC_score: 0.9733\n",
      "Early stopping triggered\n",
      "acc save\n",
      "28.000000000000004% node features transform to 0: F1: 0.9267, AUC_score: 0.9733\n",
      "Epoch 0: Train Loss: 0.0020, Macro_F1: 0.7998, AUC_score: 0.9722\n",
      "Validation loss decreased (0.002047 --> 0.002047).\n",
      "Epoch 50: Train Loss: 0.0031, Macro_F1: 0.9160, AUC_score: 0.9730\n",
      "Validation loss decreased (0.001983 --> 0.001983).\n",
      "Validation loss decreased (0.001670 --> 0.001670).\n",
      "Epoch 100: Train Loss: 0.0021, Macro_F1: 0.9196, AUC_score: 0.9706\n",
      "Validation loss decreased (0.001385 --> 0.001385).\n",
      "Epoch 150: Train Loss: 0.0023, Macro_F1: 0.9232, AUC_score: 0.9701\n",
      "Validation loss decreased (0.001255 --> 0.001255).\n",
      "Validation loss decreased (0.000960 --> 0.000960).\n",
      "Epoch 200: Train Loss: 0.0016, Macro_F1: 0.9233, AUC_score: 0.9711\n",
      "Epoch 250: Train Loss: 0.0021, Macro_F1: 0.9234, AUC_score: 0.9718\n",
      "Epoch 00293: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.000835 --> 0.000835).\n",
      "Epoch 300: Train Loss: 0.0118, Macro_F1: 0.9234, AUC_score: 0.9715\n",
      "Epoch 350: Train Loss: 0.0010, Macro_F1: 0.9196, AUC_score: 0.9717\n",
      "Epoch 00400: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0015, Macro_F1: 0.9270, AUC_score: 0.9717\n",
      "Epoch 450: Train Loss: 0.0011, Macro_F1: 0.9233, AUC_score: 0.9719\n",
      "Early stopping triggered\n",
      "acc save\n",
      "27.0% node features transform to 0: F1: 0.9233, AUC_score: 0.9720\n",
      "Epoch 0: Train Loss: 0.0095, Macro_F1: 0.8591, AUC_score: 0.9578\n",
      "Validation loss decreased (0.009511 --> 0.009511).\n",
      "Validation loss decreased (0.006123 --> 0.006123).\n",
      "Validation loss decreased (0.005554 --> 0.005554).\n",
      "Validation loss decreased (0.004116 --> 0.004116).\n",
      "Validation loss decreased (0.003322 --> 0.003322).\n",
      "Validation loss decreased (0.003298 --> 0.003298).\n",
      "Validation loss decreased (0.003247 --> 0.003247).\n",
      "Validation loss decreased (0.002319 --> 0.002319).\n",
      "Validation loss decreased (0.002000 --> 0.002000).\n",
      "Epoch 50: Train Loss: 0.0025, Macro_F1: 0.9376, AUC_score: 0.9732\n",
      "Validation loss decreased (0.001847 --> 0.001847).\n",
      "Validation loss decreased (0.001590 --> 0.001590).\n",
      "Validation loss decreased (0.001199 --> 0.001199).\n",
      "Epoch 100: Train Loss: 0.0026, Macro_F1: 0.9233, AUC_score: 0.9734\n",
      "Validation loss decreased (0.001163 --> 0.001163).\n",
      "Validation loss decreased (0.001068 --> 0.001068).\n",
      "Validation loss decreased (0.000950 --> 0.000950).\n",
      "Epoch 150: Train Loss: 0.0021, Macro_F1: 0.9234, AUC_score: 0.9739\n",
      "Validation loss decreased (0.000751 --> 0.000751).\n",
      "Epoch 200: Train Loss: 0.0018, Macro_F1: 0.9269, AUC_score: 0.9734\n",
      "Epoch 250: Train Loss: 0.0012, Macro_F1: 0.9268, AUC_score: 0.9732\n",
      "Epoch 00283: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0012, Macro_F1: 0.9305, AUC_score: 0.9732\n",
      "Validation loss decreased (0.000593 --> 0.000593).\n",
      "Epoch 350: Train Loss: 0.0023, Macro_F1: 0.9268, AUC_score: 0.9733\n",
      "Epoch 400: Train Loss: 0.0013, Macro_F1: 0.9268, AUC_score: 0.9737\n",
      "Epoch 00448: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.0008, Macro_F1: 0.9268, AUC_score: 0.9737\n",
      "Epoch 500: Train Loss: 0.0006, Macro_F1: 0.9342, AUC_score: 0.9740\n",
      "Validation loss decreased (0.000567 --> 0.000567).\n",
      "Epoch 550: Train Loss: 0.0011, Macro_F1: 0.9269, AUC_score: 0.9743\n",
      "Epoch 600: Train Loss: 0.0012, Macro_F1: 0.9269, AUC_score: 0.9742\n",
      "Epoch 00602: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 650: Train Loss: 0.0009, Macro_F1: 0.9305, AUC_score: 0.9739\n",
      "Epoch 700: Train Loss: 0.0010, Macro_F1: 0.9305, AUC_score: 0.9741\n",
      "Early stopping triggered\n",
      "acc save\n",
      "26.0% node features transform to 0: F1: 0.9305, AUC_score: 0.9741\n",
      "Epoch 0: Train Loss: 0.0041, Macro_F1: 0.8827, AUC_score: 0.9673\n",
      "Validation loss decreased (0.004069 --> 0.004069).\n",
      "Validation loss decreased (0.001303 --> 0.001303).\n",
      "Epoch 50: Train Loss: 0.0063, Macro_F1: 0.9269, AUC_score: 0.9744\n",
      "Validation loss decreased (0.001097 --> 0.001097).\n",
      "Epoch 100: Train Loss: 0.0038, Macro_F1: 0.9268, AUC_score: 0.9764\n",
      "Epoch 150: Train Loss: 0.0027, Macro_F1: 0.9269, AUC_score: 0.9765\n",
      "Validation loss decreased (0.000991 --> 0.000991).\n",
      "Validation loss decreased (0.000751 --> 0.000751).\n",
      "Epoch 200: Train Loss: 0.0014, Macro_F1: 0.9270, AUC_score: 0.9767\n",
      "Validation loss decreased (0.000714 --> 0.000714).\n",
      "Validation loss decreased (0.000492 --> 0.000492).\n",
      "Epoch 250: Train Loss: 0.0044, Macro_F1: 0.9234, AUC_score: 0.9766\n",
      "Epoch 300: Train Loss: 0.0023, Macro_F1: 0.9305, AUC_score: 0.9767\n",
      "Epoch 00317: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 350: Train Loss: 0.0027, Macro_F1: 0.9342, AUC_score: 0.9752\n",
      "Epoch 400: Train Loss: 0.0009, Macro_F1: 0.9306, AUC_score: 0.9763\n",
      "Early stopping triggered\n",
      "acc save\n",
      "25.0% node features transform to 0: F1: 0.9269, AUC_score: 0.9756\n",
      "Epoch 0: Train Loss: 0.0012, Macro_F1: 0.8755, AUC_score: 0.9668\n",
      "Validation loss decreased (0.001167 --> 0.001167).\n",
      "Epoch 50: Train Loss: 0.0031, Macro_F1: 0.9195, AUC_score: 0.9756\n",
      "Validation loss decreased (0.001086 --> 0.001086).\n",
      "Validation loss decreased (0.000979 --> 0.000979).\n",
      "Epoch 100: Train Loss: 0.0014, Macro_F1: 0.9341, AUC_score: 0.9747\n",
      "Validation loss decreased (0.000832 --> 0.000832).\n",
      "Epoch 150: Train Loss: 0.0008, Macro_F1: 0.9232, AUC_score: 0.9757\n",
      "Validation loss decreased (0.000795 --> 0.000795).\n",
      "Validation loss decreased (0.000710 --> 0.000710).\n",
      "Epoch 200: Train Loss: 0.0015, Macro_F1: 0.9342, AUC_score: 0.9742\n",
      "Epoch 250: Train Loss: 0.0014, Macro_F1: 0.9338, AUC_score: 0.9726\n",
      "Validation loss decreased (0.000696 --> 0.000696).\n",
      "Epoch 300: Train Loss: 0.0018, Macro_F1: 0.9339, AUC_score: 0.9741\n",
      "Validation loss decreased (0.000646 --> 0.000646).\n",
      "Epoch 350: Train Loss: 0.0011, Macro_F1: 0.9270, AUC_score: 0.9769\n",
      "Validation loss decreased (0.000622 --> 0.000622).\n",
      "Epoch 400: Train Loss: 0.0013, Macro_F1: 0.9235, AUC_score: 0.9751\n",
      "Epoch 450: Train Loss: 0.0011, Macro_F1: 0.9305, AUC_score: 0.9765\n",
      "Epoch 00484: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 500: Train Loss: 0.0016, Macro_F1: 0.9305, AUC_score: 0.9758\n",
      "Epoch 550: Train Loss: 0.0015, Macro_F1: 0.9305, AUC_score: 0.9752\n",
      "Early stopping triggered\n",
      "acc save\n",
      "24.0% node features transform to 0: F1: 0.9305, AUC_score: 0.9761\n",
      "Epoch 0: Train Loss: 0.0014, Macro_F1: 0.8143, AUC_score: 0.9773\n",
      "Validation loss decreased (0.001363 --> 0.001363).\n",
      "Epoch 50: Train Loss: 0.0060, Macro_F1: 0.9270, AUC_score: 0.9757\n",
      "Epoch 100: Train Loss: 0.0052, Macro_F1: 0.9306, AUC_score: 0.9720\n",
      "Epoch 00102: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.0025, Macro_F1: 0.9306, AUC_score: 0.9729\n",
      "Validation loss decreased (0.001046 --> 0.001046).\n",
      "Epoch 200: Train Loss: 0.0020, Macro_F1: 0.9306, AUC_score: 0.9732\n",
      "Epoch 250: Train Loss: 0.0016, Macro_F1: 0.9306, AUC_score: 0.9737\n",
      "Epoch 00259: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 300: Train Loss: 0.0016, Macro_F1: 0.9306, AUC_score: 0.9736\n",
      "Epoch 350: Train Loss: 0.0015, Macro_F1: 0.9306, AUC_score: 0.9737\n",
      "Early stopping triggered\n",
      "acc save\n",
      "23.0% node features transform to 0: F1: 0.9306, AUC_score: 0.9737\n",
      "Epoch 0: Train Loss: 0.0017, Macro_F1: 0.8550, AUC_score: 0.9765\n",
      "Validation loss decreased (0.001727 --> 0.001727).\n",
      "Epoch 50: Train Loss: 0.0034, Macro_F1: 0.9304, AUC_score: 0.9737\n",
      "Validation loss decreased (0.001622 --> 0.001622).\n",
      "Validation loss decreased (0.001130 --> 0.001130).\n",
      "Validation loss decreased (0.000999 --> 0.000999).\n",
      "Epoch 100: Train Loss: 0.0013, Macro_F1: 0.9233, AUC_score: 0.9738\n",
      "Validation loss decreased (0.000956 --> 0.000956).\n",
      "Validation loss decreased (0.000842 --> 0.000842).\n",
      "Epoch 150: Train Loss: 0.0022, Macro_F1: 0.9305, AUC_score: 0.9741\n",
      "Validation loss decreased (0.000809 --> 0.000809).\n",
      "Epoch 200: Train Loss: 0.0010, Macro_F1: 0.9270, AUC_score: 0.9735\n",
      "Validation loss decreased (0.000710 --> 0.000710).\n",
      "Epoch 250: Train Loss: 0.0024, Macro_F1: 0.9235, AUC_score: 0.9749\n",
      "Epoch 300: Train Loss: 0.0012, Macro_F1: 0.9234, AUC_score: 0.9744\n",
      "Epoch 00318: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 350: Train Loss: 0.0020, Macro_F1: 0.9269, AUC_score: 0.9747\n",
      "Validation loss decreased (0.000666 --> 0.000666).\n",
      "Epoch 400: Train Loss: 0.0051, Macro_F1: 0.9305, AUC_score: 0.9747\n",
      "Epoch 450: Train Loss: 0.0026, Macro_F1: 0.9233, AUC_score: 0.9756\n",
      "Validation loss decreased (0.000625 --> 0.000625).\n",
      "Validation loss decreased (0.000575 --> 0.000575).\n",
      "Epoch 500: Train Loss: 0.0023, Macro_F1: 0.9305, AUC_score: 0.9749\n",
      "Epoch 550: Train Loss: 0.0023, Macro_F1: 0.9305, AUC_score: 0.9762\n",
      "Epoch 00591: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 600: Train Loss: 0.0019, Macro_F1: 0.9305, AUC_score: 0.9757\n",
      "Epoch 650: Train Loss: 0.0007, Macro_F1: 0.9270, AUC_score: 0.9760\n",
      "Early stopping triggered\n",
      "acc save\n",
      "21.999999999999996% node features transform to 0: F1: 0.9305, AUC_score: 0.9759\n",
      "Epoch 0: Train Loss: 0.0012, Macro_F1: 0.8367, AUC_score: 0.9726\n",
      "Validation loss decreased (0.001215 --> 0.001215).\n",
      "Epoch 50: Train Loss: 0.0031, Macro_F1: 0.9269, AUC_score: 0.9759\n",
      "Epoch 100: Train Loss: 0.0017, Macro_F1: 0.9306, AUC_score: 0.9756\n",
      "Epoch 00102: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.001061 --> 0.001061).\n",
      "Validation loss decreased (0.000807 --> 0.000807).\n",
      "Epoch 150: Train Loss: 0.0018, Macro_F1: 0.9306, AUC_score: 0.9755\n",
      "Epoch 200: Train Loss: 0.0013, Macro_F1: 0.9306, AUC_score: 0.9751\n",
      "Epoch 00235: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 250: Train Loss: 0.0015, Macro_F1: 0.9306, AUC_score: 0.9756\n",
      "Epoch 300: Train Loss: 0.0025, Macro_F1: 0.9306, AUC_score: 0.9756\n",
      "Early stopping triggered\n",
      "acc save\n",
      "20.999999999999996% node features transform to 0: F1: 0.9306, AUC_score: 0.9756\n",
      "Epoch 0: Train Loss: 0.0014, Macro_F1: 0.8659, AUC_score: 0.9785\n",
      "Validation loss decreased (0.001407 --> 0.001407).\n",
      "Epoch 50: Train Loss: 0.0020, Macro_F1: 0.9270, AUC_score: 0.9744\n",
      "Validation loss decreased (0.001389 --> 0.001389).\n",
      "Validation loss decreased (0.001324 --> 0.001324).\n",
      "Validation loss decreased (0.001194 --> 0.001194).\n",
      "Epoch 100: Train Loss: 0.0022, Macro_F1: 0.9306, AUC_score: 0.9743\n",
      "Validation loss decreased (0.000713 --> 0.000713).\n",
      "Epoch 150: Train Loss: 0.0013, Macro_F1: 0.9307, AUC_score: 0.9740\n",
      "Validation loss decreased (0.000631 --> 0.000631).\n",
      "Epoch 200: Train Loss: 0.0010, Macro_F1: 0.9269, AUC_score: 0.9733\n",
      "Epoch 250: Train Loss: 0.0026, Macro_F1: 0.9270, AUC_score: 0.9766\n",
      "Epoch 00281: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.000601 --> 0.000601).\n",
      "Epoch 300: Train Loss: 0.0011, Macro_F1: 0.9270, AUC_score: 0.9747\n",
      "Epoch 350: Train Loss: 0.0010, Macro_F1: 0.9269, AUC_score: 0.9750\n",
      "Validation loss decreased (0.000535 --> 0.000535).\n",
      "Epoch 400: Train Loss: 0.0006, Macro_F1: 0.9270, AUC_score: 0.9755\n",
      "Epoch 450: Train Loss: 0.0008, Macro_F1: 0.9270, AUC_score: 0.9764\n",
      "Epoch 00482: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.0016, Macro_F1: 0.9306, AUC_score: 0.9759\n",
      "Epoch 550: Train Loss: 0.0012, Macro_F1: 0.9306, AUC_score: 0.9759\n",
      "Early stopping triggered\n",
      "acc save\n",
      "19.999999999999996% node features transform to 0: F1: 0.9306, AUC_score: 0.9757\n",
      "Epoch 0: Train Loss: 0.0018, Macro_F1: 0.8948, AUC_score: 0.9680\n",
      "Validation loss decreased (0.001757 --> 0.001757).\n",
      "Validation loss decreased (0.001539 --> 0.001539).\n",
      "Validation loss decreased (0.001081 --> 0.001081).\n",
      "Epoch 50: Train Loss: 0.0022, Macro_F1: 0.9269, AUC_score: 0.9773\n",
      "Validation loss decreased (0.000994 --> 0.000994).\n",
      "Validation loss decreased (0.000894 --> 0.000894).\n",
      "Validation loss decreased (0.000869 --> 0.000869).\n",
      "Epoch 100: Train Loss: 0.0012, Macro_F1: 0.9305, AUC_score: 0.9747\n",
      "Validation loss decreased (0.000731 --> 0.000731).\n",
      "Epoch 150: Train Loss: 0.0016, Macro_F1: 0.9233, AUC_score: 0.9743\n",
      "Epoch 200: Train Loss: 0.0050, Macro_F1: 0.9199, AUC_score: 0.9737\n",
      "Validation loss decreased (0.000706 --> 0.000706).\n",
      "Epoch 250: Train Loss: 0.0019, Macro_F1: 0.9269, AUC_score: 0.9753\n",
      "Epoch 300: Train Loss: 0.0043, Macro_F1: 0.9306, AUC_score: 0.9708\n",
      "Epoch 00323: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 350: Train Loss: 0.0025, Macro_F1: 0.9306, AUC_score: 0.9717\n",
      "Validation loss decreased (0.000688 --> 0.000688).\n",
      "Epoch 400: Train Loss: 0.0017, Macro_F1: 0.9306, AUC_score: 0.9728\n",
      "Validation loss decreased (0.000626 --> 0.000626).\n",
      "Validation loss decreased (0.000532 --> 0.000532).\n",
      "Epoch 450: Train Loss: 0.0016, Macro_F1: 0.9306, AUC_score: 0.9727\n",
      "Epoch 500: Train Loss: 0.0009, Macro_F1: 0.9270, AUC_score: 0.9726\n",
      "Epoch 00551: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 550: Train Loss: 0.0010, Macro_F1: 0.9270, AUC_score: 0.9725\n",
      "Epoch 600: Train Loss: 0.0012, Macro_F1: 0.9269, AUC_score: 0.9731\n",
      "Validation loss decreased (0.000519 --> 0.000519).\n",
      "Epoch 650: Train Loss: 0.0015, Macro_F1: 0.9269, AUC_score: 0.9728\n",
      "Epoch 700: Train Loss: 0.0004, Macro_F1: 0.9269, AUC_score: 0.9725\n",
      "Validation loss decreased (0.000446 --> 0.000446).\n",
      "Epoch 750: Train Loss: 0.0010, Macro_F1: 0.9269, AUC_score: 0.9727\n",
      "Epoch 800: Train Loss: 0.0010, Macro_F1: 0.9269, AUC_score: 0.9726\n",
      "Epoch 00802: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 850: Train Loss: 0.0016, Macro_F1: 0.9306, AUC_score: 0.9731\n",
      "Epoch 900: Train Loss: 0.0011, Macro_F1: 0.9306, AUC_score: 0.9730\n",
      "Early stopping triggered\n",
      "acc save\n",
      "18.999999999999993% node features transform to 0: F1: 0.9306, AUC_score: 0.9730\n",
      "Epoch 0: Train Loss: 0.0009, Macro_F1: 0.8463, AUC_score: 0.9631\n",
      "Validation loss decreased (0.000915 --> 0.000915).\n",
      "Epoch 50: Train Loss: 0.0036, Macro_F1: 0.9306, AUC_score: 0.9724\n",
      "Epoch 100: Train Loss: 0.0012, Macro_F1: 0.9269, AUC_score: 0.9724\n",
      "Epoch 00102: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.0016, Macro_F1: 0.9269, AUC_score: 0.9726\n",
      "Epoch 200: Train Loss: 0.0019, Macro_F1: 0.9305, AUC_score: 0.9719\n",
      "Early stopping triggered\n",
      "acc save\n",
      "17.999999999999993% node features transform to 0: F1: 0.9305, AUC_score: 0.9719\n",
      "Epoch 0: Train Loss: 0.0010, Macro_F1: 0.9234, AUC_score: 0.9733\n",
      "Validation loss decreased (0.000981 --> 0.000981).\n",
      "Epoch 50: Train Loss: 0.0014, Macro_F1: 0.9267, AUC_score: 0.9699\n",
      "Validation loss decreased (0.000961 --> 0.000961).\n",
      "Validation loss decreased (0.000717 --> 0.000717).\n",
      "Epoch 100: Train Loss: 0.0004, Macro_F1: 0.9342, AUC_score: 0.9734\n",
      "Validation loss decreased (0.000411 --> 0.000411).\n",
      "Epoch 150: Train Loss: 0.0009, Macro_F1: 0.9305, AUC_score: 0.9690\n",
      "Epoch 200: Train Loss: 0.0011, Macro_F1: 0.9306, AUC_score: 0.9724\n",
      "Epoch 00202: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0015, Macro_F1: 0.9341, AUC_score: 0.9741\n",
      "Epoch 300: Train Loss: 0.0020, Macro_F1: 0.9233, AUC_score: 0.9739\n",
      "Early stopping triggered\n",
      "acc save\n",
      "16.999999999999993% node features transform to 0: F1: 0.9233, AUC_score: 0.9739\n",
      "Epoch 0: Train Loss: 0.0008, Macro_F1: 0.9222, AUC_score: 0.9739\n",
      "Validation loss decreased (0.000768 --> 0.000768).\n",
      "Epoch 50: Train Loss: 0.0017, Macro_F1: 0.9342, AUC_score: 0.9740\n",
      "Epoch 100: Train Loss: 0.0063, Macro_F1: 0.9340, AUC_score: 0.9734\n",
      "Epoch 00102: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.000677 --> 0.000677).\n",
      "Validation loss decreased (0.000661 --> 0.000661).\n",
      "Epoch 150: Train Loss: 0.0014, Macro_F1: 0.9450, AUC_score: 0.9742\n",
      "Validation loss decreased (0.000589 --> 0.000589).\n",
      "Epoch 200: Train Loss: 0.0009, Macro_F1: 0.9414, AUC_score: 0.9732\n",
      "Validation loss decreased (0.000513 --> 0.000513).\n",
      "Epoch 250: Train Loss: 0.0023, Macro_F1: 0.9340, AUC_score: 0.9737\n",
      "Validation loss decreased (0.000388 --> 0.000388).\n",
      "Epoch 300: Train Loss: 0.0013, Macro_F1: 0.9270, AUC_score: 0.9745\n",
      "Epoch 350: Train Loss: 0.0007, Macro_F1: 0.9270, AUC_score: 0.9740\n",
      "Epoch 00372: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0010, Macro_F1: 0.9342, AUC_score: 0.9740\n",
      "Epoch 450: Train Loss: 0.0010, Macro_F1: 0.9270, AUC_score: 0.9742\n",
      "Early stopping triggered\n",
      "acc save\n",
      "16.000000000000004% node features transform to 0: F1: 0.9270, AUC_score: 0.9745\n",
      "Epoch 0: Train Loss: 0.0007, Macro_F1: 0.9339, AUC_score: 0.9747\n",
      "Validation loss decreased (0.000679 --> 0.000679).\n",
      "Epoch 50: Train Loss: 0.0036, Macro_F1: 0.9306, AUC_score: 0.9722\n",
      "Epoch 100: Train Loss: 0.0013, Macro_F1: 0.9305, AUC_score: 0.9721\n",
      "Epoch 00102: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.000654 --> 0.000654).\n",
      "Epoch 150: Train Loss: 0.0012, Macro_F1: 0.9305, AUC_score: 0.9730\n",
      "Epoch 200: Train Loss: 0.0014, Macro_F1: 0.9305, AUC_score: 0.9735\n",
      "Epoch 250: Train Loss: 0.0011, Macro_F1: 0.9305, AUC_score: 0.9727\n",
      "Validation loss decreased (0.000528 --> 0.000528).\n",
      "Epoch 300: Train Loss: 0.0007, Macro_F1: 0.9233, AUC_score: 0.9732\n",
      "Epoch 350: Train Loss: 0.0018, Macro_F1: 0.9233, AUC_score: 0.9729\n",
      "Epoch 00354: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0055, Macro_F1: 0.9305, AUC_score: 0.9725\n",
      "Epoch 450: Train Loss: 0.0014, Macro_F1: 0.9305, AUC_score: 0.9728\n",
      "Early stopping triggered\n",
      "acc save\n",
      "15.000000000000002% node features transform to 0: F1: 0.9305, AUC_score: 0.9729\n",
      "Epoch 0: Train Loss: 0.0044, Macro_F1: 0.8296, AUC_score: 0.9626\n",
      "Validation loss decreased (0.004357 --> 0.004357).\n",
      "Validation loss decreased (0.002617 --> 0.002617).\n",
      "Validation loss decreased (0.002544 --> 0.002544).\n",
      "Epoch 50: Train Loss: 0.0046, Macro_F1: 0.9343, AUC_score: 0.9725\n",
      "Validation loss decreased (0.002074 --> 0.002074).\n",
      "Validation loss decreased (0.001442 --> 0.001442).\n",
      "Validation loss decreased (0.001411 --> 0.001411).\n",
      "Validation loss decreased (0.001398 --> 0.001398).\n",
      "Epoch 100: Train Loss: 0.0032, Macro_F1: 0.9304, AUC_score: 0.9717\n",
      "Validation loss decreased (0.001277 --> 0.001277).\n",
      "Validation loss decreased (0.000998 --> 0.000998).\n",
      "Epoch 150: Train Loss: 0.0015, Macro_F1: 0.9341, AUC_score: 0.9714\n",
      "Epoch 200: Train Loss: 0.0019, Macro_F1: 0.9233, AUC_score: 0.9720\n",
      "Validation loss decreased (0.000842 --> 0.000842).\n",
      "Epoch 250: Train Loss: 0.0012, Macro_F1: 0.9341, AUC_score: 0.9723\n",
      "Validation loss decreased (0.000716 --> 0.000716).\n",
      "Validation loss decreased (0.000661 --> 0.000661).\n",
      "Epoch 300: Train Loss: 0.0017, Macro_F1: 0.9269, AUC_score: 0.9715\n",
      "Epoch 350: Train Loss: 0.0019, Macro_F1: 0.9269, AUC_score: 0.9719\n",
      "Validation loss decreased (0.000625 --> 0.000625).\n",
      "Epoch 400: Train Loss: 0.0009, Macro_F1: 0.9269, AUC_score: 0.9740\n",
      "Validation loss decreased (0.000519 --> 0.000519).\n",
      "Epoch 450: Train Loss: 0.0013, Macro_F1: 0.9413, AUC_score: 0.9724\n",
      "Epoch 500: Train Loss: 0.0013, Macro_F1: 0.9343, AUC_score: 0.9738\n",
      "Epoch 00535: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 550: Train Loss: 0.0009, Macro_F1: 0.9341, AUC_score: 0.9736\n",
      "Epoch 600: Train Loss: 0.0015, Macro_F1: 0.9341, AUC_score: 0.9739\n",
      "Early stopping triggered\n",
      "acc save\n",
      "14.000000000000002% node features transform to 0: F1: 0.9269, AUC_score: 0.9739\n",
      "Epoch 0: Train Loss: 0.0008, Macro_F1: 0.8468, AUC_score: 0.9643\n",
      "Validation loss decreased (0.000848 --> 0.000848).\n",
      "Epoch 50: Train Loss: 0.0018, Macro_F1: 0.9305, AUC_score: 0.9719\n",
      "Validation loss decreased (0.000804 --> 0.000804).\n",
      "Epoch 100: Train Loss: 0.0010, Macro_F1: 0.9341, AUC_score: 0.9737\n",
      "Epoch 150: Train Loss: 0.0011, Macro_F1: 0.9305, AUC_score: 0.9729\n",
      "Validation loss decreased (0.000727 --> 0.000727).\n",
      "Validation loss decreased (0.000714 --> 0.000714).\n",
      "Epoch 200: Train Loss: 0.0022, Macro_F1: 0.9304, AUC_score: 0.9726\n",
      "Epoch 250: Train Loss: 0.0030, Macro_F1: 0.9232, AUC_score: 0.9720\n",
      "Epoch 00291: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0017, Macro_F1: 0.9270, AUC_score: 0.9731\n",
      "Validation loss decreased (0.000648 --> 0.000648).\n",
      "Epoch 350: Train Loss: 0.0013, Macro_F1: 0.9305, AUC_score: 0.9732\n",
      "Validation loss decreased (0.000600 --> 0.000600).\n",
      "Validation loss decreased (0.000579 --> 0.000579).\n",
      "Epoch 400: Train Loss: 0.0010, Macro_F1: 0.9305, AUC_score: 0.9733\n",
      "Epoch 450: Train Loss: 0.0010, Macro_F1: 0.9341, AUC_score: 0.9737\n",
      "Epoch 00482: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.0007, Macro_F1: 0.9305, AUC_score: 0.9741\n",
      "Epoch 550: Train Loss: 0.0011, Macro_F1: 0.9305, AUC_score: 0.9739\n",
      "Early stopping triggered\n",
      "acc save\n",
      "13.0% node features transform to 0: F1: 0.9305, AUC_score: 0.9737\n",
      "Epoch 0: Train Loss: 0.0012, Macro_F1: 0.8367, AUC_score: 0.9774\n",
      "Validation loss decreased (0.001200 --> 0.001200).\n",
      "Epoch 50: Train Loss: 0.0044, Macro_F1: 0.9270, AUC_score: 0.9734\n",
      "Epoch 100: Train Loss: 0.0018, Macro_F1: 0.9307, AUC_score: 0.9731\n",
      "Epoch 00102: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.0016, Macro_F1: 0.9378, AUC_score: 0.9723\n",
      "Validation loss decreased (0.001158 --> 0.001158).\n",
      "Epoch 200: Train Loss: 0.0014, Macro_F1: 0.9378, AUC_score: 0.9722\n",
      "Validation loss decreased (0.000992 --> 0.000992).\n",
      "Validation loss decreased (0.000853 --> 0.000853).\n",
      "Epoch 250: Train Loss: 0.0019, Macro_F1: 0.9378, AUC_score: 0.9724\n",
      "Epoch 300: Train Loss: 0.0017, Macro_F1: 0.9306, AUC_score: 0.9725\n",
      "Epoch 00343: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.0021, Macro_F1: 0.9306, AUC_score: 0.9725\n",
      "Epoch 400: Train Loss: 0.0030, Macro_F1: 0.9378, AUC_score: 0.9725\n",
      "Early stopping triggered\n",
      "acc save\n",
      "12.0% node features transform to 0: F1: 0.9378, AUC_score: 0.9723\n",
      "Epoch 0: Train Loss: 0.0013, Macro_F1: 0.8804, AUC_score: 0.9778\n",
      "Validation loss decreased (0.001349 --> 0.001349).\n",
      "Validation loss decreased (0.001330 --> 0.001330).\n",
      "Validation loss decreased (0.000976 --> 0.000976).\n",
      "Epoch 50: Train Loss: 0.0017, Macro_F1: 0.9378, AUC_score: 0.9722\n",
      "Validation loss decreased (0.000632 --> 0.000632).\n",
      "Epoch 100: Train Loss: 0.0029, Macro_F1: 0.9307, AUC_score: 0.9743\n",
      "Epoch 150: Train Loss: 0.0014, Macro_F1: 0.9233, AUC_score: 0.9745\n",
      "Epoch 00166: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.000576 --> 0.000576).\n",
      "Epoch 200: Train Loss: 0.0010, Macro_F1: 0.9306, AUC_score: 0.9752\n",
      "Epoch 250: Train Loss: 0.0023, Macro_F1: 0.9306, AUC_score: 0.9759\n",
      "Epoch 00297: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 300: Train Loss: 0.0032, Macro_F1: 0.9307, AUC_score: 0.9753\n",
      "Validation loss decreased (0.000510 --> 0.000510).\n",
      "Validation loss decreased (0.000490 --> 0.000490).\n",
      "Epoch 350: Train Loss: 0.0006, Macro_F1: 0.9269, AUC_score: 0.9753\n",
      "Epoch 400: Train Loss: 0.0020, Macro_F1: 0.9305, AUC_score: 0.9751\n",
      "Epoch 00451: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 450: Train Loss: 0.0012, Macro_F1: 0.9306, AUC_score: 0.9754\n",
      "Epoch 500: Train Loss: 0.0011, Macro_F1: 0.9269, AUC_score: 0.9754\n",
      "Early stopping triggered\n",
      "acc save\n",
      "10.999999999999998% node features transform to 0: F1: 0.9305, AUC_score: 0.9753\n",
      "Epoch 0: Train Loss: 0.0018, Macro_F1: 0.8550, AUC_score: 0.9778\n",
      "Validation loss decreased (0.001820 --> 0.001820).\n",
      "Validation loss decreased (0.001579 --> 0.001579).\n",
      "Epoch 50: Train Loss: 0.0030, Macro_F1: 0.9342, AUC_score: 0.9756\n",
      "Validation loss decreased (0.001347 --> 0.001347).\n",
      "Validation loss decreased (0.001314 --> 0.001314).\n",
      "Validation loss decreased (0.001291 --> 0.001291).\n",
      "Validation loss decreased (0.000946 --> 0.000946).\n",
      "Validation loss decreased (0.000714 --> 0.000714).\n",
      "Epoch 100: Train Loss: 0.0012, Macro_F1: 0.9342, AUC_score: 0.9750\n",
      "Validation loss decreased (0.000667 --> 0.000667).\n",
      "Epoch 150: Train Loss: 0.0016, Macro_F1: 0.9305, AUC_score: 0.9756\n",
      "Epoch 200: Train Loss: 0.0022, Macro_F1: 0.9271, AUC_score: 0.9748\n",
      "Epoch 00216: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.000466 --> 0.000466).\n",
      "Epoch 250: Train Loss: 0.0010, Macro_F1: 0.9342, AUC_score: 0.9749\n",
      "Epoch 300: Train Loss: 0.0009, Macro_F1: 0.9305, AUC_score: 0.9754\n",
      "Epoch 00345: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.0028, Macro_F1: 0.9270, AUC_score: 0.9754\n",
      "Epoch 400: Train Loss: 0.0013, Macro_F1: 0.9305, AUC_score: 0.9751\n",
      "Early stopping triggered\n",
      "acc save\n",
      "9.999999999999998% node features transform to 0: F1: 0.9305, AUC_score: 0.9749\n",
      "Epoch 0: Train Loss: 0.0010, Macro_F1: 0.9343, AUC_score: 0.9767\n",
      "Validation loss decreased (0.001035 --> 0.001035).\n",
      "Epoch 50: Train Loss: 0.0012, Macro_F1: 0.9305, AUC_score: 0.9735\n",
      "Validation loss decreased (0.000953 --> 0.000953).\n",
      "Validation loss decreased (0.000820 --> 0.000820).\n",
      "Validation loss decreased (0.000670 --> 0.000670).\n",
      "Epoch 100: Train Loss: 0.0013, Macro_F1: 0.9376, AUC_score: 0.9740\n",
      "Epoch 150: Train Loss: 0.0107, Macro_F1: 0.9339, AUC_score: 0.9741\n",
      "Epoch 00186: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0036, Macro_F1: 0.9305, AUC_score: 0.9759\n",
      "Validation loss decreased (0.000636 --> 0.000636).\n",
      "Validation loss decreased (0.000523 --> 0.000523).\n",
      "Epoch 250: Train Loss: 0.0008, Macro_F1: 0.9305, AUC_score: 0.9753\n",
      "Epoch 300: Train Loss: 0.0017, Macro_F1: 0.9197, AUC_score: 0.9756\n",
      "Epoch 00323: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.0027, Macro_F1: 0.9305, AUC_score: 0.9752\n",
      "Epoch 400: Train Loss: 0.0009, Macro_F1: 0.9305, AUC_score: 0.9748\n",
      "Early stopping triggered\n",
      "acc save\n",
      "8.999999999999996% node features transform to 0: F1: 0.9305, AUC_score: 0.9747\n",
      "Epoch 0: Train Loss: 0.0026, Macro_F1: 0.8671, AUC_score: 0.9671\n",
      "Validation loss decreased (0.002634 --> 0.002634).\n",
      "Validation loss decreased (0.001181 --> 0.001181).\n",
      "Epoch 50: Train Loss: 0.0014, Macro_F1: 0.9306, AUC_score: 0.9745\n",
      "Validation loss decreased (0.001134 --> 0.001134).\n",
      "Validation loss decreased (0.000912 --> 0.000912).\n",
      "Validation loss decreased (0.000793 --> 0.000793).\n",
      "Epoch 100: Train Loss: 0.0016, Macro_F1: 0.9270, AUC_score: 0.9759\n",
      "Validation loss decreased (0.000625 --> 0.000625).\n",
      "Validation loss decreased (0.000594 --> 0.000594).\n",
      "Epoch 150: Train Loss: 0.0014, Macro_F1: 0.9270, AUC_score: 0.9755\n",
      "Validation loss decreased (0.000579 --> 0.000579).\n",
      "Epoch 200: Train Loss: 0.0015, Macro_F1: 0.9196, AUC_score: 0.9751\n",
      "Epoch 250: Train Loss: 0.0037, Macro_F1: 0.9341, AUC_score: 0.9751\n",
      "Epoch 00273: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.000528 --> 0.000528).\n",
      "Epoch 300: Train Loss: 0.0014, Macro_F1: 0.9305, AUC_score: 0.9745\n",
      "Validation loss decreased (0.000461 --> 0.000461).\n",
      "Epoch 350: Train Loss: 0.0010, Macro_F1: 0.9342, AUC_score: 0.9754\n",
      "Epoch 400: Train Loss: 0.0006, Macro_F1: 0.9233, AUC_score: 0.9753\n",
      "Epoch 450: Train Loss: 0.0015, Macro_F1: 0.9341, AUC_score: 0.9750\n",
      "Validation loss decreased (0.000400 --> 0.000400).\n",
      "Epoch 500: Train Loss: 0.0005, Macro_F1: 0.9270, AUC_score: 0.9756\n",
      "Epoch 550: Train Loss: 0.0011, Macro_F1: 0.9341, AUC_score: 0.9750\n",
      "Epoch 00598: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 600: Train Loss: 0.0013, Macro_F1: 0.9305, AUC_score: 0.9755\n",
      "Epoch 650: Train Loss: 0.0011, Macro_F1: 0.9305, AUC_score: 0.9757\n",
      "Early stopping triggered\n",
      "acc save\n",
      "7.9999999999999964% node features transform to 0: F1: 0.9341, AUC_score: 0.9758\n",
      "Epoch 0: Train Loss: 0.0014, Macro_F1: 0.9092, AUC_score: 0.9771\n",
      "Validation loss decreased (0.001445 --> 0.001445).\n",
      "Validation loss decreased (0.001072 --> 0.001072).\n",
      "Epoch 50: Train Loss: 0.0016, Macro_F1: 0.9231, AUC_score: 0.9728\n",
      "Validation loss decreased (0.001062 --> 0.001062).\n",
      "Validation loss decreased (0.000892 --> 0.000892).\n",
      "Validation loss decreased (0.000832 --> 0.000832).\n",
      "Epoch 100: Train Loss: 0.0011, Macro_F1: 0.9269, AUC_score: 0.9748\n",
      "Validation loss decreased (0.000758 --> 0.000758).\n",
      "Validation loss decreased (0.000669 --> 0.000669).\n",
      "Validation loss decreased (0.000515 --> 0.000515).\n",
      "Epoch 150: Train Loss: 0.0019, Macro_F1: 0.9197, AUC_score: 0.9744\n",
      "Epoch 200: Train Loss: 0.0073, Macro_F1: 0.9344, AUC_score: 0.9755\n",
      "Epoch 00248: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0009, Macro_F1: 0.9234, AUC_score: 0.9751\n",
      "Epoch 300: Train Loss: 0.0041, Macro_F1: 0.9232, AUC_score: 0.9739\n",
      "Early stopping triggered\n",
      "acc save\n",
      "6.999999999999995% node features transform to 0: F1: 0.9197, AUC_score: 0.9749\n",
      "Epoch 0: Train Loss: 0.0015, Macro_F1: 0.8440, AUC_score: 0.9761\n",
      "Validation loss decreased (0.001455 --> 0.001455).\n",
      "Validation loss decreased (0.001227 --> 0.001227).\n",
      "Epoch 50: Train Loss: 0.0034, Macro_F1: 0.9414, AUC_score: 0.9745\n",
      "Validation loss decreased (0.001202 --> 0.001202).\n",
      "Validation loss decreased (0.001166 --> 0.001166).\n",
      "Validation loss decreased (0.000775 --> 0.000775).\n",
      "Epoch 100: Train Loss: 0.0026, Macro_F1: 0.9306, AUC_score: 0.9750\n",
      "Validation loss decreased (0.000582 --> 0.000582).\n",
      "Epoch 150: Train Loss: 0.0007, Macro_F1: 0.9341, AUC_score: 0.9746\n",
      "Epoch 200: Train Loss: 0.0021, Macro_F1: 0.9305, AUC_score: 0.9745\n",
      "Epoch 00233: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.000540 --> 0.000540).\n",
      "Epoch 250: Train Loss: 0.0038, Macro_F1: 0.9305, AUC_score: 0.9740\n",
      "Epoch 300: Train Loss: 0.0006, Macro_F1: 0.9306, AUC_score: 0.9747\n",
      "Epoch 00348: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.0009, Macro_F1: 0.9269, AUC_score: 0.9744\n",
      "Epoch 400: Train Loss: 0.0010, Macro_F1: 0.9269, AUC_score: 0.9747\n",
      "Validation loss decreased (0.000485 --> 0.000485).\n",
      "Epoch 450: Train Loss: 0.0010, Macro_F1: 0.9269, AUC_score: 0.9748\n",
      "Epoch 500: Train Loss: 0.0006, Macro_F1: 0.9269, AUC_score: 0.9750\n",
      "Epoch 00541: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0009, Macro_F1: 0.9269, AUC_score: 0.9750\n",
      "Epoch 600: Train Loss: 0.0023, Macro_F1: 0.9269, AUC_score: 0.9750\n",
      "Early stopping triggered\n",
      "acc save\n",
      "5.999999999999995% node features transform to 0: F1: 0.9269, AUC_score: 0.9750\n",
      "Epoch 0: Train Loss: 0.0008, Macro_F1: 0.8949, AUC_score: 0.9776\n",
      "Validation loss decreased (0.000828 --> 0.000828).\n",
      "Epoch 50: Train Loss: 0.0012, Macro_F1: 0.9307, AUC_score: 0.9778\n",
      "Validation loss decreased (0.000631 --> 0.000631).\n",
      "Epoch 100: Train Loss: 0.0038, Macro_F1: 0.9269, AUC_score: 0.9764\n",
      "Epoch 150: Train Loss: 0.0041, Macro_F1: 0.9341, AUC_score: 0.9735\n",
      "Validation loss decreased (0.000511 --> 0.000511).\n",
      "Epoch 200: Train Loss: 0.0013, Macro_F1: 0.9307, AUC_score: 0.9747\n",
      "Epoch 250: Train Loss: 0.0017, Macro_F1: 0.9341, AUC_score: 0.9761\n",
      "Epoch 00271: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0011, Macro_F1: 0.9306, AUC_score: 0.9756\n",
      "Epoch 350: Train Loss: 0.0049, Macro_F1: 0.9340, AUC_score: 0.9745\n",
      "Validation loss decreased (0.000465 --> 0.000465).\n",
      "Epoch 400: Train Loss: 0.0012, Macro_F1: 0.9271, AUC_score: 0.9755\n",
      "Epoch 450: Train Loss: 0.0017, Macro_F1: 0.9307, AUC_score: 0.9757\n",
      "Epoch 00471: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.0010, Macro_F1: 0.9269, AUC_score: 0.9759\n",
      "Validation loss decreased (0.000434 --> 0.000434).\n",
      "Epoch 550: Train Loss: 0.0009, Macro_F1: 0.9232, AUC_score: 0.9754\n",
      "Epoch 600: Train Loss: 0.0024, Macro_F1: 0.9269, AUC_score: 0.9760\n",
      "Epoch 00625: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 650: Train Loss: 0.0047, Macro_F1: 0.9232, AUC_score: 0.9756\n",
      "Epoch 700: Train Loss: 0.0013, Macro_F1: 0.9232, AUC_score: 0.9757\n",
      "Early stopping triggered\n",
      "acc save\n",
      "4.999999999999993% node features transform to 0: F1: 0.9232, AUC_score: 0.9757\n",
      "Epoch 0: Train Loss: 0.0043, Macro_F1: 0.8514, AUC_score: 0.9769\n",
      "Validation loss decreased (0.004291 --> 0.004291).\n",
      "Validation loss decreased (0.001807 --> 0.001807).\n",
      "Epoch 50: Train Loss: 0.0013, Macro_F1: 0.9305, AUC_score: 0.9754\n",
      "Validation loss decreased (0.001279 --> 0.001279).\n",
      "Validation loss decreased (0.001116 --> 0.001116).\n",
      "Validation loss decreased (0.001031 --> 0.001031).\n",
      "Validation loss decreased (0.000995 --> 0.000995).\n",
      "Epoch 100: Train Loss: 0.0028, Macro_F1: 0.9377, AUC_score: 0.9769\n",
      "Validation loss decreased (0.000963 --> 0.000963).\n",
      "Validation loss decreased (0.000921 --> 0.000921).\n",
      "Epoch 150: Train Loss: 0.0010, Macro_F1: 0.9340, AUC_score: 0.9757\n",
      "Validation loss decreased (0.000735 --> 0.000735).\n",
      "Epoch 200: Train Loss: 0.0013, Macro_F1: 0.9305, AUC_score: 0.9768\n",
      "Epoch 250: Train Loss: 0.0020, Macro_F1: 0.9344, AUC_score: 0.9775\n",
      "Validation loss decreased (0.000677 --> 0.000677).\n",
      "Epoch 300: Train Loss: 0.0008, Macro_F1: 0.9233, AUC_score: 0.9753\n",
      "Validation loss decreased (0.000501 --> 0.000501).\n",
      "Epoch 350: Train Loss: 0.0047, Macro_F1: 0.9378, AUC_score: 0.9751\n",
      "Epoch 400: Train Loss: 0.0007, Macro_F1: 0.9307, AUC_score: 0.9756\n",
      "Epoch 00445: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 450: Train Loss: 0.0031, Macro_F1: 0.9268, AUC_score: 0.9728\n",
      "Epoch 500: Train Loss: 0.0017, Macro_F1: 0.9233, AUC_score: 0.9735\n",
      "Early stopping triggered\n",
      "acc save\n",
      "4.0000000000000036% node features transform to 0: F1: 0.9233, AUC_score: 0.9748\n",
      "Epoch 0: Train Loss: 0.0014, Macro_F1: 0.8254, AUC_score: 0.9641\n",
      "Validation loss decreased (0.001406 --> 0.001406).\n",
      "Epoch 50: Train Loss: 0.0018, Macro_F1: 0.9343, AUC_score: 0.9745\n",
      "Validation loss decreased (0.001357 --> 0.001357).\n",
      "Validation loss decreased (0.001316 --> 0.001316).\n",
      "Validation loss decreased (0.001278 --> 0.001278).\n",
      "Epoch 100: Train Loss: 0.0018, Macro_F1: 0.9270, AUC_score: 0.9761\n",
      "Validation loss decreased (0.001089 --> 0.001089).\n",
      "Validation loss decreased (0.001038 --> 0.001038).\n",
      "Epoch 150: Train Loss: 0.0017, Macro_F1: 0.9269, AUC_score: 0.9748\n",
      "Validation loss decreased (0.000842 --> 0.000842).\n",
      "Epoch 200: Train Loss: 0.0032, Macro_F1: 0.9269, AUC_score: 0.9752\n",
      "Validation loss decreased (0.000733 --> 0.000733).\n",
      "Epoch 250: Train Loss: 0.0023, Macro_F1: 0.9271, AUC_score: 0.9760\n",
      "Epoch 300: Train Loss: 0.0014, Macro_F1: 0.9270, AUC_score: 0.9753\n",
      "Epoch 350: Train Loss: 0.0050, Macro_F1: 0.9232, AUC_score: 0.9745\n",
      "Epoch 00368: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.000711 --> 0.000711).\n",
      "Epoch 400: Train Loss: 0.0050, Macro_F1: 0.9196, AUC_score: 0.9743\n",
      "Validation loss decreased (0.000598 --> 0.000598).\n",
      "Epoch 450: Train Loss: 0.0019, Macro_F1: 0.9233, AUC_score: 0.9750\n",
      "Validation loss decreased (0.000501 --> 0.000501).\n",
      "Epoch 500: Train Loss: 0.0010, Macro_F1: 0.9270, AUC_score: 0.9756\n",
      "Epoch 550: Train Loss: 0.0016, Macro_F1: 0.9233, AUC_score: 0.9752\n",
      "Epoch 00594: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 600: Train Loss: 0.0007, Macro_F1: 0.9233, AUC_score: 0.9753\n",
      "Epoch 650: Train Loss: 0.0012, Macro_F1: 0.9196, AUC_score: 0.9752\n",
      "Validation loss decreased (0.000477 --> 0.000477).\n",
      "Epoch 700: Train Loss: 0.0016, Macro_F1: 0.9196, AUC_score: 0.9755\n",
      "Epoch 750: Train Loss: 0.0015, Macro_F1: 0.9232, AUC_score: 0.9750\n",
      "Epoch 00781: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Validation loss decreased (0.000455 --> 0.000455).\n",
      "Epoch 800: Train Loss: 0.0008, Macro_F1: 0.9233, AUC_score: 0.9754\n",
      "Epoch 850: Train Loss: 0.0012, Macro_F1: 0.9196, AUC_score: 0.9753\n",
      "Epoch 00890: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 900: Train Loss: 0.0035, Macro_F1: 0.9196, AUC_score: 0.9753\n",
      "Epoch 950: Train Loss: 0.0011, Macro_F1: 0.9196, AUC_score: 0.9754\n",
      "Early stopping triggered\n",
      "acc save\n",
      "3.0000000000000027% node features transform to 0: F1: 0.9196, AUC_score: 0.9754\n",
      "Epoch 0: Train Loss: 0.0024, Macro_F1: 0.8329, AUC_score: 0.9764\n",
      "Validation loss decreased (0.002381 --> 0.002381).\n",
      "Validation loss decreased (0.002339 --> 0.002339).\n",
      "Validation loss decreased (0.002272 --> 0.002272).\n",
      "Validation loss decreased (0.001730 --> 0.001730).\n",
      "Epoch 50: Train Loss: 0.0015, Macro_F1: 0.9306, AUC_score: 0.9760\n",
      "Validation loss decreased (0.001495 --> 0.001495).\n",
      "Validation loss decreased (0.001301 --> 0.001301).\n",
      "Validation loss decreased (0.001173 --> 0.001173).\n",
      "Epoch 100: Train Loss: 0.0026, Macro_F1: 0.9270, AUC_score: 0.9751\n",
      "Validation loss decreased (0.001002 --> 0.001002).\n",
      "Epoch 150: Train Loss: 0.0015, Macro_F1: 0.9270, AUC_score: 0.9758\n",
      "Validation loss decreased (0.000816 --> 0.000816).\n",
      "Validation loss decreased (0.000779 --> 0.000779).\n",
      "Validation loss decreased (0.000677 --> 0.000677).\n",
      "Epoch 200: Train Loss: 0.0024, Macro_F1: 0.9344, AUC_score: 0.9763\n",
      "Epoch 250: Train Loss: 0.0016, Macro_F1: 0.9307, AUC_score: 0.9762\n",
      "Epoch 00273: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0009, Macro_F1: 0.9270, AUC_score: 0.9756\n",
      "Validation loss decreased (0.000568 --> 0.000568).\n",
      "Epoch 350: Train Loss: 0.0022, Macro_F1: 0.9233, AUC_score: 0.9745\n",
      "Epoch 400: Train Loss: 0.0006, Macro_F1: 0.9269, AUC_score: 0.9750\n",
      "Epoch 00425: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.0012, Macro_F1: 0.9233, AUC_score: 0.9752\n",
      "Epoch 500: Train Loss: 0.0013, Macro_F1: 0.9233, AUC_score: 0.9755\n",
      "Early stopping triggered\n",
      "acc save\n",
      "2.0000000000000018% node features transform to 0: F1: 0.9233, AUC_score: 0.9755\n",
      "Epoch 0: Train Loss: 0.0008, Macro_F1: 0.9163, AUC_score: 0.9739\n",
      "Validation loss decreased (0.000773 --> 0.000773).\n",
      "Epoch 50: Train Loss: 0.0022, Macro_F1: 0.9307, AUC_score: 0.9732\n",
      "Epoch 100: Train Loss: 0.0012, Macro_F1: 0.9234, AUC_score: 0.9753\n",
      "Epoch 00102: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.000697 --> 0.000697).\n",
      "Validation loss decreased (0.000556 --> 0.000556).\n",
      "Epoch 150: Train Loss: 0.0020, Macro_F1: 0.9232, AUC_score: 0.9748\n",
      "Epoch 200: Train Loss: 0.0024, Macro_F1: 0.9306, AUC_score: 0.9748\n",
      "Epoch 00229: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.000467 --> 0.000467).\n",
      "Epoch 250: Train Loss: 0.0031, Macro_F1: 0.9269, AUC_score: 0.9751\n",
      "Epoch 300: Train Loss: 0.0008, Macro_F1: 0.9269, AUC_score: 0.9755\n",
      "Epoch 00339: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 350: Train Loss: 0.0006, Macro_F1: 0.9269, AUC_score: 0.9756\n",
      "Validation loss decreased (0.000374 --> 0.000374).\n",
      "Epoch 400: Train Loss: 0.0008, Macro_F1: 0.9269, AUC_score: 0.9756\n",
      "Epoch 450: Train Loss: 0.0010, Macro_F1: 0.9269, AUC_score: 0.9756\n",
      "Epoch 00474: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 500: Train Loss: 0.0007, Macro_F1: 0.9269, AUC_score: 0.9756\n",
      "Epoch 550: Train Loss: 0.0010, Macro_F1: 0.9269, AUC_score: 0.9756\n",
      "Early stopping triggered\n",
      "acc save\n",
      "1.0000000000000009% node features transform to 0: F1: 0.9269, AUC_score: 0.9757\n",
      "Epoch 0: Train Loss: 0.0009, Macro_F1: 0.8545, AUC_score: 0.9650\n",
      "Validation loss decreased (0.000904 --> 0.000904).\n",
      "Epoch 50: Train Loss: 0.0019, Macro_F1: 0.9379, AUC_score: 0.9747\n",
      "Epoch 100: Train Loss: 0.0011, Macro_F1: 0.9377, AUC_score: 0.9758\n",
      "Epoch 00102: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.000760 --> 0.000760).\n",
      "Epoch 150: Train Loss: 0.0010, Macro_F1: 0.9378, AUC_score: 0.9763\n",
      "Validation loss decreased (0.000706 --> 0.000706).\n",
      "Epoch 200: Train Loss: 0.0008, Macro_F1: 0.9378, AUC_score: 0.9762\n",
      "Epoch 250: Train Loss: 0.0012, Macro_F1: 0.9378, AUC_score: 0.9763\n",
      "Epoch 00254: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.000684 --> 0.000684).\n",
      "Epoch 300: Train Loss: 0.0011, Macro_F1: 0.9378, AUC_score: 0.9768\n",
      "Epoch 350: Train Loss: 0.0014, Macro_F1: 0.9378, AUC_score: 0.9767\n",
      "Epoch 00356: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Validation loss decreased (0.000610 --> 0.000610).\n",
      "Epoch 400: Train Loss: 0.0023, Macro_F1: 0.9378, AUC_score: 0.9767\n",
      "Epoch 450: Train Loss: 0.0014, Macro_F1: 0.9378, AUC_score: 0.9767\n",
      "Epoch 00486: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 500: Train Loss: 0.0011, Macro_F1: 0.9378, AUC_score: 0.9767\n",
      "Epoch 550: Train Loss: 0.0021, Macro_F1: 0.9378, AUC_score: 0.9767\n",
      "Early stopping triggered\n",
      "acc save\n",
      "0.0% node features transform to 0: F1: 0.9378, AUC_score: 0.9767\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1')\n",
    "data = data.to(device)\n",
    "results = []\n",
    "num_epochs = 1000\n",
    "# 加载数据\n",
    "features = data.x\n",
    "labels = data.y # 根据你的数据加载函数进行调整\n",
    "num_iterations = 100\n",
    "original_features = features.clone()\n",
    "# 总共需要迭代的次数，这里以逐步增加5%为例，直到100%\n",
    "model = GCN(num_features=data.x.shape[1], hidden_dim=64, num_classes=2, num_layers=3, activation=F.relu, dropout=0.5)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=50, verbose=True)\n",
    "for i in range(num_iterations):\n",
    "    # 加载这次迭代的selected_indices和remaining_indices\n",
    "    selected_indices = np.load(f'DIVIDED_DATA/without_{i}%.npy')\n",
    "    \n",
    "    # 根据selected_indices和remaining_indices调整特征\n",
    "    masked_features = features.clone()\n",
    "    masked_features[torch.tensor(selected_indices)] = 0  # 假设features是一个PyTorch tensor\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=200, verbose=True, delta=0.00001)\n",
    "    \n",
    "    if i > 0:\n",
    "        # 从上一个迭代保存的模型中加载参数\n",
    "        model.load_state_dict(torch.load(f'G-G_DATA/G-G_model_WITHOUT{i-1}.pth'))\n",
    "        # 重新初始化优化器和调度器\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=100, verbose=True)\n",
    "    \n",
    "    # 模型训练和评估逻辑\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model_scheduler(model, masked_features, data.y, data.edge_index, optimizer, loss_fn, scheduler, train_mask)\n",
    "        test_f1, test_auc = evaluate_model(model, masked_features, data.y, data.edge_index, test_mask)\n",
    "        #high_f1, high_auc = evaluate_model(model, data.x, data.y, data.edge_index, test_high)\n",
    "        #low_f1, low_auc = evaluate_model(model, data.x, data.y, data.edge_index, test_low)\n",
    "        if epoch % 50 == 0:  # 每10个epoch打印一次信息\n",
    "            print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Macro_F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "        early_stopping(train_loss, model, i)\n",
    "        if early_stopping.early_stop or epoch == num_epochs - 1:\n",
    "            results.append({\n",
    "                            'Train Loss': train_loss,\n",
    "                            'F1': test_f1,\n",
    "                            'AUC_score': test_auc\n",
    "                        })\n",
    "                            #'highinfo F1': high_f1,                                                            \n",
    "                            #'highinfo AUC_score': high_auc,\n",
    "                            #'lowinfo F1': low_f1,\n",
    "                            #'lowinfo AUC_score': low_auc\n",
    "            print(\"acc save\")\n",
    "            rate = 1 - count * (i + 1)\n",
    "            rate = rate * 100\n",
    "            print(f'{rate}% node features transform to 0: F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "            torch.save(model.state_dict(), f'G-G_DATA/G-G_model_WITHOUT{i}.pth')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2e1a19e-5bd1-4be1-b80a-9eb7deecd42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from numpy import array, float32\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, np.generic):\n",
    "            return obj.item()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "\n",
    "# 保存为 JSON 文件\n",
    "with open('GNN/zeroed_GO_network.json', 'w') as f:\n",
    "    json.dump(results, f, cls=NumpyEncoder, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1de92cdf-c5e2-471c-961e-277b437d3570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1103\n",
      "tensor([False, False, False,  ..., False, False, False])\n"
     ]
    }
   ],
   "source": [
    "print(num_train)\n",
    "print(train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f324d33-db3d-4a84-89e3-54373bddf9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss:0.9409, Macro_F1: 0.3191, AUC_score: 0.8265\n",
      "Epoch 50: Train Loss:0.2024, Macro_F1: 0.9344, AUC_score: 0.9660\n",
      "Epoch 100: Train Loss:0.1137, Macro_F1: 0.9308, AUC_score: 0.9674\n",
      "Epoch 150: Train Loss:0.0977, Macro_F1: 0.9417, AUC_score: 0.9679\n",
      "Epoch 200: Train Loss:0.0781, Macro_F1: 0.9378, AUC_score: 0.9666\n",
      "Epoch 250: Train Loss:0.0966, Macro_F1: 0.9237, AUC_score: 0.9645\n",
      "Epoch 300: Train Loss:0.0590, Macro_F1: 0.9310, AUC_score: 0.9746\n",
      "Epoch 350: Train Loss:0.0691, Macro_F1: 0.9378, AUC_score: 0.9726\n",
      "Epoch 400: Train Loss:0.0365, Macro_F1: 0.9272, AUC_score: 0.9756\n",
      "Epoch 450: Train Loss:0.0414, Macro_F1: 0.9165, AUC_score: 0.9768\n",
      "Epoch 500: Train Loss:0.0392, Macro_F1: 0.9163, AUC_score: 0.9732\n",
      "Epoch 550: Train Loss:0.0268, Macro_F1: 0.9201, AUC_score: 0.9774\n",
      "Epoch 600: Train Loss:0.0295, Macro_F1: 0.9124, AUC_score: 0.9618\n",
      "Epoch 650: Train Loss:0.0218, Macro_F1: 0.9232, AUC_score: 0.9732\n",
      "Epoch 00686: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 700: Train Loss:0.0183, Macro_F1: 0.9197, AUC_score: 0.9705\n",
      "Epoch 750: Train Loss:0.0231, Macro_F1: 0.9234, AUC_score: 0.9733\n",
      "Epoch 800: Train Loss:0.0272, Macro_F1: 0.9196, AUC_score: 0.9712\n",
      "Epoch 850: Train Loss:0.0264, Macro_F1: 0.9125, AUC_score: 0.9712\n",
      "Epoch 00864: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 900: Train Loss:0.0199, Macro_F1: 0.9196, AUC_score: 0.9682\n",
      "Epoch 950: Train Loss:0.0270, Macro_F1: 0.9160, AUC_score: 0.9699\n",
      "Epoch 00993: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 1000: Train Loss:0.0152, Macro_F1: 0.9161, AUC_score: 0.9709\n",
      "Epoch 1050: Train Loss:0.0185, Macro_F1: 0.9160, AUC_score: 0.9699\n",
      "Epoch 1100: Train Loss:0.0190, Macro_F1: 0.9197, AUC_score: 0.9708\n",
      "Epoch 1150: Train Loss:0.0129, Macro_F1: 0.9197, AUC_score: 0.9708\n",
      "Epoch 1200: Train Loss:0.0189, Macro_F1: 0.9160, AUC_score: 0.9705\n",
      "Epoch 1250: Train Loss:0.0148, Macro_F1: 0.9197, AUC_score: 0.9711\n",
      "Epoch 01258: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 1300: Train Loss:0.0192, Macro_F1: 0.9197, AUC_score: 0.9712\n",
      "Epoch 1350: Train Loss:0.0156, Macro_F1: 0.9197, AUC_score: 0.9711\n",
      "Epoch 01359: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 1400: Train Loss:0.0147, Macro_F1: 0.9197, AUC_score: 0.9711\n",
      "Epoch 1450: Train Loss:0.0203, Macro_F1: 0.9197, AUC_score: 0.9711\n",
      "Epoch 01460: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 1500: Train Loss:0.0195, Macro_F1: 0.9197, AUC_score: 0.9711\n",
      "Epoch 1550: Train Loss:0.0146, Macro_F1: 0.9197, AUC_score: 0.9711\n",
      "Epoch 01561: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 1600: Train Loss:0.0206, Macro_F1: 0.9197, AUC_score: 0.9711\n",
      "Epoch 1650: Train Loss:0.0238, Macro_F1: 0.9197, AUC_score: 0.9711\n",
      "Epoch 01662: reducing learning rate of group 0 to 1.2800e-08.\n",
      "Epoch 1700: Train Loss:0.0153, Macro_F1: 0.9197, AUC_score: 0.9711\n",
      "Epoch 1750: Train Loss:0.0157, Macro_F1: 0.9197, AUC_score: 0.9711\n",
      "Epoch 01763: reducing learning rate of group 0 to 2.5600e-09.\n",
      "Epoch 1800: Train Loss:0.0292, Macro_F1: 0.9197, AUC_score: 0.9711\n",
      "Epoch 1850: Train Loss:0.0330, Macro_F1: 0.9197, AUC_score: 0.9711\n",
      "Epoch 1900: Train Loss:0.0243, Macro_F1: 0.9197, AUC_score: 0.9711\n",
      "Epoch 1950: Train Loss:0.0319, Macro_F1: 0.9197, AUC_score: 0.9711\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_model_scheduler(model, data.x, data.y, data.edge_index, optimizer, loss_fn, scheduler, train_mask)\n",
    "    test_f1, test_auc = evaluate_model(model, data.x, data.y, data.edge_index, test_mask)\n",
    "    \n",
    "    if epoch % 50 == 0: \n",
    "        print(f'Epoch {epoch}: Train Loss:{train_loss:.4f}, Macro_F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a65440ac-c6c3-4a21-a3ff-89785f4d6df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_layers=1\n",
    "n_heads = 8\n",
    "heads = ([n_heads] * num_layers) + [1]\n",
    "heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0eea03b0-21ec-4b4d-9e6b-6d8e94bf8739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3667016/3157765969.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_indices = torch.tensor(label_indices, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2316, 6630, 4048,  ..., 5884, 5710, 7093])\n",
      "276\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 0: Train Loss: 0.7221, Macro_F1: 0.6192, AUC_score: 0.7652\n",
      "Epoch 10: Train Loss: 0.4897, Macro_F1: 0.8261, AUC_score: 0.9151\n",
      "Epoch 20: Train Loss: 0.4087, Macro_F1: 0.8687, AUC_score: 0.9277\n",
      "Epoch 30: Train Loss: 0.3778, Macro_F1: 0.8764, AUC_score: 0.9368\n",
      "Epoch 40: Train Loss: 0.3309, Macro_F1: 0.8983, AUC_score: 0.9407\n",
      "Epoch 50: Train Loss: 0.3248, Macro_F1: 0.9091, AUC_score: 0.9432\n",
      "Epoch 60: Train Loss: 0.3187, Macro_F1: 0.9092, AUC_score: 0.9435\n",
      "Epoch 70: Train Loss: 0.3118, Macro_F1: 0.9165, AUC_score: 0.9478\n",
      "Epoch 80: Train Loss: 0.2821, Macro_F1: 0.9236, AUC_score: 0.9480\n",
      "Epoch 90: Train Loss: 0.2945, Macro_F1: 0.9165, AUC_score: 0.9496\n",
      "Epoch 100: Train Loss: 0.3178, Macro_F1: 0.9273, AUC_score: 0.9500\n",
      "Epoch 110: Train Loss: 0.2957, Macro_F1: 0.9129, AUC_score: 0.9480\n",
      "Epoch 120: Train Loss: 0.2959, Macro_F1: 0.9165, AUC_score: 0.9470\n",
      "Epoch 130: Train Loss: 0.2609, Macro_F1: 0.9309, AUC_score: 0.9509\n",
      "Epoch 140: Train Loss: 0.2498, Macro_F1: 0.9345, AUC_score: 0.9534\n",
      "Epoch 150: Train Loss: 0.2694, Macro_F1: 0.9273, AUC_score: 0.9542\n",
      "Epoch 160: Train Loss: 0.2878, Macro_F1: 0.9201, AUC_score: 0.9558\n",
      "Epoch 170: Train Loss: 0.2495, Macro_F1: 0.9237, AUC_score: 0.9553\n",
      "Epoch 180: Train Loss: 0.2433, Macro_F1: 0.9273, AUC_score: 0.9545\n",
      "Epoch 190: Train Loss: 0.2266, Macro_F1: 0.9273, AUC_score: 0.9559\n",
      "Epoch 200: Train Loss: 0.2433, Macro_F1: 0.9273, AUC_score: 0.9604\n",
      "Epoch 210: Train Loss: 0.2491, Macro_F1: 0.9310, AUC_score: 0.9595\n",
      "Epoch 220: Train Loss: 0.2359, Macro_F1: 0.9237, AUC_score: 0.9582\n",
      "Epoch 230: Train Loss: 0.2307, Macro_F1: 0.9309, AUC_score: 0.9579\n",
      "Epoch 240: Train Loss: 0.2304, Macro_F1: 0.9273, AUC_score: 0.9594\n",
      "Epoch 250: Train Loss: 0.2345, Macro_F1: 0.9310, AUC_score: 0.9592\n",
      "Epoch 260: Train Loss: 0.2199, Macro_F1: 0.9273, AUC_score: 0.9594\n",
      "Epoch 270: Train Loss: 0.2326, Macro_F1: 0.9273, AUC_score: 0.9598\n",
      "Epoch 280: Train Loss: 0.2233, Macro_F1: 0.9273, AUC_score: 0.9622\n",
      "Epoch 290: Train Loss: 0.2098, Macro_F1: 0.9273, AUC_score: 0.9628\n",
      "Epoch 300: Train Loss: 0.2167, Macro_F1: 0.9273, AUC_score: 0.9627\n",
      "Epoch 310: Train Loss: 0.2149, Macro_F1: 0.9309, AUC_score: 0.9672\n",
      "Epoch 320: Train Loss: 0.1987, Macro_F1: 0.9309, AUC_score: 0.9660\n",
      "Epoch 330: Train Loss: 0.1992, Macro_F1: 0.9273, AUC_score: 0.9640\n",
      "Epoch 340: Train Loss: 0.1969, Macro_F1: 0.9309, AUC_score: 0.9634\n",
      "Epoch 350: Train Loss: 0.1908, Macro_F1: 0.9345, AUC_score: 0.9648\n",
      "Epoch 360: Train Loss: 0.1991, Macro_F1: 0.9274, AUC_score: 0.9636\n",
      "Epoch 370: Train Loss: 0.1861, Macro_F1: 0.9309, AUC_score: 0.9647\n",
      "Epoch 380: Train Loss: 0.2247, Macro_F1: 0.9309, AUC_score: 0.9673\n",
      "Epoch 390: Train Loss: 0.1966, Macro_F1: 0.9310, AUC_score: 0.9711\n",
      "Epoch 400: Train Loss: 0.1788, Macro_F1: 0.9381, AUC_score: 0.9704\n",
      "Epoch 410: Train Loss: 0.1870, Macro_F1: 0.9347, AUC_score: 0.9670\n",
      "Epoch 420: Train Loss: 0.2001, Macro_F1: 0.9310, AUC_score: 0.9716\n",
      "Epoch 430: Train Loss: 0.1792, Macro_F1: 0.9381, AUC_score: 0.9696\n",
      "Epoch 440: Train Loss: 0.1880, Macro_F1: 0.9346, AUC_score: 0.9675\n",
      "Epoch 450: Train Loss: 0.1772, Macro_F1: 0.9309, AUC_score: 0.9710\n",
      "Epoch 460: Train Loss: 0.1653, Macro_F1: 0.9345, AUC_score: 0.9716\n",
      "Epoch 470: Train Loss: 0.1849, Macro_F1: 0.9345, AUC_score: 0.9717\n",
      "Epoch 480: Train Loss: 0.1651, Macro_F1: 0.9310, AUC_score: 0.9693\n",
      "Epoch 490: Train Loss: 0.1688, Macro_F1: 0.9382, AUC_score: 0.9716\n",
      "Epoch 500: Train Loss: 0.1679, Macro_F1: 0.9345, AUC_score: 0.9749\n",
      "Epoch 510: Train Loss: 0.1548, Macro_F1: 0.9345, AUC_score: 0.9739\n",
      "Epoch 520: Train Loss: 0.1601, Macro_F1: 0.9346, AUC_score: 0.9744\n",
      "Epoch 530: Train Loss: 0.1563, Macro_F1: 0.9345, AUC_score: 0.9759\n",
      "Epoch 540: Train Loss: 0.1585, Macro_F1: 0.9382, AUC_score: 0.9733\n",
      "Epoch 550: Train Loss: 0.1473, Macro_F1: 0.9310, AUC_score: 0.9768\n",
      "Epoch 560: Train Loss: 0.1544, Macro_F1: 0.9345, AUC_score: 0.9777\n",
      "Epoch 570: Train Loss: 0.1451, Macro_F1: 0.9382, AUC_score: 0.9738\n",
      "Epoch 580: Train Loss: 0.1541, Macro_F1: 0.9418, AUC_score: 0.9774\n",
      "Epoch 590: Train Loss: 0.1555, Macro_F1: 0.9382, AUC_score: 0.9758\n",
      "Epoch 600: Train Loss: 0.1408, Macro_F1: 0.9382, AUC_score: 0.9764\n",
      "Epoch 610: Train Loss: 0.1523, Macro_F1: 0.9310, AUC_score: 0.9750\n",
      "Epoch 620: Train Loss: 0.1495, Macro_F1: 0.9346, AUC_score: 0.9749\n",
      "Epoch 630: Train Loss: 0.1568, Macro_F1: 0.9381, AUC_score: 0.9781\n",
      "Epoch 640: Train Loss: 0.1508, Macro_F1: 0.9382, AUC_score: 0.9759\n",
      "Epoch 650: Train Loss: 0.1446, Macro_F1: 0.9346, AUC_score: 0.9730\n",
      "Epoch 660: Train Loss: 0.1400, Macro_F1: 0.9454, AUC_score: 0.9782\n",
      "Epoch 670: Train Loss: 0.1533, Macro_F1: 0.9382, AUC_score: 0.9755\n",
      "Epoch 680: Train Loss: 0.1501, Macro_F1: 0.9418, AUC_score: 0.9735\n",
      "Epoch 690: Train Loss: 0.1319, Macro_F1: 0.9382, AUC_score: 0.9751\n",
      "Epoch 700: Train Loss: 0.1402, Macro_F1: 0.9418, AUC_score: 0.9744\n",
      "Epoch 710: Train Loss: 0.1326, Macro_F1: 0.9310, AUC_score: 0.9725\n",
      "Epoch 720: Train Loss: 0.1371, Macro_F1: 0.9454, AUC_score: 0.9756\n",
      "Epoch 730: Train Loss: 0.1280, Macro_F1: 0.9418, AUC_score: 0.9752\n",
      "Epoch 740: Train Loss: 0.1280, Macro_F1: 0.9454, AUC_score: 0.9764\n",
      "Epoch 750: Train Loss: 0.1310, Macro_F1: 0.9454, AUC_score: 0.9735\n",
      "Epoch 760: Train Loss: 0.1146, Macro_F1: 0.9418, AUC_score: 0.9722\n",
      "Epoch 770: Train Loss: 0.1294, Macro_F1: 0.9454, AUC_score: 0.9737\n",
      "Epoch 780: Train Loss: 0.1153, Macro_F1: 0.9491, AUC_score: 0.9743\n",
      "Epoch 790: Train Loss: 0.1263, Macro_F1: 0.9454, AUC_score: 0.9737\n",
      "Epoch 800: Train Loss: 0.1342, Macro_F1: 0.9454, AUC_score: 0.9733\n",
      "Epoch 810: Train Loss: 0.1083, Macro_F1: 0.9491, AUC_score: 0.9762\n",
      "Epoch 00819: reducing learning rate of group 0 to 2.0000e-05.\n",
      "Epoch 820: Train Loss: 0.1198, Macro_F1: 0.9418, AUC_score: 0.9744\n",
      "Epoch 830: Train Loss: 0.1171, Macro_F1: 0.9454, AUC_score: 0.9751\n",
      "Epoch 840: Train Loss: 0.1328, Macro_F1: 0.9454, AUC_score: 0.9759\n",
      "Epoch 850: Train Loss: 0.1346, Macro_F1: 0.9454, AUC_score: 0.9743\n",
      "Epoch 860: Train Loss: 0.1321, Macro_F1: 0.9454, AUC_score: 0.9735\n",
      "Epoch 870: Train Loss: 0.1195, Macro_F1: 0.9418, AUC_score: 0.9743\n",
      "Epoch 880: Train Loss: 0.1014, Macro_F1: 0.9418, AUC_score: 0.9729\n",
      "Epoch 890: Train Loss: 0.1294, Macro_F1: 0.9454, AUC_score: 0.9736\n",
      "Epoch 900: Train Loss: 0.1239, Macro_F1: 0.9454, AUC_score: 0.9736\n",
      "Epoch 910: Train Loss: 0.1239, Macro_F1: 0.9454, AUC_score: 0.9742\n",
      "Epoch 920: Train Loss: 0.1085, Macro_F1: 0.9491, AUC_score: 0.9746\n",
      "Epoch 930: Train Loss: 0.1142, Macro_F1: 0.9491, AUC_score: 0.9744\n",
      "Epoch 00935: reducing learning rate of group 0 to 4.0000e-06.\n",
      "Epoch 940: Train Loss: 0.1099, Macro_F1: 0.9454, AUC_score: 0.9739\n",
      "Epoch 950: Train Loss: 0.1014, Macro_F1: 0.9454, AUC_score: 0.9739\n",
      "Epoch 960: Train Loss: 0.1177, Macro_F1: 0.9454, AUC_score: 0.9737\n",
      "Epoch 970: Train Loss: 0.1179, Macro_F1: 0.9454, AUC_score: 0.9734\n",
      "Epoch 980: Train Loss: 0.1067, Macro_F1: 0.9454, AUC_score: 0.9734\n",
      "Epoch 990: Train Loss: 0.1119, Macro_F1: 0.9454, AUC_score: 0.9734\n",
      "Epoch 1000: Train Loss: 0.1186, Macro_F1: 0.9454, AUC_score: 0.9734\n",
      "Epoch 1010: Train Loss: 0.1337, Macro_F1: 0.9454, AUC_score: 0.9734\n",
      "Epoch 1020: Train Loss: 0.1131, Macro_F1: 0.9454, AUC_score: 0.9733\n",
      "Epoch 1030: Train Loss: 0.1144, Macro_F1: 0.9454, AUC_score: 0.9733\n",
      "Epoch 1040: Train Loss: 0.1167, Macro_F1: 0.9454, AUC_score: 0.9734\n",
      "Epoch 1050: Train Loss: 0.1125, Macro_F1: 0.9454, AUC_score: 0.9734\n",
      "Epoch 1060: Train Loss: 0.1032, Macro_F1: 0.9454, AUC_score: 0.9734\n",
      "Epoch 1070: Train Loss: 0.1174, Macro_F1: 0.9454, AUC_score: 0.9734\n",
      "Epoch 1080: Train Loss: 0.1086, Macro_F1: 0.9454, AUC_score: 0.9729\n",
      "Epoch 1090: Train Loss: 0.1187, Macro_F1: 0.9454, AUC_score: 0.9729\n",
      "Epoch 1100: Train Loss: 0.1102, Macro_F1: 0.9454, AUC_score: 0.9734\n",
      "Epoch 01106: reducing learning rate of group 0 to 8.0000e-07.\n",
      "Epoch 1110: Train Loss: 0.1113, Macro_F1: 0.9454, AUC_score: 0.9735\n",
      "Epoch 1120: Train Loss: 0.1127, Macro_F1: 0.9491, AUC_score: 0.9736\n",
      "Epoch 1130: Train Loss: 0.1156, Macro_F1: 0.9491, AUC_score: 0.9736\n",
      "Epoch 1140: Train Loss: 0.1064, Macro_F1: 0.9454, AUC_score: 0.9734\n",
      "Epoch 1150: Train Loss: 0.1063, Macro_F1: 0.9454, AUC_score: 0.9733\n",
      "Epoch 1160: Train Loss: 0.1219, Macro_F1: 0.9454, AUC_score: 0.9733\n",
      "Epoch 1170: Train Loss: 0.1025, Macro_F1: 0.9454, AUC_score: 0.9734\n",
      "Epoch 1180: Train Loss: 0.1159, Macro_F1: 0.9454, AUC_score: 0.9734\n",
      "Epoch 1190: Train Loss: 0.1130, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1200: Train Loss: 0.1160, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1210: Train Loss: 0.1106, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1220: Train Loss: 0.1272, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1230: Train Loss: 0.1142, Macro_F1: 0.9454, AUC_score: 0.9735\n",
      "Epoch 1240: Train Loss: 0.1111, Macro_F1: 0.9454, AUC_score: 0.9735\n",
      "Epoch 01251: reducing learning rate of group 0 to 1.6000e-07.\n",
      "Epoch 1250: Train Loss: 0.1092, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1260: Train Loss: 0.1193, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1270: Train Loss: 0.1168, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1280: Train Loss: 0.1315, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1290: Train Loss: 0.1238, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1300: Train Loss: 0.1178, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1310: Train Loss: 0.1137, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1320: Train Loss: 0.1122, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1330: Train Loss: 0.1179, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1340: Train Loss: 0.1123, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1350: Train Loss: 0.1272, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 01352: reducing learning rate of group 0 to 3.2000e-08.\n",
      "Epoch 1360: Train Loss: 0.0967, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1370: Train Loss: 0.1126, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1380: Train Loss: 0.1369, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1390: Train Loss: 0.1228, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1400: Train Loss: 0.0951, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1410: Train Loss: 0.1261, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1420: Train Loss: 0.1157, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1430: Train Loss: 0.1131, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1440: Train Loss: 0.1042, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1450: Train Loss: 0.1098, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 01453: reducing learning rate of group 0 to 6.4000e-09.\n",
      "Epoch 1460: Train Loss: 0.1158, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1470: Train Loss: 0.1241, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1480: Train Loss: 0.1181, Macro_F1: 0.9491, AUC_score: 0.9735\n",
      "Epoch 1490: Train Loss: 0.1018, Macro_F1: 0.9491, AUC_score: 0.9735\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "device = torch.device('cuda:1')\n",
    "data = data.to(device)\n",
    "\n",
    "model = GAT(num_layers=1, \n",
    "            in_dim=features.shape[1], \n",
    "            num_hidden=64, \n",
    "            num_classes=2, \n",
    "            heads = heads, \n",
    "            activation=F.elu, dropout=0.6, negative_slope=0.2, residual=True).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.001)#0.0005\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=100, verbose=True)\n",
    "\n",
    "label_indices = torch.tensor(label_indices, dtype=torch.long)\n",
    "print(label_indices)\n",
    "# 随机打乱有标签的节点索引\n",
    "labeled_indices = label_indices[torch.randperm(label_indices.size(0))]\n",
    "#print(labeled_indices)\n",
    "labeled_indices = label_indices\n",
    "\n",
    "# 定义训练和测试集的大小\n",
    "num_labeled = labeled_indices.size(0)\n",
    "num_train = int(num_labeled * 0.8)\n",
    "num_test = num_labeled - num_train\n",
    "print(num_test)\n",
    "\n",
    "# 创建训练和测试掩码\n",
    "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "\n",
    "train_mask[labeled_indices[:num_train]] = True\n",
    "test_mask[labeled_indices[num_train:num_train+num_test]] = True\n",
    "print(test_mask)\n",
    "num_epochs = 1500\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_model_scheduler(model, data.x, data.y, data.edge_index, optimizer, loss_fn, scheduler, train_mask)\n",
    "    #train_loss = train_model(model, data.x, data.y, data.edge_index, optimizer, loss_fn, train_mask)\n",
    "    test_acc, test_auc = evaluate_model(model, data.x, data.y, data.edge_index, test_mask)\n",
    "    \n",
    "    if epoch % 10 == 0: \n",
    "        print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Macro_F1: {test_acc:.4f}, AUC_score: {test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c425c2f3-6e51-42d0-b9e3-882dbbf6d3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5144,\n",
       " 4197,\n",
       " 664,\n",
       " 11362,\n",
       " 236,\n",
       " 8828,\n",
       " 1834,\n",
       " 3013,\n",
       " 2986,\n",
       " 10862,\n",
       " 2476,\n",
       " 2730,\n",
       " 3943,\n",
       " 1124,\n",
       " 402,\n",
       " 2455,\n",
       " 4616,\n",
       " 9836,\n",
       " 8139,\n",
       " 13295,\n",
       " 6528,\n",
       " 8356,\n",
       " 13083,\n",
       " 1327,\n",
       " 7108,\n",
       " 5146,\n",
       " 116,\n",
       " 13186,\n",
       " 10443,\n",
       " 894,\n",
       " 8231,\n",
       " 14052,\n",
       " 7139,\n",
       " 9937,\n",
       " 2556,\n",
       " 586,\n",
       " 3454,\n",
       " 4935,\n",
       " 5351,\n",
       " 9013,\n",
       " 9639,\n",
       " 5866,\n",
       " 12272,\n",
       " 3959,\n",
       " 596,\n",
       " 2308,\n",
       " 2277,\n",
       " 7065,\n",
       " 9535,\n",
       " 11609,\n",
       " 10090,\n",
       " 4040,\n",
       " 9659,\n",
       " 8865,\n",
       " 4526,\n",
       " 3707,\n",
       " 1849,\n",
       " 13582,\n",
       " 8567,\n",
       " 3797,\n",
       " 6885,\n",
       " 11340,\n",
       " 321,\n",
       " 6038,\n",
       " 10151,\n",
       " 13924,\n",
       " 6168,\n",
       " 11984,\n",
       " 8653,\n",
       " 5703,\n",
       " 1170,\n",
       " 5556,\n",
       " 8632,\n",
       " 1960,\n",
       " 5589,\n",
       " 5736,\n",
       " 14325,\n",
       " 6886,\n",
       " 4417,\n",
       " 4721,\n",
       " 8716,\n",
       " 5634,\n",
       " 11345,\n",
       " 3810,\n",
       " 13978,\n",
       " 3064,\n",
       " 11022,\n",
       " 14117,\n",
       " 12954,\n",
       " 1609,\n",
       " 4169,\n",
       " 8266,\n",
       " 6782,\n",
       " 4760,\n",
       " 8595,\n",
       " 9437,\n",
       " 7125,\n",
       " 9708,\n",
       " 2937,\n",
       " 7389,\n",
       " 167,\n",
       " 3751,\n",
       " 10581,\n",
       " 11707,\n",
       " 6211,\n",
       " 13130,\n",
       " 8041,\n",
       " 14012,\n",
       " 3117,\n",
       " 9126,\n",
       " 81,\n",
       " 2115,\n",
       " 10479,\n",
       " 10323,\n",
       " 2140,\n",
       " 9423,\n",
       " 7769,\n",
       " 5555,\n",
       " 7047,\n",
       " 228,\n",
       " 11655,\n",
       " 8290,\n",
       " 9416,\n",
       " 558,\n",
       " 9098,\n",
       " 9263,\n",
       " 641,\n",
       " 11746,\n",
       " 7564,\n",
       " 7390,\n",
       " 11140,\n",
       " 7665,\n",
       " 3150,\n",
       " 773,\n",
       " 7073,\n",
       " 5306,\n",
       " 340,\n",
       " 8720,\n",
       " 3921,\n",
       " 10934,\n",
       " 3792,\n",
       " 2957,\n",
       " 13901,\n",
       " 1197,\n",
       " 8360,\n",
       " 5510,\n",
       " 10424,\n",
       " 13422,\n",
       " 3319,\n",
       " 9225,\n",
       " 5178,\n",
       " 12151,\n",
       " 11289,\n",
       " 9115,\n",
       " 8942,\n",
       " 5243,\n",
       " 9561,\n",
       " 4185,\n",
       " 5899,\n",
       " 1256,\n",
       " 7456,\n",
       " 8749,\n",
       " 6438,\n",
       " 7609,\n",
       " 12918,\n",
       " 1105,\n",
       " 11749,\n",
       " 6832,\n",
       " 7017,\n",
       " 10500,\n",
       " 12345,\n",
       " 5139,\n",
       " 11634,\n",
       " 10010,\n",
       " 9853,\n",
       " 10988,\n",
       " 6043,\n",
       " 2370,\n",
       " 10364,\n",
       " 12958,\n",
       " 3054,\n",
       " 7427,\n",
       " 12174,\n",
       " 9776,\n",
       " 10300,\n",
       " 13566,\n",
       " 7040,\n",
       " 3408,\n",
       " 2903,\n",
       " 5216,\n",
       " 7573,\n",
       " 3536,\n",
       " 13003,\n",
       " 983,\n",
       " 13559,\n",
       " 10655,\n",
       " 10580,\n",
       " 10619,\n",
       " 10705,\n",
       " 12323,\n",
       " 12107,\n",
       " 5209,\n",
       " 2660,\n",
       " 2804,\n",
       " 5823,\n",
       " 2672,\n",
       " 5312,\n",
       " 1492,\n",
       " 2016,\n",
       " 9323,\n",
       " 7387,\n",
       " 8497,\n",
       " 653,\n",
       " 2427,\n",
       " 10217,\n",
       " 1823,\n",
       " 8073,\n",
       " 8672,\n",
       " 2465,\n",
       " 7653,\n",
       " 10246,\n",
       " 595,\n",
       " 4159,\n",
       " 399,\n",
       " 1681,\n",
       " 10695,\n",
       " 1074,\n",
       " 12776,\n",
       " 13898,\n",
       " 2421,\n",
       " 4995,\n",
       " 4751,\n",
       " 3291,\n",
       " 11164,\n",
       " 6226,\n",
       " 10701,\n",
       " 2354,\n",
       " 3850,\n",
       " 5124,\n",
       " 3305,\n",
       " 4048,\n",
       " 2634,\n",
       " 334,\n",
       " 8329,\n",
       " 5702,\n",
       " 2708,\n",
       " 3123,\n",
       " 7170,\n",
       " 4926,\n",
       " 2265,\n",
       " 3929,\n",
       " 8226,\n",
       " 11533,\n",
       " 13594,\n",
       " 8285,\n",
       " 1237,\n",
       " 4888,\n",
       " 11523,\n",
       " 7266,\n",
       " 5151,\n",
       " 2214,\n",
       " 7049,\n",
       " 7280,\n",
       " 13666,\n",
       " 1833,\n",
       " 5954,\n",
       " 4711,\n",
       " 10708,\n",
       " 6628,\n",
       " 3990,\n",
       " 7631,\n",
       " 7465,\n",
       " 1196,\n",
       " 6556,\n",
       " 5593,\n",
       " 319,\n",
       " 5233,\n",
       " 7202,\n",
       " 2481,\n",
       " 10063,\n",
       " 6703,\n",
       " 3356,\n",
       " 1854,\n",
       " 11987,\n",
       " 8243,\n",
       " 6567,\n",
       " 8600,\n",
       " 8078,\n",
       " 1137,\n",
       " 11601,\n",
       " 10046,\n",
       " 9514,\n",
       " 13741,\n",
       " 957,\n",
       " 13314,\n",
       " 7151,\n",
       " 6494,\n",
       " 9552,\n",
       " 5884,\n",
       " 3502,\n",
       " 4572,\n",
       " 1108,\n",
       " 7654,\n",
       " 11562,\n",
       " 3886,\n",
       " 10542,\n",
       " 5586,\n",
       " 13847,\n",
       " 2048,\n",
       " 5693,\n",
       " 8012,\n",
       " 9026,\n",
       " 12487,\n",
       " 11844,\n",
       " 13181,\n",
       " 9900,\n",
       " 243,\n",
       " 9134,\n",
       " 7839,\n",
       " 4433,\n",
       " 6717,\n",
       " 2292,\n",
       " 12784,\n",
       " 4133,\n",
       " 6823,\n",
       " 13586,\n",
       " 7510,\n",
       " 7081,\n",
       " 2096,\n",
       " 6963,\n",
       " 3715,\n",
       " 8927,\n",
       " 3099,\n",
       " 1619,\n",
       " 2928,\n",
       " 4800,\n",
       " 7341,\n",
       " 6177,\n",
       " 3465,\n",
       " 6389,\n",
       " 7520,\n",
       " 754,\n",
       " 10561,\n",
       " 13871,\n",
       " 4558,\n",
       " 2950,\n",
       " 895,\n",
       " 6223,\n",
       " 10794,\n",
       " 3969,\n",
       " 10119,\n",
       " 1377,\n",
       " 12103,\n",
       " 6819,\n",
       " 4963,\n",
       " 13984,\n",
       " 13512,\n",
       " 6850,\n",
       " 6099,\n",
       " 6702,\n",
       " 599,\n",
       " 1527,\n",
       " 14263,\n",
       " 6460,\n",
       " 8946,\n",
       " 10552,\n",
       " 2099,\n",
       " 1315,\n",
       " 11178,\n",
       " 13165,\n",
       " 9027,\n",
       " 4853,\n",
       " 11548,\n",
       " 736,\n",
       " 3960,\n",
       " 9066,\n",
       " 2362,\n",
       " 8323,\n",
       " 13469,\n",
       " 3272,\n",
       " 9144,\n",
       " 12199,\n",
       " 6699,\n",
       " 6066,\n",
       " 3829,\n",
       " 1173,\n",
       " 3802,\n",
       " 10409,\n",
       " 3949,\n",
       " 4361,\n",
       " 9885,\n",
       " 12262,\n",
       " 12819,\n",
       " 2505,\n",
       " 11428,\n",
       " 7278,\n",
       " 142,\n",
       " 7760,\n",
       " 1470,\n",
       " 1852,\n",
       " 202,\n",
       " 8576,\n",
       " 2188,\n",
       " 8077,\n",
       " 7221,\n",
       " 4375,\n",
       " 13251,\n",
       " 2788,\n",
       " 11184,\n",
       " 12124,\n",
       " 9302,\n",
       " 1115,\n",
       " 5792,\n",
       " 14288,\n",
       " 6006,\n",
       " 3410,\n",
       " 14324,\n",
       " 10877,\n",
       " 1848,\n",
       " 3543,\n",
       " 3285,\n",
       " 10168,\n",
       " 5264,\n",
       " 9573,\n",
       " 7203,\n",
       " 9828,\n",
       " 1614,\n",
       " 8741,\n",
       " 3839,\n",
       " 3268,\n",
       " 287,\n",
       " 13727,\n",
       " 12205,\n",
       " 7930,\n",
       " 3353,\n",
       " 14398,\n",
       " 2879,\n",
       " 9431,\n",
       " 1475,\n",
       " 654,\n",
       " 7067,\n",
       " 8262,\n",
       " 4485,\n",
       " 294,\n",
       " 10418,\n",
       " 4071,\n",
       " 2785,\n",
       " 1064,\n",
       " 14234,\n",
       " 6844,\n",
       " 1970,\n",
       " 258,\n",
       " 11210,\n",
       " 9097,\n",
       " 3684,\n",
       " 1559,\n",
       " 1795,\n",
       " 5992,\n",
       " 1040,\n",
       " 4520,\n",
       " 2915,\n",
       " 4019,\n",
       " 11803,\n",
       " 5041,\n",
       " 11482,\n",
       " 12955,\n",
       " 10053,\n",
       " 11542,\n",
       " 12457,\n",
       " 1548,\n",
       " 4656,\n",
       " 917,\n",
       " 3147,\n",
       " 10917,\n",
       " 7252,\n",
       " 1261,\n",
       " 8931,\n",
       " 7958,\n",
       " 5506,\n",
       " 12289,\n",
       " 1978,\n",
       " 11187,\n",
       " 3882,\n",
       " 1986,\n",
       " 5913,\n",
       " 10321,\n",
       " 1075,\n",
       " 6328,\n",
       " 12848,\n",
       " 5085,\n",
       " 4637,\n",
       " 13872,\n",
       " 276,\n",
       " 4961,\n",
       " 369,\n",
       " 3180,\n",
       " 2302,\n",
       " 10887,\n",
       " 1079,\n",
       " 7019,\n",
       " 938,\n",
       " 6559,\n",
       " 3991,\n",
       " 8683,\n",
       " 1397,\n",
       " 384,\n",
       " 13951,\n",
       " 12889,\n",
       " 3985,\n",
       " 6583,\n",
       " 10789,\n",
       " 3326,\n",
       " 7865,\n",
       " 10147,\n",
       " 7721,\n",
       " 569,\n",
       " 13266,\n",
       " 12102,\n",
       " 1484,\n",
       " 6131,\n",
       " 1378,\n",
       " 3000,\n",
       " 7440,\n",
       " 1236,\n",
       " 13334,\n",
       " 8215,\n",
       " 6203,\n",
       " 10452,\n",
       " 13943,\n",
       " 10075,\n",
       " 6774,\n",
       " 11324,\n",
       " 2496,\n",
       " 1065,\n",
       " 1067,\n",
       " 9589,\n",
       " 4795,\n",
       " 11248,\n",
       " 13657,\n",
       " 11150,\n",
       " 4049,\n",
       " 14015,\n",
       " 7415,\n",
       " 10172,\n",
       " 1469,\n",
       " 6772,\n",
       " 682,\n",
       " 7527,\n",
       " 474,\n",
       " 915,\n",
       " 982,\n",
       " 4440,\n",
       " 9607,\n",
       " 3917,\n",
       " 12284,\n",
       " 10764,\n",
       " 3057,\n",
       " 3327,\n",
       " 3752,\n",
       " 8998,\n",
       " 13088,\n",
       " 5890,\n",
       " 1807,\n",
       " 10060,\n",
       " 10325,\n",
       " 11458,\n",
       " 7610,\n",
       " 9410,\n",
       " 8366,\n",
       " 2896,\n",
       " 11000,\n",
       " 3024,\n",
       " 7312,\n",
       " 13042,\n",
       " 5072,\n",
       " 13874,\n",
       " 4565,\n",
       " 6134,\n",
       " 12071,\n",
       " 2227,\n",
       " 11028,\n",
       " 1116,\n",
       " 9638,\n",
       " 12358,\n",
       " 4545,\n",
       " 1277,\n",
       " 615,\n",
       " 14279,\n",
       " 3255,\n",
       " 12348,\n",
       " 11887,\n",
       " 10536,\n",
       " 8647,\n",
       " 9231,\n",
       " 3159,\n",
       " 4676,\n",
       " 7782,\n",
       " 518,\n",
       " 6517,\n",
       " 4465,\n",
       " 2337,\n",
       " 2982,\n",
       " 10485,\n",
       " 2365,\n",
       " 5670,\n",
       " 7590,\n",
       " 10327,\n",
       " 10436,\n",
       " 5486,\n",
       " 4607,\n",
       " 3884,\n",
       " 375,\n",
       " 10666,\n",
       " 2293,\n",
       " 241,\n",
       " 6518,\n",
       " 2620,\n",
       " 4323,\n",
       " 1972,\n",
       " 777,\n",
       " 8480,\n",
       " 10125,\n",
       " 3538,\n",
       " 1274,\n",
       " 5902,\n",
       " 11296,\n",
       " 7591,\n",
       " 6871,\n",
       " 5192,\n",
       " 14214,\n",
       " 2888,\n",
       " 7921,\n",
       " 4365,\n",
       " 1427,\n",
       " 13831,\n",
       " 2929,\n",
       " 3779,\n",
       " 5793,\n",
       " 7891,\n",
       " 9246,\n",
       " 8156,\n",
       " 9457,\n",
       " 9800,\n",
       " 4913,\n",
       " 8234,\n",
       " 1445,\n",
       " 2533,\n",
       " 11247,\n",
       " 2676,\n",
       " 5226,\n",
       " 8903,\n",
       " 5566,\n",
       " 10183,\n",
       " 13906,\n",
       " 6081,\n",
       " 7313,\n",
       " 13826,\n",
       " 10045,\n",
       " 3330,\n",
       " 10038,\n",
       " 11125,\n",
       " 762,\n",
       " 9926,\n",
       " 13045,\n",
       " 12587,\n",
       " 5418,\n",
       " 1617,\n",
       " 5575,\n",
       " 13243,\n",
       " 3620,\n",
       " 7185,\n",
       " 10765,\n",
       " 3680,\n",
       " 4484,\n",
       " 11492,\n",
       " 7688,\n",
       " 9325,\n",
       " 11810,\n",
       " 11758,\n",
       " 6667,\n",
       " 8592,\n",
       " 418,\n",
       " 9908,\n",
       " 8519,\n",
       " 944,\n",
       " 12304,\n",
       " 12373,\n",
       " 587,\n",
       " 6058,\n",
       " 11406,\n",
       " 1645,\n",
       " 5410,\n",
       " 8350,\n",
       " 1383,\n",
       " 8535,\n",
       " 8343,\n",
       " 5399,\n",
       " 8060,\n",
       " 1019,\n",
       " 1003,\n",
       " 5077,\n",
       " 5154,\n",
       " 5582,\n",
       " 7469,\n",
       " 8557,\n",
       " 1311,\n",
       " 3043,\n",
       " 1844,\n",
       " 2424,\n",
       " 13206,\n",
       " 9512,\n",
       " 6136,\n",
       " 10776,\n",
       " 3014,\n",
       " 2101,\n",
       " 9823,\n",
       " 67,\n",
       " 3389,\n",
       " 4792,\n",
       " 13662,\n",
       " 8454,\n",
       " 2071,\n",
       " 3833,\n",
       " 6078,\n",
       " 9978,\n",
       " 10751,\n",
       " 1882,\n",
       " 2701,\n",
       " 12432,\n",
       " 14315,\n",
       " 5396,\n",
       " 3530,\n",
       " 6612,\n",
       " 7549,\n",
       " 6057,\n",
       " 3092,\n",
       " 7357,\n",
       " 10622,\n",
       " 4630,\n",
       " 2551,\n",
       " 13936,\n",
       " 1273,\n",
       " 3977,\n",
       " 2353,\n",
       " 13571,\n",
       " 4464,\n",
       " 833,\n",
       " 1431,\n",
       " 5522,\n",
       " 8815,\n",
       " 12271,\n",
       " 5738,\n",
       " 2358,\n",
       " 10146,\n",
       " 5687,\n",
       " 2117,\n",
       " 455,\n",
       " 3968,\n",
       " 6246,\n",
       " 3044,\n",
       " 12154,\n",
       " 13965,\n",
       " 13580,\n",
       " 13084,\n",
       " 7153,\n",
       " 2015,\n",
       " 7481,\n",
       " 7152,\n",
       " 6595,\n",
       " 4439,\n",
       " 585,\n",
       " 6639,\n",
       " 4258,\n",
       " 4833,\n",
       " 129,\n",
       " 3619,\n",
       " 10000,\n",
       " 8405,\n",
       " 4560,\n",
       " 11343,\n",
       " 7157,\n",
       " 7093,\n",
       " 13192,\n",
       " 7286,\n",
       " 1347,\n",
       " 7089,\n",
       " 8746,\n",
       " 447,\n",
       " 6995,\n",
       " 764,\n",
       " 2842,\n",
       " 11109,\n",
       " 1489,\n",
       " 4930,\n",
       " 4513,\n",
       " 6427,\n",
       " 11688,\n",
       " 14318,\n",
       " 4716,\n",
       " 4166,\n",
       " 10779,\n",
       " 7645,\n",
       " 11752,\n",
       " 9822,\n",
       " 10031,\n",
       " 10372,\n",
       " 10534,\n",
       " 9500,\n",
       " 7365,\n",
       " 6943,\n",
       " 3786,\n",
       " 6714,\n",
       " 4836,\n",
       " 7449,\n",
       " 6011,\n",
       " 11805,\n",
       " 4776,\n",
       " 11774,\n",
       " 4102,\n",
       " 157,\n",
       " 8451,\n",
       " 7793,\n",
       " 8441,\n",
       " 5167,\n",
       " 11820,\n",
       " 1120,\n",
       " 3781,\n",
       " 511,\n",
       " 13760,\n",
       " 14090,\n",
       " 8608,\n",
       " 6859,\n",
       " 2209,\n",
       " 8075,\n",
       " 4165,\n",
       " 2364,\n",
       " 2738,\n",
       " 12959,\n",
       " 12470,\n",
       " 8250,\n",
       " 5898,\n",
       " 6302,\n",
       " 4653,\n",
       " 3727,\n",
       " 2231,\n",
       " 4999,\n",
       " 6679,\n",
       " 1927,\n",
       " 8991,\n",
       " 12706,\n",
       " 3393,\n",
       " 13567,\n",
       " 6112,\n",
       " 9809,\n",
       " 9364,\n",
       " 1435,\n",
       " 11638,\n",
       " 2031,\n",
       " 2281,\n",
       " 12386,\n",
       " 13862,\n",
       " 2958,\n",
       " 6815,\n",
       " 14036,\n",
       " 373,\n",
       " 8492,\n",
       " 3382,\n",
       " 8447,\n",
       " 5820,\n",
       " 13214,\n",
       " 14146,\n",
       " 484,\n",
       " 8066,\n",
       " 10951,\n",
       " 10527,\n",
       " 8212,\n",
       " 6128,\n",
       " 13536,\n",
       " 3417,\n",
       " 5278,\n",
       " 4780,\n",
       " 11033,\n",
       " 13443,\n",
       " 8303,\n",
       " 8349,\n",
       " 2166,\n",
       " 5943,\n",
       " 6319,\n",
       " 6681,\n",
       " 3486,\n",
       " 13001,\n",
       " 7116,\n",
       " 3663,\n",
       " 11407,\n",
       " 1575,\n",
       " 13892,\n",
       " 4804,\n",
       " 8204,\n",
       " 9517,\n",
       " 2250,\n",
       " 12805,\n",
       " 5006,\n",
       " 7375,\n",
       " 4025,\n",
       " 8357,\n",
       " 11307,\n",
       " 1540,\n",
       " 7563,\n",
       " 12666,\n",
       " 2046,\n",
       " 14281,\n",
       " 6785,\n",
       " 6549,\n",
       " 305,\n",
       " 94,\n",
       " 9862,\n",
       " 4507,\n",
       " 9627,\n",
       " 12760,\n",
       " 12684,\n",
       " 6064,\n",
       " 1776,\n",
       " 2862,\n",
       " 13022,\n",
       " 6947,\n",
       " 1195,\n",
       " 11583,\n",
       " 13409,\n",
       " 8229,\n",
       " 5784,\n",
       " 5173,\n",
       " 5635,\n",
       " 7083,\n",
       " 4376,\n",
       " 8568,\n",
       " 11938,\n",
       " 10158,\n",
       " 8517,\n",
       " 8348,\n",
       " 10890,\n",
       " 3647,\n",
       " 11658,\n",
       " 1914,\n",
       " 12429,\n",
       " 11222,\n",
       " 5383,\n",
       " 2256,\n",
       " 170,\n",
       " 2151,\n",
       " 5020,\n",
       " 5628,\n",
       " 12142,\n",
       " 2532,\n",
       " 4796,\n",
       " 1737,\n",
       " 10828,\n",
       " 8786,\n",
       " 3456,\n",
       " 14241,\n",
       " 6416,\n",
       " 4500,\n",
       " 14317,\n",
       " 12873,\n",
       " 7296,\n",
       " 14151,\n",
       " 6101,\n",
       " 7420,\n",
       " 4539,\n",
       " 5242,\n",
       " 4806,\n",
       " 7201,\n",
       " 6593,\n",
       " 1426,\n",
       " 5280,\n",
       " 8013,\n",
       " 7018,\n",
       " 13672,\n",
       " 8419,\n",
       " 5958,\n",
       " 2238,\n",
       " 1606,\n",
       " 6527,\n",
       " 383,\n",
       " 10502,\n",
       " 4668,\n",
       " 8351,\n",
       " 5569,\n",
       " 11538,\n",
       " 5379,\n",
       " 899,\n",
       " 9301,\n",
       " 7412,\n",
       " 12193,\n",
       " 5079,\n",
       " 2400,\n",
       " 4251,\n",
       " 8748,\n",
       " 12080,\n",
       " 594,\n",
       " 2589,\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_indices = label_indices\n",
    "labeled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fa9fa61-2c54-4cc6-9c6c-539240b84c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5144, 4197, 664, 11362, 236, 8828, 1834, 3013, 2986, 10862, 2476, 2730, 3943, 1124, 402, 2455, 4616, 9836, 8139, 13295, 6528, 8356, 13083, 1327, 7108, 5146, 116, 13186, 10443, 894, 8231, 14052, 7139, 9937, 2556, 586, 3454, 4935, 5351, 9013, 9639, 5866, 12272, 3959, 596, 2308, 2277, 7065, 9535, 11609, 10090, 4040, 9659, 8865, 4526, 3707, 1849, 13582, 8567, 3797, 6885, 11340, 321, 6038, 10151, 13924, 6168, 11984, 8653, 5703, 1170, 5556, 8632, 1960, 5589, 5736, 14325, 6886, 4417, 4721, 8716, 5634, 11345, 3810, 13978, 3064, 11022, 14117, 12954, 1609, 4169, 8266, 6782, 4760, 8595, 9437, 7125, 9708, 2937, 7389, 167, 3751, 10581, 11707, 6211, 13130, 8041, 14012, 3117, 9126, 81, 2115, 10479, 10323, 2140, 9423, 7769, 5555, 7047, 228, 11655, 8290, 9416, 558, 9098, 9263, 641, 11746, 7564, 7390, 11140, 7665, 3150, 773, 7073, 5306, 340, 8720, 3921, 10934, 3792, 2957, 13901, 1197, 8360, 5510, 10424, 13422, 3319, 9225, 5178, 12151, 11289, 9115, 8942, 5243, 9561, 4185, 5899, 1256, 7456, 8749, 6438, 7609, 12918, 1105, 11749, 6832, 7017, 10500, 12345, 5139, 11634, 10010, 9853, 10988, 6043, 2370, 10364, 12958, 3054, 7427, 12174, 9776, 10300, 13566, 7040, 3408, 2903, 5216, 7573, 3536, 13003, 983, 13559, 10655, 10580, 10619, 10705, 12323, 12107, 5209, 2660, 2804, 5823, 2672, 5312, 1492, 2016, 9323, 7387, 8497, 653, 2427, 10217, 1823, 8073, 8672, 2465, 7653, 10246, 595, 4159, 399, 1681, 10695, 1074, 12776, 13898, 2421, 4995, 4751, 3291, 11164, 6226, 10701, 2354, 3850, 5124, 3305, 4048, 2634, 334, 8329, 5702, 2708, 3123, 7170, 4926, 2265, 3929, 8226, 11533, 13594, 8285, 1237, 4888, 11523, 7266, 5151, 2214, 7049, 7280, 13666, 1833, 5954, 4711, 10708, 6628, 3990, 7631, 7465, 1196, 6556, 5593, 319, 5233, 7202, 2481, 10063, 6703, 3356, 1854, 11987, 8243, 6567, 8600, 8078, 1137, 11601, 10046, 9514, 13741, 957, 13314, 7151, 6494, 9552, 5884, 3502, 4572, 1108, 7654, 11562, 3886, 10542, 5586, 13847, 2048, 5693, 8012, 9026, 12487, 11844, 13181, 9900, 243, 9134, 7839, 4433, 6717, 2292, 12784, 4133, 6823, 13586, 7510, 7081, 2096, 6963, 3715, 8927, 3099, 1619, 2928, 4800, 7341, 6177, 3465, 6389, 7520, 754, 10561, 13871, 4558, 2950, 895, 6223, 10794, 3969, 10119, 1377, 12103, 6819, 4963, 13984, 13512, 6850, 6099, 6702, 599, 1527, 14263, 6460, 8946, 10552, 2099, 1315, 11178, 13165, 9027, 4853, 11548, 736, 3960, 9066, 2362, 8323, 13469, 3272, 9144, 12199, 6699, 6066, 3829, 1173, 3802, 10409, 3949, 4361, 9885, 12262, 12819, 2505, 11428, 7278, 142, 7760, 1470, 1852, 202, 8576, 2188, 8077, 7221, 4375, 13251, 2788, 11184, 12124, 9302, 1115, 5792, 14288, 6006, 3410, 14324, 10877, 1848, 3543, 3285, 10168, 5264, 9573, 7203, 9828, 1614, 8741, 3839, 3268, 287, 13727, 12205, 7930, 3353, 14398, 2879, 9431, 1475, 654, 7067, 8262, 4485, 294, 10418, 4071, 2785, 1064, 14234, 6844, 1970, 258, 11210, 9097, 3684, 1559, 1795, 5992, 1040, 4520, 2915, 4019, 11803, 5041, 11482, 12955, 10053, 11542, 12457, 1548, 4656, 917, 3147, 10917, 7252, 1261, 8931, 7958, 5506, 12289, 1978, 11187, 3882, 1986, 5913, 10321, 1075, 6328, 12848, 5085, 4637, 13872, 276, 4961, 369, 3180, 2302, 10887, 1079, 7019, 938, 6559, 3991, 8683, 1397, 384, 13951, 12889, 3985, 6583, 10789, 3326, 7865, 10147, 7721, 569, 13266, 12102, 1484, 6131, 1378, 3000, 7440, 1236, 13334, 8215, 6203, 10452, 13943, 10075, 6774, 11324, 2496, 1065, 1067, 9589, 4795, 11248, 13657, 11150, 4049, 14015, 7415, 10172, 1469, 6772, 682, 7527, 474, 915, 982, 4440, 9607, 3917, 12284, 10764, 3057, 3327, 3752, 8998, 13088, 5890, 1807, 10060, 10325, 11458, 7610, 9410, 8366, 2896, 11000, 3024, 7312, 13042, 5072, 13874, 4565, 6134, 12071, 2227, 11028, 1116, 9638, 12358, 4545, 1277, 615, 14279, 3255, 12348, 11887, 10536, 8647, 9231, 3159, 4676, 7782, 518, 6517, 4465, 2337, 2982, 10485, 2365, 5670, 7590, 10327, 10436, 5486, 4607, 3884, 375, 10666, 2293, 241, 6518, 2620, 4323, 1972, 777, 8480, 10125, 3538, 1274, 5902, 11296, 7591, 6871, 5192, 14214, 2888, 7921, 4365, 1427, 13831, 2929, 3779, 5793, 7891, 9246, 8156, 9457, 9800, 4913, 8234, 1445, 2533, 11247, 2676, 5226, 8903, 5566, 10183, 13906, 6081, 7313, 13826, 10045, 3330, 10038, 11125, 762, 9926, 13045, 12587, 5418, 1617, 5575, 13243, 3620, 7185, 10765, 3680, 4484, 11492, 7688, 9325, 11810, 11758, 6667, 8592, 418, 9908, 8519, 944, 12304, 12373, 587, 6058, 11406, 1645, 5410, 8350, 1383, 8535, 8343, 5399, 8060, 1019, 1003, 5077, 5154, 5582, 7469, 8557, 1311, 3043, 1844, 2424, 13206, 9512, 6136, 10776, 3014, 2101, 9823, 67, 3389, 4792, 13662, 8454, 2071, 3833, 6078, 9978, 10751, 1882, 2701, 12432, 14315, 5396, 3530, 6612, 7549, 6057, 3092, 7357, 10622, 4630, 2551, 13936, 1273, 3977, 2353, 13571, 4464, 833, 1431, 5522, 8815, 12271, 5738, 2358, 10146, 5687, 2117, 455, 3968, 6246, 3044, 12154, 13965, 13580, 13084, 7153, 2015, 7481, 7152, 6595, 4439, 585, 6639, 4258, 4833, 129, 3619, 10000, 8405, 4560, 11343, 7157, 7093, 13192, 7286, 1347, 7089, 8746, 447, 6995, 764, 2842, 11109, 1489, 4930, 4513, 6427, 11688, 14318, 4716, 4166, 10779, 7645, 11752, 9822, 10031, 10372, 10534, 9500, 7365, 6943, 3786, 6714, 4836, 7449, 6011, 11805, 4776, 11774, 4102, 157, 8451, 7793, 8441, 5167, 11820, 1120, 3781, 511, 13760, 14090, 8608, 6859, 2209, 8075, 4165, 2364, 2738, 12959, 12470, 8250, 5898, 6302, 4653, 3727, 2231, 4999, 6679, 1927, 8991, 12706, 3393, 13567, 6112, 9809, 9364, 1435, 11638, 2031, 2281, 12386, 13862, 2958, 6815, 14036, 373, 8492, 3382, 8447, 5820, 13214, 14146, 484, 8066, 10951, 10527, 8212, 6128, 13536, 3417, 5278, 4780, 11033, 13443, 8303, 8349, 2166, 5943, 6319, 6681, 3486, 13001, 7116, 3663, 11407, 1575, 13892, 4804, 8204, 9517, 2250, 12805, 5006, 7375, 4025, 8357, 11307, 1540, 7563, 12666, 2046, 14281, 6785, 6549, 305, 94, 9862, 4507, 9627, 12760, 12684, 6064, 1776, 2862, 13022, 6947, 1195, 11583, 13409, 8229, 5784, 5173, 5635, 7083, 4376, 8568, 11938, 10158, 8517, 8348, 10890, 3647, 11658, 1914, 12429, 11222, 5383, 2256, 170, 2151, 5020, 5628, 12142, 2532, 4796, 1737, 10828, 8786, 3456, 14241, 6416, 4500, 14317, 12873, 7296, 14151, 6101, 7420, 4539, 5242, 4806, 7201, 6593, 1426, 5280, 8013, 7018, 13672, 8419, 5958, 2238, 1606, 6527, 383, 10502, 4668, 8351, 5569, 11538, 5379, 899, 9301, 7412, 12193, 5079, 2400, 4251, 8748, 12080, 594, 2589, 7634, 13394, 2489, 11711, 5013, 1078, 8467, 3615, 2284, 8560, 1521, 7956, 11418, 3692, 11035, 6227, 3027, 9547, 10023, 1995, 11142, 7528, 10459, 4387, 4288, 6804, 13017, 12169, 11089, 5253, 2316, 8826, 3923, 5900, 10754, 8691, 11040, 8733, 2104, 5963, 7561, 2535, 6239, 1363, 1143, 7349, 9293, 1895, 6094, 12296, 781, 5710, 10094, 7377, 2907, 11108, 12130, 9784, 3630, 1923, 2625, 3774, 4495, 11286, 4431, 3240, 3902, 9165, 6120, 5713, 8272, 5061, 9385, 8327, 5991, 1444, 13502, 5170, 11676, 10483, 2372, 13321, 1392, 10629, 5327, 2109, 3037, 6562, 2097, 10730, 11928, 10700, 10138, 6516, 7715, 7755, 9875, 8818, 8124, 7599, 11175, 4882, 10699]\n",
      "[2716, 2422, 6196, 6127, 7705, 2274, 8389, 6377, 5785, 13724, 3783, 6728, 12194, 1243, 7074, 4249, 4712, 1047, 3535, 4438, 11497, 8491, 14024, 1371, 12511, 13049, 9852, 1157, 12667, 11273, 9056, 13838, 7192, 1062, 12956, 3706, 14268, 12705, 11951, 5787, 3511, 8554, 11572, 12111, 2836, 5271, 5298, 9757, 9189, 8196, 8228, 10432, 11002, 8808, 7692, 7281, 3021, 8225, 9148, 1493, 9135, 12517, 12116, 10159, 12578, 148, 3217, 10685, 8031, 2995, 4343, 14293, 13138, 5488, 4815, 14426, 1262, 3244, 3084, 1736, 6448, 6301, 2960, 4225, 5512, 7026, 10390, 2827, 2871, 13855, 10379, 8383, 11967, 483, 5644, 8305, 2196, 10410, 8631, 11549, 6606, 1893, 8413, 1136, 5405, 7640, 4120, 100, 14162, 10298, 11278, 6119, 3433, 3785, 4783, 13164, 4232, 11657, 4333, 6343, 13447, 5570, 6428, 2630, 4328, 6865, 2927, 4351, 8386, 4178, 1674, 4239, 7892, 4360, 7300, 9910, 13090, 1638, 13078, 10686, 2770, 4840, 10732, 2119, 6436, 9772, 2823, 6607, 3754, 7889, 3747, 6884, 10926, 6744, 5985, 5437, 2417, 2273, 5083, 14240, 1131, 593, 4897, 13685, 2705, 4034, 8339, 6179, 12974, 1637, 1333, 10201, 13931, 12302, 9682, 13882, 5228, 12593, 13292, 6278, 11734, 1053, 11050, 6426, 1215, 3992, 14407, 11410, 7783, 12488, 11958, 1733, 3519, 2802, 281, 3584, 2540, 6569, 4524, 8820, 10411, 13120, 2326, 2286, 10825, 2767, 12864, 6314, 490, 8633, 6630, 2154, 9190, 11277, 255, 7130, 10697, 7433, 10623, 11192, 10607, 6623, 13305, 5721, 6792, 2112, 5513, 13673, 1917, 11321, 498, 8641, 12579, 1235, 8046, 10435, 1593, 6514, 9085, 149, 10905, 5218, 1720, 10309, 8645, 13858, 8189, 1898, 3325, 93, 8814, 5674, 2283, 7000, 526, 8727, 6579, 2419, 4915, 1759, 9435, 1710, 12821, 3186, 2022, 1183, 7189, 2819, 997, 4332, 852, 4184, 2686, 8175, 13779, 4209]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "train_indices = labeled_indices[:num_train]\n",
    "test_indices = labeled_indices[num_train:num_train+num_test]\n",
    "print(train_indices)\n",
    "print(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4459084-beac-43c6-9a05-511ddcafc9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 100\n",
    "count = 1 / num_iterations\n",
    "\n",
    "# 总节点数\n",
    "num_nodes = data.x.shape[0]\n",
    "# 所有节点的索引\n",
    "all_indices = np.arange(num_nodes)\n",
    "mask_out = torch.ones(num_nodes, dtype=torch.bool)\n",
    "# 将测试集索引处的掩码设为False\n",
    "mask_out[test_indices] = False\n",
    "mask_out[-47595:] = False\n",
    "# 使用掩码获取剩余的索引\n",
    "remaining_indices = all_indices[mask_out]\n",
    "complement_mask = ~mask_out\n",
    "complement_mask[test_indices] = False\n",
    "\n",
    "# 得到既不在 remaining_indices 也不在 test_indices 中的索引\n",
    "complement_indices = all_indices[complement_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69a1b893-4314-42d1-a47f-e0236745e352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     1     2 ... 14447 14448 14449]\n",
      "14174\n",
      "[14450 14451 14452 ... 62042 62043 62044]\n",
      "47595\n"
     ]
    }
   ],
   "source": [
    "print(remaining_indices)\n",
    "print(len(remaining_indices))\n",
    "print(complement_indices)\n",
    "print(len(complement_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "61008077-24a5-4c14-8ea6-d7e9c5b1f021",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: 47120\n",
      "s: 475\n",
      "c: 46645\n",
      "s: 475\n",
      "c: 46170\n",
      "s: 475\n",
      "c: 45695\n",
      "s: 475\n",
      "c: 45220\n",
      "s: 475\n",
      "c: 44745\n",
      "s: 475\n",
      "c: 44270\n",
      "s: 475\n",
      "c: 43795\n",
      "s: 475\n",
      "c: 43320\n",
      "s: 475\n",
      "c: 42845\n",
      "s: 475\n",
      "c: 42370\n",
      "s: 475\n",
      "c: 41895\n",
      "s: 475\n",
      "c: 41420\n",
      "s: 475\n",
      "c: 40945\n",
      "s: 475\n",
      "c: 40470\n",
      "s: 475\n",
      "c: 39995\n",
      "s: 475\n",
      "c: 39520\n",
      "s: 475\n",
      "c: 39045\n",
      "s: 475\n",
      "c: 38570\n",
      "s: 475\n",
      "c: 38095\n",
      "s: 475\n",
      "c: 37620\n",
      "s: 475\n",
      "c: 37145\n",
      "s: 475\n",
      "c: 36670\n",
      "s: 475\n",
      "c: 36195\n",
      "s: 475\n",
      "c: 35720\n",
      "s: 475\n",
      "c: 35245\n",
      "s: 475\n",
      "c: 34770\n",
      "s: 475\n",
      "c: 34295\n",
      "s: 475\n",
      "c: 33820\n",
      "s: 475\n",
      "c: 33345\n",
      "s: 475\n",
      "c: 32870\n",
      "s: 475\n",
      "c: 32395\n",
      "s: 475\n",
      "c: 31920\n",
      "s: 475\n",
      "c: 31445\n",
      "s: 475\n",
      "c: 30970\n",
      "s: 475\n",
      "c: 30495\n",
      "s: 475\n",
      "c: 30020\n",
      "s: 475\n",
      "c: 29545\n",
      "s: 475\n",
      "c: 29070\n",
      "s: 475\n",
      "c: 28595\n",
      "s: 475\n",
      "c: 28120\n",
      "s: 475\n",
      "c: 27645\n",
      "s: 475\n",
      "c: 27170\n",
      "s: 475\n",
      "c: 26695\n",
      "s: 475\n",
      "c: 26220\n",
      "s: 475\n",
      "c: 25745\n",
      "s: 475\n",
      "c: 25270\n",
      "s: 475\n",
      "c: 24795\n",
      "s: 475\n",
      "c: 24320\n",
      "s: 475\n",
      "c: 23845\n",
      "s: 475\n",
      "c: 23370\n",
      "s: 475\n",
      "c: 22895\n",
      "s: 475\n",
      "c: 22420\n",
      "s: 475\n",
      "c: 21945\n",
      "s: 475\n",
      "c: 21470\n",
      "s: 475\n",
      "c: 20995\n",
      "s: 475\n",
      "c: 20520\n",
      "s: 475\n",
      "c: 20045\n",
      "s: 475\n",
      "c: 19570\n",
      "s: 475\n",
      "c: 19095\n",
      "s: 475\n",
      "c: 18620\n",
      "s: 475\n",
      "c: 18145\n",
      "s: 475\n",
      "c: 17670\n",
      "s: 475\n",
      "c: 17195\n",
      "s: 475\n",
      "c: 16720\n",
      "s: 475\n",
      "c: 16245\n",
      "s: 475\n",
      "c: 15770\n",
      "s: 475\n",
      "c: 15295\n",
      "s: 475\n",
      "c: 14820\n",
      "s: 475\n",
      "c: 14345\n",
      "s: 475\n",
      "c: 13870\n",
      "s: 475\n",
      "c: 13395\n",
      "s: 475\n",
      "c: 12920\n",
      "s: 475\n",
      "c: 12445\n",
      "s: 475\n",
      "c: 11970\n",
      "s: 475\n",
      "c: 11495\n",
      "s: 475\n",
      "c: 11020\n",
      "s: 475\n",
      "c: 10545\n",
      "s: 475\n",
      "c: 10070\n",
      "s: 475\n",
      "c: 9595\n",
      "s: 475\n",
      "c: 9120\n",
      "s: 475\n",
      "c: 8645\n",
      "s: 475\n",
      "c: 8170\n",
      "s: 475\n",
      "c: 7695\n",
      "s: 475\n",
      "c: 7220\n",
      "s: 475\n",
      "c: 6745\n",
      "s: 475\n",
      "c: 6270\n",
      "s: 475\n",
      "c: 5795\n",
      "s: 475\n",
      "c: 5320\n",
      "s: 475\n",
      "c: 4845\n",
      "s: 475\n",
      "c: 4370\n",
      "s: 475\n",
      "c: 3895\n",
      "s: 475\n",
      "c: 3420\n",
      "s: 475\n",
      "c: 2945\n",
      "s: 475\n",
      "c: 2470\n",
      "s: 475\n",
      "c: 1995\n",
      "s: 475\n",
      "c: 1520\n",
      "s: 475\n",
      "c: 1045\n",
      "s: 475\n",
      "c: 570\n",
      "s: 475\n",
      "c: 95\n",
      "s: 475\n",
      "所有迭代的selected_indices和remaining_indices已保存到文件。\n"
     ]
    }
   ],
   "source": [
    "num_nodes_out = 47595\n",
    "num_to_select = int(num_nodes_out * count)\n",
    "for i in range(num_iterations):\n",
    "    # 随机选择节点\n",
    "    selected_indices = np.random.choice(complement_indices, num_to_select, replace=False)\n",
    "    # 更新剩余节点列表\n",
    "    complement_indices = np.setdiff1d(complement_indices, selected_indices)\n",
    "    print(\"c:\",len(complement_indices))\n",
    "    # 保存到文件\n",
    "    print(\"s:\",len(selected_indices))\n",
    "    np.save(f'DIVIDED_DATA/GO_{i}%.npy', complement_indices)  # 修改保存路径\n",
    "\n",
    "print(\"所有迭代的selected_indices和remaining_indices已保存到文件。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d16e6e67-5097-4f71-9cb0-39c512bf72b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"早停机制，用于在验证损失停止改善时终止训练。\"\"\"\n",
    "    def __init__(self, patience=200, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            patience (int): 损失没有改善的迭代次数，在这之后训练将会被停止。\n",
    "            verbose (bool): 如果为True，则打印一条消息表明早停被触发。\n",
    "            delta (float): 损失的最小改变，被认为是改善。\n",
    "            path (str): 最佳模型保存路径。\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    print(\"Early stopping triggered\")\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "    \n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        '''保存模型当验证损失减少时'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41d10184-3866-4d19-a797-929463861036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33960226, -0.03074448, -0.90138096, ...,  0.01558092,\n",
       "        -0.02386307, -0.02200161],\n",
       "       [-0.13179901, -0.02574519, -0.67730105, ..., -0.03992649,\n",
       "        -0.10278717, -0.02697964],\n",
       "       [ 0.38569278, -0.07069244, -0.8477959 , ...,  0.0253919 ,\n",
       "        -0.06603534, -0.02828273],\n",
       "       ...,\n",
       "       [ 0.02713387,  0.24139147, -0.22735251, ..., -0.82107705,\n",
       "         1.036363  , -0.83661443],\n",
       "       [ 0.13954346,  0.02888298,  0.89947975, ..., -0.9852566 ,\n",
       "         1.6735605 ,  0.10965873],\n",
       "       [ 0.08306409,  0.09089889,  0.8885408 , ..., -0.79955566,\n",
       "         1.5193683 ,  0.2632099 ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ceb1327-ac4a-484b-83da-da9000dff9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33960226, -0.03074448, -0.90138096, ...,  0.01558092,\n",
       "        -0.02386307, -0.02200161],\n",
       "       [-0.13179901, -0.02574519, -0.67730105, ..., -0.03992649,\n",
       "        -0.10278717, -0.02697964],\n",
       "       [ 0.38569278, -0.07069244, -0.8477959 , ...,  0.0253919 ,\n",
       "        -0.06603534, -0.02828273],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_features = features\n",
    "masked_features[torch.tensor(complement_indices)] = 0\n",
    "masked_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "498292e0-0f9e-4642-abf1-004d6da28559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 0.6916, Macro_F1: 0.3381, AUC_score: 0.5823\n",
      "Validation loss decreased (0.691620 --> 0.691620).\n",
      "Validation loss decreased (0.690761 --> 0.690761).\n",
      "Validation loss decreased (0.690443 --> 0.690443).\n",
      "Validation loss decreased (0.690369 --> 0.690369).\n",
      "Validation loss decreased (0.690162 --> 0.690162).\n",
      "Validation loss decreased (0.689873 --> 0.689873).\n",
      "Validation loss decreased (0.688683 --> 0.688683).\n",
      "Validation loss decreased (0.687966 --> 0.687966).\n",
      "Validation loss decreased (0.687802 --> 0.687802).\n",
      "Validation loss decreased (0.686706 --> 0.686706).\n",
      "Validation loss decreased (0.686456 --> 0.686456).\n",
      "Validation loss decreased (0.686115 --> 0.686115).\n",
      "Validation loss decreased (0.685075 --> 0.685075).\n",
      "Epoch 50: Train Loss: 0.6859, Macro_F1: 0.6323, AUC_score: 0.6673\n",
      "Validation loss decreased (0.684499 --> 0.684499).\n",
      "Validation loss decreased (0.682833 --> 0.682833).\n",
      "Validation loss decreased (0.682796 --> 0.682796).\n",
      "Validation loss decreased (0.682426 --> 0.682426).\n",
      "Validation loss decreased (0.681903 --> 0.681903).\n",
      "Epoch 100: Train Loss: 0.6822, Macro_F1: 0.6153, AUC_score: 0.6722\n",
      "Validation loss decreased (0.681460 --> 0.681460).\n",
      "Validation loss decreased (0.681036 --> 0.681036).\n",
      "Validation loss decreased (0.681007 --> 0.681007).\n",
      "Validation loss decreased (0.680194 --> 0.680194).\n",
      "Validation loss decreased (0.678985 --> 0.678985).\n",
      "Epoch 150: Train Loss: 0.6804, Macro_F1: 0.6130, AUC_score: 0.6770\n",
      "Validation loss decreased (0.678129 --> 0.678129).\n",
      "Validation loss decreased (0.677457 --> 0.677457).\n",
      "Validation loss decreased (0.677029 --> 0.677029).\n",
      "Validation loss decreased (0.676841 --> 0.676841).\n",
      "Validation loss decreased (0.676830 --> 0.676830).\n",
      "Epoch 200: Train Loss: 0.6783, Macro_F1: 0.6181, AUC_score: 0.6804\n",
      "Validation loss decreased (0.676573 --> 0.676573).\n",
      "Validation loss decreased (0.676432 --> 0.676432).\n",
      "Validation loss decreased (0.676079 --> 0.676079).\n",
      "Validation loss decreased (0.675711 --> 0.675711).\n",
      "Validation loss decreased (0.673307 --> 0.673307).\n",
      "Epoch 250: Train Loss: 0.6768, Macro_F1: 0.6276, AUC_score: 0.6851\n",
      "Validation loss decreased (0.672775 --> 0.672775).\n",
      "Validation loss decreased (0.671851 --> 0.671851).\n",
      "Validation loss decreased (0.671641 --> 0.671641).\n",
      "Validation loss decreased (0.671475 --> 0.671475).\n",
      "Epoch 300: Train Loss: 0.6746, Macro_F1: 0.6181, AUC_score: 0.6909\n",
      "Validation loss decreased (0.669580 --> 0.669580).\n",
      "Validation loss decreased (0.668868 --> 0.668868).\n",
      "Epoch 350: Train Loss: 0.6691, Macro_F1: 0.6370, AUC_score: 0.6941\n",
      "Validation loss decreased (0.668038 --> 0.668038).\n",
      "Validation loss decreased (0.666556 --> 0.666556).\n",
      "Validation loss decreased (0.666014 --> 0.666014).\n",
      "Validation loss decreased (0.665812 --> 0.665812).\n",
      "Epoch 400: Train Loss: 0.6682, Macro_F1: 0.6332, AUC_score: 0.6950\n",
      "Validation loss decreased (0.664573 --> 0.664573).\n",
      "Validation loss decreased (0.664393 --> 0.664393).\n",
      "Validation loss decreased (0.663463 --> 0.663463).\n",
      "Validation loss decreased (0.663449 --> 0.663449).\n",
      "Epoch 450: Train Loss: 0.6667, Macro_F1: 0.6220, AUC_score: 0.6997\n",
      "Validation loss decreased (0.662804 --> 0.662804).\n",
      "Validation loss decreased (0.662066 --> 0.662066).\n",
      "Validation loss decreased (0.660729 --> 0.660729).\n",
      "Epoch 500: Train Loss: 0.6655, Macro_F1: 0.6449, AUC_score: 0.7020\n",
      "Validation loss decreased (0.659603 --> 0.659603).\n",
      "Validation loss decreased (0.657600 --> 0.657600).\n",
      "Epoch 550: Train Loss: 0.6625, Macro_F1: 0.6377, AUC_score: 0.7053\n",
      "Validation loss decreased (0.656330 --> 0.656330).\n",
      "Epoch 600: Train Loss: 0.6594, Macro_F1: 0.6522, AUC_score: 0.7105\n",
      "Validation loss decreased (0.655128 --> 0.655128).\n",
      "Epoch 650: Train Loss: 0.6543, Macro_F1: 0.6485, AUC_score: 0.7115\n",
      "Validation loss decreased (0.654333 --> 0.654333).\n",
      "Validation loss decreased (0.653760 --> 0.653760).\n",
      "Validation loss decreased (0.652950 --> 0.652950).\n",
      "Validation loss decreased (0.650441 --> 0.650441).\n",
      "Epoch 700: Train Loss: 0.6574, Macro_F1: 0.6437, AUC_score: 0.7138\n",
      "Epoch 750: Train Loss: 0.6555, Macro_F1: 0.6435, AUC_score: 0.7136\n",
      "Validation loss decreased (0.650046 --> 0.650046).\n",
      "Epoch 800: Train Loss: 0.6525, Macro_F1: 0.6556, AUC_score: 0.7211\n",
      "Validation loss decreased (0.648122 --> 0.648122).\n",
      "Epoch 850: Train Loss: 0.6551, Macro_F1: 0.6475, AUC_score: 0.7230\n",
      "Validation loss decreased (0.647853 --> 0.647853).\n",
      "Validation loss decreased (0.647557 --> 0.647557).\n",
      "Validation loss decreased (0.647236 --> 0.647236).\n",
      "Validation loss decreased (0.647199 --> 0.647199).\n",
      "Epoch 900: Train Loss: 0.6495, Macro_F1: 0.6593, AUC_score: 0.7237\n",
      "Validation loss decreased (0.645959 --> 0.645959).\n",
      "Epoch 950: Train Loss: 0.6507, Macro_F1: 0.6593, AUC_score: 0.7285\n",
      "Validation loss decreased (0.645155 --> 0.645155).\n",
      "Validation loss decreased (0.644336 --> 0.644336).\n",
      "Validation loss decreased (0.643308 --> 0.643308).\n",
      "Epoch 1000: Train Loss: 0.6491, Macro_F1: 0.6422, AUC_score: 0.7263\n",
      "Validation loss decreased (0.641857 --> 0.641857).\n",
      "Validation loss decreased (0.639593 --> 0.639593).\n",
      "Epoch 1050: Train Loss: 0.6470, Macro_F1: 0.6630, AUC_score: 0.7305\n",
      "Epoch 1100: Train Loss: 0.6458, Macro_F1: 0.6548, AUC_score: 0.7307\n",
      "Epoch 01128: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 1150: Train Loss: 0.6456, Macro_F1: 0.6556, AUC_score: 0.7308\n",
      "Validation loss decreased (0.637524 --> 0.637524).\n",
      "Epoch 1200: Train Loss: 0.6465, Macro_F1: 0.6556, AUC_score: 0.7311\n",
      "Epoch 1250: Train Loss: 0.6444, Macro_F1: 0.6548, AUC_score: 0.7305\n",
      "Epoch 01255: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 1300: Train Loss: 0.6451, Macro_F1: 0.6554, AUC_score: 0.7313\n",
      "Epoch 1350: Train Loss: 0.6415, Macro_F1: 0.6556, AUC_score: 0.7317\n",
      "Epoch 01356: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 1400: Train Loss: 0.6415, Macro_F1: 0.6627, AUC_score: 0.7313\n",
      "Epoch 1450: Train Loss: 0.6407, Macro_F1: 0.6627, AUC_score: 0.7317\n",
      "Early stopping triggered\n",
      "acc save\n",
      "80.0% node features transform to 0: F1: 0.6627, AUC_score: 0.7317\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(patience=300, verbose=True, delta=0.00001)\n",
    "model = GCN(num_features=data.x.shape[1], hidden_dim=64, num_classes=2, num_layers=2, activation=F.relu, dropout=0.5)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=100, verbose=True)\n",
    "num_epochs = 2500\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=100, verbose=True)\n",
    "\n",
    "# 模型训练和评估逻辑\n",
    "for epoch in range(2000):\n",
    "    train_loss = train_model_scheduler(model, masked_features, data.y, data.edge_index, optimizer, loss_fn, scheduler, train_mask)\n",
    "    test_f1, test_auc = evaluate_model(model, masked_features, data.y, data.edge_index, test_mask)\n",
    "\n",
    "    if epoch % 50 == 0:  # 每10个epoch打印一次信息\n",
    "        print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Macro_F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "    early_stopping(train_loss, model, i)\n",
    "    if early_stopping.early_stop or epoch == num_epochs - 1:\n",
    "        results.append({\n",
    "                        'Train Loss': train_loss,\n",
    "                        'F1': test_f1,\n",
    "                        'AUC_score': test_auc\n",
    "                    })\n",
    "        print(\"acc save\")\n",
    "        rate = 1 - count * (i + 1)\n",
    "        rate = rate * 100\n",
    "        print(f'{rate}% node features transform to 0: F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "        torch.save(model.state_dict(), f'G-G_DATA/G-G_model_WITHOUT{i}.pth')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "370d2301-2315-48aa-9638-d5c101d6cb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_features = features\n",
    "masked_features[torch.tensor(remaining_indices)] = 0\n",
    "masked_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0ac5fb13-60ea-4b60-a61e-274e55eabfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 0.8644, Macro_F1: 0.6356, AUC_score: 0.5760\n",
      "Validation loss decreased (0.864432 --> 0.864432).\n",
      "Validation loss decreased (0.683808 --> 0.683808).\n",
      "Validation loss decreased (0.530536 --> 0.530536).\n",
      "Validation loss decreased (0.459387 --> 0.459387).\n",
      "Validation loss decreased (0.434727 --> 0.434727).\n",
      "Validation loss decreased (0.424424 --> 0.424424).\n",
      "Validation loss decreased (0.394828 --> 0.394828).\n",
      "Validation loss decreased (0.345164 --> 0.345164).\n",
      "Validation loss decreased (0.317922 --> 0.317922).\n",
      "Validation loss decreased (0.312071 --> 0.312071).\n",
      "Validation loss decreased (0.286887 --> 0.286887).\n",
      "Validation loss decreased (0.274502 --> 0.274502).\n",
      "Validation loss decreased (0.270713 --> 0.270713).\n",
      "Validation loss decreased (0.244378 --> 0.244378).\n",
      "Validation loss decreased (0.229917 --> 0.229917).\n",
      "Validation loss decreased (0.229369 --> 0.229369).\n",
      "Validation loss decreased (0.226182 --> 0.226182).\n",
      "Validation loss decreased (0.204944 --> 0.204944).\n",
      "Validation loss decreased (0.202765 --> 0.202765).\n",
      "Validation loss decreased (0.192387 --> 0.192387).\n",
      "Validation loss decreased (0.179755 --> 0.179755).\n",
      "Validation loss decreased (0.161278 --> 0.161278).\n",
      "Epoch 50: Train Loss: 0.1599, Macro_F1: 0.8875, AUC_score: 0.9263\n",
      "Validation loss decreased (0.159892 --> 0.159892).\n",
      "Validation loss decreased (0.158478 --> 0.158478).\n",
      "Validation loss decreased (0.145274 --> 0.145274).\n",
      "Validation loss decreased (0.134069 --> 0.134069).\n",
      "Validation loss decreased (0.133811 --> 0.133811).\n",
      "Validation loss decreased (0.133493 --> 0.133493).\n",
      "Validation loss decreased (0.131146 --> 0.131146).\n",
      "Validation loss decreased (0.130784 --> 0.130784).\n",
      "Validation loss decreased (0.128326 --> 0.128326).\n",
      "Validation loss decreased (0.124633 --> 0.124633).\n",
      "Validation loss decreased (0.122246 --> 0.122246).\n",
      "Validation loss decreased (0.111174 --> 0.111174).\n",
      "Validation loss decreased (0.107121 --> 0.107121).\n",
      "Validation loss decreased (0.103914 --> 0.103914).\n",
      "Validation loss decreased (0.096973 --> 0.096973).\n",
      "Validation loss decreased (0.095193 --> 0.095193).\n",
      "Epoch 100: Train Loss: 0.1532, Macro_F1: 0.8837, AUC_score: 0.9307\n",
      "Validation loss decreased (0.087562 --> 0.087562).\n",
      "Validation loss decreased (0.084328 --> 0.084328).\n",
      "Validation loss decreased (0.078109 --> 0.078109).\n",
      "Validation loss decreased (0.074771 --> 0.074771).\n",
      "Validation loss decreased (0.069876 --> 0.069876).\n",
      "Validation loss decreased (0.063147 --> 0.063147).\n",
      "Epoch 150: Train Loss: 0.0655, Macro_F1: 0.8911, AUC_score: 0.9305\n",
      "Validation loss decreased (0.062615 --> 0.062615).\n",
      "Validation loss decreased (0.060532 --> 0.060532).\n",
      "Validation loss decreased (0.052802 --> 0.052802).\n",
      "Validation loss decreased (0.052139 --> 0.052139).\n",
      "Validation loss decreased (0.043013 --> 0.043013).\n",
      "Validation loss decreased (0.042507 --> 0.042507).\n",
      "Epoch 200: Train Loss: 0.0708, Macro_F1: 0.8985, AUC_score: 0.9363\n",
      "Validation loss decreased (0.040076 --> 0.040076).\n",
      "Validation loss decreased (0.039030 --> 0.039030).\n",
      "Validation loss decreased (0.038046 --> 0.038046).\n",
      "Validation loss decreased (0.035720 --> 0.035720).\n",
      "Epoch 250: Train Loss: 0.0386, Macro_F1: 0.9022, AUC_score: 0.9380\n",
      "Validation loss decreased (0.032971 --> 0.032971).\n",
      "Validation loss decreased (0.031400 --> 0.031400).\n",
      "Epoch 300: Train Loss: 0.0382, Macro_F1: 0.9058, AUC_score: 0.9410\n",
      "Validation loss decreased (0.026031 --> 0.026031).\n",
      "Validation loss decreased (0.025680 --> 0.025680).\n",
      "Validation loss decreased (0.024461 --> 0.024461).\n",
      "Validation loss decreased (0.022818 --> 0.022818).\n",
      "Epoch 350: Train Loss: 0.0416, Macro_F1: 0.9130, AUC_score: 0.9428\n",
      "Validation loss decreased (0.017584 --> 0.017584).\n",
      "Epoch 400: Train Loss: 0.0266, Macro_F1: 0.9057, AUC_score: 0.9431\n",
      "Validation loss decreased (0.017393 --> 0.017393).\n",
      "Validation loss decreased (0.016541 --> 0.016541).\n",
      "Epoch 450: Train Loss: 0.0244, Macro_F1: 0.9092, AUC_score: 0.9421\n",
      "Validation loss decreased (0.016177 --> 0.016177).\n",
      "Epoch 500: Train Loss: 0.0208, Macro_F1: 0.9058, AUC_score: 0.9451\n",
      "Validation loss decreased (0.014917 --> 0.014917).\n",
      "Epoch 550: Train Loss: 0.0225, Macro_F1: 0.9167, AUC_score: 0.9471\n",
      "Validation loss decreased (0.014421 --> 0.014421).\n",
      "Validation loss decreased (0.013656 --> 0.013656).\n",
      "Validation loss decreased (0.012793 --> 0.012793).\n",
      "Epoch 600: Train Loss: 0.0186, Macro_F1: 0.9130, AUC_score: 0.9487\n",
      "Validation loss decreased (0.010683 --> 0.010683).\n",
      "Epoch 650: Train Loss: 0.0232, Macro_F1: 0.9130, AUC_score: 0.9474\n",
      "Epoch 700: Train Loss: 0.0201, Macro_F1: 0.9022, AUC_score: 0.9479\n",
      "Epoch 00712: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.010665 --> 0.010665).\n",
      "Epoch 750: Train Loss: 0.0152, Macro_F1: 0.9094, AUC_score: 0.9481\n",
      "Validation loss decreased (0.009786 --> 0.009786).\n",
      "Validation loss decreased (0.009020 --> 0.009020).\n",
      "Epoch 800: Train Loss: 0.0124, Macro_F1: 0.9166, AUC_score: 0.9478\n",
      "Epoch 850: Train Loss: 0.0108, Macro_F1: 0.9130, AUC_score: 0.9476\n",
      "Epoch 00883: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 900: Train Loss: 0.0141, Macro_F1: 0.9130, AUC_score: 0.9478\n",
      "Validation loss decreased (0.008561 --> 0.008561).\n",
      "Epoch 950: Train Loss: 0.0174, Macro_F1: 0.9167, AUC_score: 0.9479\n",
      "Epoch 1000: Train Loss: 0.0157, Macro_F1: 0.9130, AUC_score: 0.9476\n",
      "Epoch 01004: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 1050: Train Loss: 0.0099, Macro_F1: 0.9167, AUC_score: 0.9475\n",
      "Epoch 1100: Train Loss: 0.0246, Macro_F1: 0.9167, AUC_score: 0.9475\n",
      "Epoch 01105: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 1150: Train Loss: 0.0129, Macro_F1: 0.9167, AUC_score: 0.9476\n",
      "Epoch 1200: Train Loss: 0.0176, Macro_F1: 0.9167, AUC_score: 0.9476\n",
      "Early stopping triggered\n",
      "acc save\n",
      "F1: 0.9167, AUC_score: 0.9476\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(patience=300, verbose=True, delta=0.00001)\n",
    "model = GCN(num_features=data.x.shape[1], hidden_dim=64, num_classes=2, num_layers=2, activation=F.relu, dropout=0.5)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=100, verbose=True)\n",
    "num_epochs = 2500\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=100, verbose=True)\n",
    "\n",
    "# 模型训练和评估逻辑\n",
    "for epoch in range(2000):\n",
    "    train_loss = train_model_scheduler(model, masked_features, data.y, data.edge_index, optimizer, loss_fn, scheduler, train_mask)\n",
    "    test_f1, test_auc = evaluate_model(model, masked_features, data.y, data.edge_index, test_mask)\n",
    "\n",
    "    if epoch % 50 == 0:  # 每10个epoch打印一次信息\n",
    "        print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Macro_F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "    early_stopping(train_loss, model, i)\n",
    "    if early_stopping.early_stop or epoch == num_epochs - 1:\n",
    "        results.append({\n",
    "                        'Train Loss': train_loss,\n",
    "                        'F1': test_f1,\n",
    "                        'AUC_score': test_auc\n",
    "                    })\n",
    "        print(\"acc save\")\n",
    "        print(f'F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "        #torch.save(model.state_dict(), f'G-G_DATA/G-G_model_WITHOUT{i}.pth')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5bf8c92-8d3d-4179-9cd6-06563d4daabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>protein</th>\n",
       "      <th>Solubility</th>\n",
       "      <th>Label</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>Count_Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ERAP2</td>\n",
       "      <td>Membrane</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ADAMTSL5</td>\n",
       "      <td>Soluble</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>TBC1D30</td>\n",
       "      <td>Membrane</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>KCNK18</td>\n",
       "      <td>Membrane</td>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NDNF</td>\n",
       "      <td>Soluble</td>\n",
       "      <td>1</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>1374</td>\n",
       "      <td>TRABD2B</td>\n",
       "      <td>Membrane</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>1375</td>\n",
       "      <td>RPS9</td>\n",
       "      <td>Soluble</td>\n",
       "      <td>1</td>\n",
       "      <td>205</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>1376</td>\n",
       "      <td>SLC22A16</td>\n",
       "      <td>Membrane</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>1377</td>\n",
       "      <td>FBN3</td>\n",
       "      <td>Soluble</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378</th>\n",
       "      <td>1378</td>\n",
       "      <td>BDH2</td>\n",
       "      <td>Soluble</td>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1379 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0   protein Solubility  Label  Word_Count  Count_Category\n",
       "0              0     ERAP2   Membrane      0         117               0\n",
       "1              1  ADAMTSL5    Soluble      1          28               1\n",
       "2              2   TBC1D30   Membrane      0          55               0\n",
       "3              3    KCNK18   Membrane      0         184               0\n",
       "4              4      NDNF    Soluble      1         129               0\n",
       "...          ...       ...        ...    ...         ...             ...\n",
       "1374        1374   TRABD2B   Membrane      0          96               0\n",
       "1375        1375      RPS9    Soluble      1         205               0\n",
       "1376        1376  SLC22A16   Membrane      0          93               0\n",
       "1377        1377      FBN3    Soluble      1          90               0\n",
       "1378        1378      BDH2    Soluble      1         102               0\n",
       "\n",
       "[1379 rows x 6 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f11f3045-77ba-4ff9-bc52-08dd0fa0b0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67, 93, 94, 100, 116, 129, 142, 148, 157, 167, 228, 236, 241, 255, 276, 287, 294, 321, 334, 340, 369, 373, 375, 383, 384, 399, 402, 418, 447, 474, 483, 490, 498, 511, 518, 526, 558, 569, 586, 587, 593, 594, 595, 596, 599, 615, 641, 654, 664, 682, 736, 754, 764, 777, 833, 852, 894, 895, 899, 915, 917, 938, 944, 957, 982, 983, 997, 1003, 1019, 1040, 1053, 1062, 1064, 1065, 1067, 1074, 1075, 1078, 1079, 1105, 1108, 1115, 1116, 1120, 1124, 1131, 1136, 1137, 1143, 1157, 1170, 1173, 1183, 1195, 1196, 1197, 1215, 1235, 1236, 1237, 1243, 1256, 1261, 1273, 1274, 1277, 1311, 1315, 1327, 1333, 1347, 1363, 1377, 1378, 1392, 1397, 1427, 1444, 1445, 1469, 1470, 1475, 1484, 1489, 1492, 1493, 1521, 1548, 1559, 1575, 1593, 1609, 1614, 1617, 1619, 1637, 1638, 1674, 1681, 1720, 1733, 1736, 1737, 1759, 1776, 1795, 1807, 1833, 1834, 1844, 1849, 1852, 1854, 1882, 1893, 1895, 1898, 1914, 1917, 1923, 1960, 1970, 1972, 1978, 1986, 1995, 2015, 2031, 2046, 2048, 2071, 2096, 2097, 2101, 2109, 2115, 2117, 2140, 2151, 2154, 2166, 2188, 2209, 2214, 2227, 2231, 2238, 2250, 2256, 2265, 2273, 2281, 2284, 2286, 2292, 2293, 2302, 2308, 2326, 2337, 2353, 2354, 2362, 2364, 2365, 2372, 2400, 2417, 2419, 2421, 2422, 2424, 2427, 2455, 2465, 2476, 2481, 2489, 2496, 2505, 2532, 2533, 2535, 2540, 2551, 2556, 2589, 2620, 2630, 2634, 2660, 2672, 2676, 2686, 2701, 2708, 2730, 2738, 2767, 2770, 2785, 2788, 2802, 2804, 2819, 2823, 2827, 2842, 2871, 2879, 2888, 2896, 2903, 2907, 2915, 2927, 2928, 2937, 2957, 2958, 2960, 2982, 2986, 3000, 3013, 3014, 3027, 3037, 3044, 3054, 3057, 3064, 3084, 3092, 3099, 3117, 3123, 3147, 3150, 3180, 3186, 3217, 3240, 3244, 3255, 3268, 3272, 3285, 3291, 3319, 3325, 3326, 3330, 3353, 3382, 3393, 3408, 3410, 3417, 3433, 3454, 3456, 3465, 3486, 3502, 3511, 3519, 3530, 3535, 3538, 3543, 3615, 3630, 3647, 3663, 3680, 3706, 3707, 3715, 3727, 3747, 3754, 3774, 3779, 3781, 3783, 3785, 3786, 3792, 3797, 3802, 3810, 3829, 3833, 3839, 3882, 3884, 3902, 3917, 3921, 3923, 3929, 3943, 3949, 3959, 3960, 3968, 3969, 3977, 3985, 3990, 3991, 3992, 4019, 4025, 4034, 4040, 4048, 4049, 4102, 4120, 4133, 4159, 4165, 4166, 4169, 4178, 4184, 4185, 4197, 4209, 4225, 4232, 4239, 4249, 4251, 4258, 4288, 4323, 4328, 4332, 4333, 4343, 4351, 4360, 4365, 4375, 4376, 4387, 4417, 4431, 4433, 4438, 4439, 4440, 4464, 4465, 4484, 4485, 4495, 4500, 4507, 4520, 4524, 4526, 4545, 4558, 4560, 4565, 4572, 4607, 4616, 4630, 4637, 4653, 4656, 4668, 4676, 4711, 4716, 4721, 4751, 4760, 4776, 4780, 4783, 4792, 4795, 4800, 4804, 4806, 4833, 4836, 4840, 4853, 4882, 4888, 4897, 4913, 4915, 4926, 4930, 4935, 4961, 4963, 4995, 5006, 5013, 5020, 5041, 5061, 5072, 5077, 5079, 5083, 5124, 5139, 5144, 5146, 5151, 5154, 5167, 5170, 5178, 5192, 5209, 5216, 5218, 5226, 5228, 5233, 5242, 5243, 5253, 5264, 5271, 5278, 5280, 5298, 5306, 5312, 5351, 5379, 5396, 5399, 5405, 5410, 5418, 5437, 5486, 5488, 5506, 5510, 5512, 5513, 5522, 5556, 5566, 5569, 5570, 5575, 5582, 5586, 5589, 5593, 5628, 5635, 5644, 5670, 5674, 5687, 5693, 5702, 5703, 5713, 5721, 5736, 5738, 5784, 5785, 5787, 5792, 5820, 5823, 5884, 5890, 5898, 5899, 5902, 5943, 5954, 5958, 5963, 5985, 5991, 5992, 6006, 6011, 6038, 6043, 6057, 6058, 6064, 6078, 6081, 6094, 6101, 6112, 6119, 6120, 6127, 6128, 6131, 6134, 6136, 6177, 6179, 6196, 6203, 6211, 6223, 6227, 6239, 6246, 6301, 6302, 6314, 6319, 6328, 6343, 6377, 6389, 6416, 6426, 6427, 6428, 6436, 6438, 6448, 6460, 6494, 6514, 6517, 6518, 6527, 6528, 6549, 6559, 6562, 6567, 6579, 6583, 6595, 6606, 6607, 6612, 6623, 6628, 6630, 6639, 6667, 6679, 6681, 6699, 6702, 6703, 6714, 6717, 6728, 6744, 6772, 6774, 6782, 6785, 6792, 6804, 6815, 6819, 6823, 6832, 6844, 6850, 6859, 6865, 6871, 6884, 6885, 6886, 6943, 6947, 6963, 6995, 7000, 7017, 7018, 7019, 7026, 7040, 7047, 7049, 7065, 7067, 7073, 7074, 7081, 7083, 7089, 7093, 7108, 7116, 7125, 7130, 7139, 7151, 7152, 7153, 7157, 7170, 7185, 7189, 7192, 7201, 7202, 7203, 7221, 7252, 7266, 7278, 7280, 7281, 7286, 7296, 7300, 7312, 7313, 7341, 7349, 7357, 7375, 7377, 7387, 7389, 7390, 7412, 7415, 7420, 7427, 7433, 7440, 7456, 7465, 7469, 7481, 7510, 7520, 7527, 7528, 7549, 7561, 7563, 7564, 7573, 7590, 7591, 7599, 7609, 7610, 7631, 7634, 7640, 7653, 7654, 7665, 7688, 7692, 7705, 7715, 7721, 7755, 7760, 7769, 7782, 7793, 7839, 7865, 7889, 7891, 7892, 7921, 7930, 7956, 7958, 8013, 8031, 8041, 8046, 8060, 8066, 8073, 8075, 8077, 8078, 8124, 8139, 8156, 8175, 8189, 8196, 8204, 8212, 8215, 8225, 8226, 8228, 8229, 8234, 8243, 8250, 8262, 8266, 8272, 8285, 8290, 8303, 8305, 8323, 8327, 8329, 8339, 8343, 8348, 8350, 8351, 8356, 8357, 8360, 8366, 8383, 8386, 8405, 8413, 8419, 8441, 8447, 8454, 8467, 8491, 8492, 8497, 8517, 8519, 8554, 8557, 8560, 8567, 8568, 8576, 8592, 8595, 8600, 8608, 8631, 8632, 8633, 8641, 8645, 8647, 8653, 8672, 8683, 8691, 8716, 8720, 8727, 8733, 8741, 8746, 8748, 8749, 8786, 8808, 8814, 8815, 8818, 8820, 8826, 8828, 8865, 8927, 8931, 8942, 8946, 8991, 8998, 9013, 9026, 9027, 9056, 9066, 9085, 9097, 9098, 9115, 9126, 9134, 9135, 9144, 9148, 9189, 9190, 9225, 9246, 9263, 9293, 9301, 9302, 9323, 9325, 9364, 9410, 9416, 9423, 9431, 9435, 9437, 9457, 9500, 9512, 9514, 9517, 9535, 9547, 9552, 9561, 9573, 9627, 9638, 9639, 9659, 9682, 9708, 9757, 9776, 9784, 9800, 9809, 9822, 9823, 9828, 9836, 9853, 9862, 9875, 9885, 9908, 9910, 9926, 9937, 9978, 10000, 10010, 10023, 10031, 10045, 10046, 10053, 10060, 10063, 10075, 10090, 10094, 10119, 10125, 10138, 10146, 10147, 10151, 10158, 10159, 10168, 10172, 10183, 10201, 10217, 10246, 10298, 10300, 10309, 10323, 10325, 10327, 10364, 10372, 10379, 10390, 10411, 10418, 10424, 10435, 10436, 10443, 10452, 10459, 10479, 10483, 10485, 10500, 10502, 10527, 10534, 10536, 10542, 10552, 10561, 10580, 10581, 10607, 10619, 10622, 10623, 10655, 10666, 10685, 10686, 10695, 10699, 10700, 10701, 10705, 10730, 10732, 10751, 10764, 10765, 10776, 10779, 10789, 10825, 10828, 10862, 10877, 10887, 10890, 10905, 10917, 10926, 10934, 10951, 10988, 11000, 11002, 11022, 11028, 11033, 11035, 11040, 11050, 11089, 11108, 11109, 11140, 11142, 11150, 11164, 11175, 11178, 11184, 11187, 11192, 11210, 11222, 11247, 11248, 11273, 11277, 11278, 11286, 11289, 11296, 11307, 11321, 11324, 11340, 11343, 11345, 11362, 11406, 11407, 11410, 11418, 11428, 11458, 11482, 11492, 11497, 11523, 11533, 11538, 11542, 11548, 11549, 11562, 11572, 11583, 11601, 11638, 11655, 11657, 11676, 11711, 11734, 11752, 11774, 11803, 11805, 11844, 11928, 11938, 11951, 11958, 11967, 11984, 11987, 12071, 12080, 12102, 12103, 12107, 12111, 12116, 12124, 12130, 12142, 12154, 12169, 12174, 12193, 12194, 12199, 12262, 12289, 12296, 12304, 12323, 12345, 12358, 12373, 12429, 12457, 12470, 12487, 12488, 12511, 12578, 12579, 12587, 12593, 12666, 12667, 12684, 12705, 12706, 12776, 12784, 12805, 12819, 12821, 12864, 12873, 12954, 12955, 12956, 12958, 12959, 12974, 13017, 13022, 13042, 13078, 13084, 13090, 13120, 13130, 13138, 13165, 13181, 13186, 13192, 13206, 13214, 13243, 13251, 13266, 13292, 13295, 13305, 13314, 13321, 13334, 13394, 13409, 13422, 13443, 13447, 13469, 13502, 13512, 13536, 13567, 13571, 13580, 13582, 13586, 13594, 13662, 13672, 13673, 13724, 13727, 13741, 13760, 13779, 13826, 13831, 13838, 13847, 13855, 13858, 13874, 13882, 13892, 13898, 13901, 13906, 13924, 13931, 13943, 13951, 13965, 13984, 14036, 14052, 14090, 14117, 14146, 14214, 14234, 14240, 14241, 14263, 14268, 14279, 14281, 14288, 14293, 14315, 14317, 14318, 14324, 14325, 14398, 14407, 14426]\n",
      "[81, 149, 170, 202, 243, 258, 281, 305, 319, 455, 484, 585, 653, 762, 773, 781, 1047, 1262, 1371, 1383, 1426, 1431, 1435, 1527, 1540, 1606, 1645, 1710, 1823, 1848, 1927, 2016, 2022, 2099, 2104, 2112, 2119, 2196, 2274, 2277, 2283, 2316, 2358, 2370, 2625, 2705, 2716, 2836, 2862, 2929, 2950, 2995, 3021, 3024, 3043, 3159, 3305, 3327, 3356, 3389, 3536, 3584, 3619, 3620, 3684, 3692, 3751, 3752, 3850, 3886, 4071, 4361, 4513, 4539, 4712, 4796, 4815, 4999, 5085, 5173, 5327, 5383, 5555, 5634, 5710, 5793, 5866, 5900, 5913, 6066, 6099, 6168, 6226, 6278, 6516, 6556, 6569, 6593, 7365, 7449, 7645, 7783, 8012, 8231, 8349, 8389, 8451, 8480, 8535, 8903, 9165, 9231, 9385, 9589, 9607, 9772, 9852, 9900, 10038, 10321, 10409, 10410, 10432, 10629, 10697, 10708, 10754, 10794, 11125, 11609, 11634, 11658, 11688, 11707, 11746, 11749, 11758, 11810, 11820, 11887, 12151, 12205, 12271, 12272, 12284, 12302, 12348, 12386, 12432, 12517, 12760, 12848, 12889, 12918, 13001, 13003, 13045, 13049, 13083, 13088, 13164, 13559, 13566, 13657, 13666, 13685, 13862, 13871, 13872, 13936, 13978, 14012, 14015, 14024, 14151, 14162]\n"
     ]
    }
   ],
   "source": [
    "highinfo_indices = [\n",
    "    node_id_to_index[node_id]\n",
    "    for node_id, weight in zip(labels_df['protein'], labels_df['Count_Category'])\n",
    "    if weight == 0\n",
    "]\n",
    "print(highinfo_indices)\n",
    "lowinfo_indices = [\n",
    "    node_id_to_index[node_id]\n",
    "    for node_id, weight in zip(labels_df['protein'], labels_df['Count_Category'])\n",
    "    if weight == 1\n",
    "]\n",
    "print(lowinfo_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "772f0b59-7d37-446a-977a-a9cf27678c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1203\n",
      "176\n"
     ]
    }
   ],
   "source": [
    "print(len(highinfo_indices))\n",
    "print(len(lowinfo_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e5da019-cf3c-41b6-babf-569908c1ef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_indices = torch.tensor(highinfo_indices, dtype=torch.long)\n",
    "high_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "high_mask[high_indices] = True\n",
    "test_high = high_mask & test_mask\n",
    "low_indices = torch.tensor(lowinfo_indices, dtype=torch.long)\n",
    "low_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "low_mask[low_indices] = True\n",
    "test_low = low_mask & test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6088e98d-5eca-47a6-8c30-8cf14bd08556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(240)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_count = sum(test_high)\n",
    "true_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "453c8e14-b259-4ee4-97c4-08ce0719d8a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(36)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_count = sum(test_low)\n",
    "true_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa37c8f3-bc93-42bc-b51d-c5a3bfbc4369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 0.6922, Macro_F1: 0.5085, AUC_score: 0.7057\n",
      "Validation loss decreased (0.692233 --> 0.692233).\n",
      "Validation loss decreased (0.688132 --> 0.688132).\n",
      "Validation loss decreased (0.685286 --> 0.685286).\n",
      "Validation loss decreased (0.682163 --> 0.682163).\n",
      "Validation loss decreased (0.677474 --> 0.677474).\n",
      "Validation loss decreased (0.677233 --> 0.677233).\n",
      "Validation loss decreased (0.673267 --> 0.673267).\n",
      "Validation loss decreased (0.667494 --> 0.667494).\n",
      "Validation loss decreased (0.666953 --> 0.666953).\n",
      "Validation loss decreased (0.661544 --> 0.661544).\n",
      "Validation loss decreased (0.661447 --> 0.661447).\n",
      "Validation loss decreased (0.660158 --> 0.660158).\n",
      "Validation loss decreased (0.654400 --> 0.654400).\n",
      "Validation loss decreased (0.653681 --> 0.653681).\n",
      "Validation loss decreased (0.653354 --> 0.653354).\n",
      "Validation loss decreased (0.644886 --> 0.644886).\n",
      "Validation loss decreased (0.643683 --> 0.643683).\n",
      "Validation loss decreased (0.641803 --> 0.641803).\n",
      "Validation loss decreased (0.640413 --> 0.640413).\n",
      "Validation loss decreased (0.636384 --> 0.636384).\n",
      "Validation loss decreased (0.631355 --> 0.631355).\n",
      "Validation loss decreased (0.629924 --> 0.629924).\n",
      "Validation loss decreased (0.627742 --> 0.627742).\n",
      "Validation loss decreased (0.626329 --> 0.626329).\n",
      "Validation loss decreased (0.621716 --> 0.621716).\n",
      "Validation loss decreased (0.613844 --> 0.613844).\n",
      "Validation loss decreased (0.612734 --> 0.612734).\n",
      "Validation loss decreased (0.606744 --> 0.606744).\n",
      "Validation loss decreased (0.601002 --> 0.601002).\n",
      "Validation loss decreased (0.597646 --> 0.597646).\n",
      "Validation loss decreased (0.597016 --> 0.597016).\n",
      "Validation loss decreased (0.586612 --> 0.586612).\n",
      "Validation loss decreased (0.583418 --> 0.583418).\n",
      "Validation loss decreased (0.581160 --> 0.581160).\n",
      "Validation loss decreased (0.574484 --> 0.574484).\n",
      "Epoch 50: Train Loss: 0.5731, Macro_F1: 0.7753, AUC_score: 0.8453\n",
      "Validation loss decreased (0.573061 --> 0.573061).\n",
      "Validation loss decreased (0.564598 --> 0.564598).\n",
      "Validation loss decreased (0.563707 --> 0.563707).\n",
      "Validation loss decreased (0.560988 --> 0.560988).\n",
      "Validation loss decreased (0.555761 --> 0.555761).\n",
      "Validation loss decreased (0.547008 --> 0.547008).\n",
      "Validation loss decreased (0.539805 --> 0.539805).\n",
      "Validation loss decreased (0.535817 --> 0.535817).\n",
      "Validation loss decreased (0.531961 --> 0.531961).\n",
      "Validation loss decreased (0.526787 --> 0.526787).\n",
      "Validation loss decreased (0.526593 --> 0.526593).\n",
      "Validation loss decreased (0.516094 --> 0.516094).\n",
      "Validation loss decreased (0.515616 --> 0.515616).\n",
      "Validation loss decreased (0.509763 --> 0.509763).\n",
      "Epoch 100: Train Loss: 0.5174, Macro_F1: 0.8079, AUC_score: 0.8837\n",
      "Validation loss decreased (0.503529 --> 0.503529).\n",
      "Validation loss decreased (0.488903 --> 0.488903).\n",
      "Validation loss decreased (0.471100 --> 0.471100).\n",
      "Validation loss decreased (0.466669 --> 0.466669).\n",
      "Epoch 150: Train Loss: 0.4744, Macro_F1: 0.8436, AUC_score: 0.9013\n",
      "Validation loss decreased (0.465767 --> 0.465767).\n",
      "Validation loss decreased (0.460237 --> 0.460237).\n",
      "Validation loss decreased (0.458459 --> 0.458459).\n",
      "Validation loss decreased (0.451314 --> 0.451314).\n",
      "Validation loss decreased (0.449284 --> 0.449284).\n",
      "Epoch 200: Train Loss: 0.4681, Macro_F1: 0.8403, AUC_score: 0.9061\n",
      "Validation loss decreased (0.443859 --> 0.443859).\n",
      "Validation loss decreased (0.443694 --> 0.443694).\n",
      "Validation loss decreased (0.442615 --> 0.442615).\n",
      "Validation loss decreased (0.438956 --> 0.438956).\n",
      "Validation loss decreased (0.437052 --> 0.437052).\n",
      "Validation loss decreased (0.435706 --> 0.435706).\n",
      "Epoch 250: Train Loss: 0.4290, Macro_F1: 0.8404, AUC_score: 0.9086\n",
      "Validation loss decreased (0.429037 --> 0.429037).\n",
      "Validation loss decreased (0.426953 --> 0.426953).\n",
      "Validation loss decreased (0.421228 --> 0.421228).\n",
      "Validation loss decreased (0.420662 --> 0.420662).\n",
      "Epoch 300: Train Loss: 0.4225, Macro_F1: 0.8369, AUC_score: 0.9114\n",
      "Validation loss decreased (0.417141 --> 0.417141).\n",
      "Epoch 350: Train Loss: 0.4184, Macro_F1: 0.8402, AUC_score: 0.9126\n",
      "Validation loss decreased (0.413041 --> 0.413041).\n",
      "Validation loss decreased (0.409332 --> 0.409332).\n",
      "Epoch 400: Train Loss: 0.4295, Macro_F1: 0.8404, AUC_score: 0.9140\n",
      "Validation loss decreased (0.400067 --> 0.400067).\n",
      "Validation loss decreased (0.397166 --> 0.397166).\n",
      "Epoch 450: Train Loss: 0.4167, Macro_F1: 0.8333, AUC_score: 0.9167\n",
      "Validation loss decreased (0.395375 --> 0.395375).\n",
      "Validation loss decreased (0.392508 --> 0.392508).\n",
      "Validation loss decreased (0.390045 --> 0.390045).\n",
      "Epoch 500: Train Loss: 0.4021, Macro_F1: 0.8296, AUC_score: 0.9165\n",
      "Validation loss decreased (0.389316 --> 0.389316).\n",
      "Validation loss decreased (0.388635 --> 0.388635).\n",
      "Epoch 550: Train Loss: 0.4107, Macro_F1: 0.8297, AUC_score: 0.9192\n",
      "Validation loss decreased (0.387987 --> 0.387987).\n",
      "Validation loss decreased (0.386748 --> 0.386748).\n",
      "Epoch 600: Train Loss: 0.4162, Macro_F1: 0.8333, AUC_score: 0.9199\n",
      "Validation loss decreased (0.377321 --> 0.377321).\n",
      "Epoch 650: Train Loss: 0.3926, Macro_F1: 0.8404, AUC_score: 0.9204\n",
      "Validation loss decreased (0.376315 --> 0.376315).\n",
      "Validation loss decreased (0.374139 --> 0.374139).\n",
      "Epoch 700: Train Loss: 0.3961, Macro_F1: 0.8293, AUC_score: 0.9209\n",
      "Epoch 750: Train Loss: 0.3975, Macro_F1: 0.8369, AUC_score: 0.9224\n",
      "Validation loss decreased (0.369918 --> 0.369918).\n",
      "Validation loss decreased (0.363505 --> 0.363505).\n",
      "Epoch 800: Train Loss: 0.3984, Macro_F1: 0.8368, AUC_score: 0.9231\n",
      "Epoch 850: Train Loss: 0.3998, Macro_F1: 0.8333, AUC_score: 0.9247\n",
      "Epoch 00895: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 900: Train Loss: 0.3895, Macro_F1: 0.8261, AUC_score: 0.9243\n",
      "Epoch 950: Train Loss: 0.3603, Macro_F1: 0.8368, AUC_score: 0.9253\n",
      "Validation loss decreased (0.360267 --> 0.360267).\n",
      "Validation loss decreased (0.354262 --> 0.354262).\n",
      "Epoch 1000: Train Loss: 0.3795, Macro_F1: 0.8333, AUC_score: 0.9251\n",
      "Epoch 1050: Train Loss: 0.3865, Macro_F1: 0.8441, AUC_score: 0.9248\n",
      "Epoch 01092: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 1100: Train Loss: 0.3974, Macro_F1: 0.8333, AUC_score: 0.9247\n",
      "Epoch 1150: Train Loss: 0.3847, Macro_F1: 0.8369, AUC_score: 0.9251\n",
      "Epoch 01193: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 1200: Train Loss: 0.3858, Macro_F1: 0.8369, AUC_score: 0.9254\n",
      "Epoch 1250: Train Loss: 0.3982, Macro_F1: 0.8369, AUC_score: 0.9254\n",
      "Validation loss decreased (0.353428 --> 0.353428).\n",
      "Epoch 1300: Train Loss: 0.3844, Macro_F1: 0.8369, AUC_score: 0.9255\n",
      "Epoch 1350: Train Loss: 0.3768, Macro_F1: 0.8369, AUC_score: 0.9254\n",
      "Epoch 01378: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 1400: Train Loss: 0.3855, Macro_F1: 0.8369, AUC_score: 0.9253\n",
      "Epoch 1450: Train Loss: 0.3865, Macro_F1: 0.8369, AUC_score: 0.9254\n",
      "Epoch 01479: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 1500: Train Loss: 0.4048, Macro_F1: 0.8369, AUC_score: 0.9255\n",
      "Epoch 1550: Train Loss: 0.3850, Macro_F1: 0.8369, AUC_score: 0.9255\n",
      "Early stopping triggered\n",
      "acc save\n",
      "99.0% node features transform to 0: F1: 0.8369, AUC_score: 0.9255\n",
      "Epoch 0: Train Loss: 0.4907, Macro_F1: 0.8513, AUC_score: 0.9183\n",
      "Validation loss decreased (0.490678 --> 0.490678).\n",
      "Validation loss decreased (0.471546 --> 0.471546).\n",
      "Validation loss decreased (0.445339 --> 0.445339).\n",
      "Validation loss decreased (0.442142 --> 0.442142).\n",
      "Validation loss decreased (0.428407 --> 0.428407).\n",
      "Validation loss decreased (0.412868 --> 0.412868).\n",
      "Validation loss decreased (0.410482 --> 0.410482).\n",
      "Validation loss decreased (0.402105 --> 0.402105).\n",
      "Validation loss decreased (0.390190 --> 0.390190).\n",
      "Validation loss decreased (0.388524 --> 0.388524).\n",
      "Validation loss decreased (0.368872 --> 0.368872).\n",
      "Validation loss decreased (0.359633 --> 0.359633).\n",
      "Validation loss decreased (0.351439 --> 0.351439).\n",
      "Validation loss decreased (0.349685 --> 0.349685).\n",
      "Validation loss decreased (0.347978 --> 0.347978).\n",
      "Validation loss decreased (0.341186 --> 0.341186).\n",
      "Epoch 50: Train Loss: 0.3552, Macro_F1: 0.8478, AUC_score: 0.9267\n",
      "Validation loss decreased (0.338519 --> 0.338519).\n",
      "Validation loss decreased (0.333326 --> 0.333326).\n",
      "Validation loss decreased (0.325929 --> 0.325929).\n",
      "Validation loss decreased (0.324930 --> 0.324930).\n",
      "Validation loss decreased (0.323521 --> 0.323521).\n",
      "Epoch 100: Train Loss: 0.3347, Macro_F1: 0.8549, AUC_score: 0.9318\n",
      "Validation loss decreased (0.321304 --> 0.321304).\n",
      "Validation loss decreased (0.318788 --> 0.318788).\n",
      "Validation loss decreased (0.316984 --> 0.316984).\n",
      "Validation loss decreased (0.308642 --> 0.308642).\n",
      "Epoch 150: Train Loss: 0.3202, Macro_F1: 0.8550, AUC_score: 0.9320\n",
      "Validation loss decreased (0.308064 --> 0.308064).\n",
      "Validation loss decreased (0.307668 --> 0.307668).\n",
      "Epoch 200: Train Loss: 0.3317, Macro_F1: 0.8585, AUC_score: 0.9317\n",
      "Validation loss decreased (0.306371 --> 0.306371).\n",
      "Validation loss decreased (0.306041 --> 0.306041).\n",
      "Validation loss decreased (0.301913 --> 0.301913).\n",
      "Validation loss decreased (0.296808 --> 0.296808).\n",
      "Validation loss decreased (0.296208 --> 0.296208).\n",
      "Validation loss decreased (0.295967 --> 0.295967).\n",
      "Epoch 250: Train Loss: 0.3038, Macro_F1: 0.8478, AUC_score: 0.9307\n",
      "Validation loss decreased (0.293403 --> 0.293403).\n",
      "Validation loss decreased (0.293171 --> 0.293171).\n",
      "Epoch 300: Train Loss: 0.3076, Macro_F1: 0.8586, AUC_score: 0.9302\n",
      "Validation loss decreased (0.291789 --> 0.291789).\n",
      "Epoch 350: Train Loss: 0.3111, Macro_F1: 0.8514, AUC_score: 0.9315\n",
      "Validation loss decreased (0.284882 --> 0.284882).\n",
      "Epoch 400: Train Loss: 0.3100, Macro_F1: 0.8478, AUC_score: 0.9291\n",
      "Validation loss decreased (0.283426 --> 0.283426).\n",
      "Validation loss decreased (0.283411 --> 0.283411).\n",
      "Validation loss decreased (0.283155 --> 0.283155).\n",
      "Epoch 450: Train Loss: 0.3029, Macro_F1: 0.8514, AUC_score: 0.9295\n",
      "Epoch 500: Train Loss: 0.2995, Macro_F1: 0.8477, AUC_score: 0.9320\n",
      "Epoch 00548: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 550: Train Loss: 0.2802, Macro_F1: 0.8478, AUC_score: 0.9295\n",
      "Validation loss decreased (0.280200 --> 0.280200).\n",
      "Validation loss decreased (0.279881 --> 0.279881).\n",
      "Validation loss decreased (0.276152 --> 0.276152).\n",
      "Epoch 600: Train Loss: 0.2852, Macro_F1: 0.8478, AUC_score: 0.9310\n",
      "Epoch 650: Train Loss: 0.2821, Macro_F1: 0.8477, AUC_score: 0.9321\n",
      "Epoch 00697: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 700: Train Loss: 0.2933, Macro_F1: 0.8441, AUC_score: 0.9314\n",
      "Validation loss decreased (0.274408 --> 0.274408).\n",
      "Epoch 750: Train Loss: 0.2894, Macro_F1: 0.8514, AUC_score: 0.9308\n",
      "Epoch 800: Train Loss: 0.2857, Macro_F1: 0.8478, AUC_score: 0.9306\n",
      "Epoch 00811: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 850: Train Loss: 0.2976, Macro_F1: 0.8478, AUC_score: 0.9310\n",
      "Validation loss decreased (0.273110 --> 0.273110).\n",
      "Epoch 900: Train Loss: 0.3079, Macro_F1: 0.8441, AUC_score: 0.9311\n",
      "Epoch 950: Train Loss: 0.2940, Macro_F1: 0.8441, AUC_score: 0.9312\n",
      "Epoch 01000: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 1000: Train Loss: 0.2819, Macro_F1: 0.8441, AUC_score: 0.9311\n",
      "Validation loss decreased (0.272368 --> 0.272368).\n",
      "Epoch 1050: Train Loss: 0.2906, Macro_F1: 0.8441, AUC_score: 0.9311\n",
      "Epoch 1100: Train Loss: 0.2861, Macro_F1: 0.8478, AUC_score: 0.9311\n",
      "Epoch 01122: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 1150: Train Loss: 0.2775, Macro_F1: 0.8478, AUC_score: 0.9311\n",
      "Epoch 1200: Train Loss: 0.2963, Macro_F1: 0.8478, AUC_score: 0.9311\n",
      "Epoch 01223: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 1250: Train Loss: 0.2996, Macro_F1: 0.8478, AUC_score: 0.9311\n",
      "Epoch 1300: Train Loss: 0.2866, Macro_F1: 0.8478, AUC_score: 0.9311\n",
      "Validation loss decreased (0.272204 --> 0.272204).\n",
      "Epoch 1350: Train Loss: 0.2851, Macro_F1: 0.8478, AUC_score: 0.9311\n",
      "Epoch 1400: Train Loss: 0.2986, Macro_F1: 0.8478, AUC_score: 0.9311\n",
      "Epoch 01415: reducing learning rate of group 0 to 1.2800e-08.\n",
      "Epoch 1450: Train Loss: 0.2901, Macro_F1: 0.8478, AUC_score: 0.9311\n",
      "Epoch 1500: Train Loss: 0.2878, Macro_F1: 0.8478, AUC_score: 0.9311\n",
      "Epoch 01516: reducing learning rate of group 0 to 2.5600e-09.\n",
      "Epoch 1550: Train Loss: 0.2719, Macro_F1: 0.8478, AUC_score: 0.9311\n",
      "Validation loss decreased (0.271879 --> 0.271879).\n",
      "Epoch 1600: Train Loss: 0.2879, Macro_F1: 0.8478, AUC_score: 0.9311\n",
      "Epoch 1650: Train Loss: 0.2865, Macro_F1: 0.8478, AUC_score: 0.9311\n",
      "Epoch 1700: Train Loss: 0.2906, Macro_F1: 0.8478, AUC_score: 0.9311\n",
      "Epoch 1750: Train Loss: 0.2893, Macro_F1: 0.8478, AUC_score: 0.9311\n",
      "Epoch 1800: Train Loss: 0.3068, Macro_F1: 0.8478, AUC_score: 0.9311\n",
      "Epoch 1850: Train Loss: 0.2899, Macro_F1: 0.8478, AUC_score: 0.9311\n",
      "Early stopping triggered\n",
      "acc save\n",
      "98.0% node features transform to 0: F1: 0.8478, AUC_score: 0.9311\n",
      "Epoch 0: Train Loss: 0.3786, Macro_F1: 0.8403, AUC_score: 0.9146\n",
      "Validation loss decreased (0.378617 --> 0.378617).\n",
      "Validation loss decreased (0.356586 --> 0.356586).\n",
      "Validation loss decreased (0.341740 --> 0.341740).\n",
      "Validation loss decreased (0.336856 --> 0.336856).\n",
      "Validation loss decreased (0.335272 --> 0.335272).\n",
      "Validation loss decreased (0.321206 --> 0.321206).\n",
      "Validation loss decreased (0.310369 --> 0.310369).\n",
      "Validation loss decreased (0.305209 --> 0.305209).\n",
      "Validation loss decreased (0.298953 --> 0.298953).\n",
      "Validation loss decreased (0.292138 --> 0.292138).\n",
      "Validation loss decreased (0.286293 --> 0.286293).\n",
      "Validation loss decreased (0.283210 --> 0.283210).\n",
      "Validation loss decreased (0.279458 --> 0.279458).\n",
      "Validation loss decreased (0.278296 --> 0.278296).\n",
      "Validation loss decreased (0.276415 --> 0.276415).\n",
      "Validation loss decreased (0.276311 --> 0.276311).\n",
      "Validation loss decreased (0.263157 --> 0.263157).\n",
      "Validation loss decreased (0.261436 --> 0.261436).\n",
      "Epoch 50: Train Loss: 0.2611, Macro_F1: 0.8223, AUC_score: 0.9093\n",
      "Validation loss decreased (0.261115 --> 0.261115).\n",
      "Validation loss decreased (0.257745 --> 0.257745).\n",
      "Validation loss decreased (0.254899 --> 0.254899).\n",
      "Validation loss decreased (0.248721 --> 0.248721).\n",
      "Epoch 100: Train Loss: 0.2707, Macro_F1: 0.8218, AUC_score: 0.9071\n",
      "Validation loss decreased (0.245981 --> 0.245981).\n",
      "Epoch 150: Train Loss: 0.2456, Macro_F1: 0.8149, AUC_score: 0.9006\n",
      "Validation loss decreased (0.245571 --> 0.245571).\n",
      "Validation loss decreased (0.244590 --> 0.244590).\n",
      "Validation loss decreased (0.240586 --> 0.240586).\n",
      "Validation loss decreased (0.234839 --> 0.234839).\n",
      "Epoch 200: Train Loss: 0.2433, Macro_F1: 0.8179, AUC_score: 0.9024\n",
      "Validation loss decreased (0.234289 --> 0.234289).\n",
      "Epoch 250: Train Loss: 0.2392, Macro_F1: 0.8111, AUC_score: 0.9013\n",
      "Validation loss decreased (0.231646 --> 0.231646).\n",
      "Validation loss decreased (0.229031 --> 0.229031).\n",
      "Validation loss decreased (0.227440 --> 0.227440).\n",
      "Epoch 300: Train Loss: 0.2320, Macro_F1: 0.8253, AUC_score: 0.8987\n",
      "Validation loss decreased (0.226844 --> 0.226844).\n",
      "Epoch 350: Train Loss: 0.2333, Macro_F1: 0.8250, AUC_score: 0.8998\n",
      "Validation loss decreased (0.224644 --> 0.224644).\n",
      "Epoch 400: Train Loss: 0.2373, Macro_F1: 0.8110, AUC_score: 0.8991\n",
      "Validation loss decreased (0.224443 --> 0.224443).\n",
      "Validation loss decreased (0.224039 --> 0.224039).\n",
      "Epoch 450: Train Loss: 0.2417, Macro_F1: 0.8186, AUC_score: 0.8992\n",
      "Validation loss decreased (0.222849 --> 0.222849).\n",
      "Validation loss decreased (0.221012 --> 0.221012).\n",
      "Epoch 500: Train Loss: 0.2372, Macro_F1: 0.8248, AUC_score: 0.8998\n",
      "Validation loss decreased (0.220597 --> 0.220597).\n",
      "Epoch 550: Train Loss: 0.2387, Macro_F1: 0.8252, AUC_score: 0.8965\n",
      "Validation loss decreased (0.215659 --> 0.215659).\n",
      "Epoch 600: Train Loss: 0.2373, Macro_F1: 0.8252, AUC_score: 0.8977\n",
      "Validation loss decreased (0.209699 --> 0.209699).\n",
      "Epoch 650: Train Loss: 0.2358, Macro_F1: 0.8222, AUC_score: 0.8945\n",
      "Epoch 700: Train Loss: 0.2161, Macro_F1: 0.8252, AUC_score: 0.8967\n",
      "Validation loss decreased (0.205954 --> 0.205954).\n",
      "Epoch 750: Train Loss: 0.2283, Macro_F1: 0.8182, AUC_score: 0.8967\n",
      "Epoch 800: Train Loss: 0.2122, Macro_F1: 0.8221, AUC_score: 0.8983\n",
      "Epoch 00805: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 850: Train Loss: 0.2253, Macro_F1: 0.8179, AUC_score: 0.8971\n",
      "Epoch 900: Train Loss: 0.2169, Macro_F1: 0.8259, AUC_score: 0.8955\n",
      "Epoch 00906: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 950: Train Loss: 0.2193, Macro_F1: 0.8256, AUC_score: 0.8961\n",
      "Epoch 1000: Train Loss: 0.2370, Macro_F1: 0.8219, AUC_score: 0.8959\n",
      "Early stopping triggered\n",
      "acc save\n",
      "97.0% node features transform to 0: F1: 0.8219, AUC_score: 0.8960\n",
      "Epoch 0: Train Loss: 0.2806, Macro_F1: 0.8543, AUC_score: 0.9205\n",
      "Validation loss decreased (0.280554 --> 0.280554).\n",
      "Validation loss decreased (0.219307 --> 0.219307).\n",
      "Validation loss decreased (0.203824 --> 0.203824).\n",
      "Validation loss decreased (0.201804 --> 0.201804).\n",
      "Validation loss decreased (0.186405 --> 0.186405).\n",
      "Validation loss decreased (0.176976 --> 0.176976).\n",
      "Validation loss decreased (0.176936 --> 0.176936).\n",
      "Validation loss decreased (0.174187 --> 0.174187).\n",
      "Validation loss decreased (0.157183 --> 0.157183).\n",
      "Validation loss decreased (0.151619 --> 0.151619).\n",
      "Validation loss decreased (0.148355 --> 0.148355).\n",
      "Validation loss decreased (0.145763 --> 0.145763).\n",
      "Validation loss decreased (0.136914 --> 0.136914).\n",
      "Validation loss decreased (0.131722 --> 0.131722).\n",
      "Validation loss decreased (0.125701 --> 0.125701).\n",
      "Validation loss decreased (0.119660 --> 0.119660).\n",
      "Validation loss decreased (0.116646 --> 0.116646).\n",
      "Validation loss decreased (0.113200 --> 0.113200).\n",
      "Validation loss decreased (0.112655 --> 0.112655).\n",
      "Validation loss decreased (0.110345 --> 0.110345).\n",
      "Epoch 50: Train Loss: 0.1411, Macro_F1: 0.9055, AUC_score: 0.9554\n",
      "Validation loss decreased (0.107980 --> 0.107980).\n",
      "Validation loss decreased (0.107767 --> 0.107767).\n",
      "Validation loss decreased (0.107238 --> 0.107238).\n",
      "Validation loss decreased (0.103189 --> 0.103189).\n",
      "Validation loss decreased (0.101305 --> 0.101305).\n",
      "Validation loss decreased (0.100537 --> 0.100537).\n",
      "Validation loss decreased (0.094763 --> 0.094763).\n",
      "Epoch 100: Train Loss: 0.1223, Macro_F1: 0.9090, AUC_score: 0.9598\n",
      "Validation loss decreased (0.093525 --> 0.093525).\n",
      "Validation loss decreased (0.090923 --> 0.090923).\n",
      "Validation loss decreased (0.090309 --> 0.090309).\n",
      "Validation loss decreased (0.086379 --> 0.086379).\n",
      "Validation loss decreased (0.085101 --> 0.085101).\n",
      "Epoch 150: Train Loss: 0.1191, Macro_F1: 0.9127, AUC_score: 0.9600\n",
      "Epoch 200: Train Loss: 0.0991, Macro_F1: 0.9237, AUC_score: 0.9612\n",
      "Validation loss decreased (0.084847 --> 0.084847).\n",
      "Validation loss decreased (0.081251 --> 0.081251).\n",
      "Epoch 250: Train Loss: 0.0868, Macro_F1: 0.9311, AUC_score: 0.9620\n",
      "Epoch 300: Train Loss: 0.0939, Macro_F1: 0.9162, AUC_score: 0.9653\n",
      "Validation loss decreased (0.080826 --> 0.080826).\n",
      "Validation loss decreased (0.079708 --> 0.079708).\n",
      "Epoch 350: Train Loss: 0.0867, Macro_F1: 0.9163, AUC_score: 0.9628\n",
      "Validation loss decreased (0.077301 --> 0.077301).\n",
      "Epoch 400: Train Loss: 0.1014, Macro_F1: 0.9384, AUC_score: 0.9647\n",
      "Epoch 450: Train Loss: 0.0871, Macro_F1: 0.9126, AUC_score: 0.9653\n",
      "Epoch 00476: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 500: Train Loss: 0.0940, Macro_F1: 0.9310, AUC_score: 0.9658\n",
      "Epoch 550: Train Loss: 0.0845, Macro_F1: 0.9347, AUC_score: 0.9652\n",
      "Epoch 00577: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 600: Train Loss: 0.0839, Macro_F1: 0.9163, AUC_score: 0.9654\n",
      "Validation loss decreased (0.077239 --> 0.077239).\n",
      "Epoch 650: Train Loss: 0.0779, Macro_F1: 0.9237, AUC_score: 0.9655\n",
      "Validation loss decreased (0.076715 --> 0.076715).\n",
      "Validation loss decreased (0.075823 --> 0.075823).\n",
      "Epoch 700: Train Loss: 0.0881, Macro_F1: 0.9163, AUC_score: 0.9655\n",
      "Validation loss decreased (0.075092 --> 0.075092).\n",
      "Validation loss decreased (0.073150 --> 0.073150).\n",
      "Epoch 750: Train Loss: 0.0813, Macro_F1: 0.9163, AUC_score: 0.9649\n",
      "Epoch 800: Train Loss: 0.0780, Macro_F1: 0.9237, AUC_score: 0.9656\n",
      "Epoch 00849: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 850: Train Loss: 0.0888, Macro_F1: 0.9274, AUC_score: 0.9656\n",
      "Epoch 900: Train Loss: 0.0864, Macro_F1: 0.9273, AUC_score: 0.9655\n",
      "Epoch 00950: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 950: Train Loss: 0.0849, Macro_F1: 0.9273, AUC_score: 0.9655\n",
      "Epoch 1000: Train Loss: 0.0937, Macro_F1: 0.9273, AUC_score: 0.9654\n",
      "Early stopping triggered\n",
      "acc save\n",
      "96.0% node features transform to 0: F1: 0.9273, AUC_score: 0.9654\n",
      "Epoch 0: Train Loss: 0.2297, Macro_F1: 0.9130, AUC_score: 0.9613\n",
      "Validation loss decreased (0.229745 --> 0.229745).\n",
      "Validation loss decreased (0.213404 --> 0.213404).\n",
      "Validation loss decreased (0.154269 --> 0.154269).\n",
      "Validation loss decreased (0.146614 --> 0.146614).\n",
      "Validation loss decreased (0.138869 --> 0.138869).\n",
      "Validation loss decreased (0.123410 --> 0.123410).\n",
      "Validation loss decreased (0.120613 --> 0.120613).\n",
      "Validation loss decreased (0.115731 --> 0.115731).\n",
      "Validation loss decreased (0.113759 --> 0.113759).\n",
      "Validation loss decreased (0.100624 --> 0.100624).\n",
      "Validation loss decreased (0.098642 --> 0.098642).\n",
      "Validation loss decreased (0.089598 --> 0.089598).\n",
      "Validation loss decreased (0.089342 --> 0.089342).\n",
      "Validation loss decreased (0.081871 --> 0.081871).\n",
      "Epoch 50: Train Loss: 0.0930, Macro_F1: 0.9203, AUC_score: 0.9653\n",
      "Validation loss decreased (0.078803 --> 0.078803).\n",
      "Validation loss decreased (0.078280 --> 0.078280).\n",
      "Validation loss decreased (0.075527 --> 0.075527).\n",
      "Epoch 100: Train Loss: 0.0880, Macro_F1: 0.9275, AUC_score: 0.9662\n",
      "Validation loss decreased (0.075201 --> 0.075201).\n",
      "Epoch 150: Train Loss: 0.0792, Macro_F1: 0.9202, AUC_score: 0.9654\n",
      "Validation loss decreased (0.074214 --> 0.074214).\n",
      "Epoch 200: Train Loss: 0.0821, Macro_F1: 0.9165, AUC_score: 0.9637\n",
      "Validation loss decreased (0.073765 --> 0.073765).\n",
      "Validation loss decreased (0.073128 --> 0.073128).\n",
      "Epoch 250: Train Loss: 0.0797, Macro_F1: 0.9275, AUC_score: 0.9645\n",
      "Validation loss decreased (0.071942 --> 0.071942).\n",
      "Epoch 300: Train Loss: 0.0969, Macro_F1: 0.9163, AUC_score: 0.9636\n",
      "Epoch 350: Train Loss: 0.0816, Macro_F1: 0.9239, AUC_score: 0.9638\n",
      "Validation loss decreased (0.071631 --> 0.071631).\n",
      "Validation loss decreased (0.071048 --> 0.071048).\n",
      "Validation loss decreased (0.070555 --> 0.070555).\n",
      "Validation loss decreased (0.069810 --> 0.069810).\n",
      "Epoch 400: Train Loss: 0.0788, Macro_F1: 0.9239, AUC_score: 0.9651\n",
      "Validation loss decreased (0.068923 --> 0.068923).\n",
      "Validation loss decreased (0.068372 --> 0.068372).\n",
      "Epoch 450: Train Loss: 0.0934, Macro_F1: 0.9202, AUC_score: 0.9646\n",
      "Epoch 500: Train Loss: 0.0679, Macro_F1: 0.9274, AUC_score: 0.9648\n",
      "Validation loss decreased (0.067904 --> 0.067904).\n",
      "Epoch 550: Train Loss: 0.1112, Macro_F1: 0.9311, AUC_score: 0.9649\n",
      "Validation loss decreased (0.067115 --> 0.067115).\n",
      "Epoch 600: Train Loss: 0.0847, Macro_F1: 0.9203, AUC_score: 0.9635\n",
      "Epoch 650: Train Loss: 0.0913, Macro_F1: 0.9239, AUC_score: 0.9640\n",
      "Epoch 00701: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 700: Train Loss: 0.0768, Macro_F1: 0.9199, AUC_score: 0.9644\n",
      "Epoch 750: Train Loss: 0.0743, Macro_F1: 0.9203, AUC_score: 0.9635\n",
      "Epoch 800: Train Loss: 0.0712, Macro_F1: 0.9239, AUC_score: 0.9631\n",
      "Validation loss decreased (0.066971 --> 0.066971).\n",
      "Epoch 850: Train Loss: 0.1027, Macro_F1: 0.9203, AUC_score: 0.9631\n",
      "Epoch 900: Train Loss: 0.0771, Macro_F1: 0.9239, AUC_score: 0.9628\n",
      "Epoch 00903: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.066785 --> 0.066785).\n",
      "Validation loss decreased (0.066634 --> 0.066634).\n",
      "Epoch 950: Train Loss: 0.0888, Macro_F1: 0.9238, AUC_score: 0.9627\n",
      "Validation loss decreased (0.066291 --> 0.066291).\n",
      "Epoch 1000: Train Loss: 0.0772, Macro_F1: 0.9239, AUC_score: 0.9626\n",
      "Epoch 1050: Train Loss: 0.0659, Macro_F1: 0.9239, AUC_score: 0.9626\n",
      "Validation loss decreased (0.065898 --> 0.065898).\n",
      "Epoch 1100: Train Loss: 0.0837, Macro_F1: 0.9239, AUC_score: 0.9628\n",
      "Epoch 1150: Train Loss: 0.0753, Macro_F1: 0.9275, AUC_score: 0.9625\n",
      "Epoch 01152: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 1200: Train Loss: 0.0774, Macro_F1: 0.9239, AUC_score: 0.9626\n",
      "Epoch 1250: Train Loss: 0.1176, Macro_F1: 0.9239, AUC_score: 0.9626\n",
      "Epoch 01253: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 1300: Train Loss: 0.0850, Macro_F1: 0.9239, AUC_score: 0.9627\n",
      "Epoch 1350: Train Loss: 0.1199, Macro_F1: 0.9239, AUC_score: 0.9626\n",
      "Early stopping triggered\n",
      "acc save\n",
      "95.0% node features transform to 0: F1: 0.9239, AUC_score: 0.9626\n",
      "Epoch 0: Train Loss: 0.1233, Macro_F1: 0.9055, AUC_score: 0.9355\n",
      "Validation loss decreased (0.123338 --> 0.123338).\n",
      "Validation loss decreased (0.114155 --> 0.114155).\n",
      "Validation loss decreased (0.107711 --> 0.107711).\n",
      "Validation loss decreased (0.096524 --> 0.096524).\n",
      "Validation loss decreased (0.092339 --> 0.092339).\n",
      "Validation loss decreased (0.090555 --> 0.090555).\n",
      "Validation loss decreased (0.087219 --> 0.087219).\n",
      "Validation loss decreased (0.085603 --> 0.085603).\n",
      "Validation loss decreased (0.080607 --> 0.080607).\n",
      "Validation loss decreased (0.072763 --> 0.072763).\n",
      "Validation loss decreased (0.069941 --> 0.069941).\n",
      "Epoch 50: Train Loss: 0.0778, Macro_F1: 0.9273, AUC_score: 0.9567\n",
      "Validation loss decreased (0.068177 --> 0.068177).\n",
      "Validation loss decreased (0.065997 --> 0.065997).\n",
      "Epoch 100: Train Loss: 0.0678, Macro_F1: 0.9200, AUC_score: 0.9546\n",
      "Validation loss decreased (0.064720 --> 0.064720).\n",
      "Epoch 150: Train Loss: 0.0650, Macro_F1: 0.9201, AUC_score: 0.9530\n",
      "Validation loss decreased (0.063152 --> 0.063152).\n",
      "Epoch 200: Train Loss: 0.0826, Macro_F1: 0.9200, AUC_score: 0.9538\n",
      "Validation loss decreased (0.060739 --> 0.060739).\n",
      "Epoch 250: Train Loss: 0.0698, Macro_F1: 0.9238, AUC_score: 0.9513\n",
      "Epoch 300: Train Loss: 0.0652, Macro_F1: 0.9237, AUC_score: 0.9520\n",
      "Epoch 00320: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 350: Train Loss: 0.0842, Macro_F1: 0.9129, AUC_score: 0.9507\n",
      "Validation loss decreased (0.060532 --> 0.060532).\n",
      "Validation loss decreased (0.059237 --> 0.059237).\n",
      "Epoch 400: Train Loss: 0.0598, Macro_F1: 0.9201, AUC_score: 0.9505\n",
      "Validation loss decreased (0.056564 --> 0.056564).\n",
      "Epoch 450: Train Loss: 0.0672, Macro_F1: 0.9201, AUC_score: 0.9494\n",
      "Epoch 500: Train Loss: 0.0621, Macro_F1: 0.9201, AUC_score: 0.9503\n",
      "Epoch 00506: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 550: Train Loss: 0.0602, Macro_F1: 0.9165, AUC_score: 0.9499\n",
      "Epoch 600: Train Loss: 0.0631, Macro_F1: 0.9201, AUC_score: 0.9500\n",
      "Epoch 00607: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 650: Train Loss: 0.0727, Macro_F1: 0.9165, AUC_score: 0.9499\n",
      "Epoch 700: Train Loss: 0.0843, Macro_F1: 0.9201, AUC_score: 0.9500\n",
      "Early stopping triggered\n",
      "acc save\n",
      "94.0% node features transform to 0: F1: 0.9201, AUC_score: 0.9500\n",
      "Epoch 0: Train Loss: 0.0873, Macro_F1: 0.9237, AUC_score: 0.9505\n",
      "Validation loss decreased (0.087321 --> 0.087321).\n",
      "Validation loss decreased (0.079584 --> 0.079584).\n",
      "Validation loss decreased (0.075189 --> 0.075189).\n",
      "Validation loss decreased (0.072607 --> 0.072607).\n",
      "Validation loss decreased (0.067563 --> 0.067563).\n",
      "Validation loss decreased (0.060819 --> 0.060819).\n",
      "Validation loss decreased (0.059588 --> 0.059588).\n",
      "Validation loss decreased (0.058973 --> 0.058973).\n",
      "Epoch 50: Train Loss: 0.0685, Macro_F1: 0.9128, AUC_score: 0.9495\n",
      "Validation loss decreased (0.057563 --> 0.057563).\n",
      "Validation loss decreased (0.055451 --> 0.055451).\n",
      "Validation loss decreased (0.055411 --> 0.055411).\n",
      "Validation loss decreased (0.054929 --> 0.054929).\n",
      "Validation loss decreased (0.052547 --> 0.052547).\n",
      "Epoch 100: Train Loss: 0.0586, Macro_F1: 0.9130, AUC_score: 0.9497\n",
      "Epoch 150: Train Loss: 0.0588, Macro_F1: 0.9130, AUC_score: 0.9474\n",
      "Validation loss decreased (0.049360 --> 0.049360).\n",
      "Epoch 200: Train Loss: 0.0564, Macro_F1: 0.9092, AUC_score: 0.9456\n",
      "Epoch 250: Train Loss: 0.0661, Macro_F1: 0.9094, AUC_score: 0.9475\n",
      "Epoch 00272: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0837, Macro_F1: 0.9130, AUC_score: 0.9486\n",
      "Epoch 350: Train Loss: 0.0639, Macro_F1: 0.9130, AUC_score: 0.9468\n",
      "Epoch 00373: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.047249 --> 0.047249).\n",
      "Epoch 400: Train Loss: 0.0599, Macro_F1: 0.9129, AUC_score: 0.9465\n",
      "Epoch 450: Train Loss: 0.0569, Macro_F1: 0.9129, AUC_score: 0.9468\n",
      "Validation loss decreased (0.047135 --> 0.047135).\n",
      "Epoch 500: Train Loss: 0.0763, Macro_F1: 0.9129, AUC_score: 0.9465\n",
      "Epoch 550: Train Loss: 0.0625, Macro_F1: 0.9129, AUC_score: 0.9458\n",
      "Epoch 00594: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 600: Train Loss: 0.0572, Macro_F1: 0.9129, AUC_score: 0.9458\n",
      "Epoch 650: Train Loss: 0.0641, Macro_F1: 0.9129, AUC_score: 0.9460\n",
      "Epoch 00695: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 700: Train Loss: 0.0674, Macro_F1: 0.9129, AUC_score: 0.9460\n",
      "Epoch 750: Train Loss: 0.0505, Macro_F1: 0.9129, AUC_score: 0.9460\n",
      "Early stopping triggered\n",
      "acc save\n",
      "93.0% node features transform to 0: F1: 0.9129, AUC_score: 0.9460\n",
      "Epoch 0: Train Loss: 0.0735, Macro_F1: 0.9055, AUC_score: 0.9440\n",
      "Validation loss decreased (0.073533 --> 0.073533).\n",
      "Validation loss decreased (0.071182 --> 0.071182).\n",
      "Validation loss decreased (0.066922 --> 0.066922).\n",
      "Validation loss decreased (0.060598 --> 0.060598).\n",
      "Validation loss decreased (0.051553 --> 0.051553).\n",
      "Validation loss decreased (0.048742 --> 0.048742).\n",
      "Validation loss decreased (0.047104 --> 0.047104).\n",
      "Epoch 50: Train Loss: 0.0748, Macro_F1: 0.9021, AUC_score: 0.9451\n",
      "Validation loss decreased (0.045614 --> 0.045614).\n",
      "Validation loss decreased (0.045473 --> 0.045473).\n",
      "Epoch 100: Train Loss: 0.0487, Macro_F1: 0.9057, AUC_score: 0.9445\n",
      "Validation loss decreased (0.045272 --> 0.045272).\n",
      "Epoch 150: Train Loss: 0.0599, Macro_F1: 0.9165, AUC_score: 0.9445\n",
      "Validation loss decreased (0.043943 --> 0.043943).\n",
      "Validation loss decreased (0.043803 --> 0.043803).\n",
      "Validation loss decreased (0.043299 --> 0.043299).\n",
      "Validation loss decreased (0.042708 --> 0.042708).\n",
      "Validation loss decreased (0.042085 --> 0.042085).\n",
      "Epoch 200: Train Loss: 0.0525, Macro_F1: 0.9057, AUC_score: 0.9422\n",
      "Validation loss decreased (0.041285 --> 0.041285).\n",
      "Epoch 250: Train Loss: 0.0466, Macro_F1: 0.9056, AUC_score: 0.9442\n",
      "Epoch 300: Train Loss: 0.0593, Macro_F1: 0.9057, AUC_score: 0.9447\n",
      "Validation loss decreased (0.039528 --> 0.039528).\n",
      "Validation loss decreased (0.038408 --> 0.038408).\n",
      "Epoch 350: Train Loss: 0.0447, Macro_F1: 0.9092, AUC_score: 0.9411\n",
      "Epoch 400: Train Loss: 0.0427, Macro_F1: 0.9093, AUC_score: 0.9426\n",
      "Epoch 00432: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 450: Train Loss: 0.0491, Macro_F1: 0.9057, AUC_score: 0.9451\n",
      "Epoch 500: Train Loss: 0.0394, Macro_F1: 0.9056, AUC_score: 0.9447\n",
      "Epoch 00533: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 550: Train Loss: 0.0419, Macro_F1: 0.9056, AUC_score: 0.9439\n",
      "Validation loss decreased (0.037366 --> 0.037366).\n",
      "Epoch 600: Train Loss: 0.0410, Macro_F1: 0.9056, AUC_score: 0.9436\n",
      "Epoch 650: Train Loss: 0.0430, Macro_F1: 0.9056, AUC_score: 0.9435\n",
      "Epoch 00672: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 700: Train Loss: 0.0426, Macro_F1: 0.9056, AUC_score: 0.9434\n",
      "Validation loss decreased (0.037085 --> 0.037085).\n",
      "Epoch 750: Train Loss: 0.0530, Macro_F1: 0.9056, AUC_score: 0.9433\n",
      "Epoch 800: Train Loss: 0.0576, Macro_F1: 0.9056, AUC_score: 0.9430\n",
      "Validation loss decreased (0.035995 --> 0.035995).\n",
      "Epoch 850: Train Loss: 0.0745, Macro_F1: 0.9056, AUC_score: 0.9430\n",
      "Epoch 900: Train Loss: 0.0443, Macro_F1: 0.9056, AUC_score: 0.9430\n",
      "Epoch 00903: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Validation loss decreased (0.035226 --> 0.035226).\n",
      "Epoch 950: Train Loss: 0.0414, Macro_F1: 0.9020, AUC_score: 0.9430\n",
      "Epoch 1000: Train Loss: 0.0595, Macro_F1: 0.9057, AUC_score: 0.9429\n",
      "Epoch 01013: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 1050: Train Loss: 0.0449, Macro_F1: 0.9057, AUC_score: 0.9430\n",
      "Epoch 1100: Train Loss: 0.0416, Macro_F1: 0.9057, AUC_score: 0.9430\n",
      "Epoch 01114: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 1150: Train Loss: 0.0509, Macro_F1: 0.9020, AUC_score: 0.9430\n",
      "Epoch 1200: Train Loss: 0.0507, Macro_F1: 0.9020, AUC_score: 0.9430\n",
      "Early stopping triggered\n",
      "acc save\n",
      "92.0% node features transform to 0: F1: 0.9020, AUC_score: 0.9430\n",
      "Epoch 0: Train Loss: 0.0507, Macro_F1: 0.8943, AUC_score: 0.9383\n",
      "Validation loss decreased (0.050714 --> 0.050714).\n",
      "Validation loss decreased (0.046216 --> 0.046216).\n",
      "Validation loss decreased (0.043750 --> 0.043750).\n",
      "Validation loss decreased (0.040927 --> 0.040927).\n",
      "Epoch 50: Train Loss: 0.0452, Macro_F1: 0.9019, AUC_score: 0.9408\n",
      "Validation loss decreased (0.039297 --> 0.039297).\n",
      "Validation loss decreased (0.037870 --> 0.037870).\n",
      "Validation loss decreased (0.037464 --> 0.037464).\n",
      "Epoch 100: Train Loss: 0.0372, Macro_F1: 0.9092, AUC_score: 0.9421\n",
      "Validation loss decreased (0.037177 --> 0.037177).\n",
      "Validation loss decreased (0.035106 --> 0.035106).\n",
      "Epoch 150: Train Loss: 0.0425, Macro_F1: 0.8985, AUC_score: 0.9416\n",
      "Epoch 200: Train Loss: 0.0589, Macro_F1: 0.8985, AUC_score: 0.9409\n",
      "Epoch 00221: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.035050 --> 0.035050).\n",
      "Epoch 250: Train Loss: 0.0359, Macro_F1: 0.9130, AUC_score: 0.9434\n",
      "Epoch 300: Train Loss: 0.0412, Macro_F1: 0.9166, AUC_score: 0.9425\n",
      "Validation loss decreased (0.034484 --> 0.034484).\n",
      "Validation loss decreased (0.032803 --> 0.032803).\n",
      "Epoch 350: Train Loss: 0.0435, Macro_F1: 0.9094, AUC_score: 0.9410\n",
      "Epoch 400: Train Loss: 0.0357, Macro_F1: 0.9093, AUC_score: 0.9417\n",
      "Epoch 00417: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.0432, Macro_F1: 0.9130, AUC_score: 0.9422\n",
      "Epoch 500: Train Loss: 0.0351, Macro_F1: 0.9130, AUC_score: 0.9420\n",
      "Epoch 00518: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Validation loss decreased (0.031546 --> 0.031546).\n",
      "Validation loss decreased (0.030884 --> 0.030884).\n",
      "Epoch 550: Train Loss: 0.0393, Macro_F1: 0.9129, AUC_score: 0.9421\n",
      "Epoch 600: Train Loss: 0.0403, Macro_F1: 0.9166, AUC_score: 0.9420\n",
      "Epoch 00647: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 650: Train Loss: 0.0395, Macro_F1: 0.9166, AUC_score: 0.9419\n",
      "Epoch 700: Train Loss: 0.0520, Macro_F1: 0.9166, AUC_score: 0.9419\n",
      "Epoch 00748: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 750: Train Loss: 0.0353, Macro_F1: 0.9166, AUC_score: 0.9420\n",
      "Epoch 800: Train Loss: 0.0351, Macro_F1: 0.9166, AUC_score: 0.9420\n",
      "Early stopping triggered\n",
      "acc save\n",
      "91.0% node features transform to 0: F1: 0.9166, AUC_score: 0.9420\n",
      "Epoch 0: Train Loss: 0.0548, Macro_F1: 0.8913, AUC_score: 0.9390\n",
      "Validation loss decreased (0.054812 --> 0.054812).\n",
      "Validation loss decreased (0.044691 --> 0.044691).\n",
      "Validation loss decreased (0.038625 --> 0.038625).\n",
      "Validation loss decreased (0.036847 --> 0.036847).\n",
      "Validation loss decreased (0.034137 --> 0.034137).\n",
      "Epoch 50: Train Loss: 0.0782, Macro_F1: 0.9058, AUC_score: 0.9372\n",
      "Epoch 100: Train Loss: 0.0375, Macro_F1: 0.9058, AUC_score: 0.9386\n",
      "Epoch 00125: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.0587, Macro_F1: 0.9058, AUC_score: 0.9409\n",
      "Validation loss decreased (0.032882 --> 0.032882).\n",
      "Epoch 200: Train Loss: 0.0428, Macro_F1: 0.9058, AUC_score: 0.9385\n",
      "Validation loss decreased (0.032702 --> 0.032702).\n",
      "Epoch 250: Train Loss: 0.0467, Macro_F1: 0.9058, AUC_score: 0.9392\n",
      "Epoch 300: Train Loss: 0.0428, Macro_F1: 0.9021, AUC_score: 0.9377\n",
      "Validation loss decreased (0.031153 --> 0.031153).\n",
      "Epoch 350: Train Loss: 0.0529, Macro_F1: 0.9021, AUC_score: 0.9382\n",
      "Epoch 400: Train Loss: 0.0363, Macro_F1: 0.9058, AUC_score: 0.9387\n",
      "Epoch 00407: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.030492 --> 0.030492).\n",
      "Epoch 450: Train Loss: 0.0338, Macro_F1: 0.9021, AUC_score: 0.9375\n",
      "Epoch 500: Train Loss: 0.0417, Macro_F1: 0.9058, AUC_score: 0.9379\n",
      "Epoch 00533: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0370, Macro_F1: 0.9058, AUC_score: 0.9374\n",
      "Epoch 600: Train Loss: 0.0543, Macro_F1: 0.9058, AUC_score: 0.9374\n",
      "Validation loss decreased (0.028048 --> 0.028048).\n",
      "Epoch 650: Train Loss: 0.0400, Macro_F1: 0.9058, AUC_score: 0.9376\n",
      "Epoch 700: Train Loss: 0.0335, Macro_F1: 0.9058, AUC_score: 0.9376\n",
      "Epoch 00714: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 750: Train Loss: 0.0361, Macro_F1: 0.9058, AUC_score: 0.9376\n",
      "Epoch 800: Train Loss: 0.0419, Macro_F1: 0.9058, AUC_score: 0.9376\n",
      "Epoch 00815: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 850: Train Loss: 0.0402, Macro_F1: 0.9058, AUC_score: 0.9376\n",
      "Epoch 900: Train Loss: 0.0374, Macro_F1: 0.9058, AUC_score: 0.9376\n",
      "Early stopping triggered\n",
      "acc save\n",
      "90.0% node features transform to 0: F1: 0.9058, AUC_score: 0.9376\n",
      "Epoch 0: Train Loss: 0.1319, Macro_F1: 0.8913, AUC_score: 0.9397\n",
      "Validation loss decreased (0.131907 --> 0.131907).\n",
      "Validation loss decreased (0.065175 --> 0.065175).\n",
      "Validation loss decreased (0.045683 --> 0.045683).\n",
      "Validation loss decreased (0.044917 --> 0.044917).\n",
      "Validation loss decreased (0.043070 --> 0.043070).\n",
      "Validation loss decreased (0.038459 --> 0.038459).\n",
      "Validation loss decreased (0.035170 --> 0.035170).\n",
      "Validation loss decreased (0.033832 --> 0.033832).\n",
      "Validation loss decreased (0.033768 --> 0.033768).\n",
      "Validation loss decreased (0.033568 --> 0.033568).\n",
      "Validation loss decreased (0.033483 --> 0.033483).\n",
      "Validation loss decreased (0.033224 --> 0.033224).\n",
      "Epoch 50: Train Loss: 0.0466, Macro_F1: 0.8947, AUC_score: 0.9431\n",
      "Validation loss decreased (0.032997 --> 0.032997).\n",
      "Validation loss decreased (0.031608 --> 0.031608).\n",
      "Validation loss decreased (0.029979 --> 0.029979).\n",
      "Epoch 100: Train Loss: 0.0447, Macro_F1: 0.9019, AUC_score: 0.9470\n",
      "Validation loss decreased (0.028247 --> 0.028247).\n",
      "Epoch 150: Train Loss: 0.0652, Macro_F1: 0.8985, AUC_score: 0.9460\n",
      "Epoch 200: Train Loss: 0.0288, Macro_F1: 0.9057, AUC_score: 0.9452\n",
      "Validation loss decreased (0.026968 --> 0.026968).\n",
      "Validation loss decreased (0.026817 --> 0.026817).\n",
      "Epoch 250: Train Loss: 0.0367, Macro_F1: 0.9056, AUC_score: 0.9430\n",
      "Epoch 300: Train Loss: 0.0309, Macro_F1: 0.8913, AUC_score: 0.9461\n",
      "Validation loss decreased (0.024365 --> 0.024365).\n",
      "Epoch 350: Train Loss: 0.0339, Macro_F1: 0.8982, AUC_score: 0.9429\n",
      "Epoch 400: Train Loss: 0.0312, Macro_F1: 0.8877, AUC_score: 0.9462\n",
      "Epoch 00403: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 450: Train Loss: 0.0402, Macro_F1: 0.8913, AUC_score: 0.9462\n",
      "Epoch 500: Train Loss: 0.0285, Macro_F1: 0.8913, AUC_score: 0.9459\n",
      "Epoch 00504: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 550: Train Loss: 0.0340, Macro_F1: 0.8949, AUC_score: 0.9451\n",
      "Epoch 600: Train Loss: 0.0473, Macro_F1: 0.8913, AUC_score: 0.9448\n",
      "Early stopping triggered\n",
      "acc save\n",
      "89.0% node features transform to 0: F1: 0.8913, AUC_score: 0.9449\n",
      "Epoch 0: Train Loss: 0.0557, Macro_F1: 0.8949, AUC_score: 0.9487\n",
      "Validation loss decreased (0.055666 --> 0.055666).\n",
      "Validation loss decreased (0.042384 --> 0.042384).\n",
      "Validation loss decreased (0.038078 --> 0.038078).\n",
      "Validation loss decreased (0.037033 --> 0.037033).\n",
      "Validation loss decreased (0.031410 --> 0.031410).\n",
      "Validation loss decreased (0.028715 --> 0.028715).\n",
      "Validation loss decreased (0.025781 --> 0.025781).\n",
      "Validation loss decreased (0.023343 --> 0.023343).\n",
      "Validation loss decreased (0.023168 --> 0.023168).\n",
      "Epoch 50: Train Loss: 0.0255, Macro_F1: 0.9058, AUC_score: 0.9445\n",
      "Validation loss decreased (0.022651 --> 0.022651).\n",
      "Validation loss decreased (0.022549 --> 0.022549).\n",
      "Validation loss decreased (0.022329 --> 0.022329).\n",
      "Epoch 100: Train Loss: 0.0268, Macro_F1: 0.9058, AUC_score: 0.9444\n",
      "Validation loss decreased (0.021767 --> 0.021767).\n",
      "Validation loss decreased (0.020649 --> 0.020649).\n",
      "Validation loss decreased (0.020629 --> 0.020629).\n",
      "Epoch 150: Train Loss: 0.0223, Macro_F1: 0.8985, AUC_score: 0.9409\n",
      "Validation loss decreased (0.019883 --> 0.019883).\n",
      "Epoch 200: Train Loss: 0.0494, Macro_F1: 0.8986, AUC_score: 0.9441\n",
      "Epoch 250: Train Loss: 0.0261, Macro_F1: 0.9021, AUC_score: 0.9370\n",
      "Validation loss decreased (0.019776 --> 0.019776).\n",
      "Epoch 300: Train Loss: 0.0233, Macro_F1: 0.9058, AUC_score: 0.9396\n",
      "Validation loss decreased (0.019565 --> 0.019565).\n",
      "Epoch 350: Train Loss: 0.0257, Macro_F1: 0.8949, AUC_score: 0.9420\n",
      "Validation loss decreased (0.017637 --> 0.017637).\n",
      "Epoch 400: Train Loss: 0.0217, Macro_F1: 0.8985, AUC_score: 0.9376\n",
      "Epoch 450: Train Loss: 0.0383, Macro_F1: 0.8913, AUC_score: 0.9404\n",
      "Epoch 00489: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 500: Train Loss: 0.0273, Macro_F1: 0.8985, AUC_score: 0.9394\n",
      "Epoch 550: Train Loss: 0.0423, Macro_F1: 0.8985, AUC_score: 0.9367\n",
      "Epoch 00590: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 600: Train Loss: 0.0228, Macro_F1: 0.8949, AUC_score: 0.9369\n",
      "Epoch 650: Train Loss: 0.0249, Macro_F1: 0.8985, AUC_score: 0.9369\n",
      "Early stopping triggered\n",
      "acc save\n",
      "88.0% node features transform to 0: F1: 0.8985, AUC_score: 0.9376\n",
      "Epoch 0: Train Loss: 0.0479, Macro_F1: 0.8913, AUC_score: 0.9382\n",
      "Validation loss decreased (0.047866 --> 0.047866).\n",
      "Validation loss decreased (0.041600 --> 0.041600).\n",
      "Validation loss decreased (0.037100 --> 0.037100).\n",
      "Validation loss decreased (0.035667 --> 0.035667).\n",
      "Validation loss decreased (0.023390 --> 0.023390).\n",
      "Validation loss decreased (0.023024 --> 0.023024).\n",
      "Validation loss decreased (0.022071 --> 0.022071).\n",
      "Epoch 50: Train Loss: 0.0246, Macro_F1: 0.9056, AUC_score: 0.9457\n",
      "Validation loss decreased (0.021427 --> 0.021427).\n",
      "Validation loss decreased (0.020508 --> 0.020508).\n",
      "Validation loss decreased (0.020010 --> 0.020010).\n",
      "Validation loss decreased (0.019504 --> 0.019504).\n",
      "Validation loss decreased (0.019255 --> 0.019255).\n",
      "Validation loss decreased (0.018744 --> 0.018744).\n",
      "Validation loss decreased (0.018622 --> 0.018622).\n",
      "Epoch 100: Train Loss: 0.0192, Macro_F1: 0.9058, AUC_score: 0.9448\n",
      "Validation loss decreased (0.018366 --> 0.018366).\n",
      "Epoch 150: Train Loss: 0.0227, Macro_F1: 0.8985, AUC_score: 0.9446\n",
      "Epoch 200: Train Loss: 0.0269, Macro_F1: 0.9021, AUC_score: 0.9446\n",
      "Validation loss decreased (0.018065 --> 0.018065).\n",
      "Validation loss decreased (0.017466 --> 0.017466).\n",
      "Epoch 250: Train Loss: 0.0247, Macro_F1: 0.8986, AUC_score: 0.9454\n",
      "Epoch 300: Train Loss: 0.0264, Macro_F1: 0.9058, AUC_score: 0.9467\n",
      "Epoch 00317: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 350: Train Loss: 0.0237, Macro_F1: 0.9094, AUC_score: 0.9465\n",
      "Epoch 400: Train Loss: 0.0179, Macro_F1: 0.9058, AUC_score: 0.9459\n",
      "Validation loss decreased (0.016725 --> 0.016725).\n",
      "Validation loss decreased (0.016464 --> 0.016464).\n",
      "Validation loss decreased (0.016177 --> 0.016177).\n",
      "Epoch 450: Train Loss: 0.0187, Macro_F1: 0.9058, AUC_score: 0.9454\n",
      "Epoch 500: Train Loss: 0.0314, Macro_F1: 0.9058, AUC_score: 0.9464\n",
      "Epoch 00519: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 550: Train Loss: 0.0204, Macro_F1: 0.9058, AUC_score: 0.9461\n",
      "Validation loss decreased (0.015479 --> 0.015479).\n",
      "Epoch 600: Train Loss: 0.0226, Macro_F1: 0.9094, AUC_score: 0.9459\n",
      "Epoch 650: Train Loss: 0.0289, Macro_F1: 0.9058, AUC_score: 0.9452\n",
      "Epoch 00677: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 700: Train Loss: 0.0244, Macro_F1: 0.9058, AUC_score: 0.9454\n",
      "Epoch 750: Train Loss: 0.0193, Macro_F1: 0.9058, AUC_score: 0.9454\n",
      "Epoch 00778: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 800: Train Loss: 0.0399, Macro_F1: 0.9058, AUC_score: 0.9454\n",
      "Epoch 850: Train Loss: 0.0295, Macro_F1: 0.9058, AUC_score: 0.9455\n",
      "Early stopping triggered\n",
      "acc save\n",
      "87.0% node features transform to 0: F1: 0.9058, AUC_score: 0.9455\n",
      "Epoch 0: Train Loss: 0.0219, Macro_F1: 0.9021, AUC_score: 0.9453\n",
      "Validation loss decreased (0.021858 --> 0.021858).\n",
      "Validation loss decreased (0.021097 --> 0.021097).\n",
      "Validation loss decreased (0.020558 --> 0.020558).\n",
      "Validation loss decreased (0.018611 --> 0.018611).\n",
      "Epoch 50: Train Loss: 0.0315, Macro_F1: 0.8949, AUC_score: 0.9492\n",
      "Validation loss decreased (0.018347 --> 0.018347).\n",
      "Validation loss decreased (0.017229 --> 0.017229).\n",
      "Validation loss decreased (0.016855 --> 0.016855).\n",
      "Validation loss decreased (0.015459 --> 0.015459).\n",
      "Epoch 100: Train Loss: 0.0186, Macro_F1: 0.9202, AUC_score: 0.9458\n",
      "Epoch 150: Train Loss: 0.0226, Macro_F1: 0.9058, AUC_score: 0.9494\n",
      "Epoch 00200: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0178, Macro_F1: 0.9128, AUC_score: 0.9449\n",
      "Validation loss decreased (0.015094 --> 0.015094).\n",
      "Epoch 250: Train Loss: 0.0193, Macro_F1: 0.9058, AUC_score: 0.9474\n",
      "Epoch 300: Train Loss: 0.0177, Macro_F1: 0.9022, AUC_score: 0.9479\n",
      "Epoch 00328: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.014892 --> 0.014892).\n",
      "Validation loss decreased (0.014475 --> 0.014475).\n",
      "Epoch 350: Train Loss: 0.0287, Macro_F1: 0.9058, AUC_score: 0.9476\n",
      "Validation loss decreased (0.014454 --> 0.014454).\n",
      "Epoch 400: Train Loss: 0.0381, Macro_F1: 0.9130, AUC_score: 0.9473\n",
      "Validation loss decreased (0.014309 --> 0.014309).\n",
      "Validation loss decreased (0.013502 --> 0.013502).\n",
      "Epoch 450: Train Loss: 0.0223, Macro_F1: 0.9094, AUC_score: 0.9478\n",
      "Epoch 500: Train Loss: 0.0202, Macro_F1: 0.9094, AUC_score: 0.9478\n",
      "Epoch 00530: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0232, Macro_F1: 0.9094, AUC_score: 0.9479\n",
      "Epoch 600: Train Loss: 0.0165, Macro_F1: 0.9094, AUC_score: 0.9477\n",
      "Epoch 00631: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 650: Train Loss: 0.0343, Macro_F1: 0.9094, AUC_score: 0.9478\n",
      "Epoch 700: Train Loss: 0.0264, Macro_F1: 0.9094, AUC_score: 0.9478\n",
      "Early stopping triggered\n",
      "acc save\n",
      "86.0% node features transform to 0: F1: 0.9094, AUC_score: 0.9479\n",
      "Epoch 0: Train Loss: 0.0432, Macro_F1: 0.9091, AUC_score: 0.9398\n",
      "Validation loss decreased (0.043164 --> 0.043164).\n",
      "Validation loss decreased (0.042324 --> 0.042324).\n",
      "Validation loss decreased (0.039316 --> 0.039316).\n",
      "Validation loss decreased (0.030991 --> 0.030991).\n",
      "Validation loss decreased (0.020816 --> 0.020816).\n",
      "Validation loss decreased (0.018311 --> 0.018311).\n",
      "Validation loss decreased (0.017950 --> 0.017950).\n",
      "Epoch 50: Train Loss: 0.0197, Macro_F1: 0.9129, AUC_score: 0.9457\n",
      "Validation loss decreased (0.016960 --> 0.016960).\n",
      "Validation loss decreased (0.014529 --> 0.014529).\n",
      "Epoch 100: Train Loss: 0.0197, Macro_F1: 0.9021, AUC_score: 0.9467\n",
      "Epoch 150: Train Loss: 0.0341, Macro_F1: 0.9165, AUC_score: 0.9467\n",
      "Epoch 00200: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0227, Macro_F1: 0.8985, AUC_score: 0.9480\n",
      "Validation loss decreased (0.013900 --> 0.013900).\n",
      "Epoch 250: Train Loss: 0.0166, Macro_F1: 0.9058, AUC_score: 0.9475\n",
      "Validation loss decreased (0.013725 --> 0.013725).\n",
      "Epoch 300: Train Loss: 0.0202, Macro_F1: 0.9094, AUC_score: 0.9475\n",
      "Epoch 350: Train Loss: 0.0349, Macro_F1: 0.9058, AUC_score: 0.9478\n",
      "Validation loss decreased (0.013649 --> 0.013649).\n",
      "Epoch 400: Train Loss: 0.0240, Macro_F1: 0.9058, AUC_score: 0.9470\n",
      "Epoch 450: Train Loss: 0.0194, Macro_F1: 0.9058, AUC_score: 0.9474\n",
      "Validation loss decreased (0.013361 --> 0.013361).\n",
      "Epoch 500: Train Loss: 0.0162, Macro_F1: 0.9165, AUC_score: 0.9454\n",
      "Epoch 550: Train Loss: 0.0290, Macro_F1: 0.9058, AUC_score: 0.9479\n",
      "Epoch 00556: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 600: Train Loss: 0.0178, Macro_F1: 0.9094, AUC_score: 0.9476\n",
      "Epoch 650: Train Loss: 0.0255, Macro_F1: 0.9058, AUC_score: 0.9475\n",
      "Epoch 00657: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Validation loss decreased (0.013193 --> 0.013193).\n",
      "Validation loss decreased (0.013013 --> 0.013013).\n",
      "Epoch 700: Train Loss: 0.0191, Macro_F1: 0.9058, AUC_score: 0.9475\n",
      "Epoch 750: Train Loss: 0.0254, Macro_F1: 0.9058, AUC_score: 0.9475\n",
      "Validation loss decreased (0.012685 --> 0.012685).\n",
      "Epoch 800: Train Loss: 0.0202, Macro_F1: 0.9058, AUC_score: 0.9475\n",
      "Epoch 850: Train Loss: 0.0335, Macro_F1: 0.9058, AUC_score: 0.9475\n",
      "Epoch 00865: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 900: Train Loss: 0.0155, Macro_F1: 0.9058, AUC_score: 0.9476\n",
      "Epoch 950: Train Loss: 0.0214, Macro_F1: 0.9058, AUC_score: 0.9475\n",
      "Epoch 00966: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 1000: Train Loss: 0.0296, Macro_F1: 0.9058, AUC_score: 0.9475\n",
      "Epoch 1050: Train Loss: 0.0165, Macro_F1: 0.9058, AUC_score: 0.9475\n",
      "Early stopping triggered\n",
      "acc save\n",
      "85.0% node features transform to 0: F1: 0.9058, AUC_score: 0.9475\n",
      "Epoch 0: Train Loss: 0.0366, Macro_F1: 0.9094, AUC_score: 0.9460\n",
      "Validation loss decreased (0.036580 --> 0.036580).\n",
      "Validation loss decreased (0.029887 --> 0.029887).\n",
      "Validation loss decreased (0.024158 --> 0.024158).\n",
      "Validation loss decreased (0.020467 --> 0.020467).\n",
      "Validation loss decreased (0.017836 --> 0.017836).\n",
      "Validation loss decreased (0.016929 --> 0.016929).\n",
      "Epoch 50: Train Loss: 0.0250, Macro_F1: 0.9057, AUC_score: 0.9476\n",
      "Validation loss decreased (0.016437 --> 0.016437).\n",
      "Validation loss decreased (0.016386 --> 0.016386).\n",
      "Epoch 100: Train Loss: 0.0182, Macro_F1: 0.9021, AUC_score: 0.9499\n",
      "Validation loss decreased (0.014559 --> 0.014559).\n",
      "Epoch 150: Train Loss: 0.0197, Macro_F1: 0.9058, AUC_score: 0.9487\n",
      "Epoch 200: Train Loss: 0.0341, Macro_F1: 0.9057, AUC_score: 0.9484\n",
      "Validation loss decreased (0.014192 --> 0.014192).\n",
      "Epoch 250: Train Loss: 0.0197, Macro_F1: 0.9129, AUC_score: 0.9496\n",
      "Epoch 300: Train Loss: 0.0393, Macro_F1: 0.9021, AUC_score: 0.9508\n",
      "Epoch 00307: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.014037 --> 0.014037).\n",
      "Epoch 350: Train Loss: 0.0216, Macro_F1: 0.9129, AUC_score: 0.9509\n",
      "Validation loss decreased (0.012704 --> 0.012704).\n",
      "Epoch 400: Train Loss: 0.0272, Macro_F1: 0.9129, AUC_score: 0.9493\n",
      "Validation loss decreased (0.012598 --> 0.012598).\n",
      "Epoch 450: Train Loss: 0.0186, Macro_F1: 0.9129, AUC_score: 0.9490\n",
      "Validation loss decreased (0.012219 --> 0.012219).\n",
      "Epoch 500: Train Loss: 0.0196, Macro_F1: 0.9130, AUC_score: 0.9491\n",
      "Epoch 550: Train Loss: 0.0278, Macro_F1: 0.9057, AUC_score: 0.9490\n",
      "Epoch 00580: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 600: Train Loss: 0.0140, Macro_F1: 0.9165, AUC_score: 0.9494\n",
      "Epoch 650: Train Loss: 0.0299, Macro_F1: 0.9057, AUC_score: 0.9490\n",
      "Validation loss decreased (0.011180 --> 0.011180).\n",
      "Epoch 700: Train Loss: 0.0146, Macro_F1: 0.9165, AUC_score: 0.9491\n",
      "Epoch 750: Train Loss: 0.0145, Macro_F1: 0.9057, AUC_score: 0.9490\n",
      "Epoch 00755: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 800: Train Loss: 0.0517, Macro_F1: 0.9165, AUC_score: 0.9489\n",
      "Epoch 850: Train Loss: 0.0306, Macro_F1: 0.9165, AUC_score: 0.9487\n",
      "Epoch 00856: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 900: Train Loss: 0.0203, Macro_F1: 0.9165, AUC_score: 0.9487\n",
      "Epoch 950: Train Loss: 0.0229, Macro_F1: 0.9165, AUC_score: 0.9488\n",
      "Early stopping triggered\n",
      "acc save\n",
      "84.0% node features transform to 0: F1: 0.9165, AUC_score: 0.9488\n",
      "Epoch 0: Train Loss: 0.0250, Macro_F1: 0.8768, AUC_score: 0.9473\n",
      "Validation loss decreased (0.025032 --> 0.025032).\n",
      "Validation loss decreased (0.019176 --> 0.019176).\n",
      "Validation loss decreased (0.018620 --> 0.018620).\n",
      "Validation loss decreased (0.016661 --> 0.016661).\n",
      "Validation loss decreased (0.015133 --> 0.015133).\n",
      "Validation loss decreased (0.014553 --> 0.014553).\n",
      "Validation loss decreased (0.014073 --> 0.014073).\n",
      "Epoch 50: Train Loss: 0.0408, Macro_F1: 0.9058, AUC_score: 0.9476\n",
      "Validation loss decreased (0.013029 --> 0.013029).\n",
      "Epoch 100: Train Loss: 0.0158, Macro_F1: 0.8984, AUC_score: 0.9490\n",
      "Epoch 150: Train Loss: 0.0249, Macro_F1: 0.8877, AUC_score: 0.9498\n",
      "Validation loss decreased (0.012840 --> 0.012840).\n",
      "Epoch 200: Train Loss: 0.0191, Macro_F1: 0.9021, AUC_score: 0.9472\n",
      "Epoch 250: Train Loss: 0.0193, Macro_F1: 0.9092, AUC_score: 0.9506\n",
      "Epoch 00280: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0138, Macro_F1: 0.9093, AUC_score: 0.9506\n",
      "Validation loss decreased (0.012519 --> 0.012519).\n",
      "Validation loss decreased (0.010992 --> 0.010992).\n",
      "Epoch 350: Train Loss: 0.0218, Macro_F1: 0.9130, AUC_score: 0.9499\n",
      "Validation loss decreased (0.010342 --> 0.010342).\n",
      "Epoch 400: Train Loss: 0.0189, Macro_F1: 0.9130, AUC_score: 0.9498\n",
      "Epoch 450: Train Loss: 0.0117, Macro_F1: 0.9165, AUC_score: 0.9499\n",
      "Epoch 00477: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.0175, Macro_F1: 0.9130, AUC_score: 0.9499\n",
      "Epoch 550: Train Loss: 0.0132, Macro_F1: 0.9093, AUC_score: 0.9500\n",
      "Epoch 00578: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 600: Train Loss: 0.0248, Macro_F1: 0.9093, AUC_score: 0.9500\n",
      "Epoch 650: Train Loss: 0.0158, Macro_F1: 0.9093, AUC_score: 0.9500\n",
      "Early stopping triggered\n",
      "acc save\n",
      "83.0% node features transform to 0: F1: 0.9093, AUC_score: 0.9501\n",
      "Epoch 0: Train Loss: 0.0491, Macro_F1: 0.8913, AUC_score: 0.9497\n",
      "Validation loss decreased (0.049075 --> 0.049075).\n",
      "Validation loss decreased (0.037712 --> 0.037712).\n",
      "Validation loss decreased (0.034617 --> 0.034617).\n",
      "Validation loss decreased (0.023972 --> 0.023972).\n",
      "Validation loss decreased (0.016791 --> 0.016791).\n",
      "Validation loss decreased (0.014815 --> 0.014815).\n",
      "Validation loss decreased (0.014457 --> 0.014457).\n",
      "Epoch 50: Train Loss: 0.0165, Macro_F1: 0.9021, AUC_score: 0.9473\n",
      "Validation loss decreased (0.013746 --> 0.013746).\n",
      "Validation loss decreased (0.012614 --> 0.012614).\n",
      "Validation loss decreased (0.012014 --> 0.012014).\n",
      "Epoch 100: Train Loss: 0.0294, Macro_F1: 0.9021, AUC_score: 0.9486\n",
      "Validation loss decreased (0.011903 --> 0.011903).\n",
      "Epoch 150: Train Loss: 0.0332, Macro_F1: 0.9021, AUC_score: 0.9486\n",
      "Epoch 200: Train Loss: 0.0175, Macro_F1: 0.8984, AUC_score: 0.9466\n",
      "Validation loss decreased (0.011634 --> 0.011634).\n",
      "Validation loss decreased (0.011086 --> 0.011086).\n",
      "Epoch 250: Train Loss: 0.0132, Macro_F1: 0.8985, AUC_score: 0.9481\n",
      "Epoch 300: Train Loss: 0.0203, Macro_F1: 0.9057, AUC_score: 0.9473\n",
      "Validation loss decreased (0.010966 --> 0.010966).\n",
      "Epoch 350: Train Loss: 0.0209, Macro_F1: 0.9019, AUC_score: 0.9447\n",
      "Epoch 400: Train Loss: 0.0130, Macro_F1: 0.9056, AUC_score: 0.9467\n",
      "Epoch 00412: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 450: Train Loss: 0.0173, Macro_F1: 0.9058, AUC_score: 0.9478\n",
      "Epoch 500: Train Loss: 0.0283, Macro_F1: 0.9056, AUC_score: 0.9473\n",
      "Epoch 00513: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.010828 --> 0.010828).\n",
      "Epoch 550: Train Loss: 0.0266, Macro_F1: 0.9056, AUC_score: 0.9478\n",
      "Validation loss decreased (0.010662 --> 0.010662).\n",
      "Epoch 600: Train Loss: 0.0313, Macro_F1: 0.9057, AUC_score: 0.9479\n",
      "Validation loss decreased (0.010338 --> 0.010338).\n",
      "Epoch 650: Train Loss: 0.0429, Macro_F1: 0.9056, AUC_score: 0.9480\n",
      "Epoch 700: Train Loss: 0.0162, Macro_F1: 0.9094, AUC_score: 0.9477\n",
      "Epoch 00709: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 750: Train Loss: 0.0132, Macro_F1: 0.9021, AUC_score: 0.9476\n",
      "Epoch 800: Train Loss: 0.0126, Macro_F1: 0.9093, AUC_score: 0.9477\n",
      "Epoch 00810: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 850: Train Loss: 0.0184, Macro_F1: 0.9093, AUC_score: 0.9477\n",
      "Epoch 900: Train Loss: 0.0231, Macro_F1: 0.9093, AUC_score: 0.9477\n",
      "Early stopping triggered\n",
      "acc save\n",
      "82.0% node features transform to 0: F1: 0.9093, AUC_score: 0.9477\n",
      "Epoch 0: Train Loss: 0.0129, Macro_F1: 0.8913, AUC_score: 0.9455\n",
      "Validation loss decreased (0.012904 --> 0.012904).\n",
      "Validation loss decreased (0.011164 --> 0.011164).\n",
      "Epoch 50: Train Loss: 0.0192, Macro_F1: 0.9092, AUC_score: 0.9499\n",
      "Validation loss decreased (0.010986 --> 0.010986).\n",
      "Epoch 100: Train Loss: 0.0238, Macro_F1: 0.9094, AUC_score: 0.9490\n",
      "Epoch 150: Train Loss: 0.0190, Macro_F1: 0.9021, AUC_score: 0.9489\n",
      "Validation loss decreased (0.010651 --> 0.010651).\n",
      "Validation loss decreased (0.010410 --> 0.010410).\n",
      "Epoch 200: Train Loss: 0.0158, Macro_F1: 0.9022, AUC_score: 0.9487\n",
      "Epoch 250: Train Loss: 0.0405, Macro_F1: 0.9094, AUC_score: 0.9494\n",
      "Epoch 00299: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0258, Macro_F1: 0.9058, AUC_score: 0.9491\n",
      "Epoch 350: Train Loss: 0.0435, Macro_F1: 0.9020, AUC_score: 0.9490\n",
      "Validation loss decreased (0.009982 --> 0.009982).\n",
      "Validation loss decreased (0.009460 --> 0.009460).\n",
      "Epoch 400: Train Loss: 0.0112, Macro_F1: 0.9020, AUC_score: 0.9485\n",
      "Epoch 450: Train Loss: 0.0237, Macro_F1: 0.9021, AUC_score: 0.9487\n",
      "Epoch 00472: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.0173, Macro_F1: 0.9020, AUC_score: 0.9483\n",
      "Validation loss decreased (0.009446 --> 0.009446).\n",
      "Epoch 550: Train Loss: 0.0161, Macro_F1: 0.9021, AUC_score: 0.9485\n",
      "Epoch 600: Train Loss: 0.0153, Macro_F1: 0.9057, AUC_score: 0.9487\n",
      "Epoch 00630: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 650: Train Loss: 0.0238, Macro_F1: 0.8984, AUC_score: 0.9485\n",
      "Epoch 700: Train Loss: 0.0288, Macro_F1: 0.9021, AUC_score: 0.9487\n",
      "Epoch 00731: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 750: Train Loss: 0.0115, Macro_F1: 0.9020, AUC_score: 0.9486\n",
      "Epoch 800: Train Loss: 0.0154, Macro_F1: 0.9020, AUC_score: 0.9486\n",
      "Early stopping triggered\n",
      "acc save\n",
      "81.0% node features transform to 0: F1: 0.9020, AUC_score: 0.9486\n",
      "Epoch 0: Train Loss: 0.0256, Macro_F1: 0.8731, AUC_score: 0.9477\n",
      "Validation loss decreased (0.025576 --> 0.025576).\n",
      "Validation loss decreased (0.017687 --> 0.017687).\n",
      "Validation loss decreased (0.016235 --> 0.016235).\n",
      "Validation loss decreased (0.012640 --> 0.012640).\n",
      "Validation loss decreased (0.011837 --> 0.011837).\n",
      "Validation loss decreased (0.011433 --> 0.011433).\n",
      "Epoch 50: Train Loss: 0.0279, Macro_F1: 0.8983, AUC_score: 0.9508\n",
      "Validation loss decreased (0.010308 --> 0.010308).\n",
      "Epoch 100: Train Loss: 0.0147, Macro_F1: 0.8982, AUC_score: 0.9504\n",
      "Validation loss decreased (0.010078 --> 0.010078).\n",
      "Epoch 150: Train Loss: 0.0153, Macro_F1: 0.8979, AUC_score: 0.9511\n",
      "Validation loss decreased (0.009961 --> 0.009961).\n",
      "Validation loss decreased (0.009751 --> 0.009751).\n",
      "Validation loss decreased (0.009592 --> 0.009592).\n",
      "Epoch 200: Train Loss: 0.0173, Macro_F1: 0.9019, AUC_score: 0.9500\n",
      "Validation loss decreased (0.009118 --> 0.009118).\n",
      "Validation loss decreased (0.008700 --> 0.008700).\n",
      "Epoch 250: Train Loss: 0.0336, Macro_F1: 0.9055, AUC_score: 0.9515\n",
      "Epoch 300: Train Loss: 0.0112, Macro_F1: 0.9094, AUC_score: 0.9525\n",
      "Epoch 00306: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.007545 --> 0.007545).\n",
      "Epoch 350: Train Loss: 0.0118, Macro_F1: 0.9129, AUC_score: 0.9515\n",
      "Epoch 400: Train Loss: 0.0138, Macro_F1: 0.9129, AUC_score: 0.9516\n",
      "Epoch 00419: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.0152, Macro_F1: 0.9129, AUC_score: 0.9512\n",
      "Epoch 500: Train Loss: 0.0159, Macro_F1: 0.9129, AUC_score: 0.9515\n",
      "Epoch 00520: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0093, Macro_F1: 0.9166, AUC_score: 0.9515\n",
      "Epoch 600: Train Loss: 0.0120, Macro_F1: 0.9166, AUC_score: 0.9515\n",
      "Early stopping triggered\n",
      "acc save\n",
      "80.0% node features transform to 0: F1: 0.9129, AUC_score: 0.9516\n",
      "Epoch 0: Train Loss: 0.0118, Macro_F1: 0.9016, AUC_score: 0.9489\n",
      "Validation loss decreased (0.011842 --> 0.011842).\n",
      "Validation loss decreased (0.011601 --> 0.011601).\n",
      "Validation loss decreased (0.011027 --> 0.011027).\n",
      "Validation loss decreased (0.010687 --> 0.010687).\n",
      "Epoch 50: Train Loss: 0.0186, Macro_F1: 0.9237, AUC_score: 0.9526\n",
      "Validation loss decreased (0.010538 --> 0.010538).\n",
      "Validation loss decreased (0.010310 --> 0.010310).\n",
      "Validation loss decreased (0.010028 --> 0.010028).\n",
      "Validation loss decreased (0.008498 --> 0.008498).\n",
      "Epoch 100: Train Loss: 0.0251, Macro_F1: 0.9166, AUC_score: 0.9527\n",
      "Epoch 150: Train Loss: 0.0104, Macro_F1: 0.9130, AUC_score: 0.9535\n",
      "Epoch 200: Train Loss: 0.0100, Macro_F1: 0.9201, AUC_score: 0.9524\n",
      "Epoch 250: Train Loss: 0.0194, Macro_F1: 0.9201, AUC_score: 0.9533\n",
      "Epoch 00292: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0129, Macro_F1: 0.9165, AUC_score: 0.9546\n",
      "Epoch 350: Train Loss: 0.0120, Macro_F1: 0.9130, AUC_score: 0.9530\n",
      "Early stopping triggered\n",
      "acc save\n",
      "79.0% node features transform to 0: F1: 0.9202, AUC_score: 0.9526\n",
      "Epoch 0: Train Loss: 0.0134, Macro_F1: 0.8840, AUC_score: 0.9531\n",
      "Validation loss decreased (0.013387 --> 0.013387).\n",
      "Validation loss decreased (0.012082 --> 0.012082).\n",
      "Validation loss decreased (0.011678 --> 0.011678).\n",
      "Validation loss decreased (0.011171 --> 0.011171).\n",
      "Validation loss decreased (0.009646 --> 0.009646).\n",
      "Validation loss decreased (0.008480 --> 0.008480).\n",
      "Validation loss decreased (0.007958 --> 0.007958).\n",
      "Epoch 50: Train Loss: 0.0114, Macro_F1: 0.9201, AUC_score: 0.9533\n",
      "Validation loss decreased (0.007734 --> 0.007734).\n",
      "Epoch 100: Train Loss: 0.0121, Macro_F1: 0.9058, AUC_score: 0.9549\n",
      "Epoch 150: Train Loss: 0.0485, Macro_F1: 0.9093, AUC_score: 0.9521\n",
      "Epoch 00165: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0276, Macro_F1: 0.9165, AUC_score: 0.9543\n",
      "Validation loss decreased (0.007656 --> 0.007656).\n",
      "Epoch 250: Train Loss: 0.0140, Macro_F1: 0.9237, AUC_score: 0.9534\n",
      "Epoch 300: Train Loss: 0.0108, Macro_F1: 0.9238, AUC_score: 0.9546\n",
      "Epoch 00324: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.007518 --> 0.007518).\n",
      "Epoch 350: Train Loss: 0.0139, Macro_F1: 0.9201, AUC_score: 0.9535\n",
      "Validation loss decreased (0.007077 --> 0.007077).\n",
      "Epoch 400: Train Loss: 0.0245, Macro_F1: 0.9237, AUC_score: 0.9535\n",
      "Validation loss decreased (0.006913 --> 0.006913).\n",
      "Epoch 450: Train Loss: 0.0143, Macro_F1: 0.9237, AUC_score: 0.9531\n",
      "Epoch 500: Train Loss: 0.0200, Macro_F1: 0.9238, AUC_score: 0.9533\n",
      "Epoch 00511: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0091, Macro_F1: 0.9202, AUC_score: 0.9536\n",
      "Epoch 600: Train Loss: 0.0122, Macro_F1: 0.9238, AUC_score: 0.9534\n",
      "Epoch 00612: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 650: Train Loss: 0.0130, Macro_F1: 0.9202, AUC_score: 0.9537\n",
      "Epoch 700: Train Loss: 0.0095, Macro_F1: 0.9202, AUC_score: 0.9537\n",
      "Early stopping triggered\n",
      "acc save\n",
      "78.0% node features transform to 0: F1: 0.9202, AUC_score: 0.9537\n",
      "Epoch 0: Train Loss: 0.0142, Macro_F1: 0.8825, AUC_score: 0.9465\n",
      "Validation loss decreased (0.014164 --> 0.014164).\n",
      "Validation loss decreased (0.012721 --> 0.012721).\n",
      "Validation loss decreased (0.009870 --> 0.009870).\n",
      "Validation loss decreased (0.009765 --> 0.009765).\n",
      "Validation loss decreased (0.009348 --> 0.009348).\n",
      "Validation loss decreased (0.008612 --> 0.008612).\n",
      "Validation loss decreased (0.007801 --> 0.007801).\n",
      "Validation loss decreased (0.006574 --> 0.006574).\n",
      "Epoch 50: Train Loss: 0.0129, Macro_F1: 0.9163, AUC_score: 0.9520\n",
      "Epoch 100: Train Loss: 0.0152, Macro_F1: 0.9237, AUC_score: 0.9547\n",
      "Epoch 00146: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.0082, Macro_F1: 0.9093, AUC_score: 0.9552\n",
      "Epoch 200: Train Loss: 0.0054, Macro_F1: 0.9165, AUC_score: 0.9540\n",
      "Validation loss decreased (0.005364 --> 0.005364).\n",
      "Epoch 250: Train Loss: 0.0083, Macro_F1: 0.9237, AUC_score: 0.9554\n",
      "Epoch 300: Train Loss: 0.0091, Macro_F1: 0.9094, AUC_score: 0.9556\n",
      "Epoch 00302: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.0075, Macro_F1: 0.9165, AUC_score: 0.9546\n",
      "Epoch 400: Train Loss: 0.0100, Macro_F1: 0.9201, AUC_score: 0.9549\n",
      "Epoch 00403: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 450: Train Loss: 0.0083, Macro_F1: 0.9165, AUC_score: 0.9550\n",
      "Epoch 500: Train Loss: 0.0183, Macro_F1: 0.9165, AUC_score: 0.9550\n",
      "Early stopping triggered\n",
      "acc save\n",
      "77.0% node features transform to 0: F1: 0.9165, AUC_score: 0.9550\n",
      "Epoch 0: Train Loss: 0.0152, Macro_F1: 0.9088, AUC_score: 0.9526\n",
      "Validation loss decreased (0.015240 --> 0.015240).\n",
      "Validation loss decreased (0.015032 --> 0.015032).\n",
      "Validation loss decreased (0.011141 --> 0.011141).\n",
      "Validation loss decreased (0.010094 --> 0.010094).\n",
      "Validation loss decreased (0.006629 --> 0.006629).\n",
      "Epoch 50: Train Loss: 0.0152, Macro_F1: 0.8976, AUC_score: 0.9534\n",
      "Epoch 100: Train Loss: 0.0126, Macro_F1: 0.9236, AUC_score: 0.9559\n",
      "Epoch 00111: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.0102, Macro_F1: 0.9057, AUC_score: 0.9555\n",
      "Validation loss decreased (0.006359 --> 0.006359).\n",
      "Validation loss decreased (0.005739 --> 0.005739).\n",
      "Epoch 200: Train Loss: 0.0079, Macro_F1: 0.9093, AUC_score: 0.9554\n",
      "Epoch 250: Train Loss: 0.0094, Macro_F1: 0.9057, AUC_score: 0.9560\n",
      "Epoch 00269: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 300: Train Loss: 0.0095, Macro_F1: 0.9093, AUC_score: 0.9565\n",
      "Validation loss decreased (0.005709 --> 0.005709).\n",
      "Epoch 350: Train Loss: 0.0077, Macro_F1: 0.9057, AUC_score: 0.9558\n",
      "Epoch 400: Train Loss: 0.0129, Macro_F1: 0.9057, AUC_score: 0.9558\n",
      "Epoch 00437: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 450: Train Loss: 0.0120, Macro_F1: 0.9057, AUC_score: 0.9555\n",
      "Validation loss decreased (0.005544 --> 0.005544).\n",
      "Epoch 500: Train Loss: 0.0070, Macro_F1: 0.9057, AUC_score: 0.9556\n",
      "Epoch 550: Train Loss: 0.0075, Macro_F1: 0.9057, AUC_score: 0.9555\n",
      "Epoch 00570: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 600: Train Loss: 0.0104, Macro_F1: 0.9057, AUC_score: 0.9555\n",
      "Validation loss decreased (0.005309 --> 0.005309).\n",
      "Epoch 650: Train Loss: 0.0215, Macro_F1: 0.9057, AUC_score: 0.9555\n",
      "Epoch 700: Train Loss: 0.0123, Macro_F1: 0.9057, AUC_score: 0.9555\n",
      "Epoch 00705: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 750: Train Loss: 0.0078, Macro_F1: 0.9057, AUC_score: 0.9555\n",
      "Epoch 800: Train Loss: 0.0141, Macro_F1: 0.9057, AUC_score: 0.9555\n",
      "Epoch 00806: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 850: Train Loss: 0.0123, Macro_F1: 0.9057, AUC_score: 0.9555\n",
      "Epoch 900: Train Loss: 0.0120, Macro_F1: 0.9057, AUC_score: 0.9555\n",
      "Early stopping triggered\n",
      "acc save\n",
      "76.0% node features transform to 0: F1: 0.9057, AUC_score: 0.9555\n",
      "Epoch 0: Train Loss: 0.0134, Macro_F1: 0.8975, AUC_score: 0.9524\n",
      "Validation loss decreased (0.013371 --> 0.013371).\n",
      "Validation loss decreased (0.013079 --> 0.013079).\n",
      "Validation loss decreased (0.008146 --> 0.008146).\n",
      "Epoch 50: Train Loss: 0.0106, Macro_F1: 0.9021, AUC_score: 0.9537\n",
      "Epoch 100: Train Loss: 0.0078, Macro_F1: 0.9128, AUC_score: 0.9544\n",
      "Validation loss decreased (0.007792 --> 0.007792).\n",
      "Validation loss decreased (0.007306 --> 0.007306).\n",
      "Epoch 150: Train Loss: 0.0106, Macro_F1: 0.9130, AUC_score: 0.9564\n",
      "Validation loss decreased (0.007077 --> 0.007077).\n",
      "Validation loss decreased (0.006947 --> 0.006947).\n",
      "Validation loss decreased (0.006779 --> 0.006779).\n",
      "Epoch 200: Train Loss: 0.0146, Macro_F1: 0.9162, AUC_score: 0.9535\n",
      "Validation loss decreased (0.006085 --> 0.006085).\n",
      "Epoch 250: Train Loss: 0.0097, Macro_F1: 0.9092, AUC_score: 0.9535\n",
      "Epoch 300: Train Loss: 0.0109, Macro_F1: 0.9203, AUC_score: 0.9557\n",
      "Epoch 00316: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 350: Train Loss: 0.0105, Macro_F1: 0.9130, AUC_score: 0.9560\n",
      "Epoch 400: Train Loss: 0.0249, Macro_F1: 0.9166, AUC_score: 0.9556\n",
      "Epoch 00417: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.0067, Macro_F1: 0.9130, AUC_score: 0.9555\n",
      "Epoch 500: Train Loss: 0.0093, Macro_F1: 0.9093, AUC_score: 0.9550\n",
      "Early stopping triggered\n",
      "acc save\n",
      "75.0% node features transform to 0: F1: 0.9093, AUC_score: 0.9551\n",
      "Epoch 0: Train Loss: 0.0067, Macro_F1: 0.8980, AUC_score: 0.9480\n",
      "Validation loss decreased (0.006705 --> 0.006705).\n",
      "Epoch 50: Train Loss: 0.0137, Macro_F1: 0.9202, AUC_score: 0.9530\n",
      "Validation loss decreased (0.005604 --> 0.005604).\n",
      "Epoch 100: Train Loss: 0.0078, Macro_F1: 0.9094, AUC_score: 0.9546\n",
      "Epoch 150: Train Loss: 0.0089, Macro_F1: 0.9057, AUC_score: 0.9523\n",
      "Epoch 00171: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0125, Macro_F1: 0.9094, AUC_score: 0.9542\n",
      "Epoch 250: Train Loss: 0.0211, Macro_F1: 0.9130, AUC_score: 0.9545\n",
      "Epoch 00272: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 300: Train Loss: 0.0068, Macro_F1: 0.9058, AUC_score: 0.9546\n",
      "Epoch 350: Train Loss: 0.0085, Macro_F1: 0.9058, AUC_score: 0.9541\n",
      "Early stopping triggered\n",
      "acc save\n",
      "74.0% node features transform to 0: F1: 0.9058, AUC_score: 0.9544\n",
      "Epoch 0: Train Loss: 0.0103, Macro_F1: 0.8898, AUC_score: 0.9497\n",
      "Validation loss decreased (0.010308 --> 0.010308).\n",
      "Validation loss decreased (0.009721 --> 0.009721).\n",
      "Validation loss decreased (0.008741 --> 0.008741).\n",
      "Validation loss decreased (0.008541 --> 0.008541).\n",
      "Validation loss decreased (0.008511 --> 0.008511).\n",
      "Validation loss decreased (0.007893 --> 0.007893).\n",
      "Validation loss decreased (0.007859 --> 0.007859).\n",
      "Validation loss decreased (0.007516 --> 0.007516).\n",
      "Epoch 50: Train Loss: 0.0202, Macro_F1: 0.9058, AUC_score: 0.9557\n",
      "Validation loss decreased (0.007296 --> 0.007296).\n",
      "Validation loss decreased (0.007238 --> 0.007238).\n",
      "Validation loss decreased (0.005818 --> 0.005818).\n",
      "Epoch 100: Train Loss: 0.0088, Macro_F1: 0.9166, AUC_score: 0.9563\n",
      "Validation loss decreased (0.005093 --> 0.005093).\n",
      "Epoch 150: Train Loss: 0.0091, Macro_F1: 0.9058, AUC_score: 0.9553\n",
      "Epoch 200: Train Loss: 0.0124, Macro_F1: 0.9130, AUC_score: 0.9570\n",
      "Epoch 00225: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0107, Macro_F1: 0.9165, AUC_score: 0.9564\n",
      "Epoch 300: Train Loss: 0.0132, Macro_F1: 0.9094, AUC_score: 0.9573\n",
      "Epoch 00326: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.0096, Macro_F1: 0.9130, AUC_score: 0.9559\n",
      "Epoch 400: Train Loss: 0.0133, Macro_F1: 0.9130, AUC_score: 0.9566\n",
      "Early stopping triggered\n",
      "acc save\n",
      "73.0% node features transform to 0: F1: 0.9130, AUC_score: 0.9568\n",
      "Epoch 0: Train Loss: 0.0262, Macro_F1: 0.8913, AUC_score: 0.9509\n",
      "Validation loss decreased (0.026167 --> 0.026167).\n",
      "Validation loss decreased (0.013043 --> 0.013043).\n",
      "Validation loss decreased (0.011606 --> 0.011606).\n",
      "Validation loss decreased (0.010715 --> 0.010715).\n",
      "Validation loss decreased (0.010206 --> 0.010206).\n",
      "Validation loss decreased (0.008800 --> 0.008800).\n",
      "Validation loss decreased (0.008699 --> 0.008699).\n",
      "Validation loss decreased (0.008272 --> 0.008272).\n",
      "Epoch 50: Train Loss: 0.0166, Macro_F1: 0.9166, AUC_score: 0.9530\n",
      "Validation loss decreased (0.008031 --> 0.008031).\n",
      "Validation loss decreased (0.007718 --> 0.007718).\n",
      "Epoch 100: Train Loss: 0.0097, Macro_F1: 0.9057, AUC_score: 0.9517\n",
      "Validation loss decreased (0.007148 --> 0.007148).\n",
      "Epoch 150: Train Loss: 0.0123, Macro_F1: 0.9128, AUC_score: 0.9492\n",
      "Validation loss decreased (0.007021 --> 0.007021).\n",
      "Validation loss decreased (0.006897 --> 0.006897).\n",
      "Epoch 200: Train Loss: 0.0187, Macro_F1: 0.9094, AUC_score: 0.9536\n",
      "Epoch 250: Train Loss: 0.0147, Macro_F1: 0.9130, AUC_score: 0.9494\n",
      "Validation loss decreased (0.006068 --> 0.006068).\n",
      "Validation loss decreased (0.005689 --> 0.005689).\n",
      "Epoch 300: Train Loss: 0.0066, Macro_F1: 0.9163, AUC_score: 0.9533\n",
      "Epoch 350: Train Loss: 0.0067, Macro_F1: 0.9130, AUC_score: 0.9536\n",
      "Epoch 00387: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 400: Train Loss: 0.0134, Macro_F1: 0.9130, AUC_score: 0.9559\n",
      "Epoch 450: Train Loss: 0.0209, Macro_F1: 0.9130, AUC_score: 0.9541\n",
      "Validation loss decreased (0.005558 --> 0.005558).\n",
      "Validation loss decreased (0.005493 --> 0.005493).\n",
      "Epoch 500: Train Loss: 0.0120, Macro_F1: 0.9056, AUC_score: 0.9519\n",
      "Validation loss decreased (0.004750 --> 0.004750).\n",
      "Epoch 550: Train Loss: 0.0058, Macro_F1: 0.9057, AUC_score: 0.9535\n",
      "Epoch 600: Train Loss: 0.0061, Macro_F1: 0.9093, AUC_score: 0.9531\n",
      "Epoch 00618: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 650: Train Loss: 0.0106, Macro_F1: 0.9130, AUC_score: 0.9549\n",
      "Epoch 700: Train Loss: 0.0089, Macro_F1: 0.9094, AUC_score: 0.9544\n",
      "Epoch 00719: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 750: Train Loss: 0.0202, Macro_F1: 0.9094, AUC_score: 0.9543\n",
      "Epoch 800: Train Loss: 0.0125, Macro_F1: 0.9094, AUC_score: 0.9543\n",
      "Early stopping triggered\n",
      "acc save\n",
      "72.0% node features transform to 0: F1: 0.9058, AUC_score: 0.9543\n",
      "Epoch 0: Train Loss: 0.0137, Macro_F1: 0.8949, AUC_score: 0.9516\n",
      "Validation loss decreased (0.013675 --> 0.013675).\n",
      "Validation loss decreased (0.013211 --> 0.013211).\n",
      "Validation loss decreased (0.010871 --> 0.010871).\n",
      "Validation loss decreased (0.008367 --> 0.008367).\n",
      "Epoch 50: Train Loss: 0.0129, Macro_F1: 0.9130, AUC_score: 0.9581\n",
      "Validation loss decreased (0.008021 --> 0.008021).\n",
      "Validation loss decreased (0.007849 --> 0.007849).\n",
      "Epoch 100: Train Loss: 0.0088, Macro_F1: 0.9058, AUC_score: 0.9552\n",
      "Validation loss decreased (0.007635 --> 0.007635).\n",
      "Validation loss decreased (0.006974 --> 0.006974).\n",
      "Epoch 150: Train Loss: 0.0086, Macro_F1: 0.9021, AUC_score: 0.9579\n",
      "Validation loss decreased (0.006799 --> 0.006799).\n",
      "Validation loss decreased (0.006019 --> 0.006019).\n",
      "Epoch 200: Train Loss: 0.0082, Macro_F1: 0.9058, AUC_score: 0.9590\n",
      "Validation loss decreased (0.004931 --> 0.004931).\n",
      "Epoch 250: Train Loss: 0.0077, Macro_F1: 0.9094, AUC_score: 0.9543\n",
      "Epoch 300: Train Loss: 0.0093, Macro_F1: 0.9130, AUC_score: 0.9569\n",
      "Epoch 00309: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 350: Train Loss: 0.0082, Macro_F1: 0.9058, AUC_score: 0.9553\n",
      "Epoch 400: Train Loss: 0.0079, Macro_F1: 0.9094, AUC_score: 0.9549\n",
      "Epoch 00410: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.0068, Macro_F1: 0.9058, AUC_score: 0.9551\n",
      "Epoch 500: Train Loss: 0.0140, Macro_F1: 0.9058, AUC_score: 0.9547\n",
      "Early stopping triggered\n",
      "acc save\n",
      "71.0% node features transform to 0: F1: 0.9058, AUC_score: 0.9547\n",
      "Epoch 0: Train Loss: 0.0140, Macro_F1: 0.8978, AUC_score: 0.9509\n",
      "Validation loss decreased (0.014004 --> 0.014004).\n",
      "Validation loss decreased (0.010094 --> 0.010094).\n",
      "Validation loss decreased (0.009134 --> 0.009134).\n",
      "Validation loss decreased (0.008454 --> 0.008454).\n",
      "Validation loss decreased (0.008207 --> 0.008207).\n",
      "Validation loss decreased (0.007438 --> 0.007438).\n",
      "Validation loss decreased (0.007095 --> 0.007095).\n",
      "Validation loss decreased (0.006146 --> 0.006146).\n",
      "Validation loss decreased (0.005563 --> 0.005563).\n",
      "Epoch 50: Train Loss: 0.0098, Macro_F1: 0.8984, AUC_score: 0.9570\n",
      "Epoch 100: Train Loss: 0.0182, Macro_F1: 0.8976, AUC_score: 0.9531\n",
      "Epoch 00149: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.0306, Macro_F1: 0.9052, AUC_score: 0.9559\n",
      "Validation loss decreased (0.004655 --> 0.004655).\n",
      "Epoch 200: Train Loss: 0.0088, Macro_F1: 0.9129, AUC_score: 0.9566\n",
      "Epoch 250: Train Loss: 0.0076, Macro_F1: 0.9056, AUC_score: 0.9557\n",
      "Epoch 00285: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 300: Train Loss: 0.0070, Macro_F1: 0.9057, AUC_score: 0.9565\n",
      "Epoch 350: Train Loss: 0.0162, Macro_F1: 0.9094, AUC_score: 0.9571\n",
      "Epoch 00386: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 400: Train Loss: 0.0126, Macro_F1: 0.9057, AUC_score: 0.9566\n",
      "Epoch 450: Train Loss: 0.0055, Macro_F1: 0.9094, AUC_score: 0.9569\n",
      "Early stopping triggered\n",
      "acc save\n",
      "70.0% node features transform to 0: F1: 0.9094, AUC_score: 0.9570\n",
      "Epoch 0: Train Loss: 0.0100, Macro_F1: 0.9056, AUC_score: 0.9554\n",
      "Validation loss decreased (0.010032 --> 0.010032).\n",
      "Validation loss decreased (0.008390 --> 0.008390).\n",
      "Validation loss decreased (0.007235 --> 0.007235).\n",
      "Validation loss decreased (0.006871 --> 0.006871).\n",
      "Validation loss decreased (0.006678 --> 0.006678).\n",
      "Epoch 50: Train Loss: 0.0077, Macro_F1: 0.9166, AUC_score: 0.9621\n",
      "Validation loss decreased (0.006444 --> 0.006444).\n",
      "Validation loss decreased (0.006182 --> 0.006182).\n",
      "Epoch 100: Train Loss: 0.0171, Macro_F1: 0.9203, AUC_score: 0.9623\n",
      "Validation loss decreased (0.006034 --> 0.006034).\n",
      "Epoch 150: Train Loss: 0.0142, Macro_F1: 0.9092, AUC_score: 0.9580\n",
      "Epoch 200: Train Loss: 0.0135, Macro_F1: 0.9021, AUC_score: 0.9595\n",
      "Validation loss decreased (0.005784 --> 0.005784).\n",
      "Epoch 250: Train Loss: 0.0078, Macro_F1: 0.9092, AUC_score: 0.9598\n",
      "Validation loss decreased (0.005611 --> 0.005611).\n",
      "Validation loss decreased (0.004559 --> 0.004559).\n",
      "Epoch 300: Train Loss: 0.0059, Macro_F1: 0.9094, AUC_score: 0.9609\n",
      "Epoch 350: Train Loss: 0.0056, Macro_F1: 0.9167, AUC_score: 0.9603\n",
      "Epoch 00365: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 400: Train Loss: 0.0140, Macro_F1: 0.9166, AUC_score: 0.9609\n",
      "Validation loss decreased (0.004360 --> 0.004360).\n",
      "Epoch 450: Train Loss: 0.0071, Macro_F1: 0.9130, AUC_score: 0.9602\n",
      "Epoch 500: Train Loss: 0.0077, Macro_F1: 0.9094, AUC_score: 0.9599\n",
      "Epoch 00531: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 550: Train Loss: 0.0125, Macro_F1: 0.9130, AUC_score: 0.9611\n",
      "Epoch 600: Train Loss: 0.0076, Macro_F1: 0.9094, AUC_score: 0.9605\n",
      "Epoch 00632: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 650: Train Loss: 0.0079, Macro_F1: 0.9130, AUC_score: 0.9603\n",
      "Epoch 700: Train Loss: 0.0070, Macro_F1: 0.9130, AUC_score: 0.9601\n",
      "Early stopping triggered\n",
      "acc save\n",
      "69.0% node features transform to 0: F1: 0.9130, AUC_score: 0.9607\n",
      "Epoch 0: Train Loss: 0.0105, Macro_F1: 0.9130, AUC_score: 0.9607\n",
      "Validation loss decreased (0.010545 --> 0.010545).\n",
      "Validation loss decreased (0.006942 --> 0.006942).\n",
      "Validation loss decreased (0.006859 --> 0.006859).\n",
      "Epoch 50: Train Loss: 0.0129, Macro_F1: 0.9239, AUC_score: 0.9641\n",
      "Validation loss decreased (0.006812 --> 0.006812).\n",
      "Epoch 100: Train Loss: 0.0326, Macro_F1: 0.9203, AUC_score: 0.9630\n",
      "Validation loss decreased (0.006674 --> 0.006674).\n",
      "Validation loss decreased (0.006658 --> 0.006658).\n",
      "Validation loss decreased (0.006143 --> 0.006143).\n",
      "Validation loss decreased (0.005805 --> 0.005805).\n",
      "Epoch 150: Train Loss: 0.0097, Macro_F1: 0.9019, AUC_score: 0.9598\n",
      "Validation loss decreased (0.003950 --> 0.003950).\n",
      "Epoch 200: Train Loss: 0.0070, Macro_F1: 0.9130, AUC_score: 0.9628\n",
      "Epoch 250: Train Loss: 0.0141, Macro_F1: 0.9203, AUC_score: 0.9643\n",
      "Epoch 00299: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0290, Macro_F1: 0.9092, AUC_score: 0.9634\n",
      "Epoch 350: Train Loss: 0.0069, Macro_F1: 0.9203, AUC_score: 0.9632\n",
      "Epoch 00400: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0147, Macro_F1: 0.9239, AUC_score: 0.9638\n",
      "Epoch 450: Train Loss: 0.0064, Macro_F1: 0.9166, AUC_score: 0.9626\n",
      "Early stopping triggered\n",
      "acc save\n",
      "68.0% node features transform to 0: F1: 0.9166, AUC_score: 0.9625\n",
      "Epoch 0: Train Loss: 0.0078, Macro_F1: 0.8749, AUC_score: 0.9564\n",
      "Validation loss decreased (0.007822 --> 0.007822).\n",
      "Validation loss decreased (0.005338 --> 0.005338).\n",
      "Validation loss decreased (0.004808 --> 0.004808).\n",
      "Epoch 50: Train Loss: 0.0098, Macro_F1: 0.9128, AUC_score: 0.9604\n",
      "Epoch 100: Train Loss: 0.0190, Macro_F1: 0.9166, AUC_score: 0.9651\n",
      "Epoch 00123: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.0098, Macro_F1: 0.9128, AUC_score: 0.9619\n",
      "Epoch 200: Train Loss: 0.0065, Macro_F1: 0.9130, AUC_score: 0.9627\n",
      "Epoch 00224: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 250: Train Loss: 0.0090, Macro_F1: 0.9166, AUC_score: 0.9643\n",
      "Validation loss decreased (0.004547 --> 0.004547).\n",
      "Epoch 300: Train Loss: 0.0105, Macro_F1: 0.9166, AUC_score: 0.9639\n",
      "Epoch 350: Train Loss: 0.0074, Macro_F1: 0.9130, AUC_score: 0.9634\n",
      "Epoch 00380: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 400: Train Loss: 0.0050, Macro_F1: 0.9166, AUC_score: 0.9642\n",
      "Epoch 450: Train Loss: 0.0069, Macro_F1: 0.9166, AUC_score: 0.9641\n",
      "Epoch 00481: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 500: Train Loss: 0.0067, Macro_F1: 0.9166, AUC_score: 0.9640\n",
      "Epoch 550: Train Loss: 0.0079, Macro_F1: 0.9166, AUC_score: 0.9640\n",
      "Early stopping triggered\n",
      "acc save\n",
      "67.0% node features transform to 0: F1: 0.9166, AUC_score: 0.9641\n",
      "Epoch 0: Train Loss: 0.0135, Macro_F1: 0.8706, AUC_score: 0.9537\n",
      "Validation loss decreased (0.013533 --> 0.013533).\n",
      "Validation loss decreased (0.010810 --> 0.010810).\n",
      "Validation loss decreased (0.009203 --> 0.009203).\n",
      "Validation loss decreased (0.007013 --> 0.007013).\n",
      "Validation loss decreased (0.006036 --> 0.006036).\n",
      "Epoch 50: Train Loss: 0.0132, Macro_F1: 0.9129, AUC_score: 0.9625\n",
      "Validation loss decreased (0.005783 --> 0.005783).\n",
      "Validation loss decreased (0.005729 --> 0.005729).\n",
      "Validation loss decreased (0.004875 --> 0.004875).\n",
      "Epoch 100: Train Loss: 0.0074, Macro_F1: 0.9167, AUC_score: 0.9650\n",
      "Epoch 150: Train Loss: 0.0082, Macro_F1: 0.9165, AUC_score: 0.9647\n",
      "Epoch 00172: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.004764 --> 0.004764).\n",
      "Validation loss decreased (0.004749 --> 0.004749).\n",
      "Epoch 200: Train Loss: 0.0066, Macro_F1: 0.9238, AUC_score: 0.9646\n",
      "Validation loss decreased (0.004333 --> 0.004333).\n",
      "Epoch 250: Train Loss: 0.0055, Macro_F1: 0.9130, AUC_score: 0.9645\n",
      "Validation loss decreased (0.004200 --> 0.004200).\n",
      "Epoch 300: Train Loss: 0.0088, Macro_F1: 0.9167, AUC_score: 0.9642\n",
      "Epoch 350: Train Loss: 0.0230, Macro_F1: 0.9094, AUC_score: 0.9643\n",
      "Epoch 00376: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0053, Macro_F1: 0.9202, AUC_score: 0.9628\n",
      "Epoch 450: Train Loss: 0.0071, Macro_F1: 0.9167, AUC_score: 0.9639\n",
      "Epoch 00477: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 500: Train Loss: 0.0046, Macro_F1: 0.9166, AUC_score: 0.9632\n",
      "Epoch 550: Train Loss: 0.0080, Macro_F1: 0.9202, AUC_score: 0.9632\n",
      "Early stopping triggered\n",
      "acc save\n",
      "65.99999999999999% node features transform to 0: F1: 0.9202, AUC_score: 0.9631\n",
      "Epoch 0: Train Loss: 0.0095, Macro_F1: 0.8791, AUC_score: 0.9499\n",
      "Validation loss decreased (0.009500 --> 0.009500).\n",
      "Validation loss decreased (0.008869 --> 0.008869).\n",
      "Validation loss decreased (0.006999 --> 0.006999).\n",
      "Validation loss decreased (0.005879 --> 0.005879).\n",
      "Epoch 50: Train Loss: 0.0101, Macro_F1: 0.9238, AUC_score: 0.9626\n",
      "Validation loss decreased (0.005520 --> 0.005520).\n",
      "Epoch 100: Train Loss: 0.0097, Macro_F1: 0.9091, AUC_score: 0.9602\n",
      "Epoch 150: Train Loss: 0.0126, Macro_F1: 0.9274, AUC_score: 0.9627\n",
      "Validation loss decreased (0.004656 --> 0.004656).\n",
      "Validation loss decreased (0.004454 --> 0.004454).\n",
      "Epoch 200: Train Loss: 0.0122, Macro_F1: 0.8949, AUC_score: 0.9641\n",
      "Epoch 250: Train Loss: 0.0062, Macro_F1: 0.9130, AUC_score: 0.9611\n",
      "Epoch 00289: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0073, Macro_F1: 0.9129, AUC_score: 0.9615\n",
      "Epoch 350: Train Loss: 0.0056, Macro_F1: 0.9239, AUC_score: 0.9634\n",
      "Validation loss decreased (0.004224 --> 0.004224).\n",
      "Epoch 400: Train Loss: 0.0096, Macro_F1: 0.9093, AUC_score: 0.9616\n",
      "Epoch 450: Train Loss: 0.0089, Macro_F1: 0.9202, AUC_score: 0.9619\n",
      "Epoch 00477: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.0077, Macro_F1: 0.9203, AUC_score: 0.9627\n",
      "Epoch 550: Train Loss: 0.0086, Macro_F1: 0.9166, AUC_score: 0.9624\n",
      "Epoch 00578: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Validation loss decreased (0.004208 --> 0.004208).\n",
      "Epoch 600: Train Loss: 0.0119, Macro_F1: 0.9166, AUC_score: 0.9625\n",
      "Epoch 650: Train Loss: 0.0070, Macro_F1: 0.9166, AUC_score: 0.9624\n",
      "Epoch 00690: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 700: Train Loss: 0.0079, Macro_F1: 0.9166, AUC_score: 0.9628\n",
      "Epoch 750: Train Loss: 0.0082, Macro_F1: 0.9166, AUC_score: 0.9627\n",
      "Epoch 00791: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 800: Train Loss: 0.0055, Macro_F1: 0.9166, AUC_score: 0.9625\n",
      "Epoch 850: Train Loss: 0.0385, Macro_F1: 0.9166, AUC_score: 0.9625\n",
      "Early stopping triggered\n",
      "acc save\n",
      "64.99999999999999% node features transform to 0: F1: 0.9166, AUC_score: 0.9625\n",
      "Epoch 0: Train Loss: 0.0079, Macro_F1: 0.8912, AUC_score: 0.9624\n",
      "Validation loss decreased (0.007852 --> 0.007852).\n",
      "Validation loss decreased (0.007334 --> 0.007334).\n",
      "Validation loss decreased (0.007018 --> 0.007018).\n",
      "Validation loss decreased (0.006641 --> 0.006641).\n",
      "Validation loss decreased (0.006493 --> 0.006493).\n",
      "Validation loss decreased (0.005574 --> 0.005574).\n",
      "Epoch 50: Train Loss: 0.0069, Macro_F1: 0.9310, AUC_score: 0.9639\n",
      "Validation loss decreased (0.005230 --> 0.005230).\n",
      "Epoch 100: Train Loss: 0.0082, Macro_F1: 0.9202, AUC_score: 0.9634\n",
      "Validation loss decreased (0.004994 --> 0.004994).\n",
      "Validation loss decreased (0.004364 --> 0.004364).\n",
      "Epoch 150: Train Loss: 0.0098, Macro_F1: 0.9129, AUC_score: 0.9615\n",
      "Epoch 200: Train Loss: 0.0109, Macro_F1: 0.9130, AUC_score: 0.9656\n",
      "Epoch 00212: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0069, Macro_F1: 0.9239, AUC_score: 0.9647\n",
      "Epoch 300: Train Loss: 0.0082, Macro_F1: 0.9130, AUC_score: 0.9645\n",
      "Epoch 00313: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.0082, Macro_F1: 0.9239, AUC_score: 0.9629\n",
      "Validation loss decreased (0.003716 --> 0.003716).\n",
      "Epoch 400: Train Loss: 0.0083, Macro_F1: 0.9239, AUC_score: 0.9629\n",
      "Epoch 450: Train Loss: 0.0079, Macro_F1: 0.9275, AUC_score: 0.9633\n",
      "Epoch 00490: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 500: Train Loss: 0.0141, Macro_F1: 0.9275, AUC_score: 0.9635\n",
      "Epoch 550: Train Loss: 0.0127, Macro_F1: 0.9275, AUC_score: 0.9631\n",
      "Epoch 00591: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 600: Train Loss: 0.0083, Macro_F1: 0.9239, AUC_score: 0.9629\n",
      "Epoch 650: Train Loss: 0.0076, Macro_F1: 0.9239, AUC_score: 0.9630\n",
      "Early stopping triggered\n",
      "acc save\n",
      "64.0% node features transform to 0: F1: 0.9239, AUC_score: 0.9630\n",
      "Epoch 0: Train Loss: 0.0112, Macro_F1: 0.8913, AUC_score: 0.9653\n",
      "Validation loss decreased (0.011214 --> 0.011214).\n",
      "Validation loss decreased (0.010401 --> 0.010401).\n",
      "Validation loss decreased (0.009662 --> 0.009662).\n",
      "Validation loss decreased (0.008994 --> 0.008994).\n",
      "Validation loss decreased (0.007648 --> 0.007648).\n",
      "Validation loss decreased (0.007634 --> 0.007634).\n",
      "Validation loss decreased (0.007026 --> 0.007026).\n",
      "Validation loss decreased (0.006419 --> 0.006419).\n",
      "Validation loss decreased (0.006061 --> 0.006061).\n",
      "Epoch 50: Train Loss: 0.0186, Macro_F1: 0.9127, AUC_score: 0.9581\n",
      "Validation loss decreased (0.005936 --> 0.005936).\n",
      "Epoch 100: Train Loss: 0.0099, Macro_F1: 0.9237, AUC_score: 0.9627\n",
      "Validation loss decreased (0.004442 --> 0.004442).\n",
      "Epoch 150: Train Loss: 0.0069, Macro_F1: 0.9239, AUC_score: 0.9633\n",
      "Epoch 200: Train Loss: 0.0097, Macro_F1: 0.9165, AUC_score: 0.9621\n",
      "Epoch 00227: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0057, Macro_F1: 0.9201, AUC_score: 0.9619\n",
      "Validation loss decreased (0.003797 --> 0.003797).\n",
      "Epoch 300: Train Loss: 0.0079, Macro_F1: 0.9166, AUC_score: 0.9626\n",
      "Epoch 350: Train Loss: 0.0056, Macro_F1: 0.9203, AUC_score: 0.9635\n",
      "Epoch 00387: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0078, Macro_F1: 0.9202, AUC_score: 0.9636\n",
      "Epoch 450: Train Loss: 0.0063, Macro_F1: 0.9166, AUC_score: 0.9634\n",
      "Epoch 00488: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 500: Train Loss: 0.0068, Macro_F1: 0.9239, AUC_score: 0.9634\n",
      "Epoch 550: Train Loss: 0.0070, Macro_F1: 0.9202, AUC_score: 0.9631\n",
      "Early stopping triggered\n",
      "acc save\n",
      "63.0% node features transform to 0: F1: 0.9239, AUC_score: 0.9633\n",
      "Epoch 0: Train Loss: 0.0149, Macro_F1: 0.8801, AUC_score: 0.9629\n",
      "Validation loss decreased (0.014908 --> 0.014908).\n",
      "Validation loss decreased (0.011707 --> 0.011707).\n",
      "Validation loss decreased (0.006214 --> 0.006214).\n",
      "Validation loss decreased (0.005063 --> 0.005063).\n",
      "Epoch 50: Train Loss: 0.0131, Macro_F1: 0.9166, AUC_score: 0.9648\n",
      "Validation loss decreased (0.004746 --> 0.004746).\n",
      "Epoch 100: Train Loss: 0.0133, Macro_F1: 0.9166, AUC_score: 0.9626\n",
      "Validation loss decreased (0.004581 --> 0.004581).\n",
      "Validation loss decreased (0.004360 --> 0.004360).\n",
      "Epoch 150: Train Loss: 0.0066, Macro_F1: 0.9166, AUC_score: 0.9631\n",
      "Epoch 200: Train Loss: 0.0052, Macro_F1: 0.9239, AUC_score: 0.9635\n",
      "Epoch 00239: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0073, Macro_F1: 0.9239, AUC_score: 0.9633\n",
      "Epoch 300: Train Loss: 0.0059, Macro_F1: 0.9239, AUC_score: 0.9635\n",
      "Epoch 00340: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.0070, Macro_F1: 0.9165, AUC_score: 0.9615\n",
      "Epoch 400: Train Loss: 0.0107, Macro_F1: 0.9202, AUC_score: 0.9623\n",
      "Early stopping triggered\n",
      "acc save\n",
      "62.0% node features transform to 0: F1: 0.9275, AUC_score: 0.9635\n",
      "Epoch 0: Train Loss: 0.0075, Macro_F1: 0.8838, AUC_score: 0.9655\n",
      "Validation loss decreased (0.007472 --> 0.007472).\n",
      "Validation loss decreased (0.007292 --> 0.007292).\n",
      "Validation loss decreased (0.007004 --> 0.007004).\n",
      "Validation loss decreased (0.006948 --> 0.006948).\n",
      "Validation loss decreased (0.005923 --> 0.005923).\n",
      "Epoch 50: Train Loss: 0.0080, Macro_F1: 0.9275, AUC_score: 0.9643\n",
      "Validation loss decreased (0.005510 --> 0.005510).\n",
      "Validation loss decreased (0.005439 --> 0.005439).\n",
      "Validation loss decreased (0.005406 --> 0.005406).\n",
      "Validation loss decreased (0.005153 --> 0.005153).\n",
      "Epoch 100: Train Loss: 0.0088, Macro_F1: 0.9166, AUC_score: 0.9620\n",
      "Validation loss decreased (0.004498 --> 0.004498).\n",
      "Epoch 150: Train Loss: 0.0059, Macro_F1: 0.9237, AUC_score: 0.9613\n",
      "Epoch 200: Train Loss: 0.0073, Macro_F1: 0.9239, AUC_score: 0.9645\n",
      "Epoch 00203: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.004451 --> 0.004451).\n",
      "Epoch 250: Train Loss: 0.0074, Macro_F1: 0.9239, AUC_score: 0.9640\n",
      "Validation loss decreased (0.004366 --> 0.004366).\n",
      "Validation loss decreased (0.004210 --> 0.004210).\n",
      "Epoch 300: Train Loss: 0.0058, Macro_F1: 0.9166, AUC_score: 0.9624\n",
      "Validation loss decreased (0.003961 --> 0.003961).\n",
      "Epoch 350: Train Loss: 0.0074, Macro_F1: 0.9166, AUC_score: 0.9627\n",
      "Epoch 400: Train Loss: 0.0048, Macro_F1: 0.9166, AUC_score: 0.9628\n",
      "Epoch 00421: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.0085, Macro_F1: 0.9239, AUC_score: 0.9634\n",
      "Epoch 500: Train Loss: 0.0057, Macro_F1: 0.9239, AUC_score: 0.9627\n",
      "Epoch 00522: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0078, Macro_F1: 0.9239, AUC_score: 0.9634\n",
      "Epoch 600: Train Loss: 0.0061, Macro_F1: 0.9239, AUC_score: 0.9632\n",
      "Early stopping triggered\n",
      "acc save\n",
      "61.0% node features transform to 0: F1: 0.9239, AUC_score: 0.9633\n",
      "Epoch 0: Train Loss: 0.0095, Macro_F1: 0.8818, AUC_score: 0.9516\n",
      "Validation loss decreased (0.009499 --> 0.009499).\n",
      "Validation loss decreased (0.006345 --> 0.006345).\n",
      "Validation loss decreased (0.006173 --> 0.006173).\n",
      "Epoch 50: Train Loss: 0.0118, Macro_F1: 0.9239, AUC_score: 0.9648\n",
      "Validation loss decreased (0.005414 --> 0.005414).\n",
      "Validation loss decreased (0.004173 --> 0.004173).\n",
      "Epoch 100: Train Loss: 0.0143, Macro_F1: 0.9130, AUC_score: 0.9676\n",
      "Epoch 150: Train Loss: 0.0095, Macro_F1: 0.9202, AUC_score: 0.9652\n",
      "Epoch 00179: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0096, Macro_F1: 0.9311, AUC_score: 0.9633\n",
      "Epoch 250: Train Loss: 0.0058, Macro_F1: 0.9275, AUC_score: 0.9629\n",
      "Epoch 00280: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 300: Train Loss: 0.0062, Macro_F1: 0.9275, AUC_score: 0.9621\n",
      "Epoch 350: Train Loss: 0.0064, Macro_F1: 0.9275, AUC_score: 0.9631\n",
      "Validation loss decreased (0.004139 --> 0.004139).\n",
      "Validation loss decreased (0.004098 --> 0.004098).\n",
      "Epoch 400: Train Loss: 0.0091, Macro_F1: 0.9239, AUC_score: 0.9626\n",
      "Epoch 450: Train Loss: 0.0113, Macro_F1: 0.9275, AUC_score: 0.9635\n",
      "Epoch 00490: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 500: Train Loss: 0.0101, Macro_F1: 0.9239, AUC_score: 0.9624\n",
      "Epoch 550: Train Loss: 0.0065, Macro_F1: 0.9239, AUC_score: 0.9624\n",
      "Epoch 00591: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 600: Train Loss: 0.0079, Macro_F1: 0.9239, AUC_score: 0.9625\n",
      "Epoch 650: Train Loss: 0.0170, Macro_F1: 0.9239, AUC_score: 0.9625\n",
      "Early stopping triggered\n",
      "acc save\n",
      "60.0% node features transform to 0: F1: 0.9239, AUC_score: 0.9625\n",
      "Epoch 0: Train Loss: 0.0054, Macro_F1: 0.8838, AUC_score: 0.9650\n",
      "Validation loss decreased (0.005381 --> 0.005381).\n",
      "Epoch 50: Train Loss: 0.0076, Macro_F1: 0.9239, AUC_score: 0.9663\n",
      "Validation loss decreased (0.005332 --> 0.005332).\n",
      "Validation loss decreased (0.005282 --> 0.005282).\n",
      "Validation loss decreased (0.005200 --> 0.005200).\n",
      "Validation loss decreased (0.004683 --> 0.004683).\n",
      "Validation loss decreased (0.004357 --> 0.004357).\n",
      "Epoch 100: Train Loss: 0.0079, Macro_F1: 0.9166, AUC_score: 0.9613\n",
      "Validation loss decreased (0.003622 --> 0.003622).\n",
      "Epoch 150: Train Loss: 0.0067, Macro_F1: 0.9200, AUC_score: 0.9577\n",
      "Epoch 200: Train Loss: 0.0059, Macro_F1: 0.9201, AUC_score: 0.9598\n",
      "Epoch 00204: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0074, Macro_F1: 0.9202, AUC_score: 0.9626\n",
      "Epoch 300: Train Loss: 0.0050, Macro_F1: 0.9238, AUC_score: 0.9606\n",
      "Epoch 00305: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.0058, Macro_F1: 0.9201, AUC_score: 0.9604\n",
      "Epoch 400: Train Loss: 0.0051, Macro_F1: 0.9202, AUC_score: 0.9616\n",
      "Early stopping triggered\n",
      "acc save\n",
      "59.0% node features transform to 0: F1: 0.9202, AUC_score: 0.9615\n",
      "Epoch 0: Train Loss: 0.0144, Macro_F1: 0.8858, AUC_score: 0.9516\n",
      "Validation loss decreased (0.014438 --> 0.014438).\n",
      "Validation loss decreased (0.010816 --> 0.010816).\n",
      "Validation loss decreased (0.006525 --> 0.006525).\n",
      "Validation loss decreased (0.006428 --> 0.006428).\n",
      "Epoch 50: Train Loss: 0.0068, Macro_F1: 0.9165, AUC_score: 0.9596\n",
      "Validation loss decreased (0.006148 --> 0.006148).\n",
      "Validation loss decreased (0.006052 --> 0.006052).\n",
      "Validation loss decreased (0.005249 --> 0.005249).\n",
      "Epoch 100: Train Loss: 0.0156, Macro_F1: 0.9129, AUC_score: 0.9598\n",
      "Validation loss decreased (0.004872 --> 0.004872).\n",
      "Epoch 150: Train Loss: 0.0091, Macro_F1: 0.9200, AUC_score: 0.9606\n",
      "Epoch 200: Train Loss: 0.0114, Macro_F1: 0.9130, AUC_score: 0.9634\n",
      "Epoch 00211: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.004581 --> 0.004581).\n",
      "Epoch 250: Train Loss: 0.0072, Macro_F1: 0.9166, AUC_score: 0.9606\n",
      "Epoch 300: Train Loss: 0.0076, Macro_F1: 0.9165, AUC_score: 0.9601\n",
      "Validation loss decreased (0.003980 --> 0.003980).\n",
      "Validation loss decreased (0.003858 --> 0.003858).\n",
      "Epoch 350: Train Loss: 0.0079, Macro_F1: 0.9165, AUC_score: 0.9591\n",
      "Epoch 400: Train Loss: 0.0049, Macro_F1: 0.9237, AUC_score: 0.9598\n",
      "Epoch 00422: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.0102, Macro_F1: 0.9165, AUC_score: 0.9597\n",
      "Epoch 500: Train Loss: 0.0056, Macro_F1: 0.9130, AUC_score: 0.9606\n",
      "Epoch 00523: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0062, Macro_F1: 0.9129, AUC_score: 0.9597\n",
      "Epoch 600: Train Loss: 0.0096, Macro_F1: 0.9129, AUC_score: 0.9598\n",
      "Early stopping triggered\n",
      "acc save\n",
      "58.00000000000001% node features transform to 0: F1: 0.9093, AUC_score: 0.9597\n",
      "Epoch 0: Train Loss: 0.0056, Macro_F1: 0.8941, AUC_score: 0.9525\n",
      "Validation loss decreased (0.005644 --> 0.005644).\n",
      "Validation loss decreased (0.004506 --> 0.004506).\n",
      "Epoch 50: Train Loss: 0.0108, Macro_F1: 0.9239, AUC_score: 0.9617\n",
      "Epoch 100: Train Loss: 0.0145, Macro_F1: 0.9203, AUC_score: 0.9641\n",
      "Epoch 00136: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.0051, Macro_F1: 0.9165, AUC_score: 0.9599\n",
      "Validation loss decreased (0.004397 --> 0.004397).\n",
      "Epoch 200: Train Loss: 0.0050, Macro_F1: 0.9166, AUC_score: 0.9608\n",
      "Validation loss decreased (0.003574 --> 0.003574).\n",
      "Epoch 250: Train Loss: 0.0082, Macro_F1: 0.9130, AUC_score: 0.9609\n",
      "Epoch 300: Train Loss: 0.0073, Macro_F1: 0.9239, AUC_score: 0.9623\n",
      "Epoch 00332: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.0067, Macro_F1: 0.9201, AUC_score: 0.9600\n",
      "Epoch 400: Train Loss: 0.0090, Macro_F1: 0.9166, AUC_score: 0.9615\n",
      "Epoch 00433: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 450: Train Loss: 0.0077, Macro_F1: 0.9129, AUC_score: 0.9607\n",
      "Validation loss decreased (0.003482 --> 0.003482).\n",
      "Epoch 500: Train Loss: 0.0074, Macro_F1: 0.9129, AUC_score: 0.9607\n",
      "Epoch 550: Train Loss: 0.0032, Macro_F1: 0.9129, AUC_score: 0.9607\n",
      "Validation loss decreased (0.003174 --> 0.003174).\n",
      "Epoch 600: Train Loss: 0.0096, Macro_F1: 0.9129, AUC_score: 0.9604\n",
      "Epoch 650: Train Loss: 0.0095, Macro_F1: 0.9166, AUC_score: 0.9608\n",
      "Epoch 00652: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 700: Train Loss: 0.0044, Macro_F1: 0.9166, AUC_score: 0.9608\n",
      "Epoch 750: Train Loss: 0.0050, Macro_F1: 0.9166, AUC_score: 0.9608\n",
      "Epoch 00753: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Validation loss decreased (0.003000 --> 0.003000).\n",
      "Epoch 800: Train Loss: 0.0049, Macro_F1: 0.9166, AUC_score: 0.9608\n",
      "Epoch 850: Train Loss: 0.0077, Macro_F1: 0.9166, AUC_score: 0.9608\n",
      "Epoch 00860: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 900: Train Loss: 0.0065, Macro_F1: 0.9166, AUC_score: 0.9608\n",
      "Epoch 950: Train Loss: 0.0047, Macro_F1: 0.9166, AUC_score: 0.9608\n",
      "Epoch 00961: reducing learning rate of group 0 to 1.2800e-08.\n",
      "Epoch 1000: Train Loss: 0.0057, Macro_F1: 0.9166, AUC_score: 0.9608\n",
      "Epoch 1050: Train Loss: 0.0059, Macro_F1: 0.9166, AUC_score: 0.9608\n",
      "Early stopping triggered\n",
      "acc save\n",
      "57.00000000000001% node features transform to 0: F1: 0.9166, AUC_score: 0.9608\n",
      "Epoch 0: Train Loss: 0.0089, Macro_F1: 0.8706, AUC_score: 0.9464\n",
      "Validation loss decreased (0.008885 --> 0.008885).\n",
      "Validation loss decreased (0.008035 --> 0.008035).\n",
      "Validation loss decreased (0.006637 --> 0.006637).\n",
      "Validation loss decreased (0.005598 --> 0.005598).\n",
      "Epoch 50: Train Loss: 0.0156, Macro_F1: 0.9202, AUC_score: 0.9589\n",
      "Validation loss decreased (0.005388 --> 0.005388).\n",
      "Validation loss decreased (0.004213 --> 0.004213).\n",
      "Epoch 100: Train Loss: 0.0122, Macro_F1: 0.9203, AUC_score: 0.9638\n",
      "Validation loss decreased (0.003902 --> 0.003902).\n",
      "Epoch 150: Train Loss: 0.0079, Macro_F1: 0.9200, AUC_score: 0.9585\n",
      "Epoch 200: Train Loss: 0.0156, Macro_F1: 0.9201, AUC_score: 0.9641\n",
      "Epoch 00220: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0047, Macro_F1: 0.9238, AUC_score: 0.9620\n",
      "Validation loss decreased (0.003551 --> 0.003551).\n",
      "Epoch 300: Train Loss: 0.0037, Macro_F1: 0.9202, AUC_score: 0.9606\n",
      "Epoch 350: Train Loss: 0.0037, Macro_F1: 0.9202, AUC_score: 0.9608\n",
      "Epoch 00365: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0096, Macro_F1: 0.9238, AUC_score: 0.9608\n",
      "Epoch 450: Train Loss: 0.0056, Macro_F1: 0.9202, AUC_score: 0.9606\n",
      "Epoch 00466: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 500: Train Loss: 0.0040, Macro_F1: 0.9202, AUC_score: 0.9608\n",
      "Epoch 550: Train Loss: 0.0045, Macro_F1: 0.9202, AUC_score: 0.9609\n",
      "Early stopping triggered\n",
      "acc save\n",
      "56.00000000000001% node features transform to 0: F1: 0.9202, AUC_score: 0.9607\n",
      "Epoch 0: Train Loss: 0.0107, Macro_F1: 0.8985, AUC_score: 0.9681\n",
      "Validation loss decreased (0.010694 --> 0.010694).\n",
      "Validation loss decreased (0.008656 --> 0.008656).\n",
      "Validation loss decreased (0.007090 --> 0.007090).\n",
      "Validation loss decreased (0.005963 --> 0.005963).\n",
      "Validation loss decreased (0.005311 --> 0.005311).\n",
      "Validation loss decreased (0.005182 --> 0.005182).\n",
      "Validation loss decreased (0.004710 --> 0.004710).\n",
      "Epoch 50: Train Loss: 0.0089, Macro_F1: 0.9202, AUC_score: 0.9599\n",
      "Validation loss decreased (0.004596 --> 0.004596).\n",
      "Validation loss decreased (0.003913 --> 0.003913).\n",
      "Epoch 100: Train Loss: 0.0059, Macro_F1: 0.9273, AUC_score: 0.9603\n",
      "Epoch 150: Train Loss: 0.0121, Macro_F1: 0.9202, AUC_score: 0.9610\n",
      "Epoch 00169: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0049, Macro_F1: 0.9130, AUC_score: 0.9613\n",
      "Validation loss decreased (0.003799 --> 0.003799).\n",
      "Epoch 250: Train Loss: 0.0059, Macro_F1: 0.9166, AUC_score: 0.9606\n",
      "Validation loss decreased (0.003424 --> 0.003424).\n",
      "Epoch 300: Train Loss: 0.0070, Macro_F1: 0.9166, AUC_score: 0.9609\n",
      "Epoch 350: Train Loss: 0.0066, Macro_F1: 0.9130, AUC_score: 0.9608\n",
      "Epoch 00396: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0097, Macro_F1: 0.9274, AUC_score: 0.9599\n",
      "Epoch 450: Train Loss: 0.0049, Macro_F1: 0.9166, AUC_score: 0.9603\n",
      "Validation loss decreased (0.003385 --> 0.003385).\n",
      "Epoch 500: Train Loss: 0.0098, Macro_F1: 0.9238, AUC_score: 0.9596\n",
      "Validation loss decreased (0.003205 --> 0.003205).\n",
      "Epoch 550: Train Loss: 0.0098, Macro_F1: 0.9238, AUC_score: 0.9593\n",
      "Epoch 600: Train Loss: 0.0062, Macro_F1: 0.9166, AUC_score: 0.9601\n",
      "Epoch 00645: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 650: Train Loss: 0.0045, Macro_F1: 0.9202, AUC_score: 0.9596\n",
      "Validation loss decreased (0.003169 --> 0.003169).\n",
      "Epoch 700: Train Loss: 0.0056, Macro_F1: 0.9202, AUC_score: 0.9600\n",
      "Epoch 750: Train Loss: 0.0051, Macro_F1: 0.9202, AUC_score: 0.9597\n",
      "Epoch 00793: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 800: Train Loss: 0.0062, Macro_F1: 0.9202, AUC_score: 0.9601\n",
      "Epoch 850: Train Loss: 0.0043, Macro_F1: 0.9166, AUC_score: 0.9601\n",
      "Validation loss decreased (0.003132 --> 0.003132).\n",
      "Epoch 900: Train Loss: 0.0047, Macro_F1: 0.9202, AUC_score: 0.9601\n",
      "Epoch 950: Train Loss: 0.0051, Macro_F1: 0.9202, AUC_score: 0.9600\n",
      "Epoch 00991: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 1000: Train Loss: 0.0046, Macro_F1: 0.9202, AUC_score: 0.9600\n",
      "Epoch 1050: Train Loss: 0.0055, Macro_F1: 0.9202, AUC_score: 0.9600\n",
      "Epoch 01092: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 1100: Train Loss: 0.0083, Macro_F1: 0.9202, AUC_score: 0.9599\n",
      "Epoch 1150: Train Loss: 0.0124, Macro_F1: 0.9202, AUC_score: 0.9599\n",
      "Early stopping triggered\n",
      "acc save\n",
      "55.00000000000001% node features transform to 0: F1: 0.9202, AUC_score: 0.9599\n",
      "Epoch 0: Train Loss: 0.0073, Macro_F1: 0.8875, AUC_score: 0.9679\n",
      "Validation loss decreased (0.007299 --> 0.007299).\n",
      "Validation loss decreased (0.006503 --> 0.006503).\n",
      "Validation loss decreased (0.005141 --> 0.005141).\n",
      "Validation loss decreased (0.004840 --> 0.004840).\n",
      "Validation loss decreased (0.004792 --> 0.004792).\n",
      "Validation loss decreased (0.004285 --> 0.004285).\n",
      "Epoch 50: Train Loss: 0.0057, Macro_F1: 0.9236, AUC_score: 0.9623\n",
      "Validation loss decreased (0.003403 --> 0.003403).\n",
      "Epoch 100: Train Loss: 0.0087, Macro_F1: 0.9238, AUC_score: 0.9658\n",
      "Epoch 150: Train Loss: 0.0095, Macro_F1: 0.9310, AUC_score: 0.9627\n",
      "Epoch 00162: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0046, Macro_F1: 0.9310, AUC_score: 0.9629\n",
      "Epoch 250: Train Loss: 0.0048, Macro_F1: 0.9310, AUC_score: 0.9631\n",
      "Validation loss decreased (0.003226 --> 0.003226).\n",
      "Validation loss decreased (0.002975 --> 0.002975).\n",
      "Epoch 300: Train Loss: 0.0043, Macro_F1: 0.9274, AUC_score: 0.9641\n",
      "Epoch 350: Train Loss: 0.0068, Macro_F1: 0.9274, AUC_score: 0.9634\n",
      "Epoch 00371: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0087, Macro_F1: 0.9274, AUC_score: 0.9633\n",
      "Epoch 450: Train Loss: 0.0071, Macro_F1: 0.9310, AUC_score: 0.9635\n",
      "Epoch 00472: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 500: Train Loss: 0.0048, Macro_F1: 0.9274, AUC_score: 0.9639\n",
      "Epoch 550: Train Loss: 0.0050, Macro_F1: 0.9310, AUC_score: 0.9636\n",
      "Validation loss decreased (0.002904 --> 0.002904).\n",
      "Epoch 600: Train Loss: 0.0048, Macro_F1: 0.9310, AUC_score: 0.9637\n",
      "Epoch 650: Train Loss: 0.0047, Macro_F1: 0.9310, AUC_score: 0.9635\n",
      "Epoch 00661: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 700: Train Loss: 0.0047, Macro_F1: 0.9310, AUC_score: 0.9636\n",
      "Epoch 750: Train Loss: 0.0058, Macro_F1: 0.9310, AUC_score: 0.9636\n",
      "Epoch 00762: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 800: Train Loss: 0.0110, Macro_F1: 0.9310, AUC_score: 0.9636\n",
      "Epoch 850: Train Loss: 0.0040, Macro_F1: 0.9310, AUC_score: 0.9636\n",
      "Early stopping triggered\n",
      "acc save\n",
      "54.0% node features transform to 0: F1: 0.9310, AUC_score: 0.9636\n",
      "Epoch 0: Train Loss: 0.0081, Macro_F1: 0.9011, AUC_score: 0.9544\n",
      "Validation loss decreased (0.008057 --> 0.008057).\n",
      "Validation loss decreased (0.006214 --> 0.006214).\n",
      "Validation loss decreased (0.005989 --> 0.005989).\n",
      "Validation loss decreased (0.005626 --> 0.005626).\n",
      "Validation loss decreased (0.005401 --> 0.005401).\n",
      "Validation loss decreased (0.004833 --> 0.004833).\n",
      "Epoch 50: Train Loss: 0.0091, Macro_F1: 0.9238, AUC_score: 0.9654\n",
      "Validation loss decreased (0.004564 --> 0.004564).\n",
      "Validation loss decreased (0.003974 --> 0.003974).\n",
      "Epoch 100: Train Loss: 0.0088, Macro_F1: 0.9274, AUC_score: 0.9645\n",
      "Epoch 150: Train Loss: 0.0097, Macro_F1: 0.9275, AUC_score: 0.9696\n",
      "Epoch 00178: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0043, Macro_F1: 0.9310, AUC_score: 0.9645\n",
      "Validation loss decreased (0.003472 --> 0.003472).\n",
      "Epoch 250: Train Loss: 0.0057, Macro_F1: 0.9310, AUC_score: 0.9629\n",
      "Validation loss decreased (0.003430 --> 0.003430).\n",
      "Epoch 300: Train Loss: 0.0136, Macro_F1: 0.9237, AUC_score: 0.9623\n",
      "Validation loss decreased (0.003029 --> 0.003029).\n",
      "Epoch 350: Train Loss: 0.0038, Macro_F1: 0.9202, AUC_score: 0.9658\n",
      "Epoch 400: Train Loss: 0.0094, Macro_F1: 0.9274, AUC_score: 0.9648\n",
      "Epoch 00433: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.0066, Macro_F1: 0.9310, AUC_score: 0.9654\n",
      "Epoch 500: Train Loss: 0.0061, Macro_F1: 0.9274, AUC_score: 0.9654\n",
      "Epoch 00534: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0047, Macro_F1: 0.9310, AUC_score: 0.9650\n",
      "Epoch 600: Train Loss: 0.0058, Macro_F1: 0.9274, AUC_score: 0.9651\n",
      "Early stopping triggered\n",
      "acc save\n",
      "53.0% node features transform to 0: F1: 0.9274, AUC_score: 0.9652\n",
      "Epoch 0: Train Loss: 0.0067, Macro_F1: 0.8911, AUC_score: 0.9699\n",
      "Validation loss decreased (0.006733 --> 0.006733).\n",
      "Validation loss decreased (0.005441 --> 0.005441).\n",
      "Validation loss decreased (0.005147 --> 0.005147).\n",
      "Validation loss decreased (0.004937 --> 0.004937).\n",
      "Validation loss decreased (0.004160 --> 0.004160).\n",
      "Epoch 50: Train Loss: 0.0056, Macro_F1: 0.9274, AUC_score: 0.9656\n",
      "Validation loss decreased (0.004036 --> 0.004036).\n",
      "Epoch 100: Train Loss: 0.0045, Macro_F1: 0.9346, AUC_score: 0.9649\n",
      "Validation loss decreased (0.003682 --> 0.003682).\n",
      "Epoch 150: Train Loss: 0.0126, Macro_F1: 0.9310, AUC_score: 0.9656\n",
      "Epoch 200: Train Loss: 0.0068, Macro_F1: 0.9201, AUC_score: 0.9652\n",
      "Epoch 00232: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0078, Macro_F1: 0.9275, AUC_score: 0.9670\n",
      "Epoch 300: Train Loss: 0.0061, Macro_F1: 0.9203, AUC_score: 0.9674\n",
      "Epoch 00333: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.003434 --> 0.003434).\n",
      "Epoch 350: Train Loss: 0.0048, Macro_F1: 0.9274, AUC_score: 0.9661\n",
      "Validation loss decreased (0.003344 --> 0.003344).\n",
      "Epoch 400: Train Loss: 0.0049, Macro_F1: 0.9274, AUC_score: 0.9665\n",
      "Epoch 450: Train Loss: 0.0048, Macro_F1: 0.9274, AUC_score: 0.9663\n",
      "Epoch 00461: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Validation loss decreased (0.003302 --> 0.003302).\n",
      "Validation loss decreased (0.003151 --> 0.003151).\n",
      "Epoch 500: Train Loss: 0.0039, Macro_F1: 0.9274, AUC_score: 0.9661\n",
      "Epoch 550: Train Loss: 0.0074, Macro_F1: 0.9274, AUC_score: 0.9665\n",
      "Epoch 00589: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 600: Train Loss: 0.0055, Macro_F1: 0.9274, AUC_score: 0.9663\n",
      "Epoch 650: Train Loss: 0.0046, Macro_F1: 0.9274, AUC_score: 0.9665\n",
      "Validation loss decreased (0.003031 --> 0.003031).\n",
      "Epoch 700: Train Loss: 0.0040, Macro_F1: 0.9274, AUC_score: 0.9664\n",
      "Epoch 750: Train Loss: 0.0124, Macro_F1: 0.9274, AUC_score: 0.9666\n",
      "Epoch 00781: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 800: Train Loss: 0.0057, Macro_F1: 0.9274, AUC_score: 0.9666\n",
      "Epoch 850: Train Loss: 0.0037, Macro_F1: 0.9274, AUC_score: 0.9665\n",
      "Epoch 00882: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 900: Train Loss: 0.0085, Macro_F1: 0.9274, AUC_score: 0.9665\n",
      "Epoch 950: Train Loss: 0.0066, Macro_F1: 0.9274, AUC_score: 0.9665\n",
      "Early stopping triggered\n",
      "acc save\n",
      "52.0% node features transform to 0: F1: 0.9274, AUC_score: 0.9665\n",
      "Epoch 0: Train Loss: 0.0236, Macro_F1: 0.8948, AUC_score: 0.9699\n",
      "Validation loss decreased (0.023616 --> 0.023616).\n",
      "Validation loss decreased (0.010375 --> 0.010375).\n",
      "Validation loss decreased (0.009610 --> 0.009610).\n",
      "Validation loss decreased (0.009423 --> 0.009423).\n",
      "Validation loss decreased (0.008691 --> 0.008691).\n",
      "Validation loss decreased (0.007468 --> 0.007468).\n",
      "Validation loss decreased (0.006634 --> 0.006634).\n",
      "Validation loss decreased (0.006105 --> 0.006105).\n",
      "Validation loss decreased (0.004929 --> 0.004929).\n",
      "Epoch 50: Train Loss: 0.0077, Macro_F1: 0.9310, AUC_score: 0.9650\n",
      "Validation loss decreased (0.004471 --> 0.004471).\n",
      "Epoch 100: Train Loss: 0.0074, Macro_F1: 0.9311, AUC_score: 0.9674\n",
      "Validation loss decreased (0.004178 --> 0.004178).\n",
      "Epoch 150: Train Loss: 0.0056, Macro_F1: 0.9310, AUC_score: 0.9666\n",
      "Validation loss decreased (0.004035 --> 0.004035).\n",
      "Validation loss decreased (0.003807 --> 0.003807).\n",
      "Epoch 200: Train Loss: 0.0093, Macro_F1: 0.9308, AUC_score: 0.9619\n",
      "Validation loss decreased (0.003440 --> 0.003440).\n",
      "Epoch 250: Train Loss: 0.0062, Macro_F1: 0.9166, AUC_score: 0.9664\n",
      "Epoch 300: Train Loss: 0.0064, Macro_F1: 0.9309, AUC_score: 0.9639\n",
      "Epoch 00317: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 350: Train Loss: 0.0054, Macro_F1: 0.9237, AUC_score: 0.9634\n",
      "Epoch 400: Train Loss: 0.0156, Macro_F1: 0.9202, AUC_score: 0.9653\n",
      "Epoch 00418: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.0047, Macro_F1: 0.9310, AUC_score: 0.9643\n",
      "Validation loss decreased (0.003176 --> 0.003176).\n",
      "Epoch 500: Train Loss: 0.0080, Macro_F1: 0.9310, AUC_score: 0.9643\n",
      "Epoch 550: Train Loss: 0.0049, Macro_F1: 0.9274, AUC_score: 0.9645\n",
      "Epoch 00583: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 600: Train Loss: 0.0060, Macro_F1: 0.9310, AUC_score: 0.9643\n",
      "Validation loss decreased (0.003045 --> 0.003045).\n",
      "Epoch 650: Train Loss: 0.0048, Macro_F1: 0.9310, AUC_score: 0.9644\n",
      "Epoch 700: Train Loss: 0.0041, Macro_F1: 0.9274, AUC_score: 0.9641\n",
      "Epoch 00723: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 750: Train Loss: 0.0048, Macro_F1: 0.9310, AUC_score: 0.9643\n",
      "Epoch 800: Train Loss: 0.0059, Macro_F1: 0.9310, AUC_score: 0.9644\n",
      "Epoch 00824: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 850: Train Loss: 0.0100, Macro_F1: 0.9310, AUC_score: 0.9643\n",
      "Epoch 900: Train Loss: 0.0085, Macro_F1: 0.9310, AUC_score: 0.9643\n",
      "Early stopping triggered\n",
      "acc save\n",
      "51.0% node features transform to 0: F1: 0.9310, AUC_score: 0.9643\n",
      "Epoch 0: Train Loss: 0.0115, Macro_F1: 0.9162, AUC_score: 0.9604\n",
      "Validation loss decreased (0.011455 --> 0.011455).\n",
      "Validation loss decreased (0.010685 --> 0.010685).\n",
      "Validation loss decreased (0.006040 --> 0.006040).\n",
      "Validation loss decreased (0.005852 --> 0.005852).\n",
      "Validation loss decreased (0.004801 --> 0.004801).\n",
      "Validation loss decreased (0.003484 --> 0.003484).\n",
      "Validation loss decreased (0.002831 --> 0.002831).\n",
      "Epoch 50: Train Loss: 0.0090, Macro_F1: 0.9273, AUC_score: 0.9648\n",
      "Epoch 100: Train Loss: 0.0053, Macro_F1: 0.9235, AUC_score: 0.9605\n",
      "Epoch 00139: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.0041, Macro_F1: 0.9238, AUC_score: 0.9677\n",
      "Epoch 200: Train Loss: 0.0054, Macro_F1: 0.9166, AUC_score: 0.9681\n",
      "Epoch 00240: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 250: Train Loss: 0.0039, Macro_F1: 0.9238, AUC_score: 0.9662\n",
      "Epoch 300: Train Loss: 0.0067, Macro_F1: 0.9273, AUC_score: 0.9656\n",
      "Early stopping triggered\n",
      "acc save\n",
      "50.0% node features transform to 0: F1: 0.9237, AUC_score: 0.9659\n",
      "Epoch 0: Train Loss: 0.0056, Macro_F1: 0.8802, AUC_score: 0.9700\n",
      "Validation loss decreased (0.005630 --> 0.005630).\n",
      "Validation loss decreased (0.005022 --> 0.005022).\n",
      "Validation loss decreased (0.004807 --> 0.004807).\n",
      "Epoch 50: Train Loss: 0.0148, Macro_F1: 0.9201, AUC_score: 0.9639\n",
      "Validation loss decreased (0.004081 --> 0.004081).\n",
      "Validation loss decreased (0.004021 --> 0.004021).\n",
      "Validation loss decreased (0.002983 --> 0.002983).\n",
      "Epoch 100: Train Loss: 0.0063, Macro_F1: 0.9237, AUC_score: 0.9650\n",
      "Validation loss decreased (0.002806 --> 0.002806).\n",
      "Epoch 150: Train Loss: 0.0053, Macro_F1: 0.9199, AUC_score: 0.9612\n",
      "Epoch 200: Train Loss: 0.0060, Macro_F1: 0.9202, AUC_score: 0.9666\n",
      "Epoch 00221: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0053, Macro_F1: 0.9200, AUC_score: 0.9640\n",
      "Epoch 300: Train Loss: 0.0060, Macro_F1: 0.9202, AUC_score: 0.9658\n",
      "Validation loss decreased (0.002537 --> 0.002537).\n",
      "Epoch 350: Train Loss: 0.0055, Macro_F1: 0.9238, AUC_score: 0.9663\n",
      "Epoch 400: Train Loss: 0.0039, Macro_F1: 0.9202, AUC_score: 0.9660\n",
      "Epoch 00405: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.0049, Macro_F1: 0.9238, AUC_score: 0.9655\n",
      "Epoch 500: Train Loss: 0.0034, Macro_F1: 0.9202, AUC_score: 0.9662\n",
      "Epoch 00506: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0060, Macro_F1: 0.9238, AUC_score: 0.9659\n",
      "Epoch 600: Train Loss: 0.0056, Macro_F1: 0.9238, AUC_score: 0.9657\n",
      "Early stopping triggered\n",
      "acc save\n",
      "49.0% node features transform to 0: F1: 0.9238, AUC_score: 0.9657\n",
      "Epoch 0: Train Loss: 0.0049, Macro_F1: 0.8874, AUC_score: 0.9715\n",
      "Validation loss decreased (0.004857 --> 0.004857).\n",
      "Validation loss decreased (0.003913 --> 0.003913).\n",
      "Validation loss decreased (0.003758 --> 0.003758).\n",
      "Epoch 50: Train Loss: 0.0040, Macro_F1: 0.9201, AUC_score: 0.9666\n",
      "Validation loss decreased (0.003221 --> 0.003221).\n",
      "Epoch 100: Train Loss: 0.0061, Macro_F1: 0.9203, AUC_score: 0.9694\n",
      "Epoch 150: Train Loss: 0.0148, Macro_F1: 0.9272, AUC_score: 0.9610\n",
      "Epoch 00162: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.003093 --> 0.003093).\n",
      "Epoch 200: Train Loss: 0.0051, Macro_F1: 0.9165, AUC_score: 0.9641\n",
      "Epoch 250: Train Loss: 0.0088, Macro_F1: 0.9166, AUC_score: 0.9649\n",
      "Epoch 00281: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.003082 --> 0.003082).\n",
      "Epoch 300: Train Loss: 0.0044, Macro_F1: 0.9165, AUC_score: 0.9647\n",
      "Epoch 350: Train Loss: 0.0038, Macro_F1: 0.9238, AUC_score: 0.9648\n",
      "Validation loss decreased (0.003067 --> 0.003067).\n",
      "Validation loss decreased (0.002833 --> 0.002833).\n",
      "Epoch 400: Train Loss: 0.0034, Macro_F1: 0.9238, AUC_score: 0.9649\n",
      "Epoch 450: Train Loss: 0.0044, Macro_F1: 0.9202, AUC_score: 0.9648\n",
      "Epoch 00497: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 500: Train Loss: 0.0037, Macro_F1: 0.9202, AUC_score: 0.9648\n",
      "Epoch 550: Train Loss: 0.0045, Macro_F1: 0.9202, AUC_score: 0.9648\n",
      "Validation loss decreased (0.002316 --> 0.002316).\n",
      "Epoch 600: Train Loss: 0.0041, Macro_F1: 0.9202, AUC_score: 0.9645\n",
      "Epoch 650: Train Loss: 0.0082, Macro_F1: 0.9238, AUC_score: 0.9647\n",
      "Epoch 00697: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 700: Train Loss: 0.0063, Macro_F1: 0.9238, AUC_score: 0.9648\n",
      "Epoch 750: Train Loss: 0.0042, Macro_F1: 0.9238, AUC_score: 0.9647\n",
      "Epoch 00798: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 800: Train Loss: 0.0033, Macro_F1: 0.9238, AUC_score: 0.9648\n",
      "Epoch 850: Train Loss: 0.0080, Macro_F1: 0.9238, AUC_score: 0.9647\n",
      "Early stopping triggered\n",
      "acc save\n",
      "48.0% node features transform to 0: F1: 0.9238, AUC_score: 0.9647\n",
      "Epoch 0: Train Loss: 0.0027, Macro_F1: 0.9085, AUC_score: 0.9529\n",
      "Validation loss decreased (0.002667 --> 0.002667).\n",
      "Epoch 50: Train Loss: 0.0031, Macro_F1: 0.9202, AUC_score: 0.9657\n",
      "Epoch 100: Train Loss: 0.0070, Macro_F1: 0.9203, AUC_score: 0.9670\n",
      "Epoch 00102: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.0038, Macro_F1: 0.9238, AUC_score: 0.9669\n",
      "Epoch 200: Train Loss: 0.0094, Macro_F1: 0.9239, AUC_score: 0.9670\n",
      "Epoch 00203: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.002574 --> 0.002574).\n",
      "Epoch 250: Train Loss: 0.0050, Macro_F1: 0.9202, AUC_score: 0.9655\n",
      "Epoch 300: Train Loss: 0.0054, Macro_F1: 0.9202, AUC_score: 0.9666\n",
      "Epoch 00328: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 350: Train Loss: 0.0051, Macro_F1: 0.9202, AUC_score: 0.9660\n",
      "Epoch 400: Train Loss: 0.0095, Macro_F1: 0.9202, AUC_score: 0.9660\n",
      "Epoch 00429: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 450: Train Loss: 0.0040, Macro_F1: 0.9202, AUC_score: 0.9662\n",
      "Validation loss decreased (0.002240 --> 0.002240).\n",
      "Epoch 500: Train Loss: 0.0028, Macro_F1: 0.9202, AUC_score: 0.9662\n",
      "Epoch 550: Train Loss: 0.0044, Macro_F1: 0.9202, AUC_score: 0.9662\n",
      "Epoch 00558: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Validation loss decreased (0.001965 --> 0.001965).\n",
      "Epoch 600: Train Loss: 0.0027, Macro_F1: 0.9202, AUC_score: 0.9662\n",
      "Epoch 650: Train Loss: 0.0109, Macro_F1: 0.9202, AUC_score: 0.9662\n",
      "Epoch 00693: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 700: Train Loss: 0.0040, Macro_F1: 0.9202, AUC_score: 0.9662\n",
      "Epoch 750: Train Loss: 0.0039, Macro_F1: 0.9202, AUC_score: 0.9662\n",
      "Epoch 00794: reducing learning rate of group 0 to 1.2800e-08.\n",
      "Epoch 800: Train Loss: 0.0056, Macro_F1: 0.9202, AUC_score: 0.9662\n",
      "Epoch 850: Train Loss: 0.0071, Macro_F1: 0.9202, AUC_score: 0.9662\n",
      "Early stopping triggered\n",
      "acc save\n",
      "47.0% node features transform to 0: F1: 0.9202, AUC_score: 0.9662\n",
      "Epoch 0: Train Loss: 0.0065, Macro_F1: 0.8787, AUC_score: 0.9517\n",
      "Validation loss decreased (0.006472 --> 0.006472).\n",
      "Validation loss decreased (0.004558 --> 0.004558).\n",
      "Validation loss decreased (0.004271 --> 0.004271).\n",
      "Epoch 50: Train Loss: 0.0081, Macro_F1: 0.9130, AUC_score: 0.9660\n",
      "Validation loss decreased (0.004152 --> 0.004152).\n",
      "Validation loss decreased (0.003443 --> 0.003443).\n",
      "Validation loss decreased (0.003267 --> 0.003267).\n",
      "Validation loss decreased (0.003142 --> 0.003142).\n",
      "Epoch 100: Train Loss: 0.0072, Macro_F1: 0.9239, AUC_score: 0.9662\n",
      "Epoch 150: Train Loss: 0.0117, Macro_F1: 0.9238, AUC_score: 0.9651\n",
      "Epoch 00190: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0036, Macro_F1: 0.9202, AUC_score: 0.9658\n",
      "Validation loss decreased (0.003101 --> 0.003101).\n",
      "Validation loss decreased (0.002792 --> 0.002792).\n",
      "Epoch 250: Train Loss: 0.0041, Macro_F1: 0.9202, AUC_score: 0.9658\n",
      "Epoch 300: Train Loss: 0.0035, Macro_F1: 0.9202, AUC_score: 0.9652\n",
      "Validation loss decreased (0.002554 --> 0.002554).\n",
      "Epoch 350: Train Loss: 0.0057, Macro_F1: 0.9165, AUC_score: 0.9667\n",
      "Epoch 400: Train Loss: 0.0065, Macro_F1: 0.9130, AUC_score: 0.9694\n",
      "Epoch 00408: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.0047, Macro_F1: 0.9202, AUC_score: 0.9667\n",
      "Epoch 500: Train Loss: 0.0042, Macro_F1: 0.9202, AUC_score: 0.9661\n",
      "Epoch 00509: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0055, Macro_F1: 0.9202, AUC_score: 0.9660\n",
      "Validation loss decreased (0.002470 --> 0.002470).\n",
      "Validation loss decreased (0.002448 --> 0.002448).\n",
      "Epoch 600: Train Loss: 0.0045, Macro_F1: 0.9202, AUC_score: 0.9657\n",
      "Epoch 650: Train Loss: 0.0060, Macro_F1: 0.9202, AUC_score: 0.9657\n",
      "Epoch 00671: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 700: Train Loss: 0.0036, Macro_F1: 0.9202, AUC_score: 0.9658\n",
      "Epoch 750: Train Loss: 0.0035, Macro_F1: 0.9202, AUC_score: 0.9657\n",
      "Epoch 00772: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Validation loss decreased (0.002425 --> 0.002425).\n",
      "Epoch 800: Train Loss: 0.0038, Macro_F1: 0.9202, AUC_score: 0.9657\n",
      "Epoch 850: Train Loss: 0.0044, Macro_F1: 0.9202, AUC_score: 0.9657\n",
      "Epoch 00894: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 900: Train Loss: 0.0064, Macro_F1: 0.9202, AUC_score: 0.9657\n",
      "Epoch 950: Train Loss: 0.0066, Macro_F1: 0.9202, AUC_score: 0.9657\n",
      "Validation loss decreased (0.002258 --> 0.002258).\n",
      "Epoch 1000: Train Loss: 0.0096, Macro_F1: 0.9202, AUC_score: 0.9657\n",
      "Epoch 1050: Train Loss: 0.0033, Macro_F1: 0.9202, AUC_score: 0.9657\n",
      "Epoch 01078: reducing learning rate of group 0 to 1.2800e-08.\n",
      "Epoch 1100: Train Loss: 0.0034, Macro_F1: 0.9202, AUC_score: 0.9657\n",
      "Epoch 1150: Train Loss: 0.0055, Macro_F1: 0.9202, AUC_score: 0.9657\n",
      "Epoch 01179: reducing learning rate of group 0 to 2.5600e-09.\n",
      "Epoch 1200: Train Loss: 0.0045, Macro_F1: 0.9202, AUC_score: 0.9657\n",
      "Epoch 1250: Train Loss: 0.0038, Macro_F1: 0.9202, AUC_score: 0.9657\n",
      "Early stopping triggered\n",
      "acc save\n",
      "46.0% node features transform to 0: F1: 0.9202, AUC_score: 0.9657\n",
      "Epoch 0: Train Loss: 0.0091, Macro_F1: 0.8727, AUC_score: 0.9709\n",
      "Validation loss decreased (0.009102 --> 0.009102).\n",
      "Validation loss decreased (0.007990 --> 0.007990).\n",
      "Validation loss decreased (0.005605 --> 0.005605).\n",
      "Validation loss decreased (0.004880 --> 0.004880).\n",
      "Validation loss decreased (0.004867 --> 0.004867).\n",
      "Epoch 50: Train Loss: 0.0066, Macro_F1: 0.9202, AUC_score: 0.9698\n",
      "Validation loss decreased (0.003808 --> 0.003808).\n",
      "Validation loss decreased (0.003615 --> 0.003615).\n",
      "Validation loss decreased (0.003154 --> 0.003154).\n",
      "Validation loss decreased (0.002852 --> 0.002852).\n",
      "Epoch 100: Train Loss: 0.0040, Macro_F1: 0.9202, AUC_score: 0.9670\n",
      "Validation loss decreased (0.002732 --> 0.002732).\n",
      "Validation loss decreased (0.002717 --> 0.002717).\n",
      "Epoch 150: Train Loss: 0.0060, Macro_F1: 0.9166, AUC_score: 0.9678\n",
      "Epoch 200: Train Loss: 0.0043, Macro_F1: 0.9310, AUC_score: 0.9660\n",
      "Epoch 00213: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.002686 --> 0.002686).\n",
      "Epoch 250: Train Loss: 0.0052, Macro_F1: 0.9273, AUC_score: 0.9647\n",
      "Epoch 300: Train Loss: 0.0048, Macro_F1: 0.9165, AUC_score: 0.9663\n",
      "Epoch 00351: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.0029, Macro_F1: 0.9129, AUC_score: 0.9669\n",
      "Validation loss decreased (0.002435 --> 0.002435).\n",
      "Epoch 400: Train Loss: 0.0049, Macro_F1: 0.9129, AUC_score: 0.9665\n",
      "Epoch 450: Train Loss: 0.0307, Macro_F1: 0.9129, AUC_score: 0.9665\n",
      "Epoch 00475: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 500: Train Loss: 0.0046, Macro_F1: 0.9130, AUC_score: 0.9670\n",
      "Epoch 550: Train Loss: 0.0030, Macro_F1: 0.9165, AUC_score: 0.9661\n",
      "Epoch 00576: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Validation loss decreased (0.002342 --> 0.002342).\n",
      "Epoch 600: Train Loss: 0.0051, Macro_F1: 0.9201, AUC_score: 0.9659\n",
      "Epoch 650: Train Loss: 0.0037, Macro_F1: 0.9201, AUC_score: 0.9660\n",
      "Epoch 00684: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 700: Train Loss: 0.0042, Macro_F1: 0.9201, AUC_score: 0.9660\n",
      "Epoch 750: Train Loss: 0.0058, Macro_F1: 0.9201, AUC_score: 0.9660\n",
      "Validation loss decreased (0.002115 --> 0.002115).\n",
      "Epoch 800: Train Loss: 0.0029, Macro_F1: 0.9201, AUC_score: 0.9660\n",
      "Epoch 850: Train Loss: 0.0040, Macro_F1: 0.9201, AUC_score: 0.9660\n",
      "Epoch 00883: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 900: Train Loss: 0.0024, Macro_F1: 0.9201, AUC_score: 0.9660\n",
      "Epoch 950: Train Loss: 0.0096, Macro_F1: 0.9201, AUC_score: 0.9660\n",
      "Epoch 00984: reducing learning rate of group 0 to 1.2800e-08.\n",
      "Epoch 1000: Train Loss: 0.0063, Macro_F1: 0.9201, AUC_score: 0.9660\n",
      "Epoch 1050: Train Loss: 0.0037, Macro_F1: 0.9201, AUC_score: 0.9660\n",
      "Early stopping triggered\n",
      "acc save\n",
      "44.99999999999999% node features transform to 0: F1: 0.9201, AUC_score: 0.9660\n",
      "Epoch 0: Train Loss: 0.0041, Macro_F1: 0.9050, AUC_score: 0.9533\n",
      "Validation loss decreased (0.004074 --> 0.004074).\n",
      "Epoch 50: Train Loss: 0.0055, Macro_F1: 0.9129, AUC_score: 0.9663\n",
      "Validation loss decreased (0.003857 --> 0.003857).\n",
      "Validation loss decreased (0.003499 --> 0.003499).\n",
      "Validation loss decreased (0.003280 --> 0.003280).\n",
      "Epoch 100: Train Loss: 0.0035, Macro_F1: 0.9237, AUC_score: 0.9628\n",
      "Validation loss decreased (0.002946 --> 0.002946).\n",
      "Validation loss decreased (0.002746 --> 0.002746).\n",
      "Epoch 150: Train Loss: 0.0056, Macro_F1: 0.9236, AUC_score: 0.9581\n",
      "Validation loss decreased (0.002610 --> 0.002610).\n",
      "Epoch 200: Train Loss: 0.0109, Macro_F1: 0.9202, AUC_score: 0.9666\n",
      "Epoch 250: Train Loss: 0.0078, Macro_F1: 0.9202, AUC_score: 0.9660\n",
      "Epoch 00260: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0062, Macro_F1: 0.9165, AUC_score: 0.9647\n",
      "Epoch 350: Train Loss: 0.0121, Macro_F1: 0.9165, AUC_score: 0.9650\n",
      "Epoch 00361: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0055, Macro_F1: 0.9165, AUC_score: 0.9649\n",
      "Epoch 450: Train Loss: 0.0070, Macro_F1: 0.9165, AUC_score: 0.9649\n",
      "Early stopping triggered\n",
      "acc save\n",
      "43.99999999999999% node features transform to 0: F1: 0.9201, AUC_score: 0.9644\n",
      "Epoch 0: Train Loss: 0.0043, Macro_F1: 0.8709, AUC_score: 0.9497\n",
      "Validation loss decreased (0.004291 --> 0.004291).\n",
      "Validation loss decreased (0.004161 --> 0.004161).\n",
      "Validation loss decreased (0.003766 --> 0.003766).\n",
      "Epoch 50: Train Loss: 0.0148, Macro_F1: 0.9130, AUC_score: 0.9679\n",
      "Validation loss decreased (0.003444 --> 0.003444).\n",
      "Validation loss decreased (0.003286 --> 0.003286).\n",
      "Validation loss decreased (0.003092 --> 0.003092).\n",
      "Epoch 100: Train Loss: 0.0046, Macro_F1: 0.9093, AUC_score: 0.9676\n",
      "Epoch 150: Train Loss: 0.0057, Macro_F1: 0.9094, AUC_score: 0.9693\n",
      "Epoch 00199: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0075, Macro_F1: 0.9273, AUC_score: 0.9650\n",
      "Validation loss decreased (0.003029 --> 0.003029).\n",
      "Epoch 250: Train Loss: 0.0047, Macro_F1: 0.9130, AUC_score: 0.9671\n",
      "Validation loss decreased (0.002866 --> 0.002866).\n",
      "Validation loss decreased (0.002776 --> 0.002776).\n",
      "Epoch 300: Train Loss: 0.0031, Macro_F1: 0.9093, AUC_score: 0.9659\n",
      "Validation loss decreased (0.002642 --> 0.002642).\n",
      "Validation loss decreased (0.002518 --> 0.002518).\n",
      "Epoch 350: Train Loss: 0.0030, Macro_F1: 0.9129, AUC_score: 0.9656\n",
      "Epoch 400: Train Loss: 0.0026, Macro_F1: 0.9165, AUC_score: 0.9653\n",
      "Validation loss decreased (0.002280 --> 0.002280).\n",
      "Epoch 450: Train Loss: 0.0057, Macro_F1: 0.9093, AUC_score: 0.9665\n",
      "Epoch 500: Train Loss: 0.0048, Macro_F1: 0.9129, AUC_score: 0.9668\n",
      "Epoch 00511: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 550: Train Loss: 0.0038, Macro_F1: 0.9165, AUC_score: 0.9667\n",
      "Epoch 600: Train Loss: 0.0055, Macro_F1: 0.9129, AUC_score: 0.9666\n",
      "Epoch 00612: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Validation loss decreased (0.002148 --> 0.002148).\n",
      "Epoch 650: Train Loss: 0.0082, Macro_F1: 0.9129, AUC_score: 0.9667\n",
      "Epoch 700: Train Loss: 0.0056, Macro_F1: 0.9129, AUC_score: 0.9666\n",
      "Epoch 00715: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 750: Train Loss: 0.0051, Macro_F1: 0.9129, AUC_score: 0.9666\n",
      "Epoch 800: Train Loss: 0.0042, Macro_F1: 0.9129, AUC_score: 0.9666\n",
      "Epoch 00816: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 850: Train Loss: 0.0034, Macro_F1: 0.9129, AUC_score: 0.9666\n",
      "Epoch 900: Train Loss: 0.0053, Macro_F1: 0.9129, AUC_score: 0.9666\n",
      "Early stopping triggered\n",
      "acc save\n",
      "42.99999999999999% node features transform to 0: F1: 0.9129, AUC_score: 0.9666\n",
      "Epoch 0: Train Loss: 0.0037, Macro_F1: 0.9125, AUC_score: 0.9584\n",
      "Validation loss decreased (0.003724 --> 0.003724).\n",
      "Validation loss decreased (0.003688 --> 0.003688).\n",
      "Validation loss decreased (0.003608 --> 0.003608).\n",
      "Epoch 50: Train Loss: 0.0071, Macro_F1: 0.9166, AUC_score: 0.9678\n",
      "Validation loss decreased (0.003173 --> 0.003173).\n",
      "Validation loss decreased (0.002645 --> 0.002645).\n",
      "Epoch 100: Train Loss: 0.0041, Macro_F1: 0.9201, AUC_score: 0.9667\n",
      "Epoch 150: Train Loss: 0.0051, Macro_F1: 0.9238, AUC_score: 0.9681\n",
      "Epoch 00193: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0034, Macro_F1: 0.9165, AUC_score: 0.9676\n",
      "Epoch 250: Train Loss: 0.0047, Macro_F1: 0.9201, AUC_score: 0.9667\n",
      "Validation loss decreased (0.002587 --> 0.002587).\n",
      "Epoch 300: Train Loss: 0.0040, Macro_F1: 0.9165, AUC_score: 0.9671\n",
      "Validation loss decreased (0.002403 --> 0.002403).\n",
      "Epoch 350: Train Loss: 0.0044, Macro_F1: 0.9094, AUC_score: 0.9694\n",
      "Epoch 400: Train Loss: 0.0033, Macro_F1: 0.9130, AUC_score: 0.9677\n",
      "Epoch 00427: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.0086, Macro_F1: 0.9238, AUC_score: 0.9665\n",
      "Validation loss decreased (0.002387 --> 0.002387).\n",
      "Epoch 500: Train Loss: 0.0036, Macro_F1: 0.9202, AUC_score: 0.9670\n",
      "Epoch 550: Train Loss: 0.0054, Macro_F1: 0.9238, AUC_score: 0.9671\n",
      "Epoch 00592: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 600: Train Loss: 0.0044, Macro_F1: 0.9238, AUC_score: 0.9668\n",
      "Validation loss decreased (0.002200 --> 0.002200).\n",
      "Epoch 650: Train Loss: 0.0028, Macro_F1: 0.9130, AUC_score: 0.9672\n",
      "Validation loss decreased (0.002101 --> 0.002101).\n",
      "Epoch 700: Train Loss: 0.0039, Macro_F1: 0.9166, AUC_score: 0.9668\n",
      "Epoch 750: Train Loss: 0.0101, Macro_F1: 0.9238, AUC_score: 0.9669\n",
      "Epoch 00787: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 800: Train Loss: 0.0035, Macro_F1: 0.9201, AUC_score: 0.9668\n",
      "Epoch 850: Train Loss: 0.0045, Macro_F1: 0.9166, AUC_score: 0.9668\n",
      "Epoch 00888: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 900: Train Loss: 0.0048, Macro_F1: 0.9130, AUC_score: 0.9669\n",
      "Epoch 950: Train Loss: 0.0094, Macro_F1: 0.9130, AUC_score: 0.9669\n",
      "Early stopping triggered\n",
      "acc save\n",
      "42.00000000000001% node features transform to 0: F1: 0.9130, AUC_score: 0.9669\n",
      "Epoch 0: Train Loss: 0.0066, Macro_F1: 0.8874, AUC_score: 0.9695\n",
      "Validation loss decreased (0.006572 --> 0.006572).\n",
      "Validation loss decreased (0.006255 --> 0.006255).\n",
      "Validation loss decreased (0.005143 --> 0.005143).\n",
      "Validation loss decreased (0.004786 --> 0.004786).\n",
      "Validation loss decreased (0.004567 --> 0.004567).\n",
      "Epoch 50: Train Loss: 0.0047, Macro_F1: 0.9130, AUC_score: 0.9696\n",
      "Validation loss decreased (0.004030 --> 0.004030).\n",
      "Validation loss decreased (0.003832 --> 0.003832).\n",
      "Validation loss decreased (0.003681 --> 0.003681).\n",
      "Validation loss decreased (0.003019 --> 0.003019).\n",
      "Validation loss decreased (0.002750 --> 0.002750).\n",
      "Epoch 100: Train Loss: 0.0033, Macro_F1: 0.9166, AUC_score: 0.9680\n",
      "Epoch 150: Train Loss: 0.0536, Macro_F1: 0.9201, AUC_score: 0.9657\n",
      "Epoch 00194: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0078, Macro_F1: 0.9166, AUC_score: 0.9680\n",
      "Validation loss decreased (0.002676 --> 0.002676).\n",
      "Epoch 250: Train Loss: 0.0037, Macro_F1: 0.9201, AUC_score: 0.9671\n",
      "Validation loss decreased (0.002510 --> 0.002510).\n",
      "Epoch 300: Train Loss: 0.0042, Macro_F1: 0.9201, AUC_score: 0.9662\n",
      "Epoch 350: Train Loss: 0.0029, Macro_F1: 0.9238, AUC_score: 0.9674\n",
      "Epoch 00380: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.002188 --> 0.002188).\n",
      "Epoch 400: Train Loss: 0.0032, Macro_F1: 0.9201, AUC_score: 0.9670\n",
      "Epoch 450: Train Loss: 0.0026, Macro_F1: 0.9202, AUC_score: 0.9678\n",
      "Epoch 00492: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 500: Train Loss: 0.0059, Macro_F1: 0.9166, AUC_score: 0.9675\n",
      "Epoch 550: Train Loss: 0.0045, Macro_F1: 0.9238, AUC_score: 0.9674\n",
      "Epoch 00593: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 600: Train Loss: 0.0038, Macro_F1: 0.9238, AUC_score: 0.9675\n",
      "Validation loss decreased (0.002021 --> 0.002021).\n",
      "Epoch 650: Train Loss: 0.0051, Macro_F1: 0.9238, AUC_score: 0.9675\n",
      "Epoch 700: Train Loss: 0.0033, Macro_F1: 0.9238, AUC_score: 0.9675\n",
      "Epoch 00751: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 750: Train Loss: 0.0031, Macro_F1: 0.9238, AUC_score: 0.9675\n",
      "Epoch 800: Train Loss: 0.0037, Macro_F1: 0.9238, AUC_score: 0.9675\n",
      "Epoch 850: Train Loss: 0.0030, Macro_F1: 0.9238, AUC_score: 0.9675\n",
      "Epoch 00852: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 900: Train Loss: 0.0049, Macro_F1: 0.9238, AUC_score: 0.9675\n",
      "Early stopping triggered\n",
      "acc save\n",
      "41.0% node features transform to 0: F1: 0.9238, AUC_score: 0.9675\n",
      "Epoch 0: Train Loss: 0.0030, Macro_F1: 0.8591, AUC_score: 0.9496\n",
      "Validation loss decreased (0.003015 --> 0.003015).\n",
      "Epoch 50: Train Loss: 0.0040, Macro_F1: 0.9166, AUC_score: 0.9686\n",
      "Validation loss decreased (0.002927 --> 0.002927).\n",
      "Validation loss decreased (0.002911 --> 0.002911).\n",
      "Validation loss decreased (0.002619 --> 0.002619).\n",
      "Epoch 100: Train Loss: 0.0043, Macro_F1: 0.9273, AUC_score: 0.9669\n",
      "Validation loss decreased (0.002314 --> 0.002314).\n",
      "Validation loss decreased (0.002049 --> 0.002049).\n",
      "Epoch 150: Train Loss: 0.0047, Macro_F1: 0.9200, AUC_score: 0.9649\n",
      "Epoch 200: Train Loss: 0.0066, Macro_F1: 0.9237, AUC_score: 0.9696\n",
      "Epoch 00218: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0025, Macro_F1: 0.9310, AUC_score: 0.9683\n",
      "Epoch 300: Train Loss: 0.0035, Macro_F1: 0.9274, AUC_score: 0.9684\n",
      "Epoch 00319: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.0044, Macro_F1: 0.9274, AUC_score: 0.9683\n",
      "Epoch 400: Train Loss: 0.0040, Macro_F1: 0.9238, AUC_score: 0.9686\n",
      "Early stopping triggered\n",
      "acc save\n",
      "40.0% node features transform to 0: F1: 0.9238, AUC_score: 0.9685\n",
      "Epoch 0: Train Loss: 0.0028, Macro_F1: 0.9087, AUC_score: 0.9635\n",
      "Validation loss decreased (0.002760 --> 0.002760).\n",
      "Validation loss decreased (0.002716 --> 0.002716).\n",
      "Epoch 50: Train Loss: 0.0050, Macro_F1: 0.9237, AUC_score: 0.9662\n",
      "Epoch 100: Train Loss: 0.0071, Macro_F1: 0.9167, AUC_score: 0.9706\n",
      "Epoch 00147: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.0047, Macro_F1: 0.9238, AUC_score: 0.9696\n",
      "Validation loss decreased (0.002669 --> 0.002669).\n",
      "Validation loss decreased (0.002652 --> 0.002652).\n",
      "Epoch 200: Train Loss: 0.0028, Macro_F1: 0.9237, AUC_score: 0.9679\n",
      "Validation loss decreased (0.002411 --> 0.002411).\n",
      "Epoch 250: Train Loss: 0.0037, Macro_F1: 0.9238, AUC_score: 0.9681\n",
      "Epoch 300: Train Loss: 0.0027, Macro_F1: 0.9274, AUC_score: 0.9675\n",
      "Validation loss decreased (0.002213 --> 0.002213).\n",
      "Epoch 350: Train Loss: 0.0026, Macro_F1: 0.9238, AUC_score: 0.9691\n",
      "Epoch 400: Train Loss: 0.0047, Macro_F1: 0.9274, AUC_score: 0.9691\n",
      "Epoch 00403: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.0030, Macro_F1: 0.9238, AUC_score: 0.9684\n",
      "Validation loss decreased (0.001961 --> 0.001961).\n",
      "Epoch 500: Train Loss: 0.0030, Macro_F1: 0.9238, AUC_score: 0.9690\n",
      "Epoch 550: Train Loss: 0.0039, Macro_F1: 0.9238, AUC_score: 0.9685\n",
      "Epoch 00579: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 600: Train Loss: 0.0065, Macro_F1: 0.9238, AUC_score: 0.9689\n",
      "Epoch 650: Train Loss: 0.0045, Macro_F1: 0.9238, AUC_score: 0.9685\n",
      "Epoch 00680: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 700: Train Loss: 0.0062, Macro_F1: 0.9238, AUC_score: 0.9687\n",
      "Validation loss decreased (0.001851 --> 0.001851).\n",
      "Epoch 750: Train Loss: 0.0024, Macro_F1: 0.9238, AUC_score: 0.9687\n",
      "Epoch 800: Train Loss: 0.0111, Macro_F1: 0.9238, AUC_score: 0.9687\n",
      "Epoch 00851: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 850: Train Loss: 0.0030, Macro_F1: 0.9238, AUC_score: 0.9688\n",
      "Epoch 900: Train Loss: 0.0069, Macro_F1: 0.9238, AUC_score: 0.9688\n",
      "Epoch 950: Train Loss: 0.0053, Macro_F1: 0.9238, AUC_score: 0.9687\n",
      "Epoch 00952: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 1000: Train Loss: 0.0039, Macro_F1: 0.9238, AUC_score: 0.9687\n",
      "Early stopping triggered\n",
      "acc save\n",
      "39.0% node features transform to 0: F1: 0.9238, AUC_score: 0.9687\n",
      "Epoch 0: Train Loss: 0.0027, Macro_F1: 0.8898, AUC_score: 0.9552\n",
      "Validation loss decreased (0.002665 --> 0.002665).\n",
      "Epoch 50: Train Loss: 0.0055, Macro_F1: 0.9274, AUC_score: 0.9684\n",
      "Validation loss decreased (0.002546 --> 0.002546).\n",
      "Epoch 100: Train Loss: 0.0055, Macro_F1: 0.9165, AUC_score: 0.9665\n",
      "Validation loss decreased (0.002486 --> 0.002486).\n",
      "Validation loss decreased (0.002459 --> 0.002459).\n",
      "Epoch 150: Train Loss: 0.0060, Macro_F1: 0.9201, AUC_score: 0.9664\n",
      "Validation loss decreased (0.002374 --> 0.002374).\n",
      "Epoch 200: Train Loss: 0.0069, Macro_F1: 0.9165, AUC_score: 0.9655\n",
      "Epoch 250: Train Loss: 0.0110, Macro_F1: 0.9200, AUC_score: 0.9667\n",
      "Epoch 00256: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0042, Macro_F1: 0.9274, AUC_score: 0.9677\n",
      "Validation loss decreased (0.002288 --> 0.002288).\n",
      "Epoch 350: Train Loss: 0.0024, Macro_F1: 0.9274, AUC_score: 0.9686\n",
      "Validation loss decreased (0.002195 --> 0.002195).\n",
      "Epoch 400: Train Loss: 0.0038, Macro_F1: 0.9274, AUC_score: 0.9680\n",
      "Epoch 450: Train Loss: 0.0050, Macro_F1: 0.9310, AUC_score: 0.9682\n",
      "Epoch 00501: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.0036, Macro_F1: 0.9238, AUC_score: 0.9682\n",
      "Epoch 550: Train Loss: 0.0025, Macro_F1: 0.9274, AUC_score: 0.9676\n",
      "Epoch 600: Train Loss: 0.0037, Macro_F1: 0.9274, AUC_score: 0.9678\n",
      "Epoch 00602: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 650: Train Loss: 0.0020, Macro_F1: 0.9238, AUC_score: 0.9681\n",
      "Validation loss decreased (0.002032 --> 0.002032).\n",
      "Epoch 700: Train Loss: 0.0031, Macro_F1: 0.9238, AUC_score: 0.9683\n",
      "Epoch 750: Train Loss: 0.0026, Macro_F1: 0.9238, AUC_score: 0.9682\n",
      "Epoch 00752: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 800: Train Loss: 0.0046, Macro_F1: 0.9238, AUC_score: 0.9683\n",
      "Epoch 850: Train Loss: 0.0027, Macro_F1: 0.9238, AUC_score: 0.9683\n",
      "Epoch 00853: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 900: Train Loss: 0.0039, Macro_F1: 0.9238, AUC_score: 0.9683\n",
      "Epoch 950: Train Loss: 0.0031, Macro_F1: 0.9238, AUC_score: 0.9683\n",
      "Early stopping triggered\n",
      "acc save\n",
      "38.0% node features transform to 0: F1: 0.9238, AUC_score: 0.9683\n",
      "Epoch 0: Train Loss: 0.0038, Macro_F1: 0.9237, AUC_score: 0.9670\n",
      "Validation loss decreased (0.003795 --> 0.003795).\n",
      "Validation loss decreased (0.003492 --> 0.003492).\n",
      "Epoch 50: Train Loss: 0.0071, Macro_F1: 0.9274, AUC_score: 0.9688\n",
      "Validation loss decreased (0.003422 --> 0.003422).\n",
      "Epoch 100: Train Loss: 0.0032, Macro_F1: 0.9200, AUC_score: 0.9650\n",
      "Validation loss decreased (0.003224 --> 0.003224).\n",
      "Validation loss decreased (0.002669 --> 0.002669).\n",
      "Epoch 150: Train Loss: 0.0044, Macro_F1: 0.9200, AUC_score: 0.9651\n",
      "Epoch 200: Train Loss: 0.0070, Macro_F1: 0.9200, AUC_score: 0.9668\n",
      "Validation loss decreased (0.002568 --> 0.002568).\n",
      "Validation loss decreased (0.002379 --> 0.002379).\n",
      "Epoch 250: Train Loss: 0.0138, Macro_F1: 0.9202, AUC_score: 0.9673\n",
      "Epoch 300: Train Loss: 0.0038, Macro_F1: 0.9274, AUC_score: 0.9712\n",
      "Validation loss decreased (0.001904 --> 0.001904).\n",
      "Epoch 350: Train Loss: 0.0099, Macro_F1: 0.9167, AUC_score: 0.9712\n",
      "Epoch 400: Train Loss: 0.0043, Macro_F1: 0.9310, AUC_score: 0.9690\n",
      "Epoch 00417: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 450: Train Loss: 0.0041, Macro_F1: 0.9274, AUC_score: 0.9689\n",
      "Epoch 500: Train Loss: 0.0054, Macro_F1: 0.9274, AUC_score: 0.9690\n",
      "Epoch 00518: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 550: Train Loss: 0.0078, Macro_F1: 0.9310, AUC_score: 0.9678\n",
      "Epoch 600: Train Loss: 0.0038, Macro_F1: 0.9274, AUC_score: 0.9683\n",
      "Early stopping triggered\n",
      "acc save\n",
      "37.0% node features transform to 0: F1: 0.9274, AUC_score: 0.9685\n",
      "Epoch 0: Train Loss: 0.0078, Macro_F1: 0.8837, AUC_score: 0.9726\n",
      "Validation loss decreased (0.007837 --> 0.007837).\n",
      "Validation loss decreased (0.006871 --> 0.006871).\n",
      "Validation loss decreased (0.004543 --> 0.004543).\n",
      "Validation loss decreased (0.004532 --> 0.004532).\n",
      "Validation loss decreased (0.004251 --> 0.004251).\n",
      "Validation loss decreased (0.004011 --> 0.004011).\n",
      "Validation loss decreased (0.003321 --> 0.003321).\n",
      "Epoch 50: Train Loss: 0.0036, Macro_F1: 0.9166, AUC_score: 0.9701\n",
      "Validation loss decreased (0.003134 --> 0.003134).\n",
      "Validation loss decreased (0.002744 --> 0.002744).\n",
      "Validation loss decreased (0.002536 --> 0.002536).\n",
      "Validation loss decreased (0.002209 --> 0.002209).\n",
      "Epoch 100: Train Loss: 0.0085, Macro_F1: 0.9166, AUC_score: 0.9686\n",
      "Epoch 150: Train Loss: 0.0065, Macro_F1: 0.9274, AUC_score: 0.9683\n",
      "Epoch 00196: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0048, Macro_F1: 0.9310, AUC_score: 0.9679\n",
      "Epoch 250: Train Loss: 0.0039, Macro_F1: 0.9274, AUC_score: 0.9685\n",
      "Epoch 00297: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 300: Train Loss: 0.0038, Macro_F1: 0.9310, AUC_score: 0.9684\n",
      "Epoch 350: Train Loss: 0.0059, Macro_F1: 0.9310, AUC_score: 0.9684\n",
      "Validation loss decreased (0.002138 --> 0.002138).\n",
      "Epoch 400: Train Loss: 0.0043, Macro_F1: 0.9310, AUC_score: 0.9686\n",
      "Epoch 450: Train Loss: 0.0036, Macro_F1: 0.9274, AUC_score: 0.9686\n",
      "Validation loss decreased (0.002112 --> 0.002112).\n",
      "Epoch 500: Train Loss: 0.0032, Macro_F1: 0.9274, AUC_score: 0.9687\n",
      "Validation loss decreased (0.002016 --> 0.002016).\n",
      "Epoch 550: Train Loss: 0.0027, Macro_F1: 0.9274, AUC_score: 0.9685\n",
      "Epoch 600: Train Loss: 0.0039, Macro_F1: 0.9274, AUC_score: 0.9686\n",
      "Epoch 00640: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 650: Train Loss: 0.0028, Macro_F1: 0.9274, AUC_score: 0.9690\n",
      "Epoch 700: Train Loss: 0.0027, Macro_F1: 0.9274, AUC_score: 0.9683\n",
      "Epoch 750: Train Loss: 0.0035, Macro_F1: 0.9274, AUC_score: 0.9684\n",
      "Epoch 00790: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 800: Train Loss: 0.0065, Macro_F1: 0.9274, AUC_score: 0.9685\n",
      "Early stopping triggered\n",
      "acc save\n",
      "36.0% node features transform to 0: F1: 0.9274, AUC_score: 0.9685\n",
      "Epoch 0: Train Loss: 0.0027, Macro_F1: 0.9199, AUC_score: 0.9687\n",
      "Validation loss decreased (0.002681 --> 0.002681).\n",
      "Validation loss decreased (0.002396 --> 0.002396).\n",
      "Epoch 50: Train Loss: 0.0024, Macro_F1: 0.9273, AUC_score: 0.9677\n",
      "Epoch 100: Train Loss: 0.0050, Macro_F1: 0.9273, AUC_score: 0.9675\n",
      "Epoch 00145: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.0037, Macro_F1: 0.9165, AUC_score: 0.9678\n",
      "Epoch 200: Train Loss: 0.0051, Macro_F1: 0.9165, AUC_score: 0.9683\n",
      "Validation loss decreased (0.002320 --> 0.002320).\n",
      "Epoch 250: Train Loss: 0.0023, Macro_F1: 0.9129, AUC_score: 0.9687\n",
      "Epoch 300: Train Loss: 0.0027, Macro_F1: 0.9202, AUC_score: 0.9688\n",
      "Validation loss decreased (0.002233 --> 0.002233).\n",
      "Epoch 350: Train Loss: 0.0048, Macro_F1: 0.9273, AUC_score: 0.9674\n",
      "Validation loss decreased (0.001924 --> 0.001924).\n",
      "Validation loss decreased (0.001888 --> 0.001888).\n",
      "Epoch 400: Train Loss: 0.0067, Macro_F1: 0.9166, AUC_score: 0.9698\n",
      "Epoch 450: Train Loss: 0.0049, Macro_F1: 0.9201, AUC_score: 0.9674\n",
      "Epoch 00459: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.0045, Macro_F1: 0.9165, AUC_score: 0.9683\n",
      "Epoch 550: Train Loss: 0.0043, Macro_F1: 0.9201, AUC_score: 0.9681\n",
      "Epoch 00560: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 600: Train Loss: 0.0036, Macro_F1: 0.9201, AUC_score: 0.9683\n",
      "Epoch 650: Train Loss: 0.0054, Macro_F1: 0.9165, AUC_score: 0.9683\n",
      "Early stopping triggered\n",
      "acc save\n",
      "35.0% node features transform to 0: F1: 0.9165, AUC_score: 0.9684\n",
      "Epoch 0: Train Loss: 0.0045, Macro_F1: 0.8703, AUC_score: 0.9573\n",
      "Validation loss decreased (0.004461 --> 0.004461).\n",
      "Validation loss decreased (0.004072 --> 0.004072).\n",
      "Validation loss decreased (0.003788 --> 0.003788).\n",
      "Epoch 50: Train Loss: 0.0041, Macro_F1: 0.9130, AUC_score: 0.9679\n",
      "Validation loss decreased (0.003334 --> 0.003334).\n",
      "Validation loss decreased (0.002977 --> 0.002977).\n",
      "Validation loss decreased (0.002728 --> 0.002728).\n",
      "Epoch 100: Train Loss: 0.0089, Macro_F1: 0.9094, AUC_score: 0.9698\n",
      "Validation loss decreased (0.002609 --> 0.002609).\n",
      "Epoch 150: Train Loss: 0.0178, Macro_F1: 0.9127, AUC_score: 0.9649\n",
      "Epoch 200: Train Loss: 0.0039, Macro_F1: 0.9200, AUC_score: 0.9663\n",
      "Epoch 00205: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.002401 --> 0.002401).\n",
      "Epoch 250: Train Loss: 0.0042, Macro_F1: 0.9094, AUC_score: 0.9688\n",
      "Validation loss decreased (0.002027 --> 0.002027).\n",
      "Epoch 300: Train Loss: 0.0058, Macro_F1: 0.9237, AUC_score: 0.9674\n",
      "Epoch 350: Train Loss: 0.0031, Macro_F1: 0.9165, AUC_score: 0.9672\n",
      "Epoch 00381: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0075, Macro_F1: 0.9201, AUC_score: 0.9676\n",
      "Validation loss decreased (0.001953 --> 0.001953).\n",
      "Epoch 450: Train Loss: 0.0069, Macro_F1: 0.9202, AUC_score: 0.9676\n",
      "Epoch 500: Train Loss: 0.0049, Macro_F1: 0.9202, AUC_score: 0.9673\n",
      "Epoch 00511: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0084, Macro_F1: 0.9202, AUC_score: 0.9675\n",
      "Epoch 600: Train Loss: 0.0032, Macro_F1: 0.9202, AUC_score: 0.9674\n",
      "Epoch 00612: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 650: Train Loss: 0.0056, Macro_F1: 0.9202, AUC_score: 0.9675\n",
      "Epoch 700: Train Loss: 0.0039, Macro_F1: 0.9202, AUC_score: 0.9675\n",
      "Early stopping triggered\n",
      "acc save\n",
      "34.0% node features transform to 0: F1: 0.9202, AUC_score: 0.9676\n",
      "Epoch 0: Train Loss: 0.0037, Macro_F1: 0.8470, AUC_score: 0.9487\n",
      "Validation loss decreased (0.003729 --> 0.003729).\n",
      "Validation loss decreased (0.003117 --> 0.003117).\n",
      "Epoch 50: Train Loss: 0.0114, Macro_F1: 0.9166, AUC_score: 0.9693\n",
      "Validation loss decreased (0.003008 --> 0.003008).\n",
      "Validation loss decreased (0.002648 --> 0.002648).\n",
      "Validation loss decreased (0.002523 --> 0.002523).\n",
      "Validation loss decreased (0.002305 --> 0.002305).\n",
      "Epoch 100: Train Loss: 0.0042, Macro_F1: 0.9129, AUC_score: 0.9673\n",
      "Epoch 150: Train Loss: 0.0047, Macro_F1: 0.9237, AUC_score: 0.9680\n",
      "Epoch 00192: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.001935 --> 0.001935).\n",
      "Epoch 200: Train Loss: 0.0027, Macro_F1: 0.9129, AUC_score: 0.9687\n",
      "Epoch 250: Train Loss: 0.0069, Macro_F1: 0.9129, AUC_score: 0.9684\n",
      "Epoch 00300: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 300: Train Loss: 0.0056, Macro_F1: 0.9130, AUC_score: 0.9686\n",
      "Epoch 350: Train Loss: 0.0043, Macro_F1: 0.9129, AUC_score: 0.9682\n",
      "Validation loss decreased (0.001860 --> 0.001860).\n",
      "Epoch 400: Train Loss: 0.0028, Macro_F1: 0.9166, AUC_score: 0.9683\n",
      "Epoch 450: Train Loss: 0.0048, Macro_F1: 0.9166, AUC_score: 0.9683\n",
      "Epoch 00494: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 500: Train Loss: 0.0019, Macro_F1: 0.9166, AUC_score: 0.9685\n",
      "Epoch 550: Train Loss: 0.0026, Macro_F1: 0.9129, AUC_score: 0.9683\n",
      "Epoch 00595: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 600: Train Loss: 0.0031, Macro_F1: 0.9129, AUC_score: 0.9680\n",
      "Epoch 650: Train Loss: 0.0039, Macro_F1: 0.9129, AUC_score: 0.9680\n",
      "Early stopping triggered\n",
      "acc save\n",
      "32.99999999999999% node features transform to 0: F1: 0.9129, AUC_score: 0.9679\n",
      "Epoch 0: Train Loss: 0.0028, Macro_F1: 0.9092, AUC_score: 0.9635\n",
      "Validation loss decreased (0.002792 --> 0.002792).\n",
      "Epoch 50: Train Loss: 0.0103, Macro_F1: 0.9167, AUC_score: 0.9706\n",
      "Validation loss decreased (0.002780 --> 0.002780).\n",
      "Validation loss decreased (0.002741 --> 0.002741).\n",
      "Epoch 100: Train Loss: 0.0062, Macro_F1: 0.9166, AUC_score: 0.9694\n",
      "Validation loss decreased (0.002718 --> 0.002718).\n",
      "Epoch 150: Train Loss: 0.0090, Macro_F1: 0.9058, AUC_score: 0.9700\n",
      "Epoch 200: Train Loss: 0.0050, Macro_F1: 0.9163, AUC_score: 0.9640\n",
      "Epoch 00206: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.002550 --> 0.002550).\n",
      "Epoch 250: Train Loss: 0.0064, Macro_F1: 0.9129, AUC_score: 0.9687\n",
      "Validation loss decreased (0.002197 --> 0.002197).\n",
      "Validation loss decreased (0.002114 --> 0.002114).\n",
      "Epoch 300: Train Loss: 0.0024, Macro_F1: 0.9165, AUC_score: 0.9685\n",
      "Validation loss decreased (0.001648 --> 0.001648).\n",
      "Epoch 350: Train Loss: 0.0033, Macro_F1: 0.9093, AUC_score: 0.9693\n",
      "Epoch 400: Train Loss: 0.0068, Macro_F1: 0.9165, AUC_score: 0.9687\n",
      "Epoch 00440: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.0056, Macro_F1: 0.9129, AUC_score: 0.9696\n",
      "Epoch 500: Train Loss: 0.0038, Macro_F1: 0.9166, AUC_score: 0.9695\n",
      "Epoch 00541: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0066, Macro_F1: 0.9165, AUC_score: 0.9690\n",
      "Epoch 600: Train Loss: 0.0032, Macro_F1: 0.9165, AUC_score: 0.9690\n",
      "Early stopping triggered\n",
      "acc save\n",
      "31.999999999999996% node features transform to 0: F1: 0.9165, AUC_score: 0.9691\n",
      "Epoch 0: Train Loss: 0.0038, Macro_F1: 0.9084, AUC_score: 0.9615\n",
      "Validation loss decreased (0.003775 --> 0.003775).\n",
      "Validation loss decreased (0.003187 --> 0.003187).\n",
      "Validation loss decreased (0.003054 --> 0.003054).\n",
      "Validation loss decreased (0.002872 --> 0.002872).\n",
      "Epoch 50: Train Loss: 0.0030, Macro_F1: 0.9201, AUC_score: 0.9696\n",
      "Validation loss decreased (0.002759 --> 0.002759).\n",
      "Validation loss decreased (0.002659 --> 0.002659).\n",
      "Validation loss decreased (0.002526 --> 0.002526).\n",
      "Validation loss decreased (0.001956 --> 0.001956).\n",
      "Epoch 100: Train Loss: 0.0141, Macro_F1: 0.9203, AUC_score: 0.9717\n",
      "Epoch 150: Train Loss: 0.0099, Macro_F1: 0.9164, AUC_score: 0.9645\n",
      "Epoch 00187: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0041, Macro_F1: 0.9310, AUC_score: 0.9668\n",
      "Epoch 250: Train Loss: 0.0053, Macro_F1: 0.9129, AUC_score: 0.9687\n",
      "Epoch 00288: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 300: Train Loss: 0.0057, Macro_F1: 0.9201, AUC_score: 0.9676\n",
      "Validation loss decreased (0.001595 --> 0.001595).\n",
      "Epoch 350: Train Loss: 0.0029, Macro_F1: 0.9129, AUC_score: 0.9693\n",
      "Epoch 400: Train Loss: 0.0038, Macro_F1: 0.9129, AUC_score: 0.9688\n",
      "Epoch 00411: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 450: Train Loss: 0.0037, Macro_F1: 0.9165, AUC_score: 0.9687\n",
      "Epoch 500: Train Loss: 0.0054, Macro_F1: 0.9165, AUC_score: 0.9685\n",
      "Epoch 00512: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 550: Train Loss: 0.0025, Macro_F1: 0.9129, AUC_score: 0.9689\n",
      "Epoch 600: Train Loss: 0.0092, Macro_F1: 0.9129, AUC_score: 0.9687\n",
      "Early stopping triggered\n",
      "acc save\n",
      "30.999999999999993% node features transform to 0: F1: 0.9129, AUC_score: 0.9688\n",
      "Epoch 0: Train Loss: 0.0027, Macro_F1: 0.8594, AUC_score: 0.9489\n",
      "Validation loss decreased (0.002711 --> 0.002711).\n",
      "Epoch 50: Train Loss: 0.0038, Macro_F1: 0.9202, AUC_score: 0.9676\n",
      "Validation loss decreased (0.002497 --> 0.002497).\n",
      "Validation loss decreased (0.002297 --> 0.002297).\n",
      "Epoch 100: Train Loss: 0.0064, Macro_F1: 0.9166, AUC_score: 0.9650\n",
      "Epoch 150: Train Loss: 0.0020, Macro_F1: 0.9166, AUC_score: 0.9659\n",
      "Validation loss decreased (0.002026 --> 0.002026).\n",
      "Epoch 200: Train Loss: 0.0045, Macro_F1: 0.9310, AUC_score: 0.9654\n",
      "Validation loss decreased (0.001895 --> 0.001895).\n",
      "Epoch 250: Train Loss: 0.0019, Macro_F1: 0.9165, AUC_score: 0.9652\n",
      "Epoch 300: Train Loss: 0.0025, Macro_F1: 0.9201, AUC_score: 0.9645\n",
      "Validation loss decreased (0.001883 --> 0.001883).\n",
      "Epoch 350: Train Loss: 0.0056, Macro_F1: 0.9130, AUC_score: 0.9680\n",
      "Epoch 400: Train Loss: 0.0070, Macro_F1: 0.9239, AUC_score: 0.9664\n",
      "Epoch 00413: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 450: Train Loss: 0.0021, Macro_F1: 0.9129, AUC_score: 0.9651\n",
      "Epoch 500: Train Loss: 0.0059, Macro_F1: 0.9201, AUC_score: 0.9649\n",
      "Epoch 00514: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.001784 --> 0.001784).\n",
      "Epoch 550: Train Loss: 0.0032, Macro_F1: 0.9201, AUC_score: 0.9653\n",
      "Epoch 600: Train Loss: 0.0035, Macro_F1: 0.9201, AUC_score: 0.9652\n",
      "Validation loss decreased (0.001562 --> 0.001562).\n",
      "Epoch 650: Train Loss: 0.0067, Macro_F1: 0.9165, AUC_score: 0.9654\n",
      "Epoch 700: Train Loss: 0.0028, Macro_F1: 0.9201, AUC_score: 0.9653\n",
      "Epoch 00728: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 750: Train Loss: 0.0081, Macro_F1: 0.9201, AUC_score: 0.9648\n",
      "Epoch 800: Train Loss: 0.0038, Macro_F1: 0.9165, AUC_score: 0.9653\n",
      "Epoch 00829: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 850: Train Loss: 0.0050, Macro_F1: 0.9201, AUC_score: 0.9651\n",
      "Epoch 900: Train Loss: 0.0033, Macro_F1: 0.9201, AUC_score: 0.9650\n",
      "Early stopping triggered\n",
      "acc save\n",
      "29.999999999999993% node features transform to 0: F1: 0.9201, AUC_score: 0.9651\n",
      "Epoch 0: Train Loss: 0.0034, Macro_F1: 0.8548, AUC_score: 0.9497\n",
      "Validation loss decreased (0.003427 --> 0.003427).\n",
      "Epoch 50: Train Loss: 0.0042, Macro_F1: 0.9130, AUC_score: 0.9652\n",
      "Validation loss decreased (0.003254 --> 0.003254).\n",
      "Validation loss decreased (0.002824 --> 0.002824).\n",
      "Validation loss decreased (0.002078 --> 0.002078).\n",
      "Epoch 100: Train Loss: 0.0029, Macro_F1: 0.9093, AUC_score: 0.9654\n",
      "Validation loss decreased (0.001833 --> 0.001833).\n",
      "Epoch 150: Train Loss: 0.0042, Macro_F1: 0.9201, AUC_score: 0.9649\n",
      "Epoch 200: Train Loss: 0.0063, Macro_F1: 0.9058, AUC_score: 0.9657\n",
      "Epoch 00218: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0052, Macro_F1: 0.9129, AUC_score: 0.9654\n",
      "Epoch 300: Train Loss: 0.0025, Macro_F1: 0.9057, AUC_score: 0.9655\n",
      "Epoch 00319: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.0028, Macro_F1: 0.9129, AUC_score: 0.9649\n",
      "Epoch 400: Train Loss: 0.0044, Macro_F1: 0.9201, AUC_score: 0.9651\n",
      "Early stopping triggered\n",
      "acc save\n",
      "29.000000000000004% node features transform to 0: F1: 0.9129, AUC_score: 0.9655\n",
      "Epoch 0: Train Loss: 0.0060, Macro_F1: 0.8838, AUC_score: 0.9712\n",
      "Validation loss decreased (0.005995 --> 0.005995).\n",
      "Validation loss decreased (0.005495 --> 0.005495).\n",
      "Validation loss decreased (0.005041 --> 0.005041).\n",
      "Validation loss decreased (0.004826 --> 0.004826).\n",
      "Validation loss decreased (0.004316 --> 0.004316).\n",
      "Validation loss decreased (0.004144 --> 0.004144).\n",
      "Epoch 50: Train Loss: 0.0060, Macro_F1: 0.9130, AUC_score: 0.9667\n",
      "Validation loss decreased (0.003957 --> 0.003957).\n",
      "Validation loss decreased (0.003451 --> 0.003451).\n",
      "Validation loss decreased (0.003412 --> 0.003412).\n",
      "Validation loss decreased (0.002709 --> 0.002709).\n",
      "Epoch 100: Train Loss: 0.0038, Macro_F1: 0.9058, AUC_score: 0.9677\n",
      "Validation loss decreased (0.002628 --> 0.002628).\n",
      "Validation loss decreased (0.002110 --> 0.002110).\n",
      "Validation loss decreased (0.001999 --> 0.001999).\n",
      "Epoch 150: Train Loss: 0.0034, Macro_F1: 0.9130, AUC_score: 0.9668\n",
      "Epoch 200: Train Loss: 0.0039, Macro_F1: 0.9130, AUC_score: 0.9674\n",
      "Epoch 00244: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0035, Macro_F1: 0.9129, AUC_score: 0.9648\n",
      "Validation loss decreased (0.001824 --> 0.001824).\n",
      "Epoch 300: Train Loss: 0.0039, Macro_F1: 0.9130, AUC_score: 0.9660\n",
      "Epoch 350: Train Loss: 0.0040, Macro_F1: 0.9165, AUC_score: 0.9644\n",
      "Epoch 00358: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0029, Macro_F1: 0.9165, AUC_score: 0.9655\n",
      "Validation loss decreased (0.001716 --> 0.001716).\n",
      "Epoch 450: Train Loss: 0.0023, Macro_F1: 0.9166, AUC_score: 0.9656\n",
      "Epoch 500: Train Loss: 0.0033, Macro_F1: 0.9201, AUC_score: 0.9655\n",
      "Epoch 00503: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0100, Macro_F1: 0.9201, AUC_score: 0.9658\n",
      "Epoch 600: Train Loss: 0.0034, Macro_F1: 0.9201, AUC_score: 0.9658\n",
      "Epoch 00604: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 650: Train Loss: 0.0030, Macro_F1: 0.9201, AUC_score: 0.9658\n",
      "Epoch 700: Train Loss: 0.0024, Macro_F1: 0.9201, AUC_score: 0.9658\n",
      "Early stopping triggered\n",
      "acc save\n",
      "28.000000000000004% node features transform to 0: F1: 0.9201, AUC_score: 0.9658\n",
      "Epoch 0: Train Loss: 0.0034, Macro_F1: 0.8427, AUC_score: 0.9494\n",
      "Validation loss decreased (0.003417 --> 0.003417).\n",
      "Epoch 50: Train Loss: 0.0045, Macro_F1: 0.9058, AUC_score: 0.9655\n",
      "Validation loss decreased (0.003161 --> 0.003161).\n",
      "Validation loss decreased (0.003096 --> 0.003096).\n",
      "Validation loss decreased (0.002869 --> 0.002869).\n",
      "Validation loss decreased (0.002712 --> 0.002712).\n",
      "Validation loss decreased (0.002684 --> 0.002684).\n",
      "Epoch 100: Train Loss: 0.0034, Macro_F1: 0.9130, AUC_score: 0.9658\n",
      "Validation loss decreased (0.002550 --> 0.002550).\n",
      "Validation loss decreased (0.002231 --> 0.002231).\n",
      "Epoch 150: Train Loss: 0.0023, Macro_F1: 0.9201, AUC_score: 0.9631\n",
      "Epoch 200: Train Loss: 0.0038, Macro_F1: 0.9201, AUC_score: 0.9631\n",
      "Validation loss decreased (0.002187 --> 0.002187).\n",
      "Epoch 250: Train Loss: 0.0055, Macro_F1: 0.9166, AUC_score: 0.9668\n",
      "Epoch 300: Train Loss: 0.0065, Macro_F1: 0.9130, AUC_score: 0.9651\n",
      "Epoch 00346: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 350: Train Loss: 0.0038, Macro_F1: 0.9201, AUC_score: 0.9632\n",
      "Epoch 400: Train Loss: 0.0025, Macro_F1: 0.9166, AUC_score: 0.9646\n",
      "Validation loss decreased (0.002001 --> 0.002001).\n",
      "Validation loss decreased (0.001799 --> 0.001799).\n",
      "Epoch 450: Train Loss: 0.0031, Macro_F1: 0.9238, AUC_score: 0.9646\n",
      "Validation loss decreased (0.001771 --> 0.001771).\n",
      "Epoch 500: Train Loss: 0.0038, Macro_F1: 0.9238, AUC_score: 0.9645\n",
      "Epoch 550: Train Loss: 0.0030, Macro_F1: 0.9166, AUC_score: 0.9660\n",
      "Epoch 00582: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 600: Train Loss: 0.0041, Macro_F1: 0.9238, AUC_score: 0.9646\n",
      "Epoch 650: Train Loss: 0.0031, Macro_F1: 0.9166, AUC_score: 0.9654\n",
      "Validation loss decreased (0.001754 --> 0.001754).\n",
      "Epoch 700: Train Loss: 0.0030, Macro_F1: 0.9238, AUC_score: 0.9648\n",
      "Epoch 750: Train Loss: 0.0028, Macro_F1: 0.9202, AUC_score: 0.9655\n",
      "Epoch 00779: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 800: Train Loss: 0.0027, Macro_F1: 0.9238, AUC_score: 0.9650\n",
      "Validation loss decreased (0.001722 --> 0.001722).\n",
      "Epoch 850: Train Loss: 0.0024, Macro_F1: 0.9238, AUC_score: 0.9650\n",
      "Epoch 900: Train Loss: 0.0048, Macro_F1: 0.9238, AUC_score: 0.9653\n",
      "Validation loss decreased (0.001655 --> 0.001655).\n",
      "Epoch 950: Train Loss: 0.0036, Macro_F1: 0.9238, AUC_score: 0.9650\n",
      "Epoch 1000: Train Loss: 0.0058, Macro_F1: 0.9238, AUC_score: 0.9651\n",
      "Epoch 01013: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 1050: Train Loss: 0.0034, Macro_F1: 0.9238, AUC_score: 0.9650\n",
      "Epoch 1100: Train Loss: 0.0031, Macro_F1: 0.9238, AUC_score: 0.9650\n",
      "Epoch 01114: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 1150: Train Loss: 0.0029, Macro_F1: 0.9238, AUC_score: 0.9650\n",
      "Epoch 1200: Train Loss: 0.0060, Macro_F1: 0.9238, AUC_score: 0.9650\n",
      "Early stopping triggered\n",
      "acc save\n",
      "27.0% node features transform to 0: F1: 0.9238, AUC_score: 0.9650\n",
      "Epoch 0: Train Loss: 0.0031, Macro_F1: 0.9022, AUC_score: 0.9641\n",
      "Validation loss decreased (0.003144 --> 0.003144).\n",
      "Validation loss decreased (0.002579 --> 0.002579).\n",
      "Epoch 50: Train Loss: 0.0083, Macro_F1: 0.9130, AUC_score: 0.9683\n",
      "Validation loss decreased (0.002416 --> 0.002416).\n",
      "Epoch 100: Train Loss: 0.0033, Macro_F1: 0.9130, AUC_score: 0.9675\n",
      "Validation loss decreased (0.001781 --> 0.001781).\n",
      "Epoch 150: Train Loss: 0.0054, Macro_F1: 0.9238, AUC_score: 0.9643\n",
      "Epoch 200: Train Loss: 0.0080, Macro_F1: 0.9202, AUC_score: 0.9647\n",
      "Epoch 00214: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0031, Macro_F1: 0.9274, AUC_score: 0.9630\n",
      "Epoch 300: Train Loss: 0.0043, Macro_F1: 0.9274, AUC_score: 0.9643\n",
      "Epoch 00315: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.001650 --> 0.001650).\n",
      "Epoch 350: Train Loss: 0.0032, Macro_F1: 0.9238, AUC_score: 0.9649\n",
      "Epoch 400: Train Loss: 0.0035, Macro_F1: 0.9238, AUC_score: 0.9649\n",
      "Epoch 00423: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 450: Train Loss: 0.0030, Macro_F1: 0.9274, AUC_score: 0.9645\n",
      "Epoch 500: Train Loss: 0.0040, Macro_F1: 0.9238, AUC_score: 0.9648\n",
      "Epoch 00524: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 550: Train Loss: 0.0026, Macro_F1: 0.9238, AUC_score: 0.9648\n",
      "Epoch 600: Train Loss: 0.0020, Macro_F1: 0.9274, AUC_score: 0.9649\n",
      "Early stopping triggered\n",
      "acc save\n",
      "26.0% node features transform to 0: F1: 0.9274, AUC_score: 0.9648\n",
      "Epoch 0: Train Loss: 0.0035, Macro_F1: 0.8509, AUC_score: 0.9496\n",
      "Validation loss decreased (0.003492 --> 0.003492).\n",
      "Epoch 50: Train Loss: 0.0047, Macro_F1: 0.9238, AUC_score: 0.9639\n",
      "Validation loss decreased (0.003456 --> 0.003456).\n",
      "Validation loss decreased (0.003098 --> 0.003098).\n",
      "Validation loss decreased (0.003001 --> 0.003001).\n",
      "Validation loss decreased (0.002503 --> 0.002503).\n",
      "Validation loss decreased (0.002435 --> 0.002435).\n",
      "Validation loss decreased (0.002392 --> 0.002392).\n",
      "Epoch 100: Train Loss: 0.0129, Macro_F1: 0.9238, AUC_score: 0.9644\n",
      "Validation loss decreased (0.001797 --> 0.001797).\n",
      "Validation loss decreased (0.001741 --> 0.001741).\n",
      "Epoch 150: Train Loss: 0.0035, Macro_F1: 0.9237, AUC_score: 0.9627\n",
      "Epoch 200: Train Loss: 0.0059, Macro_F1: 0.9274, AUC_score: 0.9653\n",
      "Epoch 00237: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0035, Macro_F1: 0.9274, AUC_score: 0.9655\n",
      "Validation loss decreased (0.001714 --> 0.001714).\n",
      "Epoch 300: Train Loss: 0.0038, Macro_F1: 0.9166, AUC_score: 0.9666\n",
      "Epoch 350: Train Loss: 0.0085, Macro_F1: 0.9274, AUC_score: 0.9653\n",
      "Epoch 00388: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0028, Macro_F1: 0.9238, AUC_score: 0.9658\n",
      "Epoch 450: Train Loss: 0.0100, Macro_F1: 0.9274, AUC_score: 0.9647\n",
      "Epoch 00489: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 500: Train Loss: 0.0029, Macro_F1: 0.9274, AUC_score: 0.9650\n",
      "Validation loss decreased (0.001647 --> 0.001647).\n",
      "Epoch 550: Train Loss: 0.0063, Macro_F1: 0.9274, AUC_score: 0.9651\n",
      "Epoch 600: Train Loss: 0.0031, Macro_F1: 0.9274, AUC_score: 0.9652\n",
      "Epoch 00612: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 650: Train Loss: 0.0031, Macro_F1: 0.9274, AUC_score: 0.9653\n",
      "Epoch 700: Train Loss: 0.0036, Macro_F1: 0.9274, AUC_score: 0.9652\n",
      "Epoch 00713: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 750: Train Loss: 0.0035, Macro_F1: 0.9274, AUC_score: 0.9652\n",
      "Epoch 800: Train Loss: 0.0027, Macro_F1: 0.9274, AUC_score: 0.9652\n",
      "Early stopping triggered\n",
      "acc save\n",
      "25.0% node features transform to 0: F1: 0.9274, AUC_score: 0.9652\n",
      "Epoch 0: Train Loss: 0.0020, Macro_F1: 0.8668, AUC_score: 0.9521\n",
      "Validation loss decreased (0.002012 --> 0.002012).\n",
      "Epoch 50: Train Loss: 0.0044, Macro_F1: 0.9237, AUC_score: 0.9637\n",
      "Validation loss decreased (0.001861 --> 0.001861).\n",
      "Epoch 100: Train Loss: 0.0090, Macro_F1: 0.9202, AUC_score: 0.9660\n",
      "Validation loss decreased (0.001627 --> 0.001627).\n",
      "Epoch 150: Train Loss: 0.0064, Macro_F1: 0.9274, AUC_score: 0.9648\n",
      "Validation loss decreased (0.001369 --> 0.001369).\n",
      "Epoch 200: Train Loss: 0.0050, Macro_F1: 0.9127, AUC_score: 0.9606\n",
      "Epoch 250: Train Loss: 0.0059, Macro_F1: 0.9166, AUC_score: 0.9689\n",
      "Epoch 00264: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0044, Macro_F1: 0.9274, AUC_score: 0.9665\n",
      "Epoch 350: Train Loss: 0.0050, Macro_F1: 0.9274, AUC_score: 0.9648\n",
      "Epoch 00365: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0047, Macro_F1: 0.9274, AUC_score: 0.9660\n",
      "Epoch 450: Train Loss: 0.0025, Macro_F1: 0.9274, AUC_score: 0.9659\n",
      "Early stopping triggered\n",
      "acc save\n",
      "24.0% node features transform to 0: F1: 0.9274, AUC_score: 0.9658\n",
      "Epoch 0: Train Loss: 0.0040, Macro_F1: 0.9167, AUC_score: 0.9640\n",
      "Validation loss decreased (0.004020 --> 0.004020).\n",
      "Validation loss decreased (0.003677 --> 0.003677).\n",
      "Validation loss decreased (0.003558 --> 0.003558).\n",
      "Validation loss decreased (0.002081 --> 0.002081).\n",
      "Epoch 50: Train Loss: 0.0026, Macro_F1: 0.9201, AUC_score: 0.9632\n",
      "Epoch 100: Train Loss: 0.0040, Macro_F1: 0.9130, AUC_score: 0.9670\n",
      "Epoch 00135: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.0042, Macro_F1: 0.9166, AUC_score: 0.9648\n",
      "Epoch 200: Train Loss: 0.0084, Macro_F1: 0.9274, AUC_score: 0.9646\n",
      "Validation loss decreased (0.001883 --> 0.001883).\n",
      "Validation loss decreased (0.001861 --> 0.001861).\n",
      "Validation loss decreased (0.001560 --> 0.001560).\n",
      "Epoch 250: Train Loss: 0.0036, Macro_F1: 0.9238, AUC_score: 0.9640\n",
      "Epoch 300: Train Loss: 0.0023, Macro_F1: 0.9202, AUC_score: 0.9662\n",
      "Validation loss decreased (0.001525 --> 0.001525).\n",
      "Epoch 350: Train Loss: 0.0037, Macro_F1: 0.9274, AUC_score: 0.9655\n",
      "Epoch 400: Train Loss: 0.0053, Macro_F1: 0.9166, AUC_score: 0.9675\n",
      "Epoch 00419: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.0032, Macro_F1: 0.9239, AUC_score: 0.9657\n",
      "Epoch 500: Train Loss: 0.0043, Macro_F1: 0.9274, AUC_score: 0.9650\n",
      "Epoch 00520: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0036, Macro_F1: 0.9274, AUC_score: 0.9651\n",
      "Epoch 600: Train Loss: 0.0023, Macro_F1: 0.9274, AUC_score: 0.9652\n",
      "Early stopping triggered\n",
      "acc save\n",
      "23.0% node features transform to 0: F1: 0.9238, AUC_score: 0.9651\n",
      "Epoch 0: Train Loss: 0.0029, Macro_F1: 0.8594, AUC_score: 0.9480\n",
      "Validation loss decreased (0.002896 --> 0.002896).\n",
      "Epoch 50: Train Loss: 0.0077, Macro_F1: 0.9274, AUC_score: 0.9642\n",
      "Validation loss decreased (0.002351 --> 0.002351).\n",
      "Validation loss decreased (0.002291 --> 0.002291).\n",
      "Epoch 100: Train Loss: 0.0034, Macro_F1: 0.9166, AUC_score: 0.9664\n",
      "Validation loss decreased (0.002253 --> 0.002253).\n",
      "Epoch 150: Train Loss: 0.0025, Macro_F1: 0.9238, AUC_score: 0.9653\n",
      "Validation loss decreased (0.002151 --> 0.002151).\n",
      "Epoch 200: Train Loss: 0.0084, Macro_F1: 0.9202, AUC_score: 0.9650\n",
      "Validation loss decreased (0.001852 --> 0.001852).\n",
      "Epoch 250: Train Loss: 0.0028, Macro_F1: 0.9238, AUC_score: 0.9634\n",
      "Epoch 300: Train Loss: 0.0027, Macro_F1: 0.9238, AUC_score: 0.9650\n",
      "Epoch 00306: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.001808 --> 0.001808).\n",
      "Epoch 350: Train Loss: 0.0029, Macro_F1: 0.9273, AUC_score: 0.9639\n",
      "Epoch 400: Train Loss: 0.0027, Macro_F1: 0.9310, AUC_score: 0.9659\n",
      "Epoch 00411: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.0043, Macro_F1: 0.9238, AUC_score: 0.9659\n",
      "Epoch 500: Train Loss: 0.0039, Macro_F1: 0.9237, AUC_score: 0.9650\n",
      "Epoch 00512: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0033, Macro_F1: 0.9310, AUC_score: 0.9660\n",
      "Epoch 600: Train Loss: 0.0031, Macro_F1: 0.9274, AUC_score: 0.9655\n",
      "Early stopping triggered\n",
      "acc save\n",
      "21.999999999999996% node features transform to 0: F1: 0.9274, AUC_score: 0.9654\n",
      "Epoch 0: Train Loss: 0.0035, Macro_F1: 0.8978, AUC_score: 0.9587\n",
      "Validation loss decreased (0.003459 --> 0.003459).\n",
      "Validation loss decreased (0.003049 --> 0.003049).\n",
      "Epoch 50: Train Loss: 0.0037, Macro_F1: 0.9310, AUC_score: 0.9643\n",
      "Validation loss decreased (0.002683 --> 0.002683).\n",
      "Validation loss decreased (0.002363 --> 0.002363).\n",
      "Validation loss decreased (0.002337 --> 0.002337).\n",
      "Epoch 100: Train Loss: 0.0044, Macro_F1: 0.9238, AUC_score: 0.9658\n",
      "Validation loss decreased (0.002170 --> 0.002170).\n",
      "Epoch 150: Train Loss: 0.0042, Macro_F1: 0.9275, AUC_score: 0.9638\n",
      "Epoch 200: Train Loss: 0.0129, Macro_F1: 0.9203, AUC_score: 0.9657\n",
      "Epoch 00215: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0052, Macro_F1: 0.9202, AUC_score: 0.9659\n",
      "Epoch 300: Train Loss: 0.0021, Macro_F1: 0.9202, AUC_score: 0.9649\n",
      "Validation loss decreased (0.002103 --> 0.002103).\n",
      "Validation loss decreased (0.002010 --> 0.002010).\n",
      "Epoch 350: Train Loss: 0.0044, Macro_F1: 0.9202, AUC_score: 0.9661\n",
      "Epoch 400: Train Loss: 0.0040, Macro_F1: 0.9130, AUC_score: 0.9674\n",
      "Epoch 00422: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.0034, Macro_F1: 0.9310, AUC_score: 0.9656\n",
      "Epoch 500: Train Loss: 0.0056, Macro_F1: 0.9310, AUC_score: 0.9653\n",
      "Validation loss decreased (0.001728 --> 0.001728).\n",
      "Epoch 550: Train Loss: 0.0044, Macro_F1: 0.9274, AUC_score: 0.9653\n",
      "Epoch 600: Train Loss: 0.0031, Macro_F1: 0.9310, AUC_score: 0.9661\n",
      "Epoch 00605: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 650: Train Loss: 0.0036, Macro_F1: 0.9310, AUC_score: 0.9658\n",
      "Epoch 700: Train Loss: 0.0028, Macro_F1: 0.9310, AUC_score: 0.9660\n",
      "Epoch 00706: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Validation loss decreased (0.001691 --> 0.001691).\n",
      "Epoch 750: Train Loss: 0.0055, Macro_F1: 0.9310, AUC_score: 0.9659\n",
      "Epoch 800: Train Loss: 0.0029, Macro_F1: 0.9310, AUC_score: 0.9659\n",
      "Epoch 00840: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 850: Train Loss: 0.0025, Macro_F1: 0.9310, AUC_score: 0.9659\n",
      "Epoch 900: Train Loss: 0.0023, Macro_F1: 0.9310, AUC_score: 0.9659\n",
      "Epoch 00941: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 950: Train Loss: 0.0035, Macro_F1: 0.9310, AUC_score: 0.9659\n",
      "Epoch 1000: Train Loss: 0.0043, Macro_F1: 0.9310, AUC_score: 0.9659\n",
      "Early stopping triggered\n",
      "acc save\n",
      "20.999999999999996% node features transform to 0: F1: 0.9310, AUC_score: 0.9659\n",
      "Epoch 0: Train Loss: 0.0033, Macro_F1: 0.8874, AUC_score: 0.9699\n",
      "Validation loss decreased (0.003273 --> 0.003273).\n",
      "Validation loss decreased (0.003246 --> 0.003246).\n",
      "Epoch 50: Train Loss: 0.0032, Macro_F1: 0.9238, AUC_score: 0.9652\n",
      "Validation loss decreased (0.003183 --> 0.003183).\n",
      "Validation loss decreased (0.002898 --> 0.002898).\n",
      "Validation loss decreased (0.002621 --> 0.002621).\n",
      "Validation loss decreased (0.002478 --> 0.002478).\n",
      "Epoch 100: Train Loss: 0.0063, Macro_F1: 0.9166, AUC_score: 0.9654\n",
      "Epoch 150: Train Loss: 0.0037, Macro_F1: 0.9163, AUC_score: 0.9597\n",
      "Epoch 00181: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.002364 --> 0.002364).\n",
      "Epoch 200: Train Loss: 0.0030, Macro_F1: 0.9310, AUC_score: 0.9637\n",
      "Validation loss decreased (0.002121 --> 0.002121).\n",
      "Validation loss decreased (0.001575 --> 0.001575).\n",
      "Epoch 250: Train Loss: 0.0041, Macro_F1: 0.9274, AUC_score: 0.9645\n",
      "Epoch 300: Train Loss: 0.0038, Macro_F1: 0.9274, AUC_score: 0.9643\n",
      "Epoch 00319: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.0024, Macro_F1: 0.9310, AUC_score: 0.9647\n",
      "Epoch 400: Train Loss: 0.0025, Macro_F1: 0.9238, AUC_score: 0.9651\n",
      "Epoch 00420: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 450: Train Loss: 0.0024, Macro_F1: 0.9310, AUC_score: 0.9646\n",
      "Epoch 500: Train Loss: 0.0045, Macro_F1: 0.9310, AUC_score: 0.9646\n",
      "Early stopping triggered\n",
      "acc save\n",
      "19.999999999999996% node features transform to 0: F1: 0.9310, AUC_score: 0.9646\n",
      "Epoch 0: Train Loss: 0.0030, Macro_F1: 0.8874, AUC_score: 0.9683\n",
      "Validation loss decreased (0.002991 --> 0.002991).\n",
      "Validation loss decreased (0.002555 --> 0.002555).\n",
      "Validation loss decreased (0.002436 --> 0.002436).\n",
      "Epoch 50: Train Loss: 0.0026, Macro_F1: 0.9273, AUC_score: 0.9599\n",
      "Validation loss decreased (0.002410 --> 0.002410).\n",
      "Validation loss decreased (0.002168 --> 0.002168).\n",
      "Validation loss decreased (0.002023 --> 0.002023).\n",
      "Epoch 100: Train Loss: 0.0040, Macro_F1: 0.9238, AUC_score: 0.9636\n",
      "Epoch 150: Train Loss: 0.0054, Macro_F1: 0.9238, AUC_score: 0.9637\n",
      "Epoch 00196: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0035, Macro_F1: 0.9166, AUC_score: 0.9640\n",
      "Validation loss decreased (0.001998 --> 0.001998).\n",
      "Epoch 250: Train Loss: 0.0031, Macro_F1: 0.9238, AUC_score: 0.9640\n",
      "Validation loss decreased (0.001656 --> 0.001656).\n",
      "Epoch 300: Train Loss: 0.0034, Macro_F1: 0.9202, AUC_score: 0.9644\n",
      "Epoch 350: Train Loss: 0.0024, Macro_F1: 0.9274, AUC_score: 0.9640\n",
      "Epoch 00353: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.001615 --> 0.001615).\n",
      "Epoch 400: Train Loss: 0.0029, Macro_F1: 0.9238, AUC_score: 0.9638\n",
      "Validation loss decreased (0.001530 --> 0.001530).\n",
      "Epoch 450: Train Loss: 0.0023, Macro_F1: 0.9202, AUC_score: 0.9637\n",
      "Epoch 500: Train Loss: 0.0040, Macro_F1: 0.9274, AUC_score: 0.9637\n",
      "Epoch 00526: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0035, Macro_F1: 0.9274, AUC_score: 0.9635\n",
      "Epoch 600: Train Loss: 0.0039, Macro_F1: 0.9274, AUC_score: 0.9634\n",
      "Epoch 00627: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 650: Train Loss: 0.0035, Macro_F1: 0.9274, AUC_score: 0.9634\n",
      "Epoch 700: Train Loss: 0.0154, Macro_F1: 0.9274, AUC_score: 0.9635\n",
      "Early stopping triggered\n",
      "acc save\n",
      "18.999999999999993% node features transform to 0: F1: 0.9274, AUC_score: 0.9635\n",
      "Epoch 0: Train Loss: 0.0034, Macro_F1: 0.8763, AUC_score: 0.9697\n",
      "Validation loss decreased (0.003363 --> 0.003363).\n",
      "Epoch 50: Train Loss: 0.0039, Macro_F1: 0.9165, AUC_score: 0.9636\n",
      "Validation loss decreased (0.002387 --> 0.002387).\n",
      "Validation loss decreased (0.001908 --> 0.001908).\n",
      "Epoch 100: Train Loss: 0.0043, Macro_F1: 0.9238, AUC_score: 0.9636\n",
      "Epoch 150: Train Loss: 0.0031, Macro_F1: 0.9201, AUC_score: 0.9615\n",
      "Epoch 00160: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0033, Macro_F1: 0.9238, AUC_score: 0.9626\n",
      "Validation loss decreased (0.001879 --> 0.001879).\n",
      "Validation loss decreased (0.001849 --> 0.001849).\n",
      "Epoch 250: Train Loss: 0.0026, Macro_F1: 0.9166, AUC_score: 0.9631\n",
      "Validation loss decreased (0.001767 --> 0.001767).\n",
      "Epoch 300: Train Loss: 0.0033, Macro_F1: 0.9238, AUC_score: 0.9631\n",
      "Validation loss decreased (0.001602 --> 0.001602).\n",
      "Epoch 350: Train Loss: 0.0026, Macro_F1: 0.9238, AUC_score: 0.9632\n",
      "Validation loss decreased (0.001542 --> 0.001542).\n",
      "Epoch 400: Train Loss: 0.0047, Macro_F1: 0.9166, AUC_score: 0.9644\n",
      "Epoch 450: Train Loss: 0.0042, Macro_F1: 0.9166, AUC_score: 0.9644\n",
      "Epoch 00482: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.0025, Macro_F1: 0.9202, AUC_score: 0.9636\n",
      "Epoch 550: Train Loss: 0.0035, Macro_F1: 0.9202, AUC_score: 0.9638\n",
      "Validation loss decreased (0.001360 --> 0.001360).\n",
      "Epoch 600: Train Loss: 0.0028, Macro_F1: 0.9238, AUC_score: 0.9630\n",
      "Epoch 650: Train Loss: 0.0038, Macro_F1: 0.9238, AUC_score: 0.9639\n",
      "Epoch 00673: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 700: Train Loss: 0.0027, Macro_F1: 0.9238, AUC_score: 0.9632\n",
      "Epoch 750: Train Loss: 0.0056, Macro_F1: 0.9238, AUC_score: 0.9633\n",
      "Epoch 00774: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 800: Train Loss: 0.0020, Macro_F1: 0.9238, AUC_score: 0.9635\n",
      "Epoch 850: Train Loss: 0.0037, Macro_F1: 0.9238, AUC_score: 0.9636\n",
      "Early stopping triggered\n",
      "acc save\n",
      "17.999999999999993% node features transform to 0: F1: 0.9238, AUC_score: 0.9636\n",
      "Epoch 0: Train Loss: 0.0067, Macro_F1: 0.8690, AUC_score: 0.9698\n",
      "Validation loss decreased (0.006748 --> 0.006748).\n",
      "Validation loss decreased (0.004129 --> 0.004129).\n",
      "Validation loss decreased (0.004008 --> 0.004008).\n",
      "Validation loss decreased (0.002855 --> 0.002855).\n",
      "Epoch 50: Train Loss: 0.0040, Macro_F1: 0.9057, AUC_score: 0.9628\n",
      "Validation loss decreased (0.002497 --> 0.002497).\n",
      "Validation loss decreased (0.002386 --> 0.002386).\n",
      "Validation loss decreased (0.002199 --> 0.002199).\n",
      "Epoch 100: Train Loss: 0.0066, Macro_F1: 0.9202, AUC_score: 0.9650\n",
      "Validation loss decreased (0.002099 --> 0.002099).\n",
      "Validation loss decreased (0.001940 --> 0.001940).\n",
      "Epoch 150: Train Loss: 0.0036, Macro_F1: 0.9310, AUC_score: 0.9629\n",
      "Validation loss decreased (0.001777 --> 0.001777).\n",
      "Validation loss decreased (0.001492 --> 0.001492).\n",
      "Epoch 200: Train Loss: 0.0028, Macro_F1: 0.9238, AUC_score: 0.9649\n",
      "Epoch 250: Train Loss: 0.0047, Macro_F1: 0.9201, AUC_score: 0.9636\n",
      "Epoch 00295: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0036, Macro_F1: 0.9237, AUC_score: 0.9621\n",
      "Epoch 350: Train Loss: 0.0064, Macro_F1: 0.9094, AUC_score: 0.9644\n",
      "Epoch 00396: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0022, Macro_F1: 0.9166, AUC_score: 0.9656\n",
      "Epoch 450: Train Loss: 0.0032, Macro_F1: 0.9274, AUC_score: 0.9646\n",
      "Early stopping triggered\n",
      "acc save\n",
      "16.999999999999993% node features transform to 0: F1: 0.9202, AUC_score: 0.9648\n",
      "Epoch 0: Train Loss: 0.0023, Macro_F1: 0.8703, AUC_score: 0.9534\n",
      "Validation loss decreased (0.002261 --> 0.002261).\n",
      "Epoch 50: Train Loss: 0.0125, Macro_F1: 0.9238, AUC_score: 0.9650\n",
      "Epoch 100: Train Loss: 0.0031, Macro_F1: 0.9274, AUC_score: 0.9649\n",
      "Epoch 00102: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.002232 --> 0.002232).\n",
      "Validation loss decreased (0.002134 --> 0.002134).\n",
      "Validation loss decreased (0.002116 --> 0.002116).\n",
      "Epoch 150: Train Loss: 0.0030, Macro_F1: 0.9274, AUC_score: 0.9655\n",
      "Validation loss decreased (0.001852 --> 0.001852).\n",
      "Epoch 200: Train Loss: 0.0029, Macro_F1: 0.9274, AUC_score: 0.9663\n",
      "Epoch 250: Train Loss: 0.0023, Macro_F1: 0.9274, AUC_score: 0.9653\n",
      "Epoch 00273: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 300: Train Loss: 0.0031, Macro_F1: 0.9166, AUC_score: 0.9668\n",
      "Validation loss decreased (0.001810 --> 0.001810).\n",
      "Epoch 350: Train Loss: 0.0024, Macro_F1: 0.9274, AUC_score: 0.9657\n",
      "Validation loss decreased (0.001783 --> 0.001783).\n",
      "Epoch 400: Train Loss: 0.0033, Macro_F1: 0.9274, AUC_score: 0.9655\n",
      "Validation loss decreased (0.001649 --> 0.001649).\n",
      "Validation loss decreased (0.001560 --> 0.001560).\n",
      "Epoch 450: Train Loss: 0.0039, Macro_F1: 0.9274, AUC_score: 0.9659\n",
      "Validation loss decreased (0.001494 --> 0.001494).\n",
      "Epoch 500: Train Loss: 0.0051, Macro_F1: 0.9274, AUC_score: 0.9654\n",
      "Epoch 550: Train Loss: 0.0037, Macro_F1: 0.9202, AUC_score: 0.9661\n",
      "Epoch 00593: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 600: Train Loss: 0.0022, Macro_F1: 0.9274, AUC_score: 0.9658\n",
      "Epoch 650: Train Loss: 0.0024, Macro_F1: 0.9274, AUC_score: 0.9661\n",
      "Epoch 00694: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 700: Train Loss: 0.0034, Macro_F1: 0.9238, AUC_score: 0.9662\n",
      "Epoch 750: Train Loss: 0.0047, Macro_F1: 0.9274, AUC_score: 0.9661\n",
      "Early stopping triggered\n",
      "acc save\n",
      "16.000000000000004% node features transform to 0: F1: 0.9274, AUC_score: 0.9661\n",
      "Epoch 0: Train Loss: 0.0037, Macro_F1: 0.8782, AUC_score: 0.9556\n",
      "Validation loss decreased (0.003736 --> 0.003736).\n",
      "Validation loss decreased (0.002152 --> 0.002152).\n",
      "Epoch 50: Train Loss: 0.0051, Macro_F1: 0.9274, AUC_score: 0.9616\n",
      "Validation loss decreased (0.001802 --> 0.001802).\n",
      "Epoch 100: Train Loss: 0.0048, Macro_F1: 0.9130, AUC_score: 0.9659\n",
      "Epoch 150: Train Loss: 0.0043, Macro_F1: 0.9094, AUC_score: 0.9633\n",
      "Validation loss decreased (0.001735 --> 0.001735).\n",
      "Validation loss decreased (0.001667 --> 0.001667).\n",
      "Epoch 200: Train Loss: 0.0037, Macro_F1: 0.9094, AUC_score: 0.9653\n",
      "Epoch 250: Train Loss: 0.0026, Macro_F1: 0.9237, AUC_score: 0.9603\n",
      "Epoch 00279: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.001605 --> 0.001605).\n",
      "Epoch 300: Train Loss: 0.0032, Macro_F1: 0.9166, AUC_score: 0.9659\n",
      "Epoch 350: Train Loss: 0.0020, Macro_F1: 0.9202, AUC_score: 0.9643\n",
      "Validation loss decreased (0.001411 --> 0.001411).\n",
      "Epoch 400: Train Loss: 0.0019, Macro_F1: 0.9166, AUC_score: 0.9654\n",
      "Epoch 450: Train Loss: 0.0023, Macro_F1: 0.9166, AUC_score: 0.9651\n",
      "Epoch 00464: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.0018, Macro_F1: 0.9274, AUC_score: 0.9638\n",
      "Epoch 550: Train Loss: 0.0046, Macro_F1: 0.9238, AUC_score: 0.9635\n",
      "Epoch 00565: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 600: Train Loss: 0.0024, Macro_F1: 0.9202, AUC_score: 0.9638\n",
      "Epoch 650: Train Loss: 0.0049, Macro_F1: 0.9238, AUC_score: 0.9638\n",
      "Early stopping triggered\n",
      "acc save\n",
      "15.000000000000002% node features transform to 0: F1: 0.9202, AUC_score: 0.9639\n",
      "Epoch 0: Train Loss: 0.0032, Macro_F1: 0.8427, AUC_score: 0.9492\n",
      "Validation loss decreased (0.003241 --> 0.003241).\n",
      "Validation loss decreased (0.002968 --> 0.002968).\n",
      "Epoch 50: Train Loss: 0.0039, Macro_F1: 0.9202, AUC_score: 0.9634\n",
      "Validation loss decreased (0.002896 --> 0.002896).\n",
      "Validation loss decreased (0.002616 --> 0.002616).\n",
      "Validation loss decreased (0.002377 --> 0.002377).\n",
      "Validation loss decreased (0.002243 --> 0.002243).\n",
      "Validation loss decreased (0.002127 --> 0.002127).\n",
      "Epoch 100: Train Loss: 0.0032, Macro_F1: 0.9274, AUC_score: 0.9620\n",
      "Validation loss decreased (0.001862 --> 0.001862).\n",
      "Epoch 150: Train Loss: 0.0050, Macro_F1: 0.9202, AUC_score: 0.9630\n",
      "Validation loss decreased (0.001820 --> 0.001820).\n",
      "Epoch 200: Train Loss: 0.0029, Macro_F1: 0.9202, AUC_score: 0.9639\n",
      "Epoch 250: Train Loss: 0.0031, Macro_F1: 0.9166, AUC_score: 0.9627\n",
      "Epoch 300: Train Loss: 0.0160, Macro_F1: 0.9237, AUC_score: 0.9610\n",
      "Validation loss decreased (0.001728 --> 0.001728).\n",
      "Epoch 350: Train Loss: 0.0064, Macro_F1: 0.9238, AUC_score: 0.9638\n",
      "Epoch 400: Train Loss: 0.0020, Macro_F1: 0.9202, AUC_score: 0.9639\n",
      "Epoch 00413: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 450: Train Loss: 0.0034, Macro_F1: 0.9202, AUC_score: 0.9641\n",
      "Validation loss decreased (0.001669 --> 0.001669).\n",
      "Epoch 500: Train Loss: 0.0073, Macro_F1: 0.9130, AUC_score: 0.9636\n",
      "Validation loss decreased (0.001551 --> 0.001551).\n",
      "Epoch 550: Train Loss: 0.0033, Macro_F1: 0.9274, AUC_score: 0.9636\n",
      "Epoch 600: Train Loss: 0.0022, Macro_F1: 0.9274, AUC_score: 0.9627\n",
      "Validation loss decreased (0.001505 --> 0.001505).\n",
      "Epoch 650: Train Loss: 0.0019, Macro_F1: 0.9166, AUC_score: 0.9641\n",
      "Validation loss decreased (0.001461 --> 0.001461).\n",
      "Epoch 700: Train Loss: 0.0016, Macro_F1: 0.9166, AUC_score: 0.9650\n",
      "Epoch 750: Train Loss: 0.0031, Macro_F1: 0.9238, AUC_score: 0.9640\n",
      "Epoch 00765: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.001339 --> 0.001339).\n",
      "Epoch 800: Train Loss: 0.0032, Macro_F1: 0.9166, AUC_score: 0.9633\n",
      "Epoch 850: Train Loss: 0.0030, Macro_F1: 0.9238, AUC_score: 0.9634\n",
      "Epoch 00887: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 900: Train Loss: 0.0036, Macro_F1: 0.9202, AUC_score: 0.9637\n",
      "Epoch 950: Train Loss: 0.0031, Macro_F1: 0.9274, AUC_score: 0.9636\n",
      "Epoch 00988: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 1000: Train Loss: 0.0036, Macro_F1: 0.9274, AUC_score: 0.9635\n",
      "Epoch 1050: Train Loss: 0.0016, Macro_F1: 0.9274, AUC_score: 0.9635\n",
      "Early stopping triggered\n",
      "acc save\n",
      "14.000000000000002% node features transform to 0: F1: 0.9274, AUC_score: 0.9636\n",
      "Epoch 0: Train Loss: 0.0025, Macro_F1: 0.8509, AUC_score: 0.9478\n",
      "Validation loss decreased (0.002511 --> 0.002511).\n",
      "Epoch 50: Train Loss: 0.0081, Macro_F1: 0.9094, AUC_score: 0.9666\n",
      "Validation loss decreased (0.002411 --> 0.002411).\n",
      "Validation loss decreased (0.002266 --> 0.002266).\n",
      "Validation loss decreased (0.001968 --> 0.001968).\n",
      "Epoch 100: Train Loss: 0.0035, Macro_F1: 0.9238, AUC_score: 0.9607\n",
      "Validation loss decreased (0.001663 --> 0.001663).\n",
      "Epoch 150: Train Loss: 0.0029, Macro_F1: 0.9202, AUC_score: 0.9623\n",
      "Validation loss decreased (0.001578 --> 0.001578).\n",
      "Epoch 200: Train Loss: 0.0020, Macro_F1: 0.9130, AUC_score: 0.9628\n",
      "Epoch 250: Train Loss: 0.0034, Macro_F1: 0.9094, AUC_score: 0.9659\n",
      "Epoch 00287: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0032, Macro_F1: 0.9238, AUC_score: 0.9603\n",
      "Epoch 350: Train Loss: 0.0111, Macro_F1: 0.9166, AUC_score: 0.9628\n",
      "Epoch 00388: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0034, Macro_F1: 0.9238, AUC_score: 0.9625\n",
      "Epoch 450: Train Loss: 0.0042, Macro_F1: 0.9238, AUC_score: 0.9621\n",
      "Early stopping triggered\n",
      "acc save\n",
      "13.0% node features transform to 0: F1: 0.9166, AUC_score: 0.9625\n",
      "Epoch 0: Train Loss: 0.0028, Macro_F1: 0.8623, AUC_score: 0.9494\n",
      "Validation loss decreased (0.002844 --> 0.002844).\n",
      "Epoch 50: Train Loss: 0.0046, Macro_F1: 0.9166, AUC_score: 0.9606\n",
      "Validation loss decreased (0.002151 --> 0.002151).\n",
      "Validation loss decreased (0.001981 --> 0.001981).\n",
      "Epoch 100: Train Loss: 0.0039, Macro_F1: 0.9201, AUC_score: 0.9611\n",
      "Validation loss decreased (0.001742 --> 0.001742).\n",
      "Epoch 150: Train Loss: 0.0030, Macro_F1: 0.9238, AUC_score: 0.9616\n",
      "Validation loss decreased (0.001589 --> 0.001589).\n",
      "Validation loss decreased (0.001467 --> 0.001467).\n",
      "Epoch 200: Train Loss: 0.0049, Macro_F1: 0.9238, AUC_score: 0.9625\n",
      "Epoch 250: Train Loss: 0.0074, Macro_F1: 0.9238, AUC_score: 0.9626\n",
      "Epoch 00266: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0034, Macro_F1: 0.9130, AUC_score: 0.9640\n",
      "Validation loss decreased (0.001358 --> 0.001358).\n",
      "Epoch 350: Train Loss: 0.0055, Macro_F1: 0.9202, AUC_score: 0.9634\n",
      "Epoch 400: Train Loss: 0.0055, Macro_F1: 0.9238, AUC_score: 0.9618\n",
      "Validation loss decreased (0.001318 --> 0.001318).\n",
      "Validation loss decreased (0.001225 --> 0.001225).\n",
      "Epoch 450: Train Loss: 0.0032, Macro_F1: 0.9238, AUC_score: 0.9626\n",
      "Epoch 500: Train Loss: 0.0027, Macro_F1: 0.9238, AUC_score: 0.9629\n",
      "Epoch 00545: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 550: Train Loss: 0.0019, Macro_F1: 0.9238, AUC_score: 0.9632\n",
      "Epoch 600: Train Loss: 0.0029, Macro_F1: 0.9238, AUC_score: 0.9630\n",
      "Epoch 00646: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 650: Train Loss: 0.0031, Macro_F1: 0.9238, AUC_score: 0.9630\n",
      "Epoch 700: Train Loss: 0.0032, Macro_F1: 0.9238, AUC_score: 0.9634\n",
      "Early stopping triggered\n",
      "acc save\n",
      "12.0% node features transform to 0: F1: 0.9238, AUC_score: 0.9632\n",
      "Epoch 0: Train Loss: 0.0025, Macro_F1: 0.8894, AUC_score: 0.9529\n",
      "Validation loss decreased (0.002505 --> 0.002505).\n",
      "Epoch 50: Train Loss: 0.0039, Macro_F1: 0.9238, AUC_score: 0.9608\n",
      "Validation loss decreased (0.002435 --> 0.002435).\n",
      "Epoch 100: Train Loss: 0.0022, Macro_F1: 0.9238, AUC_score: 0.9606\n",
      "Validation loss decreased (0.002160 --> 0.002160).\n",
      "Validation loss decreased (0.001665 --> 0.001665).\n",
      "Validation loss decreased (0.001313 --> 0.001313).\n",
      "Epoch 150: Train Loss: 0.0024, Macro_F1: 0.9238, AUC_score: 0.9618\n",
      "Epoch 200: Train Loss: 0.0030, Macro_F1: 0.9238, AUC_score: 0.9635\n",
      "Epoch 00238: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0029, Macro_F1: 0.9130, AUC_score: 0.9628\n",
      "Epoch 300: Train Loss: 0.0021, Macro_F1: 0.9238, AUC_score: 0.9619\n",
      "Epoch 00339: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.0020, Macro_F1: 0.9238, AUC_score: 0.9633\n",
      "Epoch 400: Train Loss: 0.0024, Macro_F1: 0.9202, AUC_score: 0.9636\n",
      "Early stopping triggered\n",
      "acc save\n",
      "10.999999999999998% node features transform to 0: F1: 0.9238, AUC_score: 0.9628\n",
      "Epoch 0: Train Loss: 0.0025, Macro_F1: 0.8949, AUC_score: 0.9616\n",
      "Validation loss decreased (0.002521 --> 0.002521).\n",
      "Epoch 50: Train Loss: 0.0031, Macro_F1: 0.9201, AUC_score: 0.9587\n",
      "Validation loss decreased (0.002387 --> 0.002387).\n",
      "Validation loss decreased (0.002136 --> 0.002136).\n",
      "Epoch 100: Train Loss: 0.0085, Macro_F1: 0.9058, AUC_score: 0.9620\n",
      "Validation loss decreased (0.001824 --> 0.001824).\n",
      "Epoch 150: Train Loss: 0.0087, Macro_F1: 0.9130, AUC_score: 0.9590\n",
      "Epoch 200: Train Loss: 0.0028, Macro_F1: 0.9094, AUC_score: 0.9608\n",
      "Epoch 00216: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.001788 --> 0.001788).\n",
      "Validation loss decreased (0.001515 --> 0.001515).\n",
      "Epoch 250: Train Loss: 0.0026, Macro_F1: 0.9094, AUC_score: 0.9626\n",
      "Epoch 300: Train Loss: 0.0020, Macro_F1: 0.9274, AUC_score: 0.9597\n",
      "Epoch 00343: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.0037, Macro_F1: 0.9202, AUC_score: 0.9600\n",
      "Epoch 400: Train Loss: 0.0013, Macro_F1: 0.9202, AUC_score: 0.9610\n",
      "Validation loss decreased (0.001285 --> 0.001285).\n",
      "Epoch 450: Train Loss: 0.0046, Macro_F1: 0.9202, AUC_score: 0.9614\n",
      "Epoch 500: Train Loss: 0.0020, Macro_F1: 0.9130, AUC_score: 0.9619\n",
      "Epoch 00502: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0027, Macro_F1: 0.9166, AUC_score: 0.9614\n",
      "Epoch 600: Train Loss: 0.0033, Macro_F1: 0.9202, AUC_score: 0.9612\n",
      "Epoch 00603: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 650: Train Loss: 0.0020, Macro_F1: 0.9202, AUC_score: 0.9612\n",
      "Epoch 700: Train Loss: 0.0029, Macro_F1: 0.9202, AUC_score: 0.9613\n",
      "Early stopping triggered\n",
      "acc save\n",
      "9.999999999999998% node features transform to 0: F1: 0.9202, AUC_score: 0.9613\n",
      "Epoch 0: Train Loss: 0.0051, Macro_F1: 0.8307, AUC_score: 0.9480\n",
      "Validation loss decreased (0.005064 --> 0.005064).\n",
      "Validation loss decreased (0.004692 --> 0.004692).\n",
      "Validation loss decreased (0.003902 --> 0.003902).\n",
      "Validation loss decreased (0.003386 --> 0.003386).\n",
      "Epoch 50: Train Loss: 0.0066, Macro_F1: 0.9130, AUC_score: 0.9612\n",
      "Validation loss decreased (0.003182 --> 0.003182).\n",
      "Validation loss decreased (0.002753 --> 0.002753).\n",
      "Validation loss decreased (0.002554 --> 0.002554).\n",
      "Validation loss decreased (0.002303 --> 0.002303).\n",
      "Epoch 100: Train Loss: 0.0022, Macro_F1: 0.9202, AUC_score: 0.9587\n",
      "Validation loss decreased (0.002182 --> 0.002182).\n",
      "Validation loss decreased (0.002025 --> 0.002025).\n",
      "Validation loss decreased (0.001555 --> 0.001555).\n",
      "Epoch 150: Train Loss: 0.0028, Macro_F1: 0.9166, AUC_score: 0.9598\n",
      "Validation loss decreased (0.001505 --> 0.001505).\n",
      "Epoch 200: Train Loss: 0.0026, Macro_F1: 0.9130, AUC_score: 0.9625\n",
      "Epoch 250: Train Loss: 0.0017, Macro_F1: 0.9166, AUC_score: 0.9604\n",
      "Epoch 00285: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0054, Macro_F1: 0.9202, AUC_score: 0.9600\n",
      "Validation loss decreased (0.001490 --> 0.001490).\n",
      "Epoch 350: Train Loss: 0.0019, Macro_F1: 0.9166, AUC_score: 0.9605\n",
      "Epoch 400: Train Loss: 0.0030, Macro_F1: 0.9202, AUC_score: 0.9607\n",
      "Epoch 00449: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.0032, Macro_F1: 0.9166, AUC_score: 0.9613\n",
      "Epoch 500: Train Loss: 0.0026, Macro_F1: 0.9166, AUC_score: 0.9615\n",
      "Epoch 00550: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0019, Macro_F1: 0.9166, AUC_score: 0.9614\n",
      "Validation loss decreased (0.001190 --> 0.001190).\n",
      "Epoch 600: Train Loss: 0.0020, Macro_F1: 0.9166, AUC_score: 0.9615\n",
      "Epoch 650: Train Loss: 0.0135, Macro_F1: 0.9166, AUC_score: 0.9615\n",
      "Epoch 00693: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Validation loss decreased (0.001098 --> 0.001098).\n",
      "Epoch 700: Train Loss: 0.0026, Macro_F1: 0.9166, AUC_score: 0.9618\n",
      "Epoch 750: Train Loss: 0.0021, Macro_F1: 0.9166, AUC_score: 0.9619\n",
      "Epoch 00801: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 800: Train Loss: 0.0021, Macro_F1: 0.9166, AUC_score: 0.9619\n",
      "Epoch 850: Train Loss: 0.0041, Macro_F1: 0.9166, AUC_score: 0.9618\n",
      "Epoch 900: Train Loss: 0.0022, Macro_F1: 0.9166, AUC_score: 0.9617\n",
      "Epoch 00902: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 950: Train Loss: 0.0022, Macro_F1: 0.9166, AUC_score: 0.9617\n",
      "Early stopping triggered\n",
      "acc save\n",
      "8.999999999999996% node features transform to 0: F1: 0.9166, AUC_score: 0.9617\n",
      "Epoch 0: Train Loss: 0.0024, Macro_F1: 0.8435, AUC_score: 0.9449\n",
      "Validation loss decreased (0.002442 --> 0.002442).\n",
      "Epoch 50: Train Loss: 0.0049, Macro_F1: 0.9166, AUC_score: 0.9613\n",
      "Validation loss decreased (0.002195 --> 0.002195).\n",
      "Validation loss decreased (0.001735 --> 0.001735).\n",
      "Epoch 100: Train Loss: 0.0045, Macro_F1: 0.9130, AUC_score: 0.9614\n",
      "Epoch 150: Train Loss: 0.0019, Macro_F1: 0.9274, AUC_score: 0.9599\n",
      "Validation loss decreased (0.001554 --> 0.001554).\n",
      "Epoch 200: Train Loss: 0.0021, Macro_F1: 0.9130, AUC_score: 0.9620\n",
      "Validation loss decreased (0.001497 --> 0.001497).\n",
      "Epoch 250: Train Loss: 0.0037, Macro_F1: 0.9310, AUC_score: 0.9575\n",
      "Epoch 300: Train Loss: 0.0019, Macro_F1: 0.9274, AUC_score: 0.9605\n",
      "Epoch 00322: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.001473 --> 0.001473).\n",
      "Epoch 350: Train Loss: 0.0029, Macro_F1: 0.9166, AUC_score: 0.9614\n",
      "Validation loss decreased (0.001439 --> 0.001439).\n",
      "Epoch 400: Train Loss: 0.0017, Macro_F1: 0.9166, AUC_score: 0.9618\n",
      "Validation loss decreased (0.001414 --> 0.001414).\n",
      "Epoch 450: Train Loss: 0.0048, Macro_F1: 0.9166, AUC_score: 0.9617\n",
      "Validation loss decreased (0.001372 --> 0.001372).\n",
      "Epoch 500: Train Loss: 0.0028, Macro_F1: 0.9166, AUC_score: 0.9629\n",
      "Epoch 550: Train Loss: 0.0029, Macro_F1: 0.9166, AUC_score: 0.9623\n",
      "Epoch 00595: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 600: Train Loss: 0.0048, Macro_F1: 0.9238, AUC_score: 0.9619\n",
      "Epoch 650: Train Loss: 0.0020, Macro_F1: 0.9166, AUC_score: 0.9624\n",
      "Epoch 00696: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 700: Train Loss: 0.0015, Macro_F1: 0.9238, AUC_score: 0.9620\n",
      "Validation loss decreased (0.001329 --> 0.001329).\n",
      "Validation loss decreased (0.001304 --> 0.001304).\n",
      "Epoch 750: Train Loss: 0.0021, Macro_F1: 0.9238, AUC_score: 0.9618\n",
      "Epoch 800: Train Loss: 0.0021, Macro_F1: 0.9238, AUC_score: 0.9619\n",
      "Epoch 00843: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 850: Train Loss: 0.0026, Macro_F1: 0.9166, AUC_score: 0.9620\n",
      "Epoch 900: Train Loss: 0.0035, Macro_F1: 0.9166, AUC_score: 0.9619\n",
      "Epoch 00944: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 950: Train Loss: 0.0045, Macro_F1: 0.9166, AUC_score: 0.9620\n",
      "Epoch 1000: Train Loss: 0.0031, Macro_F1: 0.9166, AUC_score: 0.9620\n",
      "Early stopping triggered\n",
      "acc save\n",
      "7.9999999999999964% node features transform to 0: F1: 0.9166, AUC_score: 0.9620\n",
      "Epoch 0: Train Loss: 0.0028, Macro_F1: 0.8545, AUC_score: 0.9516\n",
      "Validation loss decreased (0.002825 --> 0.002825).\n",
      "Epoch 50: Train Loss: 0.0074, Macro_F1: 0.9274, AUC_score: 0.9618\n",
      "Validation loss decreased (0.002751 --> 0.002751).\n",
      "Validation loss decreased (0.002627 --> 0.002627).\n",
      "Validation loss decreased (0.002547 --> 0.002547).\n",
      "Validation loss decreased (0.002127 --> 0.002127).\n",
      "Validation loss decreased (0.001949 --> 0.001949).\n",
      "Validation loss decreased (0.001886 --> 0.001886).\n",
      "Epoch 100: Train Loss: 0.0057, Macro_F1: 0.9166, AUC_score: 0.9641\n",
      "Validation loss decreased (0.001850 --> 0.001850).\n",
      "Validation loss decreased (0.001670 --> 0.001670).\n",
      "Epoch 150: Train Loss: 0.0021, Macro_F1: 0.9274, AUC_score: 0.9611\n",
      "Epoch 200: Train Loss: 0.0024, Macro_F1: 0.9273, AUC_score: 0.9588\n",
      "Validation loss decreased (0.001635 --> 0.001635).\n",
      "Validation loss decreased (0.001467 --> 0.001467).\n",
      "Epoch 250: Train Loss: 0.0021, Macro_F1: 0.9274, AUC_score: 0.9636\n",
      "Epoch 300: Train Loss: 0.0030, Macro_F1: 0.9274, AUC_score: 0.9619\n",
      "Epoch 00351: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 350: Train Loss: 0.0022, Macro_F1: 0.9130, AUC_score: 0.9637\n",
      "Epoch 400: Train Loss: 0.0044, Macro_F1: 0.9238, AUC_score: 0.9638\n",
      "Validation loss decreased (0.001313 --> 0.001313).\n",
      "Epoch 450: Train Loss: 0.0023, Macro_F1: 0.9202, AUC_score: 0.9634\n",
      "Epoch 500: Train Loss: 0.0023, Macro_F1: 0.9238, AUC_score: 0.9629\n",
      "Epoch 00530: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 550: Train Loss: 0.0036, Macro_F1: 0.9166, AUC_score: 0.9648\n",
      "Epoch 600: Train Loss: 0.0019, Macro_F1: 0.9202, AUC_score: 0.9643\n",
      "Epoch 00631: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 650: Train Loss: 0.0051, Macro_F1: 0.9202, AUC_score: 0.9642\n",
      "Epoch 700: Train Loss: 0.0026, Macro_F1: 0.9202, AUC_score: 0.9641\n",
      "Early stopping triggered\n",
      "acc save\n",
      "6.999999999999995% node features transform to 0: F1: 0.9202, AUC_score: 0.9640\n",
      "Epoch 0: Train Loss: 0.0027, Macro_F1: 0.8545, AUC_score: 0.9649\n",
      "Validation loss decreased (0.002722 --> 0.002722).\n",
      "Validation loss decreased (0.002312 --> 0.002312).\n",
      "Epoch 50: Train Loss: 0.0041, Macro_F1: 0.9202, AUC_score: 0.9635\n",
      "Validation loss decreased (0.002042 --> 0.002042).\n",
      "Validation loss decreased (0.001733 --> 0.001733).\n",
      "Epoch 100: Train Loss: 0.0034, Macro_F1: 0.9274, AUC_score: 0.9628\n",
      "Validation loss decreased (0.001710 --> 0.001710).\n",
      "Epoch 150: Train Loss: 0.0035, Macro_F1: 0.9274, AUC_score: 0.9632\n",
      "Validation loss decreased (0.001424 --> 0.001424).\n",
      "Epoch 200: Train Loss: 0.0032, Macro_F1: 0.9202, AUC_score: 0.9643\n",
      "Epoch 250: Train Loss: 0.0017, Macro_F1: 0.9274, AUC_score: 0.9612\n",
      "Epoch 00286: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0033, Macro_F1: 0.9094, AUC_score: 0.9637\n",
      "Epoch 350: Train Loss: 0.0042, Macro_F1: 0.9202, AUC_score: 0.9639\n",
      "Epoch 00387: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0026, Macro_F1: 0.9202, AUC_score: 0.9637\n",
      "Epoch 450: Train Loss: 0.0061, Macro_F1: 0.9274, AUC_score: 0.9635\n",
      "Early stopping triggered\n",
      "acc save\n",
      "5.999999999999995% node features transform to 0: F1: 0.9166, AUC_score: 0.9635\n",
      "Epoch 0: Train Loss: 0.0051, Macro_F1: 0.8623, AUC_score: 0.9518\n",
      "Validation loss decreased (0.005143 --> 0.005143).\n",
      "Validation loss decreased (0.002861 --> 0.002861).\n",
      "Validation loss decreased (0.002539 --> 0.002539).\n",
      "Validation loss decreased (0.002445 --> 0.002445).\n",
      "Validation loss decreased (0.002142 --> 0.002142).\n",
      "Epoch 50: Train Loss: 0.0040, Macro_F1: 0.9238, AUC_score: 0.9628\n",
      "Validation loss decreased (0.001769 --> 0.001769).\n",
      "Epoch 100: Train Loss: 0.0016, Macro_F1: 0.9238, AUC_score: 0.9641\n",
      "Validation loss decreased (0.001574 --> 0.001574).\n",
      "Epoch 150: Train Loss: 0.0048, Macro_F1: 0.9130, AUC_score: 0.9603\n",
      "Epoch 200: Train Loss: 0.0025, Macro_F1: 0.9130, AUC_score: 0.9667\n",
      "Epoch 00202: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0023, Macro_F1: 0.9310, AUC_score: 0.9649\n",
      "Epoch 300: Train Loss: 0.0057, Macro_F1: 0.9238, AUC_score: 0.9647\n",
      "Validation loss decreased (0.001431 --> 0.001431).\n",
      "Validation loss decreased (0.001296 --> 0.001296).\n",
      "Epoch 350: Train Loss: 0.0016, Macro_F1: 0.9202, AUC_score: 0.9651\n",
      "Validation loss decreased (0.001149 --> 0.001149).\n",
      "Validation loss decreased (0.001045 --> 0.001045).\n",
      "Epoch 400: Train Loss: 0.0024, Macro_F1: 0.9166, AUC_score: 0.9655\n",
      "Epoch 450: Train Loss: 0.0077, Macro_F1: 0.9202, AUC_score: 0.9643\n",
      "Epoch 00465: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.0017, Macro_F1: 0.9202, AUC_score: 0.9648\n",
      "Epoch 550: Train Loss: 0.0017, Macro_F1: 0.9166, AUC_score: 0.9657\n",
      "Epoch 00566: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 600: Train Loss: 0.0026, Macro_F1: 0.9166, AUC_score: 0.9657\n",
      "Epoch 650: Train Loss: 0.0016, Macro_F1: 0.9166, AUC_score: 0.9658\n",
      "Early stopping triggered\n",
      "acc save\n",
      "4.999999999999993% node features transform to 0: F1: 0.9166, AUC_score: 0.9657\n",
      "Epoch 0: Train Loss: 0.0023, Macro_F1: 0.8466, AUC_score: 0.9510\n",
      "Validation loss decreased (0.002274 --> 0.002274).\n",
      "Epoch 50: Train Loss: 0.0035, Macro_F1: 0.9166, AUC_score: 0.9639\n",
      "Validation loss decreased (0.002227 --> 0.002227).\n",
      "Epoch 100: Train Loss: 0.0039, Macro_F1: 0.9202, AUC_score: 0.9642\n",
      "Validation loss decreased (0.002124 --> 0.002124).\n",
      "Validation loss decreased (0.001862 --> 0.001862).\n",
      "Validation loss decreased (0.001777 --> 0.001777).\n",
      "Validation loss decreased (0.001573 --> 0.001573).\n",
      "Epoch 150: Train Loss: 0.0020, Macro_F1: 0.9202, AUC_score: 0.9637\n",
      "Validation loss decreased (0.001384 --> 0.001384).\n",
      "Epoch 200: Train Loss: 0.0049, Macro_F1: 0.9202, AUC_score: 0.9638\n",
      "Epoch 250: Train Loss: 0.0021, Macro_F1: 0.9202, AUC_score: 0.9626\n",
      "Epoch 00288: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0059, Macro_F1: 0.9166, AUC_score: 0.9641\n",
      "Epoch 350: Train Loss: 0.0022, Macro_F1: 0.9202, AUC_score: 0.9646\n",
      "Epoch 00389: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0023, Macro_F1: 0.9202, AUC_score: 0.9643\n",
      "Epoch 450: Train Loss: 0.0029, Macro_F1: 0.9202, AUC_score: 0.9645\n",
      "Early stopping triggered\n",
      "acc save\n",
      "4.0000000000000036% node features transform to 0: F1: 0.9202, AUC_score: 0.9641\n",
      "Epoch 0: Train Loss: 0.0019, Macro_F1: 0.8387, AUC_score: 0.9524\n",
      "Validation loss decreased (0.001861 --> 0.001861).\n",
      "Epoch 50: Train Loss: 0.0044, Macro_F1: 0.9130, AUC_score: 0.9633\n",
      "Epoch 100: Train Loss: 0.0039, Macro_F1: 0.9202, AUC_score: 0.9633\n",
      "Epoch 00102: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.001645 --> 0.001645).\n",
      "Epoch 150: Train Loss: 0.0025, Macro_F1: 0.9166, AUC_score: 0.9640\n",
      "Validation loss decreased (0.001413 --> 0.001413).\n",
      "Epoch 200: Train Loss: 0.0044, Macro_F1: 0.9202, AUC_score: 0.9632\n",
      "Epoch 250: Train Loss: 0.0036, Macro_F1: 0.9202, AUC_score: 0.9634\n",
      "Epoch 00278: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 300: Train Loss: 0.0041, Macro_F1: 0.9238, AUC_score: 0.9630\n",
      "Epoch 350: Train Loss: 0.0045, Macro_F1: 0.9238, AUC_score: 0.9629\n",
      "Epoch 00379: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 400: Train Loss: 0.0017, Macro_F1: 0.9202, AUC_score: 0.9631\n",
      "Epoch 450: Train Loss: 0.0031, Macro_F1: 0.9238, AUC_score: 0.9631\n",
      "Early stopping triggered\n",
      "acc save\n",
      "3.0000000000000027% node features transform to 0: F1: 0.9238, AUC_score: 0.9631\n",
      "Epoch 0: Train Loss: 0.0019, Macro_F1: 0.9053, AUC_score: 0.9558\n",
      "Validation loss decreased (0.001868 --> 0.001868).\n",
      "Epoch 50: Train Loss: 0.0020, Macro_F1: 0.9310, AUC_score: 0.9610\n",
      "Validation loss decreased (0.001536 --> 0.001536).\n",
      "Epoch 100: Train Loss: 0.0018, Macro_F1: 0.9310, AUC_score: 0.9634\n",
      "Epoch 150: Train Loss: 0.0019, Macro_F1: 0.9238, AUC_score: 0.9648\n",
      "Epoch 00163: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0024, Macro_F1: 0.9238, AUC_score: 0.9636\n",
      "Validation loss decreased (0.001446 --> 0.001446).\n",
      "Validation loss decreased (0.001304 --> 0.001304).\n",
      "Validation loss decreased (0.001229 --> 0.001229).\n",
      "Epoch 250: Train Loss: 0.0027, Macro_F1: 0.9166, AUC_score: 0.9647\n",
      "Epoch 300: Train Loss: 0.0031, Macro_F1: 0.9274, AUC_score: 0.9631\n",
      "Epoch 350: Train Loss: 0.0036, Macro_F1: 0.9237, AUC_score: 0.9625\n",
      "Epoch 00399: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0052, Macro_F1: 0.9203, AUC_score: 0.9660\n",
      "Epoch 450: Train Loss: 0.0022, Macro_F1: 0.9202, AUC_score: 0.9646\n",
      "Epoch 00500: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 500: Train Loss: 0.0037, Macro_F1: 0.9202, AUC_score: 0.9639\n",
      "Early stopping triggered\n",
      "acc save\n",
      "2.0000000000000018% node features transform to 0: F1: 0.9202, AUC_score: 0.9640\n",
      "Epoch 0: Train Loss: 0.0028, Macro_F1: 0.9310, AUC_score: 0.9590\n",
      "Validation loss decreased (0.002780 --> 0.002780).\n",
      "Validation loss decreased (0.002605 --> 0.002605).\n",
      "Validation loss decreased (0.002238 --> 0.002238).\n",
      "Epoch 50: Train Loss: 0.0044, Macro_F1: 0.9310, AUC_score: 0.9627\n",
      "Validation loss decreased (0.001701 --> 0.001701).\n",
      "Validation loss decreased (0.001634 --> 0.001634).\n",
      "Epoch 100: Train Loss: 0.0083, Macro_F1: 0.9202, AUC_score: 0.9626\n",
      "Epoch 150: Train Loss: 0.0021, Macro_F1: 0.9201, AUC_score: 0.9636\n",
      "Epoch 00168: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0020, Macro_F1: 0.9202, AUC_score: 0.9629\n",
      "Validation loss decreased (0.001533 --> 0.001533).\n",
      "Validation loss decreased (0.001512 --> 0.001512).\n",
      "Validation loss decreased (0.001469 --> 0.001469).\n",
      "Validation loss decreased (0.001195 --> 0.001195).\n",
      "Epoch 250: Train Loss: 0.0036, Macro_F1: 0.9166, AUC_score: 0.9656\n",
      "Epoch 300: Train Loss: 0.0041, Macro_F1: 0.9202, AUC_score: 0.9641\n",
      "Validation loss decreased (0.001177 --> 0.001177).\n",
      "Epoch 350: Train Loss: 0.0060, Macro_F1: 0.9203, AUC_score: 0.9651\n",
      "Epoch 400: Train Loss: 0.0031, Macro_F1: 0.9202, AUC_score: 0.9635\n",
      "Epoch 00416: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.001145 --> 0.001145).\n",
      "Epoch 450: Train Loss: 0.0028, Macro_F1: 0.9238, AUC_score: 0.9644\n",
      "Epoch 500: Train Loss: 0.0036, Macro_F1: 0.9273, AUC_score: 0.9639\n",
      "Epoch 00545: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0019, Macro_F1: 0.9202, AUC_score: 0.9645\n",
      "Epoch 600: Train Loss: 0.0045, Macro_F1: 0.9202, AUC_score: 0.9649\n",
      "Epoch 00646: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 650: Train Loss: 0.0021, Macro_F1: 0.9202, AUC_score: 0.9646\n",
      "Epoch 700: Train Loss: 0.0025, Macro_F1: 0.9202, AUC_score: 0.9645\n",
      "Early stopping triggered\n",
      "acc save\n",
      "1.0000000000000009% node features transform to 0: F1: 0.9202, AUC_score: 0.9644\n",
      "Epoch 0: Train Loss: 0.0049, Macro_F1: 0.8545, AUC_score: 0.9520\n",
      "Validation loss decreased (0.004916 --> 0.004916).\n",
      "Validation loss decreased (0.004264 --> 0.004264).\n",
      "Validation loss decreased (0.004121 --> 0.004121).\n",
      "Validation loss decreased (0.003928 --> 0.003928).\n",
      "Validation loss decreased (0.003150 --> 0.003150).\n",
      "Validation loss decreased (0.003054 --> 0.003054).\n",
      "Epoch 50: Train Loss: 0.0037, Macro_F1: 0.9130, AUC_score: 0.9619\n",
      "Validation loss decreased (0.002866 --> 0.002866).\n",
      "Validation loss decreased (0.002481 --> 0.002481).\n",
      "Validation loss decreased (0.002397 --> 0.002397).\n",
      "Validation loss decreased (0.002231 --> 0.002231).\n",
      "Validation loss decreased (0.002123 --> 0.002123).\n",
      "Validation loss decreased (0.001958 --> 0.001958).\n",
      "Epoch 100: Train Loss: 0.0055, Macro_F1: 0.9237, AUC_score: 0.9605\n",
      "Validation loss decreased (0.001816 --> 0.001816).\n",
      "Validation loss decreased (0.001606 --> 0.001606).\n",
      "Epoch 150: Train Loss: 0.0130, Macro_F1: 0.9202, AUC_score: 0.9599\n",
      "Validation loss decreased (0.001538 --> 0.001538).\n",
      "Epoch 200: Train Loss: 0.0023, Macro_F1: 0.9237, AUC_score: 0.9626\n",
      "Validation loss decreased (0.001472 --> 0.001472).\n",
      "Validation loss decreased (0.001441 --> 0.001441).\n",
      "Epoch 250: Train Loss: 0.0075, Macro_F1: 0.9237, AUC_score: 0.9635\n",
      "Epoch 300: Train Loss: 0.0044, Macro_F1: 0.9237, AUC_score: 0.9609\n",
      "Epoch 00340: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 350: Train Loss: 0.0038, Macro_F1: 0.9202, AUC_score: 0.9631\n",
      "Epoch 400: Train Loss: 0.0016, Macro_F1: 0.9165, AUC_score: 0.9644\n",
      "Epoch 00441: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.0035, Macro_F1: 0.9165, AUC_score: 0.9640\n",
      "Validation loss decreased (0.001320 --> 0.001320).\n",
      "Epoch 500: Train Loss: 0.0017, Macro_F1: 0.9165, AUC_score: 0.9639\n",
      "Epoch 550: Train Loss: 0.0034, Macro_F1: 0.9165, AUC_score: 0.9640\n",
      "Epoch 00587: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 600: Train Loss: 0.0028, Macro_F1: 0.9165, AUC_score: 0.9643\n",
      "Epoch 650: Train Loss: 0.0020, Macro_F1: 0.9165, AUC_score: 0.9644\n",
      "Epoch 00688: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 700: Train Loss: 0.0019, Macro_F1: 0.9165, AUC_score: 0.9645\n",
      "Epoch 750: Train Loss: 0.0018, Macro_F1: 0.9165, AUC_score: 0.9645\n",
      "Early stopping triggered\n",
      "acc save\n",
      "0.0% node features transform to 0: F1: 0.9165, AUC_score: 0.9645\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1')\n",
    "data = data.to(device)\n",
    "results = []\n",
    "# 加载数据\n",
    "features = data.x\n",
    "labels = data.y # 根据你的数据加载函数进行调整\n",
    "num_iterations = 50\n",
    "original_features = features.clone()\n",
    "# 总共需要迭代的次数，这里以逐步增加5%为例，直到100%\n",
    "model = GCN(num_features=data.x.shape[1], hidden_dim=64, num_classes=2, num_layers=2, activation=F.relu, dropout=0.5)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=100, verbose=True)\n",
    "num_epochs = 2500\n",
    "for i in range(100):\n",
    "    # 加载这次迭代的selected_indices和remaining_indices\n",
    "    selected_indices = np.load(f'DIVIDED_DATA/GO_{i}%.npy')\n",
    "    \n",
    "    # 根据selected_indices和remaining_indices调整特征\n",
    "    masked_features = features.clone()\n",
    "    masked_features[torch.tensor(selected_indices)] = 0  # 假设features是一个PyTorch tensor\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=300, verbose=True, delta=0.00001)\n",
    "    \n",
    "    if i > 0:\n",
    "        # 从上一个迭代保存的模型中加载参数\n",
    "        model.load_state_dict(torch.load(f'G-G_DATA/G-G_model_WITHOUT{i-1}.pth'))\n",
    "        # 重新初始化优化器和调度器\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=100, verbose=True)\n",
    "    \n",
    "    # 模型训练和评估逻辑\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model_scheduler(model, masked_features, data.y, data.edge_index, optimizer, loss_fn, scheduler, train_mask)\n",
    "        test_f1, test_auc = evaluate_model(model, masked_features, data.y, data.edge_index, test_mask)\n",
    "        high_f1, high_auc = evaluate_model(model, data.x, data.y, data.edge_index, test_high)\n",
    "        low_f1, low_auc = evaluate_model(model, data.x, data.y, data.edge_index, test_low)\n",
    "        if epoch % 50 == 0:  # 每10个epoch打印一次信息\n",
    "            print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Macro_F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "        early_stopping(train_loss, model, i)\n",
    "        if early_stopping.early_stop or epoch == num_epochs - 1:\n",
    "            results.append({\n",
    "                            'Train Loss': train_loss,\n",
    "                            'F1': test_f1,\n",
    "                            'AUC_score': test_auc,\n",
    "                            'highinfo F1': high_f1,\n",
    "                            'highinfo AUC_score': high_auc,\n",
    "                            'lowinfo F1': low_f1,\n",
    "                            'lowinfo AUC_score': low_auc\n",
    "                        })\n",
    "            print(\"acc save\")\n",
    "            rate = 1 - count * (i + 1)\n",
    "            rate = rate * 100\n",
    "            print(f'{rate}% node features transform to 0: F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "            torch.save(model.state_dict(), f'G-G_DATA/G-G_model_WITHOUT{i}.pth')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9e58e0d-d1f8-4ef0-bf20-79e2666f92d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Train Loss': 0.36859965324401855,\n",
       "  'F1': array(0.83690304, dtype=float32),\n",
       "  'AUC_score': 0.9255067122927086,\n",
       "  'highinfo F1': array(0.77232796, dtype=float32),\n",
       "  'highinfo AUC_score': 0.8329036304090308,\n",
       "  'lowinfo F1': array(0.9166023, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9197530864197532},\n",
       " {'Train Loss': 0.2899118959903717,\n",
       "  'F1': array(0.8477541, dtype=float32),\n",
       "  'AUC_score': 0.9310871281916294,\n",
       "  'highinfo F1': array(0.8043443, dtype=float32),\n",
       "  'highinfo AUC_score': 0.8786844122360811,\n",
       "  'lowinfo F1': array(0.8885449, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9320987654320988},\n",
       " {'Train Loss': 0.22125475108623505,\n",
       "  'F1': array(0.8219378, dtype=float32),\n",
       "  'AUC_score': 0.8960252698078441,\n",
       "  'highinfo F1': array(0.76496243, dtype=float32),\n",
       "  'highinfo AUC_score': 0.8899728241934362,\n",
       "  'lowinfo F1': array(0.8017309, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.904320987654321},\n",
       " {'Train Loss': 0.0839753970503807,\n",
       "  'F1': array(0.9273493, dtype=float32),\n",
       "  'AUC_score': 0.9653593050802843,\n",
       "  'highinfo F1': array(0.8528149, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9091352518988225,\n",
       "  'lowinfo F1': array(0.9166023, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9228395061728395},\n",
       " {'Train Loss': 0.11989011615514755,\n",
       "  'F1': array(0.92386407, dtype=float32),\n",
       "  'AUC_score': 0.9626217425638326,\n",
       "  'highinfo F1': array(0.86591244, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9340812486934709,\n",
       "  'lowinfo F1': array(0.9166023, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9320987654320987},\n",
       " {'Train Loss': 0.06734295934438705,\n",
       "  'F1': array(0.9201389, dtype=float32),\n",
       "  'AUC_score': 0.9499868386417478,\n",
       "  'highinfo F1': array(0.87869275, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9358232875757787,\n",
       "  'lowinfo F1': array(0.8888889, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9444444444444444},\n",
       " {'Train Loss': 0.06181521341204643,\n",
       "  'F1': array(0.9128788, dtype=float32),\n",
       "  'AUC_score': 0.9460384311660963,\n",
       "  'highinfo F1': array(0.87032676, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9404222702250714,\n",
       "  'lowinfo F1': array(0.8888889, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9444444444444444},\n",
       " {'Train Loss': 0.04415540397167206,\n",
       "  'F1': array(0.9020183, dtype=float32),\n",
       "  'AUC_score': 0.9430376414846012,\n",
       "  'highinfo F1': array(0.86941636, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9466936102013797,\n",
       "  'lowinfo F1': array(0.8888889, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9537037037037036},\n",
       " {'Train Loss': 0.03480144590139389,\n",
       "  'F1': array(0.91657794, dtype=float32),\n",
       "  'AUC_score': 0.9419847328244275,\n",
       "  'highinfo F1': array(0.8735156, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9482962859731029,\n",
       "  'lowinfo F1': array(0.8888889, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9506172839506173},\n",
       " {'Train Loss': 0.03832719475030899,\n",
       "  'F1': array(0.90575254, dtype=float32),\n",
       "  'AUC_score': 0.9376151618847065,\n",
       "  'highinfo F1': array(0.87804663, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9510138666295033,\n",
       "  'lowinfo F1': array(0.8888889, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9537037037037036},\n",
       " {'Train Loss': 0.02965329773724079,\n",
       "  'F1': array(0.89125293, dtype=float32),\n",
       "  'AUC_score': 0.9448802316399052,\n",
       "  'highinfo F1': array(0.8739407, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9462058393143334,\n",
       "  'lowinfo F1': array(0.8888889, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.941358024691358},\n",
       " {'Train Loss': 0.022568242624402046,\n",
       "  'F1': array(0.8985294, dtype=float32),\n",
       "  'AUC_score': 0.9376151618847065,\n",
       "  'highinfo F1': array(0.8869326, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9394467284509789,\n",
       "  'lowinfo F1': array(0.8888889, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9444444444444444},\n",
       " {'Train Loss': 0.019915031269192696,\n",
       "  'F1': array(0.90575254, dtype=float32),\n",
       "  'AUC_score': 0.9454593314030008,\n",
       "  'highinfo F1': array(0.891183, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9440457111002717,\n",
       "  'lowinfo F1': array(0.8888889, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9444444444444444},\n",
       " {'Train Loss': 0.019127439707517624,\n",
       "  'F1': array(0.90939057, dtype=float32),\n",
       "  'AUC_score': 0.9478810213214004,\n",
       "  'highinfo F1': array(0.8826734, dtype=float32),\n",
       "  'highinfo AUC_score': 0.940561633335656,\n",
       "  'lowinfo F1': array(0.8888889, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9506172839506173},\n",
       " {'Train Loss': 0.017917701974511147,\n",
       "  'F1': array(0.90575254, dtype=float32),\n",
       "  'AUC_score': 0.9475125032903395,\n",
       "  'highinfo F1': array(0.87412584, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9378440526792559,\n",
       "  'lowinfo F1': array(0.8888889, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9783950617283951},\n",
       " {'Train Loss': 0.01686493493616581,\n",
       "  'F1': array(0.91653407, dtype=float32),\n",
       "  'AUC_score': 0.9487759936825481,\n",
       "  'highinfo F1': array(0.88251746, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9374956449027941,\n",
       "  'lowinfo F1': array(0.8888889, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9753086419753086},\n",
       " {'Train Loss': 0.013858291320502758,\n",
       "  'F1': array(0.9093239, dtype=float32),\n",
       "  'AUC_score': 0.9501447749407739,\n",
       "  'highinfo F1': array(0.89517635, dtype=float32),\n",
       "  'highinfo AUC_score': 0.942512716883841,\n",
       "  'lowinfo F1': array(0.8888889, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9753086419753086},\n",
       " {'Train Loss': 0.016418496146798134,\n",
       "  'F1': array(0.9092762, dtype=float32),\n",
       "  'AUC_score': 0.9476704395893656,\n",
       "  'highinfo F1': array(0.8869326, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9436276217685178,\n",
       "  'lowinfo F1': array(0.8888889, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9783950617283951},\n",
       " {'Train Loss': 0.0250579621642828,\n",
       "  'F1': array(0.9020183, dtype=float32),\n",
       "  'AUC_score': 0.9485654119505134,\n",
       "  'highinfo F1': array(0.8785573, dtype=float32),\n",
       "  'highinfo AUC_score': 0.947878196641349,\n",
       "  'lowinfo F1': array(0.8888889, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9814814814814815},\n",
       " {'Train Loss': 0.013911056332290173,\n",
       "  'F1': array(0.9129292, dtype=float32),\n",
       "  'AUC_score': 0.9515662016320084,\n",
       "  'highinfo F1': array(0.891183, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9483659675283952,\n",
       "  'lowinfo F1': array(0.8888889, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9845679012345679},\n",
       " {'Train Loss': 0.01118291076272726,\n",
       "  'F1': array(0.9201851, dtype=float32),\n",
       "  'AUC_score': 0.9525664648591734,\n",
       "  'highinfo F1': array(0.8870588, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9486446937495645,\n",
       "  'lowinfo F1': array(0.8888889, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9845679012345679},\n",
       " {'Train Loss': 0.007659365888684988,\n",
       "  'F1': array(0.9201851, dtype=float32),\n",
       "  'AUC_score': 0.9536720189523559,\n",
       "  'highinfo F1': array(0.891183, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9493415093024877,\n",
       "  'lowinfo F1': array(0.9166023, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876543},\n",
       " {'Train Loss': 0.01829446479678154,\n",
       "  'F1': array(0.91653407, dtype=float32),\n",
       "  'AUC_score': 0.9550408002105818,\n",
       "  'highinfo F1': array(0.90379083, dtype=float32),\n",
       "  'highinfo AUC_score': 0.952964950177688,\n",
       "  'lowinfo F1': array(0.8888889, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876543},\n",
       " {'Train Loss': 0.007817248813807964,\n",
       "  'F1': array(0.90567327, dtype=float32),\n",
       "  'AUC_score': 0.95551460910766,\n",
       "  'highinfo F1': array(0.89965856, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9516410006271341,\n",
       "  'lowinfo F1': array(0.9166023, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876543},\n",
       " {'Train Loss': 0.010296311229467392,\n",
       "  'F1': array(0.9092762, dtype=float32),\n",
       "  'AUC_score': 0.9551460910765991,\n",
       "  'highinfo F1': array(0.89965856, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9535920841753188,\n",
       "  'lowinfo F1': array(0.9166023, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876543},\n",
       " {'Train Loss': 0.01091026607900858,\n",
       "  'F1': array(0.90575254, dtype=float32),\n",
       "  'AUC_score': 0.9543564095814688,\n",
       "  'highinfo F1': array(0.89988875, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9562399832764268,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876543},\n",
       " {'Train Loss': 0.01076161302626133,\n",
       "  'F1': array(0.9130024, dtype=float32),\n",
       "  'AUC_score': 0.9567780994998684,\n",
       "  'highinfo F1': array(0.9081739, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9561703017211345,\n",
       "  'lowinfo F1': array(0.9166023, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876543},\n",
       " {'Train Loss': 0.010076716542243958,\n",
       "  'F1': array(0.90575254, dtype=float32),\n",
       "  'AUC_score': 0.9542511187154514,\n",
       "  'highinfo F1': array(0.91224253, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9517803637377186,\n",
       "  'lowinfo F1': array(0.9166023, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876543},\n",
       " {'Train Loss': 0.00882361177355051,\n",
       "  'F1': array(0.90575254, dtype=float32),\n",
       "  'AUC_score': 0.9547249276125296,\n",
       "  'highinfo F1': array(0.8956866, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9521984530694725,\n",
       "  'lowinfo F1': array(0.8888889, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876543},\n",
       " {'Train Loss': 0.01163968164473772,\n",
       "  'F1': array(0.90936196, dtype=float32),\n",
       "  'AUC_score': 0.9570413266649118,\n",
       "  'highinfo F1': array(0.8957447, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9519197268483033,\n",
       "  'lowinfo F1': array(0.8888889, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876543},\n",
       " {'Train Loss': 0.007443310227245092,\n",
       "  'F1': array(0.9130024, dtype=float32),\n",
       "  'AUC_score': 0.9607265069755199,\n",
       "  'highinfo F1': array(0.90412503, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9562399832764268,\n",
       "  'lowinfo F1': array(0.9166023, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876543},\n",
       " {'Train Loss': 0.004732996225357056,\n",
       "  'F1': array(0.91661304, dtype=float32),\n",
       "  'AUC_score': 0.9625164516978152,\n",
       "  'highinfo F1': array(0.9082314, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9566580726081806,\n",
       "  'lowinfo F1': array(0.9166023, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876543},\n",
       " {'Train Loss': 0.008071492426097393,\n",
       "  'F1': array(0.91661304, dtype=float32),\n",
       "  'AUC_score': 0.9640958146880757,\n",
       "  'highinfo F1': array(0.916574, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9570761619399346,\n",
       "  'lowinfo F1': array(0.9166023, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876543},\n",
       " {'Train Loss': 0.005611649248749018,\n",
       "  'F1': array(0.9201851, dtype=float32),\n",
       "  'AUC_score': 0.9631481968939194,\n",
       "  'highinfo F1': array(0.9165217, dtype=float32),\n",
       "  'highinfo AUC_score': 0.956518709497596,\n",
       "  'lowinfo F1': array(0.9166023, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876543},\n",
       " {'Train Loss': 0.007298608776181936,\n",
       "  'F1': array(0.91661304, dtype=float32),\n",
       "  'AUC_score': 0.9625164516978152,\n",
       "  'highinfo F1': array(0.916574, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9567974357187654,\n",
       "  'lowinfo F1': array(0.9166023, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876543},\n",
       " {'Train Loss': 0.008300382643938065,\n",
       "  'F1': array(0.92386407, dtype=float32),\n",
       "  'AUC_score': 0.9629902605948933,\n",
       "  'highinfo F1': array(0.9207219, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9588182008222423,\n",
       "  'lowinfo F1': array(0.9166023, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876543},\n",
       " {'Train Loss': 0.008261335082352161,\n",
       "  'F1': array(0.92386407, dtype=float32),\n",
       "  'AUC_score': 0.9633061331929454,\n",
       "  'highinfo F1': array(0.916574, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9605602397045503,\n",
       "  'lowinfo F1': array(0.9166023, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876544},\n",
       " {'Train Loss': 0.006033885292708874,\n",
       "  'F1': array(0.9275019, dtype=float32),\n",
       "  'AUC_score': 0.9635167149249803,\n",
       "  'highinfo F1': array(0.9124255, dtype=float32),\n",
       "  'highinfo AUC_score': 0.960072468817504,\n",
       "  'lowinfo F1': array(0.9166023, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876544},\n",
       " {'Train Loss': 0.0070802620612084866,\n",
       "  'F1': array(0.92386407, dtype=float32),\n",
       "  'AUC_score': 0.9632534877599368,\n",
       "  'highinfo F1': array(0.9207219, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9595150163751656,\n",
       "  'lowinfo F1': array(0.9166023, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876544},\n",
       " {'Train Loss': 0.006025262642651796,\n",
       "  'F1': array(0.92386407, dtype=float32),\n",
       "  'AUC_score': 0.9625164516978152,\n",
       "  'highinfo F1': array(0.91237676, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9593756532645809,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876544},\n",
       " {'Train Loss': 0.006605042610317469,\n",
       "  'F1': array(0.9201851, dtype=float32),\n",
       "  'AUC_score': 0.9615161884706501,\n",
       "  'highinfo F1': array(0.90408504, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9599331057069194,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9907407407407407},\n",
       " {'Train Loss': 0.005723185371607542,\n",
       "  'F1': array(0.9093239, dtype=float32),\n",
       "  'AUC_score': 0.9597262437483549,\n",
       "  'highinfo F1': array(0.9082314, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9581213852693191,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876543},\n",
       " {'Train Loss': 0.005670665297657251,\n",
       "  'F1': array(0.91657794, dtype=float32),\n",
       "  'AUC_score': 0.9608317978415373,\n",
       "  'highinfo F1': array(0.9165217, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9574942512716883,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876543},\n",
       " {'Train Loss': 0.009280772879719734,\n",
       "  'F1': array(0.9201851, dtype=float32),\n",
       "  'AUC_score': 0.96072650697552,\n",
       "  'highinfo F1': array(0.9165217, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9556825308340882,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876543},\n",
       " {'Train Loss': 0.0061386963352561,\n",
       "  'F1': array(0.9201851, dtype=float32),\n",
       "  'AUC_score': 0.9598841800473809,\n",
       "  'highinfo F1': array(0.91237676, dtype=float32),\n",
       "  'highinfo AUC_score': 0.959375653264581,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876543},\n",
       " {'Train Loss': 0.0059568933211266994,\n",
       "  'F1': array(0.9310499, dtype=float32),\n",
       "  'AUC_score': 0.9635693603579889,\n",
       "  'highinfo F1': array(0.9124255, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9632778203609504,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876543},\n",
       " {'Train Loss': 0.004035841207951307,\n",
       "  'F1': array(0.927441, dtype=float32),\n",
       "  'AUC_score': 0.9652013687812582,\n",
       "  'highinfo F1': array(0.90412503, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9641139990244583,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876543},\n",
       " {'Train Loss': 0.0051999702118337154,\n",
       "  'F1': array(0.92739904, dtype=float32),\n",
       "  'AUC_score': 0.9665175046064755,\n",
       "  'highinfo F1': array(0.90412503, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9637655912479965,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9907407407407407},\n",
       " {'Train Loss': 0.005193789955228567,\n",
       "  'F1': array(0.9310499, dtype=float32),\n",
       "  'AUC_score': 0.9643063964201106,\n",
       "  'highinfo F1': array(0.8999722, dtype=float32),\n",
       "  'highinfo AUC_score': 0.960490558149258,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9907407407407407},\n",
       " {'Train Loss': 0.004515201784670353,\n",
       "  'F1': array(0.92374384, dtype=float32),\n",
       "  'AUC_score': 0.9658857594103711,\n",
       "  'highinfo F1': array(0.90827596, dtype=float32),\n",
       "  'highinfo AUC_score': 0.960490558149258,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9938271604938271},\n",
       " {'Train Loss': 0.003918759524822235,\n",
       "  'F1': array(0.923792, dtype=float32),\n",
       "  'AUC_score': 0.9656751776783363,\n",
       "  'highinfo F1': array(0.90827596, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9602118319280887,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9907407407407407},\n",
       " {'Train Loss': 0.004350855480879545,\n",
       "  'F1': array(0.92383206, dtype=float32),\n",
       "  'AUC_score': 0.96472755988418,\n",
       "  'highinfo F1': array(0.90412503, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9594453348198733,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9907407407407407},\n",
       " {'Train Loss': 0.0034164905082434416,\n",
       "  'F1': array(0.9201851, dtype=float32),\n",
       "  'AUC_score': 0.9662016320084232,\n",
       "  'highinfo F1': array(0.9083079, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9604905581492578,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9907407407407407},\n",
       " {'Train Loss': 0.004250007681548595,\n",
       "  'F1': array(0.9201851, dtype=float32),\n",
       "  'AUC_score': 0.9657278231113451,\n",
       "  'highinfo F1': array(0.9083079, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9597937425963348,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9876543209876544},\n",
       " {'Train Loss': 0.006670373026281595,\n",
       "  'F1': array(0.9201389, dtype=float32),\n",
       "  'AUC_score': 0.9659910502763885,\n",
       "  'highinfo F1': array(0.90827596, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9598634241516271,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9907407407407407},\n",
       " {'Train Loss': 0.0036948646884411573,\n",
       "  'F1': array(0.9201389, dtype=float32),\n",
       "  'AUC_score': 0.9643590418531192,\n",
       "  'highinfo F1': array(0.90412503, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9584697930457808,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9938271604938271},\n",
       " {'Train Loss': 0.004251944832503796,\n",
       "  'F1': array(0.9129292, dtype=float32),\n",
       "  'AUC_score': 0.9665701500394841,\n",
       "  'highinfo F1': array(0.90412503, dtype=float32),\n",
       "  'highinfo AUC_score': 0.960978329036304,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9938271604938271},\n",
       " {'Train Loss': 0.007948217913508415,\n",
       "  'F1': array(0.9129703, dtype=float32),\n",
       "  'AUC_score': 0.9668860226375362,\n",
       "  'highinfo F1': array(0.9083079, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9620235523656888,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9938271604938271},\n",
       " {'Train Loss': 0.0038361181505024433,\n",
       "  'F1': array(0.923792, dtype=float32),\n",
       "  'AUC_score': 0.9674651224006316,\n",
       "  'highinfo F1': array(0.90412503, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9606299212598424,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9938271604938271},\n",
       " {'Train Loss': 0.005517303012311459,\n",
       "  'F1': array(0.923792, dtype=float32),\n",
       "  'AUC_score': 0.9685180310608055,\n",
       "  'highinfo F1': array(0.912462, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9619538708103965,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9938271604938271},\n",
       " {'Train Loss': 0.0038292850367724895,\n",
       "  'F1': array(0.923792, dtype=float32),\n",
       "  'AUC_score': 0.9687286127928402,\n",
       "  'highinfo F1': array(0.90827596, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9624416416974427,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9938271604938271},\n",
       " {'Train Loss': 0.0031253905035555363,\n",
       "  'F1': array(0.923792, dtype=float32),\n",
       "  'AUC_score': 0.9683074493287707,\n",
       "  'highinfo F1': array(0.90412503, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9628597310291966,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9938271604938271},\n",
       " {'Train Loss': 0.0031226719729602337,\n",
       "  'F1': array(0.92739904, dtype=float32),\n",
       "  'AUC_score': 0.9685180310608054,\n",
       "  'highinfo F1': array(0.90412503, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9629990941397812,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9938271604938271},\n",
       " {'Train Loss': 0.00445875059813261,\n",
       "  'F1': array(0.92739904, dtype=float32),\n",
       "  'AUC_score': 0.9685180310608055,\n",
       "  'highinfo F1': array(0.9041517, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9624416416974426,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 1.0},\n",
       " {'Train Loss': 0.0033172001130878925,\n",
       "  'F1': array(0.91653407, dtype=float32),\n",
       "  'AUC_score': 0.9683600947617794,\n",
       "  'highinfo F1': array(0.904165, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9621629154762734,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 1.0},\n",
       " {'Train Loss': 0.010599736124277115,\n",
       "  'F1': array(0.9201851, dtype=float32),\n",
       "  'AUC_score': 0.9675704132666492,\n",
       "  'highinfo F1': array(0.9124863, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9591666085987041,\n",
       "  'lowinfo F1': array(0.97220075, dtype=float32),\n",
       "  'lowinfo AUC_score': 1.0},\n",
       " {'Train Loss': 0.005325163248926401,\n",
       "  'F1': array(0.9129292, dtype=float32),\n",
       "  'AUC_score': 0.96793893129771,\n",
       "  'highinfo F1': array(0.9124863, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9588182008222425,\n",
       "  'lowinfo F1': array(0.97220075, dtype=float32),\n",
       "  'lowinfo AUC_score': 1.0},\n",
       " {'Train Loss': 0.005754433106631041,\n",
       "  'F1': array(0.91653407, dtype=float32),\n",
       "  'AUC_score': 0.969097130823901,\n",
       "  'highinfo F1': array(0.9166435, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9608389659257197,\n",
       "  'lowinfo F1': array(0.97220075, dtype=float32),\n",
       "  'lowinfo AUC_score': 1.0},\n",
       " {'Train Loss': 0.0022448271047323942,\n",
       "  'F1': array(0.9129292, dtype=float32),\n",
       "  'AUC_score': 0.9687812582258489,\n",
       "  'highinfo F1': array(0.9207989, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9608389659257195,\n",
       "  'lowinfo F1': array(0.97220075, dtype=float32),\n",
       "  'lowinfo AUC_score': 1.0},\n",
       " {'Train Loss': 0.007081400603055954,\n",
       "  'F1': array(0.9201389, dtype=float32),\n",
       "  'AUC_score': 0.9650960779152408,\n",
       "  'highinfo F1': array(0.9166435, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9601421503727964,\n",
       "  'lowinfo F1': array(0.97220075, dtype=float32),\n",
       "  'lowinfo AUC_score': 1.0},\n",
       " {'Train Loss': 0.004730235319584608,\n",
       "  'F1': array(0.9129292, dtype=float32),\n",
       "  'AUC_score': 0.9654645959463016,\n",
       "  'highinfo F1': array(0.9166435, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9621629154762734,\n",
       "  'lowinfo F1': array(0.97220075, dtype=float32),\n",
       "  'lowinfo AUC_score': 1.0},\n",
       " {'Train Loss': 0.003962785471230745,\n",
       "  'F1': array(0.9201389, dtype=float32),\n",
       "  'AUC_score': 0.9658331139773623,\n",
       "  'highinfo F1': array(0.9166435, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9598634241516271,\n",
       "  'lowinfo F1': array(0.97220075, dtype=float32),\n",
       "  'lowinfo AUC_score': 1.0},\n",
       " {'Train Loss': 0.0039037286769598722,\n",
       "  'F1': array(0.923792, dtype=float32),\n",
       "  'AUC_score': 0.9650434324822321,\n",
       "  'highinfo F1': array(0.90827596, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9601421503727963,\n",
       "  'lowinfo F1': array(0.97220075, dtype=float32),\n",
       "  'lowinfo AUC_score': 1.0},\n",
       " {'Train Loss': 0.0034387821797281504,\n",
       "  'F1': array(0.92739904, dtype=float32),\n",
       "  'AUC_score': 0.9648328507501973,\n",
       "  'highinfo F1': array(0.9166435, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9594453348198732,\n",
       "  'lowinfo F1': array(0.97220075, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9969135802469136},\n",
       " {'Train Loss': 0.003153004217892885,\n",
       "  'F1': array(0.92739904, dtype=float32),\n",
       "  'AUC_score': 0.9652013687812583,\n",
       "  'highinfo F1': array(0.912462, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9611176921468887,\n",
       "  'lowinfo F1': array(0.97220075, dtype=float32),\n",
       "  'lowinfo AUC_score': 1.0},\n",
       " {'Train Loss': 0.0028004792984575033,\n",
       "  'F1': array(0.92739904, dtype=float32),\n",
       "  'AUC_score': 0.9658331139773624,\n",
       "  'highinfo F1': array(0.90827596, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9599331057069194,\n",
       "  'lowinfo F1': array(0.97220075, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9969135802469136},\n",
       " {'Train Loss': 0.005912263412028551,\n",
       "  'F1': array(0.923792, dtype=float32),\n",
       "  'AUC_score': 0.9650960779152409,\n",
       "  'highinfo F1': array(0.90412503, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9600027872622117,\n",
       "  'lowinfo F1': array(0.97220075, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9969135802469136},\n",
       " {'Train Loss': 0.008036197163164616,\n",
       "  'F1': array(0.92739904, dtype=float32),\n",
       "  'AUC_score': 0.9653593050802843,\n",
       "  'highinfo F1': array(0.90412503, dtype=float32),\n",
       "  'highinfo AUC_score': 0.961187373702181,\n",
       "  'lowinfo F1': array(0.97220075, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9969135802469136},\n",
       " {'Train Loss': 0.003360648872330785,\n",
       "  'F1': array(0.9310499, dtype=float32),\n",
       "  'AUC_score': 0.9659384048433798,\n",
       "  'highinfo F1': array(0.90412503, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9605602397045503,\n",
       "  'lowinfo F1': array(0.97220075, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9969135802469136},\n",
       " {'Train Loss': 0.003204064443707466,\n",
       "  'F1': array(0.9310499, dtype=float32),\n",
       "  'AUC_score': 0.9646222690181626,\n",
       "  'highinfo F1': array(0.89993745, dtype=float32),\n",
       "  'highinfo AUC_score': 0.959166608598704,\n",
       "  'lowinfo F1': array(0.97220075, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9938271604938271},\n",
       " {'Train Loss': 0.003456573700532317,\n",
       "  'F1': array(0.927441, dtype=float32),\n",
       "  'AUC_score': 0.9635167149249803,\n",
       "  'highinfo F1': array(0.90412503, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9588878823775346,\n",
       "  'lowinfo F1': array(0.97220075, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9907407407407407},\n",
       " {'Train Loss': 0.002770071616396308,\n",
       "  'F1': array(0.923792, dtype=float32),\n",
       "  'AUC_score': 0.963569360357989,\n",
       "  'highinfo F1': array(0.89988875, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9559612570552576,\n",
       "  'lowinfo F1': array(0.97220075, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9907407407407407},\n",
       " {'Train Loss': 0.004185761325061321,\n",
       "  'F1': array(0.9201851, dtype=float32),\n",
       "  'AUC_score': 0.9648328507501974,\n",
       "  'highinfo F1': array(0.8957447, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9566580726081806,\n",
       "  'lowinfo F1': array(0.97220075, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9907407407407407},\n",
       " {'Train Loss': 0.002805946161970496,\n",
       "  'F1': array(0.92739904, dtype=float32),\n",
       "  'AUC_score': 0.9660963411424058,\n",
       "  'highinfo F1': array(0.8957447, dtype=float32),\n",
       "  'highinfo AUC_score': 0.957633614382273,\n",
       "  'lowinfo F1': array(0.97220075, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9938271604938271},\n",
       " {'Train Loss': 0.0024452172219753265,\n",
       "  'F1': array(0.9201851, dtype=float32),\n",
       "  'AUC_score': 0.9639378783890497,\n",
       "  'highinfo F1': array(0.8957447, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9582607483799039,\n",
       "  'lowinfo F1': array(0.97220075, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9907407407407407},\n",
       " {'Train Loss': 0.003370339283719659,\n",
       "  'F1': array(0.92739904, dtype=float32),\n",
       "  'AUC_score': 0.9636220057909975,\n",
       "  'highinfo F1': array(0.90403175, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9590969270434115,\n",
       "  'lowinfo F1': array(0.97220075, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9907407407407407},\n",
       " {'Train Loss': 0.0022447367664426565,\n",
       "  'F1': array(0.91657794, dtype=float32),\n",
       "  'AUC_score': 0.9625164516978152,\n",
       "  'highinfo F1': array(0.8957447, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9568671172740575,\n",
       "  'lowinfo F1': array(0.94427246, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9938271604938271},\n",
       " {'Train Loss': 0.003910401836037636,\n",
       "  'F1': array(0.923792, dtype=float32),\n",
       "  'AUC_score': 0.9632008423269282,\n",
       "  'highinfo F1': array(0.9081739, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9558915754999651,\n",
       "  'lowinfo F1': array(0.97220075, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9938271604938271},\n",
       " {'Train Loss': 0.005742526613175869,\n",
       "  'F1': array(0.923792, dtype=float32),\n",
       "  'AUC_score': 0.9628323242958673,\n",
       "  'highinfo F1': array(0.9081739, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9561703017211344,\n",
       "  'lowinfo F1': array(0.97220075, dtype=float32),\n",
       "  'lowinfo AUC_score': 0.9938271604938271},\n",
       " {'Train Loss': 0.002912016585469246,\n",
       "  'F1': array(0.9201851, dtype=float32),\n",
       "  'AUC_score': 0.9612529613056067,\n",
       "  'highinfo F1': array(0.89988875, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9555431677235036,\n",
       "  'lowinfo F1': array(1., dtype=float32),\n",
       "  'lowinfo AUC_score': 1.0},\n",
       " {'Train Loss': 0.00437534274533391,\n",
       "  'F1': array(0.91657794, dtype=float32),\n",
       "  'AUC_score': 0.9617267702026849,\n",
       "  'highinfo F1': array(0.89988875, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9555431677235036,\n",
       "  'lowinfo F1': array(1., dtype=float32),\n",
       "  'lowinfo AUC_score': 1.0},\n",
       " {'Train Loss': 0.0037010263185948133,\n",
       "  'F1': array(0.91657794, dtype=float32),\n",
       "  'AUC_score': 0.9619899973677283,\n",
       "  'highinfo F1': array(0.90403175, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9558915754999652,\n",
       "  'lowinfo F1': array(1., dtype=float32),\n",
       "  'lowinfo AUC_score': 1.0},\n",
       " {'Train Loss': 0.0036875703372061253,\n",
       "  'F1': array(0.9201851, dtype=float32),\n",
       "  'AUC_score': 0.964043169255067,\n",
       "  'highinfo F1': array(0.9081739, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9567974357187654,\n",
       "  'lowinfo F1': array(1., dtype=float32),\n",
       "  'lowinfo AUC_score': 1.0},\n",
       " {'Train Loss': 0.0055754822678864,\n",
       "  'F1': array(0.91657794, dtype=float32),\n",
       "  'AUC_score': 0.9634640694919715,\n",
       "  'highinfo F1': array(0.90403175, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9555431677235036,\n",
       "  'lowinfo F1': array(1., dtype=float32),\n",
       "  'lowinfo AUC_score': 1.0},\n",
       " {'Train Loss': 0.0019873660057783127,\n",
       "  'F1': array(0.91657794, dtype=float32),\n",
       "  'AUC_score': 0.9657278231113451,\n",
       "  'highinfo F1': array(0.9081739, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9574245697163961,\n",
       "  'lowinfo F1': array(1., dtype=float32),\n",
       "  'lowinfo AUC_score': 1.0},\n",
       " {'Train Loss': 0.0038901944644749165,\n",
       "  'F1': array(0.9201851, dtype=float32),\n",
       "  'AUC_score': 0.9641484601210845,\n",
       "  'highinfo F1': array(0.9081739, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9561703017211345,\n",
       "  'lowinfo F1': array(1., dtype=float32),\n",
       "  'lowinfo AUC_score': 1.0},\n",
       " {'Train Loss': 0.00410675210878253,\n",
       "  'F1': array(0.923792, dtype=float32),\n",
       "  'AUC_score': 0.9630955514609109,\n",
       "  'highinfo F1': array(0.9081739, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9553341230576267,\n",
       "  'lowinfo F1': array(1., dtype=float32),\n",
       "  'lowinfo AUC_score': 1.0},\n",
       " {'Train Loss': 0.0024050662759691477,\n",
       "  'F1': array(0.9201851, dtype=float32),\n",
       "  'AUC_score': 0.9640431692550671,\n",
       "  'highinfo F1': array(0.9081739, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9561703017211345,\n",
       "  'lowinfo F1': array(1., dtype=float32),\n",
       "  'lowinfo AUC_score': 1.0},\n",
       " {'Train Loss': 0.0031123084481805563,\n",
       "  'F1': array(0.9201851, dtype=float32),\n",
       "  'AUC_score': 0.9644116872861279,\n",
       "  'highinfo F1': array(0.9081739, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9568671172740576,\n",
       "  'lowinfo F1': array(1., dtype=float32),\n",
       "  'lowinfo AUC_score': 1.0},\n",
       " {'Train Loss': 0.0023847264237701893,\n",
       "  'F1': array(0.91653407, dtype=float32),\n",
       "  'AUC_score': 0.9645169781521451,\n",
       "  'highinfo F1': array(0.90396494, dtype=float32),\n",
       "  'highinfo AUC_score': 0.9572852066058115,\n",
       "  'lowinfo F1': array(1., dtype=float32),\n",
       "  'lowinfo AUC_score': 1.0}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a2e95a1-0cf5-45b4-8752-b862b6d5971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, np.generic):\n",
    "            return obj.item()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "# 保存为 JSON 文件\n",
    "with open('GNN/results_GP_data_2.json', 'w') as f:\n",
    "    json.dump(results, f, cls=NumpyEncoder, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e990c227-7c76-40af-8f9d-b3ae3126dc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 0.0167, Macro_F1: 0.9130, AUC_score: 0.9664\n",
      "Validation loss decreased (0.016663 --> 0.016663).\n",
      "Validation loss decreased (0.012255 --> 0.012255).\n",
      "Validation loss decreased (0.011488 --> 0.011488).\n",
      "Validation loss decreased (0.010566 --> 0.010566).\n",
      "Validation loss decreased (0.009436 --> 0.009436).\n",
      "Epoch 50: Train Loss: 0.0095, Macro_F1: 0.9203, AUC_score: 0.9721\n",
      "Validation loss decreased (0.008950 --> 0.008950).\n",
      "Validation loss decreased (0.007095 --> 0.007095).\n",
      "Epoch 100: Train Loss: 0.0102, Macro_F1: 0.9094, AUC_score: 0.9687\n",
      "Epoch 150: Train Loss: 0.0117, Macro_F1: 0.9275, AUC_score: 0.9722\n",
      "Epoch 00196: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0110, Macro_F1: 0.9239, AUC_score: 0.9698\n",
      "Epoch 250: Train Loss: 0.0155, Macro_F1: 0.9239, AUC_score: 0.9703\n",
      "Epoch 00297: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 300: Train Loss: 0.0150, Macro_F1: 0.9130, AUC_score: 0.9701\n",
      "Epoch 350: Train Loss: 0.0107, Macro_F1: 0.9167, AUC_score: 0.9704\n",
      "Early stopping triggered\n",
      "acc save\n",
      "79.0% node features transform to 0: F1: 0.9130, AUC_score: 0.9706\n",
      "Epoch 0: Train Loss: 0.0100, Macro_F1: 0.8983, AUC_score: 0.9678\n",
      "Validation loss decreased (0.010001 --> 0.010001).\n",
      "Validation loss decreased (0.008972 --> 0.008972).\n",
      "Validation loss decreased (0.008546 --> 0.008546).\n",
      "Validation loss decreased (0.008086 --> 0.008086).\n",
      "Epoch 50: Train Loss: 0.0235, Macro_F1: 0.9094, AUC_score: 0.9706\n",
      "Validation loss decreased (0.007583 --> 0.007583).\n",
      "Validation loss decreased (0.007049 --> 0.007049).\n",
      "Epoch 100: Train Loss: 0.0092, Macro_F1: 0.9094, AUC_score: 0.9678\n",
      "Epoch 150: Train Loss: 0.0103, Macro_F1: 0.9203, AUC_score: 0.9705\n",
      "Epoch 00181: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0135, Macro_F1: 0.9166, AUC_score: 0.9717\n",
      "Epoch 250: Train Loss: 0.0073, Macro_F1: 0.9130, AUC_score: 0.9724\n",
      "Validation loss decreased (0.006798 --> 0.006798).\n",
      "Validation loss decreased (0.006205 --> 0.006205).\n",
      "Epoch 300: Train Loss: 0.0117, Macro_F1: 0.9130, AUC_score: 0.9720\n",
      "Epoch 350: Train Loss: 0.0090, Macro_F1: 0.9167, AUC_score: 0.9714\n",
      "Epoch 00401: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0094, Macro_F1: 0.9167, AUC_score: 0.9716\n",
      "Epoch 450: Train Loss: 0.0096, Macro_F1: 0.9130, AUC_score: 0.9720\n",
      "Epoch 500: Train Loss: 0.0185, Macro_F1: 0.9130, AUC_score: 0.9712\n",
      "Epoch 00502: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Validation loss decreased (0.005877 --> 0.005877).\n",
      "Epoch 550: Train Loss: 0.0081, Macro_F1: 0.9130, AUC_score: 0.9713\n",
      "Epoch 600: Train Loss: 0.0087, Macro_F1: 0.9167, AUC_score: 0.9714\n",
      "Epoch 00634: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 650: Train Loss: 0.0077, Macro_F1: 0.9167, AUC_score: 0.9716\n",
      "Epoch 700: Train Loss: 0.0110, Macro_F1: 0.9167, AUC_score: 0.9716\n",
      "Epoch 00735: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 750: Train Loss: 0.0103, Macro_F1: 0.9167, AUC_score: 0.9717\n",
      "Epoch 800: Train Loss: 0.0093, Macro_F1: 0.9167, AUC_score: 0.9716\n",
      "Early stopping triggered\n",
      "acc save\n",
      "78.0% node features transform to 0: F1: 0.9167, AUC_score: 0.9716\n",
      "Epoch 0: Train Loss: 0.0079, Macro_F1: 0.9239, AUC_score: 0.9741\n",
      "Validation loss decreased (0.007880 --> 0.007880).\n",
      "Validation loss decreased (0.007609 --> 0.007609).\n",
      "Epoch 50: Train Loss: 0.0116, Macro_F1: 0.9130, AUC_score: 0.9703\n",
      "Validation loss decreased (0.007117 --> 0.007117).\n",
      "Epoch 100: Train Loss: 0.0221, Macro_F1: 0.9275, AUC_score: 0.9730\n",
      "Validation loss decreased (0.006889 --> 0.006889).\n",
      "Validation loss decreased (0.006286 --> 0.006286).\n",
      "Epoch 150: Train Loss: 0.0163, Macro_F1: 0.9058, AUC_score: 0.9725\n",
      "Validation loss decreased (0.005873 --> 0.005873).\n",
      "Epoch 200: Train Loss: 0.0098, Macro_F1: 0.9094, AUC_score: 0.9718\n",
      "Epoch 250: Train Loss: 0.0075, Macro_F1: 0.9094, AUC_score: 0.9694\n",
      "Epoch 00280: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.0136, Macro_F1: 0.9203, AUC_score: 0.9714\n",
      "Validation loss decreased (0.005671 --> 0.005671).\n",
      "Epoch 350: Train Loss: 0.0103, Macro_F1: 0.9094, AUC_score: 0.9715\n",
      "Epoch 400: Train Loss: 0.0098, Macro_F1: 0.9130, AUC_score: 0.9712\n",
      "Epoch 00448: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.005507 --> 0.005507).\n",
      "Epoch 450: Train Loss: 0.0070, Macro_F1: 0.9094, AUC_score: 0.9714\n",
      "Epoch 500: Train Loss: 0.0187, Macro_F1: 0.9094, AUC_score: 0.9715\n",
      "Validation loss decreased (0.005257 --> 0.005257).\n",
      "Epoch 550: Train Loss: 0.0095, Macro_F1: 0.9094, AUC_score: 0.9719\n",
      "Epoch 600: Train Loss: 0.0074, Macro_F1: 0.9094, AUC_score: 0.9720\n",
      "Epoch 00651: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 650: Train Loss: 0.0109, Macro_F1: 0.9130, AUC_score: 0.9717\n",
      "Epoch 700: Train Loss: 0.0111, Macro_F1: 0.9094, AUC_score: 0.9717\n",
      "Epoch 750: Train Loss: 0.0079, Macro_F1: 0.9130, AUC_score: 0.9718\n",
      "Epoch 00752: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 800: Train Loss: 0.0078, Macro_F1: 0.9130, AUC_score: 0.9718\n",
      "Early stopping triggered\n",
      "acc save\n",
      "77.0% node features transform to 0: F1: 0.9130, AUC_score: 0.9718\n",
      "Epoch 0: Train Loss: 0.0138, Macro_F1: 0.8797, AUC_score: 0.9691\n",
      "Validation loss decreased (0.013838 --> 0.013838).\n",
      "Validation loss decreased (0.010861 --> 0.010861).\n",
      "Validation loss decreased (0.010482 --> 0.010482).\n",
      "Validation loss decreased (0.009081 --> 0.009081).\n",
      "Validation loss decreased (0.006782 --> 0.006782).\n",
      "Validation loss decreased (0.006588 --> 0.006588).\n",
      "Epoch 50: Train Loss: 0.0112, Macro_F1: 0.9167, AUC_score: 0.9726\n",
      "Validation loss decreased (0.006437 --> 0.006437).\n",
      "Validation loss decreased (0.005461 --> 0.005461).\n",
      "Epoch 100: Train Loss: 0.0314, Macro_F1: 0.9203, AUC_score: 0.9740\n",
      "Epoch 150: Train Loss: 0.0100, Macro_F1: 0.9203, AUC_score: 0.9730\n",
      "Epoch 00200: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0093, Macro_F1: 0.9203, AUC_score: 0.9730\n",
      "Epoch 250: Train Loss: 0.0082, Macro_F1: 0.9203, AUC_score: 0.9728\n",
      "Epoch 00301: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 300: Train Loss: 0.0111, Macro_F1: 0.9203, AUC_score: 0.9738\n",
      "Epoch 350: Train Loss: 0.0144, Macro_F1: 0.9203, AUC_score: 0.9735\n",
      "Early stopping triggered\n",
      "acc save\n",
      "76.0% node features transform to 0: F1: 0.9203, AUC_score: 0.9730\n",
      "Epoch 0: Train Loss: 0.0176, Macro_F1: 0.9200, AUC_score: 0.9731\n",
      "Validation loss decreased (0.017599 --> 0.017599).\n",
      "Validation loss decreased (0.015876 --> 0.015876).\n",
      "Validation loss decreased (0.012423 --> 0.012423).\n",
      "Validation loss decreased (0.010767 --> 0.010767).\n",
      "Validation loss decreased (0.009519 --> 0.009519).\n",
      "Validation loss decreased (0.009481 --> 0.009481).\n",
      "Validation loss decreased (0.009031 --> 0.009031).\n",
      "Validation loss decreased (0.008322 --> 0.008322).\n",
      "Validation loss decreased (0.008080 --> 0.008080).\n",
      "Epoch 50: Train Loss: 0.0159, Macro_F1: 0.9094, AUC_score: 0.9725\n",
      "Validation loss decreased (0.007161 --> 0.007161).\n",
      "Validation loss decreased (0.007074 --> 0.007074).\n",
      "Validation loss decreased (0.006337 --> 0.006337).\n",
      "Epoch 100: Train Loss: 0.0093, Macro_F1: 0.9057, AUC_score: 0.9722\n",
      "Validation loss decreased (0.005993 --> 0.005993).\n",
      "Epoch 150: Train Loss: 0.0139, Macro_F1: 0.9058, AUC_score: 0.9719\n",
      "Epoch 200: Train Loss: 0.0063, Macro_F1: 0.9275, AUC_score: 0.9712\n",
      "Epoch 00203: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.005655 --> 0.005655).\n",
      "Epoch 250: Train Loss: 0.0132, Macro_F1: 0.9058, AUC_score: 0.9721\n",
      "Epoch 300: Train Loss: 0.0074, Macro_F1: 0.9094, AUC_score: 0.9722\n",
      "Epoch 00326: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.0075, Macro_F1: 0.9094, AUC_score: 0.9720\n",
      "Epoch 400: Train Loss: 0.0076, Macro_F1: 0.9058, AUC_score: 0.9719\n",
      "Epoch 00427: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 450: Train Loss: 0.0120, Macro_F1: 0.9058, AUC_score: 0.9721\n",
      "Epoch 500: Train Loss: 0.0080, Macro_F1: 0.9058, AUC_score: 0.9721\n",
      "Validation loss decreased (0.005272 --> 0.005272).\n",
      "Validation loss decreased (0.005053 --> 0.005053).\n",
      "Epoch 550: Train Loss: 0.0088, Macro_F1: 0.9021, AUC_score: 0.9719\n",
      "Epoch 600: Train Loss: 0.0090, Macro_F1: 0.9058, AUC_score: 0.9718\n",
      "Epoch 00646: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 650: Train Loss: 0.0072, Macro_F1: 0.9058, AUC_score: 0.9717\n",
      "Epoch 700: Train Loss: 0.0107, Macro_F1: 0.9058, AUC_score: 0.9719\n",
      "Epoch 00747: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 750: Train Loss: 0.0074, Macro_F1: 0.9058, AUC_score: 0.9719\n",
      "Epoch 800: Train Loss: 0.0139, Macro_F1: 0.9058, AUC_score: 0.9718\n",
      "Early stopping triggered\n",
      "acc save\n",
      "75.0% node features transform to 0: F1: 0.9058, AUC_score: 0.9718\n",
      "Epoch 0: Train Loss: 0.0111, Macro_F1: 0.9200, AUC_score: 0.9713\n",
      "Validation loss decreased (0.011112 --> 0.011112).\n",
      "Validation loss decreased (0.007685 --> 0.007685).\n",
      "Validation loss decreased (0.007271 --> 0.007271).\n",
      "Validation loss decreased (0.006966 --> 0.006966).\n",
      "Validation loss decreased (0.006026 --> 0.006026).\n",
      "Epoch 50: Train Loss: 0.0078, Macro_F1: 0.9130, AUC_score: 0.9709\n",
      "Epoch 100: Train Loss: 0.0089, Macro_F1: 0.9347, AUC_score: 0.9726\n",
      "Epoch 00134: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.005886 --> 0.005886).\n",
      "Epoch 150: Train Loss: 0.0136, Macro_F1: 0.9094, AUC_score: 0.9713\n",
      "Validation loss decreased (0.005576 --> 0.005576).\n",
      "Epoch 200: Train Loss: 0.0150, Macro_F1: 0.9130, AUC_score: 0.9712\n",
      "Validation loss decreased (0.005114 --> 0.005114).\n",
      "Epoch 250: Train Loss: 0.0077, Macro_F1: 0.9094, AUC_score: 0.9704\n",
      "Epoch 300: Train Loss: 0.0061, Macro_F1: 0.9130, AUC_score: 0.9709\n",
      "Epoch 00338: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.0122, Macro_F1: 0.9130, AUC_score: 0.9706\n",
      "Epoch 400: Train Loss: 0.0072, Macro_F1: 0.9130, AUC_score: 0.9711\n",
      "Epoch 00439: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 450: Train Loss: 0.0121, Macro_F1: 0.9130, AUC_score: 0.9710\n",
      "Epoch 500: Train Loss: 0.0081, Macro_F1: 0.9130, AUC_score: 0.9711\n",
      "Early stopping triggered\n",
      "acc save\n",
      "74.0% node features transform to 0: F1: 0.9130, AUC_score: 0.9711\n",
      "Epoch 0: Train Loss: 0.0113, Macro_F1: 0.8797, AUC_score: 0.9650\n",
      "Validation loss decreased (0.011345 --> 0.011345).\n",
      "Validation loss decreased (0.006359 --> 0.006359).\n",
      "Validation loss decreased (0.006324 --> 0.006324).\n",
      "Epoch 50: Train Loss: 0.0081, Macro_F1: 0.9021, AUC_score: 0.9672\n",
      "Validation loss decreased (0.005644 --> 0.005644).\n",
      "Epoch 100: Train Loss: 0.0066, Macro_F1: 0.9203, AUC_score: 0.9683\n",
      "Epoch 150: Train Loss: 0.0097, Macro_F1: 0.9130, AUC_score: 0.9669\n",
      "Epoch 00180: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0067, Macro_F1: 0.9130, AUC_score: 0.9686\n",
      "Epoch 250: Train Loss: 0.0103, Macro_F1: 0.9203, AUC_score: 0.9677\n",
      "Epoch 00281: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.005476 --> 0.005476).\n",
      "Epoch 300: Train Loss: 0.0071, Macro_F1: 0.9094, AUC_score: 0.9680\n",
      "Validation loss decreased (0.005123 --> 0.005123).\n",
      "Epoch 350: Train Loss: 0.0072, Macro_F1: 0.9167, AUC_score: 0.9680\n",
      "Epoch 400: Train Loss: 0.0076, Macro_F1: 0.9167, AUC_score: 0.9678\n",
      "Epoch 00442: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 450: Train Loss: 0.0139, Macro_F1: 0.9167, AUC_score: 0.9680\n",
      "Validation loss decreased (0.004956 --> 0.004956).\n",
      "Epoch 500: Train Loss: 0.0074, Macro_F1: 0.9130, AUC_score: 0.9679\n",
      "Epoch 550: Train Loss: 0.0107, Macro_F1: 0.9167, AUC_score: 0.9678\n",
      "Epoch 00566: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 600: Train Loss: 0.0063, Macro_F1: 0.9167, AUC_score: 0.9679\n",
      "Epoch 650: Train Loss: 0.0104, Macro_F1: 0.9167, AUC_score: 0.9679\n",
      "Epoch 00667: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 700: Train Loss: 0.0074, Macro_F1: 0.9167, AUC_score: 0.9678\n",
      "Epoch 750: Train Loss: 0.0069, Macro_F1: 0.9167, AUC_score: 0.9678\n",
      "Early stopping triggered\n",
      "acc save\n",
      "73.0% node features transform to 0: F1: 0.9167, AUC_score: 0.9678\n",
      "Epoch 0: Train Loss: 0.0074, Macro_F1: 0.8760, AUC_score: 0.9567\n",
      "Validation loss decreased (0.007443 --> 0.007443).\n",
      "Validation loss decreased (0.006376 --> 0.006376).\n",
      "Validation loss decreased (0.006144 --> 0.006144).\n",
      "Epoch 50: Train Loss: 0.0069, Macro_F1: 0.9130, AUC_score: 0.9583\n",
      "Validation loss decreased (0.005453 --> 0.005453).\n",
      "Validation loss decreased (0.005375 --> 0.005375).\n",
      "Epoch 100: Train Loss: 0.0239, Macro_F1: 0.9167, AUC_score: 0.9592\n",
      "Epoch 150: Train Loss: 0.0183, Macro_F1: 0.8985, AUC_score: 0.9598\n",
      "Epoch 00192: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0122, Macro_F1: 0.9203, AUC_score: 0.9605\n",
      "Validation loss decreased (0.005354 --> 0.005354).\n",
      "Epoch 250: Train Loss: 0.0176, Macro_F1: 0.8985, AUC_score: 0.9587\n",
      "Validation loss decreased (0.005209 --> 0.005209).\n",
      "Epoch 300: Train Loss: 0.0131, Macro_F1: 0.9058, AUC_score: 0.9594\n",
      "Epoch 350: Train Loss: 0.0059, Macro_F1: 0.9021, AUC_score: 0.9604\n",
      "Validation loss decreased (0.004806 --> 0.004806).\n",
      "Epoch 400: Train Loss: 0.0125, Macro_F1: 0.9203, AUC_score: 0.9592\n",
      "Epoch 450: Train Loss: 0.0060, Macro_F1: 0.9130, AUC_score: 0.9608\n",
      "Epoch 00479: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.0173, Macro_F1: 0.9130, AUC_score: 0.9600\n",
      "Epoch 550: Train Loss: 0.0118, Macro_F1: 0.9130, AUC_score: 0.9598\n",
      "Epoch 00580: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 600: Train Loss: 0.0128, Macro_F1: 0.9203, AUC_score: 0.9600\n",
      "Epoch 650: Train Loss: 0.0061, Macro_F1: 0.9130, AUC_score: 0.9600\n",
      "Early stopping triggered\n",
      "acc save\n",
      "72.0% node features transform to 0: F1: 0.9130, AUC_score: 0.9600\n",
      "Epoch 0: Train Loss: 0.0223, Macro_F1: 0.9056, AUC_score: 0.9516\n",
      "Validation loss decreased (0.022301 --> 0.022301).\n",
      "Validation loss decreased (0.017329 --> 0.017329).\n",
      "Validation loss decreased (0.012379 --> 0.012379).\n",
      "Validation loss decreased (0.009463 --> 0.009463).\n",
      "Validation loss decreased (0.008425 --> 0.008425).\n",
      "Epoch 50: Train Loss: 0.0156, Macro_F1: 0.9203, AUC_score: 0.9554\n",
      "Validation loss decreased (0.008111 --> 0.008111).\n",
      "Validation loss decreased (0.007631 --> 0.007631).\n",
      "Validation loss decreased (0.007087 --> 0.007087).\n",
      "Epoch 100: Train Loss: 0.0104, Macro_F1: 0.9058, AUC_score: 0.9533\n",
      "Validation loss decreased (0.006970 --> 0.006970).\n",
      "Validation loss decreased (0.005898 --> 0.005898).\n",
      "Validation loss decreased (0.005585 --> 0.005585).\n",
      "Epoch 150: Train Loss: 0.0093, Macro_F1: 0.9203, AUC_score: 0.9553\n",
      "Epoch 200: Train Loss: 0.0108, Macro_F1: 0.8985, AUC_score: 0.9568\n",
      "Epoch 00236: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0147, Macro_F1: 0.9203, AUC_score: 0.9545\n",
      "Validation loss decreased (0.005453 --> 0.005453).\n",
      "Epoch 300: Train Loss: 0.0081, Macro_F1: 0.9130, AUC_score: 0.9559\n",
      "Validation loss decreased (0.005259 --> 0.005259).\n",
      "Epoch 350: Train Loss: 0.0179, Macro_F1: 0.9130, AUC_score: 0.9560\n",
      "Epoch 400: Train Loss: 0.0274, Macro_F1: 0.9203, AUC_score: 0.9565\n",
      "Epoch 00418: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.0136, Macro_F1: 0.9167, AUC_score: 0.9562\n",
      "Epoch 500: Train Loss: 0.0107, Macro_F1: 0.9167, AUC_score: 0.9563\n",
      "Validation loss decreased (0.004859 --> 0.004859).\n",
      "Epoch 550: Train Loss: 0.0066, Macro_F1: 0.9167, AUC_score: 0.9564\n",
      "Epoch 600: Train Loss: 0.0184, Macro_F1: 0.9167, AUC_score: 0.9562\n",
      "Epoch 00615: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 650: Train Loss: 0.0067, Macro_F1: 0.9130, AUC_score: 0.9562\n",
      "Epoch 700: Train Loss: 0.0068, Macro_F1: 0.9130, AUC_score: 0.9563\n",
      "Epoch 00716: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 750: Train Loss: 0.0097, Macro_F1: 0.9130, AUC_score: 0.9562\n",
      "Epoch 800: Train Loss: 0.0072, Macro_F1: 0.9130, AUC_score: 0.9562\n",
      "Early stopping triggered\n",
      "acc save\n",
      "71.0% node features transform to 0: F1: 0.9130, AUC_score: 0.9562\n",
      "Epoch 0: Train Loss: 0.0126, Macro_F1: 0.8837, AUC_score: 0.9558\n",
      "Validation loss decreased (0.012607 --> 0.012607).\n",
      "Validation loss decreased (0.010950 --> 0.010950).\n",
      "Validation loss decreased (0.009988 --> 0.009988).\n",
      "Validation loss decreased (0.008320 --> 0.008320).\n",
      "Validation loss decreased (0.007306 --> 0.007306).\n",
      "Epoch 50: Train Loss: 0.0180, Macro_F1: 0.8911, AUC_score: 0.9582\n",
      "Validation loss decreased (0.006857 --> 0.006857).\n",
      "Validation loss decreased (0.005393 --> 0.005393).\n",
      "Epoch 100: Train Loss: 0.0124, Macro_F1: 0.9058, AUC_score: 0.9555\n",
      "Validation loss decreased (0.005355 --> 0.005355).\n",
      "Epoch 150: Train Loss: 0.0072, Macro_F1: 0.9130, AUC_score: 0.9569\n",
      "Epoch 200: Train Loss: 0.0093, Macro_F1: 0.9094, AUC_score: 0.9576\n",
      "Epoch 00210: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0074, Macro_F1: 0.9094, AUC_score: 0.9562\n",
      "Validation loss decreased (0.005041 --> 0.005041).\n",
      "Validation loss decreased (0.005021 --> 0.005021).\n",
      "Epoch 300: Train Loss: 0.0076, Macro_F1: 0.9130, AUC_score: 0.9561\n",
      "Epoch 350: Train Loss: 0.0064, Macro_F1: 0.9130, AUC_score: 0.9567\n",
      "Validation loss decreased (0.004935 --> 0.004935).\n",
      "Epoch 400: Train Loss: 0.0085, Macro_F1: 0.9130, AUC_score: 0.9565\n",
      "Epoch 450: Train Loss: 0.0091, Macro_F1: 0.9130, AUC_score: 0.9566\n",
      "Epoch 00467: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.004907 --> 0.004907).\n",
      "Epoch 500: Train Loss: 0.0105, Macro_F1: 0.9130, AUC_score: 0.9573\n",
      "Epoch 550: Train Loss: 0.0063, Macro_F1: 0.9094, AUC_score: 0.9573\n",
      "Validation loss decreased (0.004480 --> 0.004480).\n",
      "Epoch 600: Train Loss: 0.0062, Macro_F1: 0.9094, AUC_score: 0.9573\n",
      "Epoch 650: Train Loss: 0.0088, Macro_F1: 0.9130, AUC_score: 0.9574\n",
      "Epoch 00667: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 700: Train Loss: 0.0093, Macro_F1: 0.9130, AUC_score: 0.9570\n",
      "Epoch 750: Train Loss: 0.0070, Macro_F1: 0.9130, AUC_score: 0.9570\n",
      "Epoch 00768: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 800: Train Loss: 0.0072, Macro_F1: 0.9130, AUC_score: 0.9571\n",
      "Epoch 850: Train Loss: 0.0162, Macro_F1: 0.9130, AUC_score: 0.9572\n",
      "Early stopping triggered\n",
      "acc save\n",
      "70.0% node features transform to 0: F1: 0.9130, AUC_score: 0.9572\n",
      "Epoch 0: Train Loss: 0.0134, Macro_F1: 0.9203, AUC_score: 0.9578\n",
      "Validation loss decreased (0.013406 --> 0.013406).\n",
      "Validation loss decreased (0.011149 --> 0.011149).\n",
      "Validation loss decreased (0.009268 --> 0.009268).\n",
      "Validation loss decreased (0.007952 --> 0.007952).\n",
      "Validation loss decreased (0.007897 --> 0.007897).\n",
      "Validation loss decreased (0.006628 --> 0.006628).\n",
      "Epoch 50: Train Loss: 0.0106, Macro_F1: 0.8948, AUC_score: 0.9549\n",
      "Validation loss decreased (0.006250 --> 0.006250).\n",
      "Validation loss decreased (0.006108 --> 0.006108).\n",
      "Epoch 100: Train Loss: 0.0125, Macro_F1: 0.9239, AUC_score: 0.9550\n",
      "Epoch 150: Train Loss: 0.0139, Macro_F1: 0.9167, AUC_score: 0.9557\n",
      "Epoch 00175: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0071, Macro_F1: 0.9203, AUC_score: 0.9560\n",
      "Validation loss decreased (0.005998 --> 0.005998).\n",
      "Epoch 250: Train Loss: 0.0093, Macro_F1: 0.9094, AUC_score: 0.9562\n",
      "Validation loss decreased (0.005485 --> 0.005485).\n",
      "Validation loss decreased (0.005081 --> 0.005081).\n",
      "Validation loss decreased (0.005064 --> 0.005064).\n",
      "Epoch 300: Train Loss: 0.0070, Macro_F1: 0.9130, AUC_score: 0.9555\n",
      "Epoch 350: Train Loss: 0.0124, Macro_F1: 0.9130, AUC_score: 0.9562\n",
      "Epoch 00400: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0096, Macro_F1: 0.9203, AUC_score: 0.9559\n",
      "Validation loss decreased (0.004914 --> 0.004914).\n",
      "Validation loss decreased (0.004563 --> 0.004563).\n",
      "Epoch 450: Train Loss: 0.0054, Macro_F1: 0.9094, AUC_score: 0.9558\n",
      "Epoch 500: Train Loss: 0.0183, Macro_F1: 0.9094, AUC_score: 0.9564\n",
      "Validation loss decreased (0.004418 --> 0.004418).\n",
      "Epoch 550: Train Loss: 0.0086, Macro_F1: 0.9094, AUC_score: 0.9559\n",
      "Validation loss decreased (0.004333 --> 0.004333).\n",
      "Epoch 600: Train Loss: 0.0063, Macro_F1: 0.9094, AUC_score: 0.9563\n",
      "Epoch 650: Train Loss: 0.0072, Macro_F1: 0.9167, AUC_score: 0.9563\n",
      "Epoch 00698: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 700: Train Loss: 0.0093, Macro_F1: 0.9094, AUC_score: 0.9560\n",
      "Epoch 750: Train Loss: 0.0058, Macro_F1: 0.9094, AUC_score: 0.9563\n",
      "Epoch 00799: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 800: Train Loss: 0.0084, Macro_F1: 0.9130, AUC_score: 0.9563\n",
      "Epoch 850: Train Loss: 0.0082, Macro_F1: 0.9094, AUC_score: 0.9563\n",
      "Early stopping triggered\n",
      "acc save\n",
      "69.0% node features transform to 0: F1: 0.9094, AUC_score: 0.9563\n",
      "Epoch 0: Train Loss: 0.0113, Macro_F1: 0.8609, AUC_score: 0.9533\n",
      "Validation loss decreased (0.011256 --> 0.011256).\n",
      "Validation loss decreased (0.006970 --> 0.006970).\n",
      "Validation loss decreased (0.006623 --> 0.006623).\n",
      "Validation loss decreased (0.005826 --> 0.005826).\n",
      "Epoch 50: Train Loss: 0.0143, Macro_F1: 0.9167, AUC_score: 0.9549\n",
      "Epoch 100: Train Loss: 0.0102, Macro_F1: 0.9094, AUC_score: 0.9539\n",
      "Epoch 00134: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.0075, Macro_F1: 0.9130, AUC_score: 0.9553\n",
      "Epoch 200: Train Loss: 0.0099, Macro_F1: 0.9130, AUC_score: 0.9545\n",
      "Validation loss decreased (0.005221 --> 0.005221).\n",
      "Epoch 250: Train Loss: 0.0102, Macro_F1: 0.9167, AUC_score: 0.9554\n",
      "Validation loss decreased (0.005127 --> 0.005127).\n",
      "Epoch 300: Train Loss: 0.0079, Macro_F1: 0.9239, AUC_score: 0.9556\n",
      "Epoch 350: Train Loss: 0.0100, Macro_F1: 0.9167, AUC_score: 0.9553\n",
      "Epoch 00366: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0094, Macro_F1: 0.9094, AUC_score: 0.9554\n",
      "Validation loss decreased (0.005049 --> 0.005049).\n",
      "Validation loss decreased (0.004536 --> 0.004536).\n",
      "Epoch 450: Train Loss: 0.0048, Macro_F1: 0.9130, AUC_score: 0.9552\n",
      "Epoch 500: Train Loss: 0.0077, Macro_F1: 0.9130, AUC_score: 0.9552\n",
      "Epoch 00548: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0162, Macro_F1: 0.9239, AUC_score: 0.9557\n",
      "Epoch 600: Train Loss: 0.0062, Macro_F1: 0.9130, AUC_score: 0.9558\n",
      "Epoch 00649: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 650: Train Loss: 0.0143, Macro_F1: 0.9130, AUC_score: 0.9555\n",
      "Epoch 700: Train Loss: 0.0053, Macro_F1: 0.9130, AUC_score: 0.9555\n",
      "Early stopping triggered\n",
      "acc save\n",
      "68.0% node features transform to 0: F1: 0.9130, AUC_score: 0.9554\n",
      "Epoch 0: Train Loss: 0.0068, Macro_F1: 0.8723, AUC_score: 0.9568\n",
      "Validation loss decreased (0.006812 --> 0.006812).\n",
      "Validation loss decreased (0.006434 --> 0.006434).\n",
      "Validation loss decreased (0.006382 --> 0.006382).\n",
      "Validation loss decreased (0.006092 --> 0.006092).\n",
      "Epoch 50: Train Loss: 0.0119, Macro_F1: 0.9130, AUC_score: 0.9570\n",
      "Validation loss decreased (0.005812 --> 0.005812).\n",
      "Validation loss decreased (0.005254 --> 0.005254).\n",
      "Epoch 100: Train Loss: 0.0070, Macro_F1: 0.8982, AUC_score: 0.9577\n",
      "Epoch 150: Train Loss: 0.0072, Macro_F1: 0.9203, AUC_score: 0.9575\n",
      "Epoch 00181: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0067, Macro_F1: 0.9203, AUC_score: 0.9579\n",
      "Validation loss decreased (0.005086 --> 0.005086).\n",
      "Epoch 250: Train Loss: 0.0081, Macro_F1: 0.9203, AUC_score: 0.9588\n",
      "Epoch 300: Train Loss: 0.0067, Macro_F1: 0.9203, AUC_score: 0.9584\n",
      "Epoch 00317: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.0077, Macro_F1: 0.9203, AUC_score: 0.9578\n",
      "Validation loss decreased (0.005069 --> 0.005069).\n",
      "Epoch 400: Train Loss: 0.0165, Macro_F1: 0.9203, AUC_score: 0.9581\n",
      "Epoch 450: Train Loss: 0.0076, Macro_F1: 0.9239, AUC_score: 0.9583\n",
      "Epoch 00467: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 500: Train Loss: 0.0068, Macro_F1: 0.9203, AUC_score: 0.9582\n",
      "Validation loss decreased (0.004927 --> 0.004927).\n",
      "Validation loss decreased (0.004855 --> 0.004855).\n",
      "Epoch 550: Train Loss: 0.0072, Macro_F1: 0.9203, AUC_score: 0.9583\n",
      "Validation loss decreased (0.004804 --> 0.004804).\n",
      "Validation loss decreased (0.004778 --> 0.004778).\n",
      "Epoch 600: Train Loss: 0.0114, Macro_F1: 0.9203, AUC_score: 0.9581\n",
      "Epoch 650: Train Loss: 0.0063, Macro_F1: 0.9239, AUC_score: 0.9581\n",
      "Validation loss decreased (0.004420 --> 0.004420).\n",
      "Epoch 700: Train Loss: 0.0071, Macro_F1: 0.9203, AUC_score: 0.9583\n",
      "Epoch 750: Train Loss: 0.0089, Macro_F1: 0.9203, AUC_score: 0.9582\n",
      "Epoch 00783: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 800: Train Loss: 0.0060, Macro_F1: 0.9203, AUC_score: 0.9583\n",
      "Epoch 850: Train Loss: 0.0056, Macro_F1: 0.9203, AUC_score: 0.9583\n",
      "Epoch 00884: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 900: Train Loss: 0.0085, Macro_F1: 0.9203, AUC_score: 0.9583\n",
      "Epoch 950: Train Loss: 0.0071, Macro_F1: 0.9203, AUC_score: 0.9583\n",
      "Validation loss decreased (0.004225 --> 0.004225).\n",
      "Epoch 1000: Train Loss: 0.0076, Macro_F1: 0.9203, AUC_score: 0.9583\n",
      "Epoch 1050: Train Loss: 0.0070, Macro_F1: 0.9203, AUC_score: 0.9583\n",
      "Epoch 01075: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 1100: Train Loss: 0.0079, Macro_F1: 0.9203, AUC_score: 0.9583\n",
      "Epoch 1150: Train Loss: 0.0055, Macro_F1: 0.9203, AUC_score: 0.9583\n",
      "Epoch 01176: reducing learning rate of group 0 to 1.2800e-08.\n",
      "Epoch 1200: Train Loss: 0.0062, Macro_F1: 0.9203, AUC_score: 0.9583\n",
      "Epoch 1250: Train Loss: 0.0071, Macro_F1: 0.9203, AUC_score: 0.9583\n",
      "Early stopping triggered\n",
      "acc save\n",
      "67.0% node features transform to 0: F1: 0.9203, AUC_score: 0.9583\n",
      "Epoch 0: Train Loss: 0.0074, Macro_F1: 0.8833, AUC_score: 0.9577\n",
      "Validation loss decreased (0.007444 --> 0.007444).\n",
      "Validation loss decreased (0.007169 --> 0.007169).\n",
      "Validation loss decreased (0.006738 --> 0.006738).\n",
      "Validation loss decreased (0.006473 --> 0.006473).\n",
      "Validation loss decreased (0.006382 --> 0.006382).\n",
      "Validation loss decreased (0.005380 --> 0.005380).\n",
      "Epoch 50: Train Loss: 0.0058, Macro_F1: 0.9166, AUC_score: 0.9585\n",
      "Validation loss decreased (0.005124 --> 0.005124).\n",
      "Epoch 100: Train Loss: 0.0083, Macro_F1: 0.8946, AUC_score: 0.9597\n",
      "Epoch 150: Train Loss: 0.0096, Macro_F1: 0.9130, AUC_score: 0.9576\n",
      "Epoch 00169: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0115, Macro_F1: 0.9203, AUC_score: 0.9595\n",
      "Epoch 250: Train Loss: 0.0113, Macro_F1: 0.9203, AUC_score: 0.9594\n",
      "Epoch 00270: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 300: Train Loss: 0.0081, Macro_F1: 0.9203, AUC_score: 0.9593\n",
      "Validation loss decreased (0.004643 --> 0.004643).\n",
      "Epoch 350: Train Loss: 0.0074, Macro_F1: 0.9203, AUC_score: 0.9598\n",
      "Epoch 400: Train Loss: 0.0061, Macro_F1: 0.9203, AUC_score: 0.9599\n",
      "Epoch 00409: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 450: Train Loss: 0.0092, Macro_F1: 0.9203, AUC_score: 0.9599\n",
      "Epoch 500: Train Loss: 0.0092, Macro_F1: 0.9203, AUC_score: 0.9598\n",
      "Epoch 00510: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 550: Train Loss: 0.0054, Macro_F1: 0.9203, AUC_score: 0.9598\n",
      "Validation loss decreased (0.004201 --> 0.004201).\n",
      "Epoch 600: Train Loss: 0.0076, Macro_F1: 0.9203, AUC_score: 0.9596\n",
      "Epoch 650: Train Loss: 0.0058, Macro_F1: 0.9203, AUC_score: 0.9596\n",
      "Epoch 00659: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 700: Train Loss: 0.0092, Macro_F1: 0.9203, AUC_score: 0.9596\n",
      "Epoch 750: Train Loss: 0.0058, Macro_F1: 0.9203, AUC_score: 0.9595\n",
      "Epoch 00760: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 800: Train Loss: 0.0067, Macro_F1: 0.9203, AUC_score: 0.9595\n",
      "Epoch 850: Train Loss: 0.0049, Macro_F1: 0.9203, AUC_score: 0.9595\n",
      "Early stopping triggered\n",
      "acc save\n",
      "65.99999999999999% node features transform to 0: F1: 0.9203, AUC_score: 0.9595\n",
      "Epoch 0: Train Loss: 0.0071, Macro_F1: 0.8684, AUC_score: 0.9549\n",
      "Validation loss decreased (0.007082 --> 0.007082).\n",
      "Validation loss decreased (0.007054 --> 0.007054).\n",
      "Validation loss decreased (0.007026 --> 0.007026).\n",
      "Validation loss decreased (0.006973 --> 0.006973).\n",
      "Epoch 50: Train Loss: 0.0094, Macro_F1: 0.9239, AUC_score: 0.9600\n",
      "Validation loss decreased (0.006075 --> 0.006075).\n",
      "Validation loss decreased (0.005098 --> 0.005098).\n",
      "Epoch 100: Train Loss: 0.0093, Macro_F1: 0.9239, AUC_score: 0.9589\n",
      "Epoch 150: Train Loss: 0.0099, Macro_F1: 0.9275, AUC_score: 0.9601\n",
      "Epoch 00193: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0118, Macro_F1: 0.9275, AUC_score: 0.9602\n",
      "Epoch 250: Train Loss: 0.0090, Macro_F1: 0.9239, AUC_score: 0.9598\n",
      "Validation loss decreased (0.005030 --> 0.005030).\n",
      "Validation loss decreased (0.004935 --> 0.004935).\n",
      "Epoch 300: Train Loss: 0.0057, Macro_F1: 0.9275, AUC_score: 0.9601\n",
      "Validation loss decreased (0.004755 --> 0.004755).\n",
      "Epoch 350: Train Loss: 0.0075, Macro_F1: 0.9275, AUC_score: 0.9599\n",
      "Validation loss decreased (0.003765 --> 0.003765).\n",
      "Epoch 400: Train Loss: 0.0074, Macro_F1: 0.9239, AUC_score: 0.9602\n",
      "Epoch 450: Train Loss: 0.0090, Macro_F1: 0.9239, AUC_score: 0.9609\n",
      "Epoch 00500: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.0079, Macro_F1: 0.9239, AUC_score: 0.9598\n",
      "Epoch 550: Train Loss: 0.0127, Macro_F1: 0.9239, AUC_score: 0.9602\n",
      "Epoch 00601: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 600: Train Loss: 0.0088, Macro_F1: 0.9275, AUC_score: 0.9598\n",
      "Epoch 650: Train Loss: 0.0080, Macro_F1: 0.9239, AUC_score: 0.9600\n",
      "Early stopping triggered\n",
      "acc save\n",
      "64.99999999999999% node features transform to 0: F1: 0.9239, AUC_score: 0.9602\n",
      "Epoch 0: Train Loss: 0.0176, Macro_F1: 0.9052, AUC_score: 0.9606\n",
      "Validation loss decreased (0.017579 --> 0.017579).\n",
      "Validation loss decreased (0.010100 --> 0.010100).\n",
      "Validation loss decreased (0.006763 --> 0.006763).\n",
      "Validation loss decreased (0.005279 --> 0.005279).\n",
      "Epoch 50: Train Loss: 0.0252, Macro_F1: 0.9239, AUC_score: 0.9595\n",
      "Validation loss decreased (0.004680 --> 0.004680).\n",
      "Validation loss decreased (0.004169 --> 0.004169).\n",
      "Epoch 100: Train Loss: 0.0060, Macro_F1: 0.9312, AUC_score: 0.9615\n",
      "Epoch 150: Train Loss: 0.0052, Macro_F1: 0.9130, AUC_score: 0.9609\n",
      "Epoch 00188: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.0103, Macro_F1: 0.9275, AUC_score: 0.9603\n",
      "Epoch 250: Train Loss: 0.0100, Macro_F1: 0.9312, AUC_score: 0.9605\n",
      "Epoch 00289: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 300: Train Loss: 0.0080, Macro_F1: 0.9275, AUC_score: 0.9605\n",
      "Epoch 350: Train Loss: 0.0081, Macro_F1: 0.9275, AUC_score: 0.9610\n",
      "Early stopping triggered\n",
      "acc save\n",
      "64.0% node features transform to 0: F1: 0.9275, AUC_score: 0.9609\n",
      "Epoch 0: Train Loss: 0.0127, Macro_F1: 0.9201, AUC_score: 0.9629\n",
      "Validation loss decreased (0.012685 --> 0.012685).\n",
      "Validation loss decreased (0.008989 --> 0.008989).\n",
      "Validation loss decreased (0.008762 --> 0.008762).\n",
      "Validation loss decreased (0.007840 --> 0.007840).\n",
      "Validation loss decreased (0.005909 --> 0.005909).\n",
      "Validation loss decreased (0.005628 --> 0.005628).\n",
      "Epoch 50: Train Loss: 0.0070, Macro_F1: 0.9166, AUC_score: 0.9618\n",
      "Validation loss decreased (0.005533 --> 0.005533).\n",
      "Epoch 100: Train Loss: 0.0172, Macro_F1: 0.9312, AUC_score: 0.9628\n",
      "Validation loss decreased (0.005040 --> 0.005040).\n",
      "Epoch 150: Train Loss: 0.0078, Macro_F1: 0.9275, AUC_score: 0.9617\n",
      "Epoch 200: Train Loss: 0.0104, Macro_F1: 0.9130, AUC_score: 0.9619\n",
      "Epoch 00218: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.0070, Macro_F1: 0.9275, AUC_score: 0.9619\n",
      "Validation loss decreased (0.004434 --> 0.004434).\n",
      "Epoch 300: Train Loss: 0.0059, Macro_F1: 0.9312, AUC_score: 0.9620\n",
      "Epoch 350: Train Loss: 0.0076, Macro_F1: 0.9348, AUC_score: 0.9627\n",
      "Epoch 00376: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.0046, Macro_F1: 0.9384, AUC_score: 0.9627\n",
      "Validation loss decreased (0.004291 --> 0.004291).\n",
      "Epoch 450: Train Loss: 0.0092, Macro_F1: 0.9312, AUC_score: 0.9627\n",
      "Epoch 500: Train Loss: 0.0082, Macro_F1: 0.9348, AUC_score: 0.9625\n",
      "Validation loss decreased (0.004117 --> 0.004117).\n",
      "Validation loss decreased (0.004017 --> 0.004017).\n",
      "Epoch 550: Train Loss: 0.0064, Macro_F1: 0.9348, AUC_score: 0.9627\n",
      "Epoch 600: Train Loss: 0.0064, Macro_F1: 0.9275, AUC_score: 0.9625\n",
      "Validation loss decreased (0.003824 --> 0.003824).\n",
      "Epoch 650: Train Loss: 0.0059, Macro_F1: 0.9275, AUC_score: 0.9625\n",
      "Epoch 700: Train Loss: 0.0094, Macro_F1: 0.9348, AUC_score: 0.9628\n",
      "Epoch 00728: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 750: Train Loss: 0.0098, Macro_F1: 0.9275, AUC_score: 0.9625\n",
      "Epoch 800: Train Loss: 0.0055, Macro_F1: 0.9348, AUC_score: 0.9627\n",
      "Epoch 00829: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 850: Train Loss: 0.0055, Macro_F1: 0.9348, AUC_score: 0.9626\n",
      "Epoch 900: Train Loss: 0.0052, Macro_F1: 0.9348, AUC_score: 0.9626\n",
      "Early stopping triggered\n",
      "acc save\n",
      "63.0% node features transform to 0: F1: 0.9348, AUC_score: 0.9626\n",
      "Epoch 0: Train Loss: 0.0089, Macro_F1: 0.9090, AUC_score: 0.9611\n",
      "Validation loss decreased (0.008889 --> 0.008889).\n",
      "Validation loss decreased (0.008542 --> 0.008542).\n",
      "Validation loss decreased (0.007956 --> 0.007956).\n",
      "Validation loss decreased (0.006017 --> 0.006017).\n",
      "Validation loss decreased (0.005185 --> 0.005185).\n",
      "Validation loss decreased (0.005043 --> 0.005043).\n",
      "Validation loss decreased (0.004694 --> 0.004694).\n",
      "Epoch 50: Train Loss: 0.0068, Macro_F1: 0.9239, AUC_score: 0.9606\n",
      "Epoch 100: Train Loss: 0.0085, Macro_F1: 0.9275, AUC_score: 0.9602\n",
      "Epoch 00150: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.0107, Macro_F1: 0.9239, AUC_score: 0.9608\n",
      "Epoch 200: Train Loss: 0.0057, Macro_F1: 0.9312, AUC_score: 0.9614\n",
      "Validation loss decreased (0.004368 --> 0.004368).\n",
      "Epoch 250: Train Loss: 0.0058, Macro_F1: 0.9239, AUC_score: 0.9605\n",
      "Validation loss decreased (0.004266 --> 0.004266).\n",
      "Epoch 300: Train Loss: 0.0106, Macro_F1: 0.9348, AUC_score: 0.9615\n",
      "Validation loss decreased (0.003663 --> 0.003663).\n",
      "Epoch 350: Train Loss: 0.0063, Macro_F1: 0.9312, AUC_score: 0.9609\n",
      "Epoch 400: Train Loss: 0.0073, Macro_F1: 0.9348, AUC_score: 0.9615\n",
      "Epoch 00428: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.0078, Macro_F1: 0.9312, AUC_score: 0.9616\n",
      "Epoch 500: Train Loss: 0.0076, Macro_F1: 0.9275, AUC_score: 0.9612\n",
      "Epoch 00529: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0086, Macro_F1: 0.9312, AUC_score: 0.9610\n",
      "Epoch 600: Train Loss: 0.0082, Macro_F1: 0.9312, AUC_score: 0.9611\n",
      "Early stopping triggered\n",
      "acc save\n",
      "62.0% node features transform to 0: F1: 0.9312, AUC_score: 0.9610\n",
      "Epoch 0: Train Loss: 0.0056, Macro_F1: 0.9166, AUC_score: 0.9598\n",
      "Validation loss decreased (0.005608 --> 0.005608).\n",
      "Validation loss decreased (0.004327 --> 0.004327).\n",
      "Epoch 50: Train Loss: 0.0099, Macro_F1: 0.9275, AUC_score: 0.9611\n",
      "Epoch 100: Train Loss: 0.0076, Macro_F1: 0.9312, AUC_score: 0.9617\n",
      "Epoch 00145: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.0066, Macro_F1: 0.9166, AUC_score: 0.9602\n",
      "Epoch 200: Train Loss: 0.0059, Macro_F1: 0.9384, AUC_score: 0.9611\n",
      "Validation loss decreased (0.003966 --> 0.003966).\n",
      "Epoch 250: Train Loss: 0.0064, Macro_F1: 0.9384, AUC_score: 0.9612\n",
      "Epoch 300: Train Loss: 0.0053, Macro_F1: 0.9348, AUC_score: 0.9618\n",
      "Epoch 00305: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.0047, Macro_F1: 0.9348, AUC_score: 0.9615\n",
      "Epoch 400: Train Loss: 0.0066, Macro_F1: 0.9384, AUC_score: 0.9616\n",
      "Validation loss decreased (0.003874 --> 0.003874).\n",
      "Validation loss decreased (0.003524 --> 0.003524).\n",
      "Epoch 450: Train Loss: 0.0098, Macro_F1: 0.9384, AUC_score: 0.9614\n",
      "Epoch 500: Train Loss: 0.0067, Macro_F1: 0.9384, AUC_score: 0.9616\n",
      "Epoch 00545: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.0063, Macro_F1: 0.9348, AUC_score: 0.9614\n",
      "Validation loss decreased (0.003126 --> 0.003126).\n",
      "Epoch 600: Train Loss: 0.0059, Macro_F1: 0.9348, AUC_score: 0.9614\n",
      "Epoch 650: Train Loss: 0.0104, Macro_F1: 0.9384, AUC_score: 0.9616\n",
      "Epoch 00688: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 700: Train Loss: 0.0066, Macro_F1: 0.9384, AUC_score: 0.9616\n",
      "Epoch 750: Train Loss: 0.0061, Macro_F1: 0.9384, AUC_score: 0.9615\n",
      "Epoch 00789: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 800: Train Loss: 0.0061, Macro_F1: 0.9384, AUC_score: 0.9616\n",
      "Epoch 850: Train Loss: 0.0072, Macro_F1: 0.9384, AUC_score: 0.9616\n",
      "Early stopping triggered\n",
      "acc save\n",
      "61.0% node features transform to 0: F1: 0.9384, AUC_score: 0.9616\n",
      "Epoch 0: Train Loss: 0.0072, Macro_F1: 0.9019, AUC_score: 0.9558\n",
      "Validation loss decreased (0.007174 --> 0.007174).\n",
      "Validation loss decreased (0.006387 --> 0.006387).\n",
      "Validation loss decreased (0.006192 --> 0.006192).\n",
      "Validation loss decreased (0.005708 --> 0.005708).\n",
      "Validation loss decreased (0.005305 --> 0.005305).\n",
      "Epoch 50: Train Loss: 0.0059, Macro_F1: 0.9348, AUC_score: 0.9607\n",
      "Validation loss decreased (0.005288 --> 0.005288).\n",
      "Epoch 100: Train Loss: 0.0069, Macro_F1: 0.9348, AUC_score: 0.9614\n",
      "Validation loss decreased (0.005213 --> 0.005213).\n",
      "Validation loss decreased (0.005188 --> 0.005188).\n",
      "Epoch 150: Train Loss: 0.0053, Macro_F1: 0.9348, AUC_score: 0.9607\n",
      "Validation loss decreased (0.005055 --> 0.005055).\n",
      "Epoch 200: Train Loss: 0.0059, Macro_F1: 0.9348, AUC_score: 0.9615\n",
      "Validation loss decreased (0.004631 --> 0.004631).\n",
      "Validation loss decreased (0.004491 --> 0.004491).\n",
      "Epoch 250: Train Loss: 0.0207, Macro_F1: 0.9384, AUC_score: 0.9614\n",
      "Epoch 300: Train Loss: 0.0073, Macro_F1: 0.9348, AUC_score: 0.9612\n",
      "Epoch 00321: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.004460 --> 0.004460).\n",
      "Epoch 350: Train Loss: 0.0099, Macro_F1: 0.9384, AUC_score: 0.9617\n",
      "Validation loss decreased (0.004403 --> 0.004403).\n",
      "Validation loss decreased (0.004212 --> 0.004212).\n",
      "Epoch 400: Train Loss: 0.0059, Macro_F1: 0.9384, AUC_score: 0.9615\n",
      "Validation loss decreased (0.003278 --> 0.003278).\n",
      "Epoch 450: Train Loss: 0.0057, Macro_F1: 0.9348, AUC_score: 0.9610\n",
      "Epoch 500: Train Loss: 0.0051, Macro_F1: 0.9384, AUC_score: 0.9615\n",
      "Epoch 00549: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 550: Train Loss: 0.0056, Macro_F1: 0.9420, AUC_score: 0.9615\n",
      "Epoch 600: Train Loss: 0.0067, Macro_F1: 0.9384, AUC_score: 0.9616\n",
      "Epoch 00650: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 650: Train Loss: 0.0066, Macro_F1: 0.9384, AUC_score: 0.9611\n",
      "Epoch 700: Train Loss: 0.0052, Macro_F1: 0.9384, AUC_score: 0.9612\n",
      "Early stopping triggered\n",
      "acc save\n",
      "60.0% node features transform to 0: F1: 0.9384, AUC_score: 0.9615\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(20):\n",
    "    # 加载这次迭代的selected_indices和remaining_indices\n",
    "    i = i+20\n",
    "    selected_indices = np.load(f'DIVIDED_DATA/GO_{i}%.npy')\n",
    "    \n",
    "    # 根据selected_indices和remaining_indices调整特征\n",
    "    masked_features = features.clone()\n",
    "    masked_features[torch.tensor(selected_indices)] = 0  # 假设features是一个PyTorch tensor\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=300, verbose=True, delta=0.00001)\n",
    "    \n",
    "    if i > 0:\n",
    "        # 从上一个迭代保存的模型中加载参数\n",
    "        model.load_state_dict(torch.load(f'G-G_DATA/G-G_model_WITHOUT{i-1}.pth'))\n",
    "        # 重新初始化优化器和调度器\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=100, verbose=True)\n",
    "    \n",
    "    # 模型训练和评估逻辑\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model_scheduler(model, masked_features, data.y, data.edge_index, optimizer, loss_fn, scheduler, train_mask)\n",
    "        test_f1, test_auc = evaluate_model(model, masked_features, data.y, data.edge_index, test_mask)\n",
    "\n",
    "        if epoch % 50 == 0:  # 每10个epoch打印一次信息\n",
    "            print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Macro_F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "        early_stopping(train_loss, model, i)\n",
    "        if early_stopping.early_stop or epoch == num_epochs - 1:\n",
    "            results.append({\n",
    "                            'Train Loss': train_loss,\n",
    "                            'F1': test_f1,\n",
    "                            'AUC_score': test_auc\n",
    "                        })\n",
    "            print(\"acc save\")\n",
    "            rate = 1 - count * (i + 1)\n",
    "            rate = rate * 100\n",
    "            print(f'{rate}% node features transform to 0: F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "            torch.save(model.state_dict(), f'G-G_DATA/G-G_model_WITHOUT{i}.pth')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1905418d-b1cf-433a-b991-83b6add974a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 0.8002, Macro_F1: 0.7219, AUC_score: 0.8225\n",
      "Validation loss decreased (0.800181 --> 0.800181).\n",
      "Validation loss decreased (0.579283 --> 0.579283).\n",
      "Validation loss decreased (0.484609 --> 0.484609).\n",
      "Validation loss decreased (0.428152 --> 0.428152).\n",
      "Validation loss decreased (0.417531 --> 0.417531).\n",
      "Validation loss decreased (0.416901 --> 0.416901).\n",
      "Validation loss decreased (0.392682 --> 0.392682).\n",
      "Validation loss decreased (0.374649 --> 0.374649).\n",
      "Validation loss decreased (0.374201 --> 0.374201).\n",
      "Validation loss decreased (0.363151 --> 0.363151).\n",
      "Validation loss decreased (0.310136 --> 0.310136).\n",
      "Validation loss decreased (0.294451 --> 0.294451).\n",
      "Validation loss decreased (0.288247 --> 0.288247).\n",
      "Validation loss decreased (0.261868 --> 0.261868).\n",
      "Validation loss decreased (0.251904 --> 0.251904).\n",
      "Validation loss decreased (0.247174 --> 0.247174).\n",
      "Validation loss decreased (0.238913 --> 0.238913).\n",
      "Validation loss decreased (0.235150 --> 0.235150).\n",
      "Validation loss decreased (0.229689 --> 0.229689).\n",
      "Validation loss decreased (0.223701 --> 0.223701).\n",
      "Validation loss decreased (0.220906 --> 0.220906).\n",
      "Validation loss decreased (0.216566 --> 0.216566).\n",
      "Validation loss decreased (0.190271 --> 0.190271).\n",
      "Validation loss decreased (0.183387 --> 0.183387).\n",
      "Validation loss decreased (0.175180 --> 0.175180).\n",
      "Validation loss decreased (0.165886 --> 0.165886).\n",
      "Epoch 50: Train Loss: 0.1715, Macro_F1: 0.8985, AUC_score: 0.9449\n",
      "Validation loss decreased (0.147356 --> 0.147356).\n",
      "Validation loss decreased (0.141188 --> 0.141188).\n",
      "Validation loss decreased (0.139444 --> 0.139444).\n",
      "Validation loss decreased (0.132452 --> 0.132452).\n",
      "Validation loss decreased (0.122869 --> 0.122869).\n",
      "Validation loss decreased (0.105958 --> 0.105958).\n",
      "Validation loss decreased (0.105394 --> 0.105394).\n",
      "Epoch 100: Train Loss: 0.1377, Macro_F1: 0.9129, AUC_score: 0.9505\n",
      "Validation loss decreased (0.092589 --> 0.092589).\n",
      "Validation loss decreased (0.091869 --> 0.091869).\n",
      "Validation loss decreased (0.086315 --> 0.086315).\n",
      "Validation loss decreased (0.082630 --> 0.082630).\n",
      "Validation loss decreased (0.077582 --> 0.077582).\n",
      "Validation loss decreased (0.075945 --> 0.075945).\n",
      "Epoch 150: Train Loss: 0.0947, Macro_F1: 0.9130, AUC_score: 0.9494\n",
      "Validation loss decreased (0.061211 --> 0.061211).\n",
      "Validation loss decreased (0.056662 --> 0.056662).\n",
      "Validation loss decreased (0.055626 --> 0.055626).\n",
      "Epoch 200: Train Loss: 0.0617, Macro_F1: 0.9130, AUC_score: 0.9526\n",
      "Validation loss decreased (0.049734 --> 0.049734).\n",
      "Validation loss decreased (0.046747 --> 0.046747).\n",
      "Validation loss decreased (0.043789 --> 0.043789).\n",
      "Validation loss decreased (0.042076 --> 0.042076).\n",
      "Epoch 250: Train Loss: 0.0438, Macro_F1: 0.9129, AUC_score: 0.9572\n",
      "Validation loss decreased (0.041833 --> 0.041833).\n",
      "Validation loss decreased (0.039075 --> 0.039075).\n",
      "Validation loss decreased (0.037442 --> 0.037442).\n",
      "Validation loss decreased (0.035970 --> 0.035970).\n",
      "Validation loss decreased (0.032540 --> 0.032540).\n",
      "Epoch 300: Train Loss: 0.0454, Macro_F1: 0.9058, AUC_score: 0.9553\n",
      "Validation loss decreased (0.032242 --> 0.032242).\n",
      "Validation loss decreased (0.030457 --> 0.030457).\n",
      "Validation loss decreased (0.029257 --> 0.029257).\n",
      "Epoch 350: Train Loss: 0.0334, Macro_F1: 0.9093, AUC_score: 0.9586\n",
      "Validation loss decreased (0.026346 --> 0.026346).\n",
      "Validation loss decreased (0.026093 --> 0.026093).\n",
      "Validation loss decreased (0.025782 --> 0.025782).\n",
      "Validation loss decreased (0.024645 --> 0.024645).\n",
      "Epoch 400: Train Loss: 0.0289, Macro_F1: 0.9275, AUC_score: 0.9585\n",
      "Validation loss decreased (0.023373 --> 0.023373).\n",
      "Validation loss decreased (0.020750 --> 0.020750).\n",
      "Epoch 450: Train Loss: 0.0293, Macro_F1: 0.9203, AUC_score: 0.9582\n",
      "Validation loss decreased (0.019979 --> 0.019979).\n",
      "Validation loss decreased (0.019576 --> 0.019576).\n",
      "Validation loss decreased (0.019563 --> 0.019563).\n",
      "Validation loss decreased (0.018400 --> 0.018400).\n",
      "Validation loss decreased (0.018363 --> 0.018363).\n",
      "Epoch 500: Train Loss: 0.0196, Macro_F1: 0.9130, AUC_score: 0.9569\n",
      "Validation loss decreased (0.016824 --> 0.016824).\n",
      "Epoch 550: Train Loss: 0.0271, Macro_F1: 0.9056, AUC_score: 0.9604\n",
      "Validation loss decreased (0.014602 --> 0.014602).\n",
      "Epoch 600: Train Loss: 0.0178, Macro_F1: 0.9311, AUC_score: 0.9576\n",
      "Validation loss decreased (0.014352 --> 0.014352).\n",
      "Validation loss decreased (0.012565 --> 0.012565).\n",
      "Validation loss decreased (0.010837 --> 0.010837).\n",
      "Epoch 650: Train Loss: 0.0227, Macro_F1: 0.9275, AUC_score: 0.9600\n",
      "Epoch 700: Train Loss: 0.0142, Macro_F1: 0.9203, AUC_score: 0.9606\n",
      "Epoch 00729: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 750: Train Loss: 0.0262, Macro_F1: 0.9202, AUC_score: 0.9597\n",
      "Validation loss decreased (0.010331 --> 0.010331).\n",
      "Epoch 800: Train Loss: 0.0152, Macro_F1: 0.9239, AUC_score: 0.9594\n",
      "Validation loss decreased (0.010153 --> 0.010153).\n",
      "Validation loss decreased (0.009997 --> 0.009997).\n",
      "Validation loss decreased (0.008939 --> 0.008939).\n",
      "Epoch 850: Train Loss: 0.0187, Macro_F1: 0.9203, AUC_score: 0.9594\n",
      "Epoch 900: Train Loss: 0.0149, Macro_F1: 0.9239, AUC_score: 0.9597\n",
      "Epoch 00929: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 950: Train Loss: 0.0123, Macro_F1: 0.9275, AUC_score: 0.9595\n",
      "Epoch 1000: Train Loss: 0.0195, Macro_F1: 0.9239, AUC_score: 0.9596\n",
      "Epoch 01030: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 1050: Train Loss: 0.0150, Macro_F1: 0.9239, AUC_score: 0.9598\n",
      "Epoch 1100: Train Loss: 0.0152, Macro_F1: 0.9239, AUC_score: 0.9601\n",
      "Early stopping triggered\n",
      "acc save\n",
      "99.0% node features transform to 0: F1: 0.9239, AUC_score: 0.9598\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1')\n",
    "data = data.to(device)\n",
    "results_2 = []\n",
    "# 加载数据\n",
    "features = data.x\n",
    "labels = data.y # 根据你的数据加载函数进行调整\n",
    "num_iterations = 1\n",
    "original_features = features.clone()\n",
    "# 总共需要迭代的次数，这里以逐步增加5%为例，直到100%\n",
    "model = GCN(num_features=data.x.shape[1], hidden_dim=64, num_classes=2, num_layers=2, activation=F.relu, dropout=0.5)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=100, verbose=True)\n",
    "num_epochs = 2500\n",
    "for i in range(num_iterations):\n",
    "    # 加载这次迭代的selected_indices和remaining_indices\n",
    "    selected_indices = np.load(f'DIVIDED_DATA/selected_indices_iteration_{i}.npy')\n",
    "    remaining_indices = np.load(f'DIVIDED_DATA/remaining_indices_after_iteration_{i}.npy')\n",
    "    \n",
    "    # 根据selected_indices和remaining_indices调整特征\n",
    "    masked_features = features.clone()\n",
    "    masked_features[torch.tensor(remaining_indices)] = 0  # 假设features是一个PyTorch tensor\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=300, verbose=True, delta=0.00001)\n",
    "    \n",
    "    if i > 0:\n",
    "        # 从上一个迭代保存的模型中加载参数\n",
    "        model.load_state_dict(torch.load(f'G-G_DATA/G-G_model_WITHOUT_2{i-1}.pth'))\n",
    "        # 重新初始化优化器和调度器\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=100, verbose=True)\n",
    "    \n",
    "    # 模型训练和评估逻辑\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model_scheduler(model, masked_features, data.y, data.edge_index, optimizer, loss_fn, scheduler, train_mask)\n",
    "        test_f1, test_auc = evaluate_model(model, masked_features, data.y, data.edge_index, test_mask)\n",
    "\n",
    "        if epoch % 50 == 0:  # 每10个epoch打印一次信息\n",
    "            print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Macro_F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "        early_stopping(train_loss, model, i)\n",
    "        if early_stopping.early_stop or epoch == num_epochs - 1:\n",
    "            results_2.append({\n",
    "                            'Train Loss': train_loss,\n",
    "                            'F1': test_f1,\n",
    "                            'AUC_score': test_auc\n",
    "                        })\n",
    "            print(\"acc save\")\n",
    "            rate = 1 - count * (i + 1)\n",
    "            rate = rate * 100\n",
    "            print(f'{rate}% node features transform to 0: F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "            torch.save(model.state_dict(), f'G-G_DATA/G-G_model_WITHOUT_2{i}.pth')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3c11247e-2a1a-421d-ac89-f55d0a643d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Train Loss': 0.4924525320529938,\n",
       "  'F1': array(0.8043854, dtype=float32),\n",
       "  'AUC_score': 0.8161652496184871}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588cb933-cde3-480d-a417-67c956f1ee39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
