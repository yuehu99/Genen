{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d74e01c1-0b50-4a4d-be92-024e9f2f8768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1224800/2362945497.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import obonet\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2fda8ea-68ef-4d52-bc3b-bff4cc85f260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein</th>\n",
       "      <th>Subcellular_location</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OR7A10</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORC5</td>\n",
       "      <td>Nucleus</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZNF596</td>\n",
       "      <td>Nucleus</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SFI1</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SUCNR1</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8592</th>\n",
       "      <td>INSR</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8593</th>\n",
       "      <td>SLC7A1</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8594</th>\n",
       "      <td>SLC44A2</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8595</th>\n",
       "      <td>PARM1</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8596</th>\n",
       "      <td>OR10C1</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8597 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      protein Subcellular_location  Label\n",
       "0      OR7A10        Cell membrane      0\n",
       "1        ORC5              Nucleus      1\n",
       "2      ZNF596              Nucleus      1\n",
       "3        SFI1            Cytoplasm      2\n",
       "4      SUCNR1        Cell membrane      0\n",
       "...       ...                  ...    ...\n",
       "8592     INSR        Cell membrane      0\n",
       "8593   SLC7A1        Cell membrane      0\n",
       "8594  SLC44A2        Cell membrane      0\n",
       "8595    PARM1        Cell membrane      0\n",
       "8596   OR10C1        Cell membrane      0\n",
       "\n",
       "[8597 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#localization\n",
    "labels_df = pd.read_csv('GNN/subcellular_location.csv')\n",
    "labels_df.dropna(inplace=True)\n",
    "location_mapping = {\n",
    "    'Cell membrane': 0,\n",
    "    'Nucleus': 1,\n",
    "    'Cytoplasm': 2\n",
    "}\n",
    "labels_df.rename(columns={'GeneSymbol': 'protein'}, inplace=True)\n",
    "labels_df['Label'] = labels_df['Subcellular_location'].map(location_mapping)\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20e16054-a9ef-45c7-9f2e-8b12ed89b80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein</th>\n",
       "      <th>DosageSensitivity</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZNF627</td>\n",
       "      <td>DosageInsensitive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZEB2</td>\n",
       "      <td>DosageSensitive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PPARD</td>\n",
       "      <td>DosageSensitive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TBX6</td>\n",
       "      <td>DosageInsensitive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MBNL2</td>\n",
       "      <td>DosageSensitive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>HNF4A</td>\n",
       "      <td>DosageSensitive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>ZNF391</td>\n",
       "      <td>DosageInsensitive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>ZNF707</td>\n",
       "      <td>DosageInsensitive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>ZFP37</td>\n",
       "      <td>DosageInsensitive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>ZNF90</td>\n",
       "      <td>DosageInsensitive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>487 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    protein  DosageSensitivity  Label\n",
       "0    ZNF627  DosageInsensitive      0\n",
       "1      ZEB2    DosageSensitive      1\n",
       "2     PPARD    DosageSensitive      1\n",
       "3      TBX6  DosageInsensitive      0\n",
       "4     MBNL2    DosageSensitive      1\n",
       "..      ...                ...    ...\n",
       "482   HNF4A    DosageSensitive      1\n",
       "483  ZNF391  DosageInsensitive      0\n",
       "484  ZNF707  DosageInsensitive      0\n",
       "485   ZFP37  DosageInsensitive      0\n",
       "486   ZNF90  DosageInsensitive      0\n",
       "\n",
       "[487 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DosageSensitivity\n",
    "labels_df = pd.read_csv('GNN/DosageSensitivity.csv')\n",
    "labels_df.dropna(inplace=True)\n",
    "location_mapping = {\n",
    "    'DosageInsensitive': 0,\n",
    "    'DosageSensitive': 1\n",
    "}\n",
    "labels_df.rename(columns={'GeneSymbol': 'protein'}, inplace=True)\n",
    "labels_df['Label'] = labels_df['DosageSensitivity'].map(location_mapping)\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d09676a6-78d3-4f86-9b28-fcfdf5542e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein</th>\n",
       "      <th>BivalentVsLys4</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HSD17B1</td>\n",
       "      <td>Bivalent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NKX2-8</td>\n",
       "      <td>Bivalent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NKX6-2</td>\n",
       "      <td>Bivalent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATP6V0A1</td>\n",
       "      <td>Lys4Only</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IRX4</td>\n",
       "      <td>Bivalent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>ZFPM2</td>\n",
       "      <td>Bivalent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>HOXD13</td>\n",
       "      <td>Bivalent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>NKX2-4</td>\n",
       "      <td>Bivalent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>CNTNAP1</td>\n",
       "      <td>Bivalent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>INSIG1</td>\n",
       "      <td>Lys4Only</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>184 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      protein BivalentVsLys4  Label\n",
       "0     HSD17B1       Bivalent      0\n",
       "1      NKX2-8       Bivalent      0\n",
       "2      NKX6-2       Bivalent      0\n",
       "3    ATP6V0A1       Lys4Only      1\n",
       "4        IRX4       Bivalent      0\n",
       "..        ...            ...    ...\n",
       "179     ZFPM2       Bivalent      0\n",
       "180    HOXD13       Bivalent      0\n",
       "181    NKX2-4       Bivalent      0\n",
       "182   CNTNAP1       Bivalent      0\n",
       "183    INSIG1       Lys4Only      1\n",
       "\n",
       "[184 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BivalentVsLys4\n",
    "labels_df = pd.read_csv('GNN/BivalentVsLys4.csv')\n",
    "labels_df.dropna(inplace=True)\n",
    "location_mapping = {\n",
    "    'Bivalent': 0,\n",
    "    'Lys4Only': 1\n",
    "}\n",
    "labels_df.rename(columns={'GeneSymbol': 'protein'}, inplace=True)\n",
    "labels_df['Label'] = labels_df['BivalentVsLys4'].map(location_mapping)\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5c2eee6-50e4-4c5b-8d7f-d1631ce2d5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein</th>\n",
       "      <th>BivalentVsNonMethylated</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KCNMA1</td>\n",
       "      <td>NonMethylated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IFI35</td>\n",
       "      <td>NonMethylated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOXD13</td>\n",
       "      <td>Bivalent</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZFPM2</td>\n",
       "      <td>Bivalent</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NKG7</td>\n",
       "      <td>NonMethylated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>SOX6</td>\n",
       "      <td>Bivalent</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>PAX1</td>\n",
       "      <td>Bivalent</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>EN2</td>\n",
       "      <td>Bivalent</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>SIX2</td>\n",
       "      <td>Bivalent</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>SATB1</td>\n",
       "      <td>Bivalent</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    protein BivalentVsNonMethylated  Label\n",
       "0    KCNMA1           NonMethylated      0\n",
       "1     IFI35           NonMethylated      0\n",
       "2    HOXD13                Bivalent      1\n",
       "3     ZFPM2                Bivalent      1\n",
       "4      NKG7           NonMethylated      0\n",
       "..      ...                     ...    ...\n",
       "142    SOX6                Bivalent      1\n",
       "143    PAX1                Bivalent      1\n",
       "144     EN2                Bivalent      1\n",
       "145    SIX2                Bivalent      1\n",
       "146   SATB1                Bivalent      1\n",
       "\n",
       "[147 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BivalentVsNonMethylated\n",
    "labels_df = pd.read_csv('GNN/BivalentVsNonMethylated.csv')\n",
    "labels_df.dropna(inplace=True)\n",
    "location_mapping = {\n",
    "    'NonMethylated': 0,\n",
    "    'Bivalent': 1\n",
    "}\n",
    "labels_df.rename(columns={'GeneSymbol': 'protein'}, inplace=True)\n",
    "labels_df['Label'] = labels_df['BivalentVsNonMethylated'].map(location_mapping)\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faef3380-ffd1-40c0-a305-447db9a73151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein</th>\n",
       "      <th>Tf_range</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PITX1</td>\n",
       "      <td>Short_range</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOS</td>\n",
       "      <td>Long_range</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EGR1</td>\n",
       "      <td>Short_range</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOXM1</td>\n",
       "      <td>Short_range</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ESR1</td>\n",
       "      <td>Short_range</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>TEAD1</td>\n",
       "      <td>Long_range</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>MAFB</td>\n",
       "      <td>Short_range</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>OTX2</td>\n",
       "      <td>Short_range</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>NKX6-1</td>\n",
       "      <td>Short_range</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>NKX2-1</td>\n",
       "      <td>Short_range</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    protein     Tf_range  Label\n",
       "0     PITX1  Short_range      0\n",
       "1       FOS   Long_range      1\n",
       "2      EGR1  Short_range      0\n",
       "3     FOXM1  Short_range      0\n",
       "4      ESR1  Short_range      0\n",
       "..      ...          ...    ...\n",
       "169   TEAD1   Long_range      1\n",
       "170    MAFB  Short_range      0\n",
       "171    OTX2  Short_range      0\n",
       "172  NKX6-1  Short_range      0\n",
       "173  NKX2-1  Short_range      0\n",
       "\n",
       "[174 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tf_range\n",
    "labels_df = pd.read_csv('GNN/Tf_range.csv')\n",
    "labels_df.dropna(inplace=True)\n",
    "location_mapping = {\n",
    "    'Short_range': 0,\n",
    "    'Long_range': 1\n",
    "}\n",
    "labels_df.rename(columns={'GeneSymbol': 'protein'}, inplace=True)\n",
    "labels_df['Label'] = labels_df['Tf_range'].map(location_mapping)\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73b5d102-ca14-460f-be91-1de9f8fcf7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein</th>\n",
       "      <th>TF_target_type</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DGKG</td>\n",
       "      <td>gata4_indirect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COPB2</td>\n",
       "      <td>gata4_indirect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GPX3</td>\n",
       "      <td>gata4_indirect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DNAH2</td>\n",
       "      <td>gata4_indirect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TBX4</td>\n",
       "      <td>gata4_indirect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1783</th>\n",
       "      <td>GPRIN3</td>\n",
       "      <td>gata4_direct</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1784</th>\n",
       "      <td>LAMA2</td>\n",
       "      <td>gata4_direct</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785</th>\n",
       "      <td>CKMT2</td>\n",
       "      <td>gata4_direct</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786</th>\n",
       "      <td>FAT3</td>\n",
       "      <td>gata4_direct</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787</th>\n",
       "      <td>HTR4</td>\n",
       "      <td>gata4_direct</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1787 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     protein  TF_target_type  Label\n",
       "0       DGKG  gata4_indirect      0\n",
       "1      COPB2  gata4_indirect      0\n",
       "2       GPX3  gata4_indirect      0\n",
       "3      DNAH2  gata4_indirect      0\n",
       "4       TBX4  gata4_indirect      0\n",
       "...      ...             ...    ...\n",
       "1783  GPRIN3    gata4_direct      1\n",
       "1784   LAMA2    gata4_direct      1\n",
       "1785   CKMT2    gata4_direct      1\n",
       "1786    FAT3    gata4_direct      1\n",
       "1787    HTR4    gata4_direct      1\n",
       "\n",
       "[1787 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF_target_type\n",
    "labels_df = pd.read_csv('GNN/TF_target_type.csv')\n",
    "labels_df.dropna(inplace=True)\n",
    "labels_df = labels_df.iloc[:1787]\n",
    "location_mapping = {\n",
    "    'gata4_indirect': 0,\n",
    "    'gata4_direct': 1\n",
    "    #'tbx5_indirect': 2,\n",
    "    #'tbx5_direct': 3,\n",
    "    #'gata4_tbx5_combo_targets': 4\n",
    "}\n",
    "labels_df.rename(columns={'GeneSymbol': 'protein'}, inplace=True)\n",
    "labels_df['Label'] = labels_df['TF_target_type'].map(location_mapping)\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d647cfd8-5910-4ac7-bea6-f0d4d4559dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein</th>\n",
       "      <th>TF_target_type</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1788</th>\n",
       "      <td>NARF</td>\n",
       "      <td>tbx5_indirect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>THNSL1</td>\n",
       "      <td>tbx5_indirect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790</th>\n",
       "      <td>F2RL1</td>\n",
       "      <td>tbx5_indirect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>GPX8</td>\n",
       "      <td>tbx5_indirect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>ACADSB</td>\n",
       "      <td>tbx5_indirect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4025</th>\n",
       "      <td>GPRIN3</td>\n",
       "      <td>gata4_tbx5_combo_targets</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4026</th>\n",
       "      <td>LAMA2</td>\n",
       "      <td>gata4_tbx5_combo_targets</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4027</th>\n",
       "      <td>CKMT2</td>\n",
       "      <td>gata4_tbx5_combo_targets</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4028</th>\n",
       "      <td>FAT3</td>\n",
       "      <td>gata4_tbx5_combo_targets</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4029</th>\n",
       "      <td>HTR4</td>\n",
       "      <td>gata4_tbx5_combo_targets</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2240 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     protein            TF_target_type  Label\n",
       "1788    NARF             tbx5_indirect      0\n",
       "1789  THNSL1             tbx5_indirect      0\n",
       "1790   F2RL1             tbx5_indirect      0\n",
       "1791    GPX8             tbx5_indirect      0\n",
       "1792  ACADSB             tbx5_indirect      0\n",
       "...      ...                       ...    ...\n",
       "4025  GPRIN3  gata4_tbx5_combo_targets      2\n",
       "4026   LAMA2  gata4_tbx5_combo_targets      2\n",
       "4027   CKMT2  gata4_tbx5_combo_targets      2\n",
       "4028    FAT3  gata4_tbx5_combo_targets      2\n",
       "4029    HTR4  gata4_tbx5_combo_targets      2\n",
       "\n",
       "[2240 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF_target_type\n",
    "labels_df = pd.read_csv('GNN/TF_target_type.csv')\n",
    "labels_df.dropna(inplace=True)\n",
    "labels_df = labels_df.iloc[1787:]\n",
    "location_mapping = {\n",
    "    #'gata4_indirect': 0,\n",
    "    #'gata4_direct': 1\n",
    "    'tbx5_indirect': 0,\n",
    "    'tbx5_direct': 1,\n",
    "    'gata4_tbx5_combo_targets': 2\n",
    "}\n",
    "labels_df.rename(columns={'GeneSymbol': 'protein'}, inplace=True)\n",
    "labels_df['Label'] = labels_df['TF_target_type'].map(location_mapping)\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f2932d6b-2879-4477-8a55-152452488985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GeneSymbol</th>\n",
       "      <th>Conservation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1BG</td>\n",
       "      <td>0.062761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NAT2</td>\n",
       "      <td>0.118469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADA</td>\n",
       "      <td>0.253069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CDH2</td>\n",
       "      <td>0.694620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GAGE12F</td>\n",
       "      <td>0.088746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30992</th>\n",
       "      <td>SLC12A6</td>\n",
       "      <td>0.659845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30993</th>\n",
       "      <td>PTBP3</td>\n",
       "      <td>0.729372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30994</th>\n",
       "      <td>KCNE2</td>\n",
       "      <td>0.160752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30995</th>\n",
       "      <td>DGCR2</td>\n",
       "      <td>0.627946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30996</th>\n",
       "      <td>SCO2</td>\n",
       "      <td>0.229875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30467 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      GeneSymbol  Conservation\n",
       "0           A1BG      0.062761\n",
       "1           NAT2      0.118469\n",
       "2            ADA      0.253069\n",
       "3           CDH2      0.694620\n",
       "4        GAGE12F      0.088746\n",
       "...          ...           ...\n",
       "30992    SLC12A6      0.659845\n",
       "30993      PTBP3      0.729372\n",
       "30994      KCNE2      0.160752\n",
       "30995      DGCR2      0.627946\n",
       "30996       SCO2      0.229875\n",
       "\n",
       "[30467 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#phastcons\n",
    "labels_df = pd.read_csv('GNN/phastcons.csv')\n",
    "labels_df.dropna(inplace=True)\n",
    "location_mapping = {\n",
    "    #'gata4_indirect': 0,\n",
    "    #'gata4_direct': 1\n",
    "    'tbx5_indirect': 0,\n",
    "    'tbx5_direct': 1,\n",
    "    'gata4_tbx5_combo_targets': 2\n",
    "}\n",
    "labels_df.rename(columns={'GeneSymbol': 'protein'}, inplace=True)\n",
    "labels_df['Label'] = labels_df['TF_target_type'].map(location_mapping)\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a848213-8293-4bf8-8f06-55d3431113bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Source      Target\n",
      "0      GO:0000001  GO:0048308\n",
      "1      GO:0000001  GO:0048311\n",
      "2      GO:0000002  GO:0007005\n",
      "3      GO:0000003  GO:0008150\n",
      "4      GO:0000006  GO:0005385\n",
      "...           ...         ...\n",
      "83792  GO:2001317  GO:0034309\n",
      "83793  GO:2001317  GO:0042181\n",
      "83794  GO:2001317  GO:0120255\n",
      "83795  GO:2001317  GO:1901362\n",
      "83796  GO:2001317  GO:2001316\n",
      "\n",
      "[83797 rows x 2 columns]\n",
      "           GO  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
      "0  GO:0000001 -1.168093 -0.355214  0.265877 -0.710051  0.515028 -0.525165   \n",
      "1  GO:0000002 -1.185879 -0.098765  0.388240 -0.295556  0.327296 -0.119842   \n",
      "2  GO:0000003  0.063323 -0.199995  0.151511 -0.942141  0.109313  0.015316   \n",
      "3  GO:0000005  0.163135  0.301527  0.219680  0.094342 -0.129769  0.225696   \n",
      "4  GO:0000006 -0.641113 -0.541363  0.413941  0.699345  0.461507 -0.497388   \n",
      "\n",
      "   feature7  feature8  feature9  ...  feature759  feature760  feature761  \\\n",
      "0 -0.186588 -0.161192  0.186984  ...   -1.350874   -0.991801   -0.648123   \n",
      "1  0.399882 -0.035890  0.853417  ...   -1.086927   -0.842870   -0.385764   \n",
      "2  0.633298  0.507875  0.665548  ...    0.174185    0.351648    0.138497   \n",
      "3  0.357577  0.819992  0.852388  ...   -0.084025   -0.291103   -0.003621   \n",
      "4 -0.044589 -0.655766 -0.596647  ...   -0.561434    0.246475   -0.029871   \n",
      "\n",
      "   feature762  feature763  feature764  feature765  feature766  feature767  \\\n",
      "0   -0.361629   -0.914965   -0.506993    0.389760    0.207266    0.070705   \n",
      "1    0.175797   -1.223772   -0.999628    0.101473   -0.051212    0.048775   \n",
      "2    0.119273   -0.295167   -0.331179    0.102570   -0.524301   -0.139264   \n",
      "3    0.245929   -0.443244    0.229245   -0.685159   -0.725621    0.285964   \n",
      "4   -0.212828   -0.985273    0.677472    0.582681    0.299317   -0.131577   \n",
      "\n",
      "   feature768  \n",
      "0    0.938593  \n",
      "1    0.780470  \n",
      "2    0.761573  \n",
      "3    0.313211  \n",
      "4    0.739702  \n",
      "\n",
      "[5 rows x 769 columns]\n"
     ]
    }
   ],
   "source": [
    "GO_graph = obonet.read_obo(\"GNN/go-basic.obo\")\n",
    "\n",
    "go_edges = []\n",
    "for u, v, data in GO_graph.edges(data=True):\n",
    "    go_edges.append([u, v])\n",
    "go_edges_df = pd.DataFrame(go_edges, columns=['Source', 'Target']).dropna()\n",
    "print(go_edges_df)\n",
    "col_name = ['GO']\n",
    "for i in range(1,769):\n",
    "  col_name.append('feature'+str(i))\n",
    "go_features_df = pd.read_csv(\"GNN/go_terms_embeddings.csv\", skiprows=1, names=col_name).dropna()\n",
    "print(go_features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2359f30-1cda-4e25-94b0-10af2c9e501c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  protein  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
      "0     FES  0.339602 -0.030744 -0.901381  0.100888  0.886443  0.383596   \n",
      "1  HADHA  -0.131799 -0.025745 -0.677301 -0.053545  0.971046  0.180315   \n",
      "2  SLC7A7  0.385693 -0.070692 -0.847796 -0.022054  0.959772  0.085487   \n",
      "3    LCK   0.650428  0.014479 -0.866163  0.053508  0.951529  0.269402   \n",
      "4   HSPA2  0.322262  0.017484 -0.849302  0.046401  0.920429  0.463832   \n",
      "\n",
      "   feature7  feature8  feature9  ...  feature759  feature760  feature761  \\\n",
      "0 -0.192082 -0.032063 -0.154869  ...   -0.549204   -0.856123    0.714672   \n",
      "1 -0.028189 -0.077389 -0.095152  ...    0.927885   -0.817812    0.809631   \n",
      "2  0.076455 -0.003006 -0.032268  ...    0.941094   -0.912443    0.789828   \n",
      "3 -0.214788  0.045179 -0.506429  ...   -0.576739   -0.969558    0.916549   \n",
      "4 -0.050414 -0.033398  0.387791  ...    0.387301   -0.860696    0.678607   \n",
      "\n",
      "   feature762  feature763  feature764  feature765  feature766  feature767  \\\n",
      "0   -0.046649   -0.894424   -0.001815    0.739485    0.015581   -0.023863   \n",
      "1   -0.005827   -0.848839    0.024516    0.526404   -0.039926   -0.102787   \n",
      "2    0.046979   -0.715636    0.085842    0.150494    0.025392   -0.066035   \n",
      "3   -0.080332   -0.927649   -0.047398    0.741663   -0.000096   -0.096318   \n",
      "4   -0.060695   -0.945793    0.040472    0.831079   -0.001711   -0.079842   \n",
      "\n",
      "   feature768  \n",
      "0   -0.022002  \n",
      "1   -0.026980  \n",
      "2   -0.028283  \n",
      "3   -0.056501  \n",
      "4   -0.011189  \n",
      "\n",
      "[5 rows x 769 columns]\n",
      "         Target      Source\n",
      "0         MT-TF  GO:0030533\n",
      "1         MT-TF  GO:0006412\n",
      "4       MT-RNR2  GO:0003735\n",
      "5       MT-RNR2  GO:0005840\n",
      "6        MT-TL1  GO:0030533\n",
      "...         ...         ...\n",
      "456584  PLEKHM2  GO:0032880\n",
      "456585  PLEKHM2  GO:0010008\n",
      "456586  PLEKHM2  GO:0019894\n",
      "456587  PLEKHM2  GO:0032418\n",
      "456588  PLEKHM2  GO:0042267\n",
      "\n",
      "[393231 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "col_name = ['protein']\n",
    "for i in range(1,769):\n",
    "  col_name.append('feature'+str(i))\n",
    "gene_features_df = pd.read_csv('GNN/gene_embedding_GeneLLM_2.csv', header=None, names=col_name).dropna()\n",
    "print(gene_features_df.head())\n",
    "\n",
    "col_name = ['Target', 'Source']\n",
    "go_protein_df = pd.read_csv(\n",
    "    \"GNN/mart_export.txt\", \n",
    "    skiprows=1, \n",
    "    names=col_name, \n",
    "    usecols=[1, 2]  # 使用列的索引来指定\n",
    ").dropna()\n",
    "print(go_protein_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f11ddd23-be2e-4b64-8510-664baa3a17e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47595\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature759</th>\n",
       "      <th>feature760</th>\n",
       "      <th>feature761</th>\n",
       "      <th>feature762</th>\n",
       "      <th>feature763</th>\n",
       "      <th>feature764</th>\n",
       "      <th>feature765</th>\n",
       "      <th>feature766</th>\n",
       "      <th>feature767</th>\n",
       "      <th>feature768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FES</td>\n",
       "      <td>0.339602</td>\n",
       "      <td>-0.030744</td>\n",
       "      <td>-0.901381</td>\n",
       "      <td>0.100888</td>\n",
       "      <td>0.886443</td>\n",
       "      <td>0.383596</td>\n",
       "      <td>-0.192082</td>\n",
       "      <td>-0.032063</td>\n",
       "      <td>-0.154869</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.549204</td>\n",
       "      <td>-0.856123</td>\n",
       "      <td>0.714672</td>\n",
       "      <td>-0.046649</td>\n",
       "      <td>-0.894424</td>\n",
       "      <td>-0.001815</td>\n",
       "      <td>0.739485</td>\n",
       "      <td>0.015581</td>\n",
       "      <td>-0.023863</td>\n",
       "      <td>-0.022002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HADHA</td>\n",
       "      <td>-0.131799</td>\n",
       "      <td>-0.025745</td>\n",
       "      <td>-0.677301</td>\n",
       "      <td>-0.053545</td>\n",
       "      <td>0.971046</td>\n",
       "      <td>0.180315</td>\n",
       "      <td>-0.028189</td>\n",
       "      <td>-0.077389</td>\n",
       "      <td>-0.095152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.927885</td>\n",
       "      <td>-0.817812</td>\n",
       "      <td>0.809631</td>\n",
       "      <td>-0.005827</td>\n",
       "      <td>-0.848839</td>\n",
       "      <td>0.024516</td>\n",
       "      <td>0.526404</td>\n",
       "      <td>-0.039926</td>\n",
       "      <td>-0.102787</td>\n",
       "      <td>-0.026980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLC7A7</td>\n",
       "      <td>0.385693</td>\n",
       "      <td>-0.070692</td>\n",
       "      <td>-0.847796</td>\n",
       "      <td>-0.022054</td>\n",
       "      <td>0.959772</td>\n",
       "      <td>0.085487</td>\n",
       "      <td>0.076455</td>\n",
       "      <td>-0.003006</td>\n",
       "      <td>-0.032268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.941094</td>\n",
       "      <td>-0.912443</td>\n",
       "      <td>0.789828</td>\n",
       "      <td>0.046979</td>\n",
       "      <td>-0.715636</td>\n",
       "      <td>0.085842</td>\n",
       "      <td>0.150494</td>\n",
       "      <td>0.025392</td>\n",
       "      <td>-0.066035</td>\n",
       "      <td>-0.028283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LCK</td>\n",
       "      <td>0.650428</td>\n",
       "      <td>0.014479</td>\n",
       "      <td>-0.866163</td>\n",
       "      <td>0.053508</td>\n",
       "      <td>0.951529</td>\n",
       "      <td>0.269402</td>\n",
       "      <td>-0.214788</td>\n",
       "      <td>0.045179</td>\n",
       "      <td>-0.506429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.576739</td>\n",
       "      <td>-0.969558</td>\n",
       "      <td>0.916549</td>\n",
       "      <td>-0.080332</td>\n",
       "      <td>-0.927649</td>\n",
       "      <td>-0.047398</td>\n",
       "      <td>0.741663</td>\n",
       "      <td>-0.000096</td>\n",
       "      <td>-0.096318</td>\n",
       "      <td>-0.056501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HSPA2</td>\n",
       "      <td>0.322262</td>\n",
       "      <td>0.017484</td>\n",
       "      <td>-0.849302</td>\n",
       "      <td>0.046401</td>\n",
       "      <td>0.920429</td>\n",
       "      <td>0.463832</td>\n",
       "      <td>-0.050414</td>\n",
       "      <td>-0.033398</td>\n",
       "      <td>0.387791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387301</td>\n",
       "      <td>-0.860696</td>\n",
       "      <td>0.678607</td>\n",
       "      <td>-0.060695</td>\n",
       "      <td>-0.945793</td>\n",
       "      <td>0.040472</td>\n",
       "      <td>0.831079</td>\n",
       "      <td>-0.001711</td>\n",
       "      <td>-0.079842</td>\n",
       "      <td>-0.011189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62040</th>\n",
       "      <td>GO:2001313</td>\n",
       "      <td>0.174428</td>\n",
       "      <td>0.194728</td>\n",
       "      <td>-0.284376</td>\n",
       "      <td>0.282102</td>\n",
       "      <td>-0.713190</td>\n",
       "      <td>-0.272055</td>\n",
       "      <td>0.121190</td>\n",
       "      <td>0.129901</td>\n",
       "      <td>-0.983496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500545</td>\n",
       "      <td>0.429651</td>\n",
       "      <td>-0.292929</td>\n",
       "      <td>-0.464941</td>\n",
       "      <td>-0.740187</td>\n",
       "      <td>0.179149</td>\n",
       "      <td>-0.960807</td>\n",
       "      <td>-0.746958</td>\n",
       "      <td>1.069112</td>\n",
       "      <td>-0.848182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62041</th>\n",
       "      <td>GO:2001314</td>\n",
       "      <td>0.025886</td>\n",
       "      <td>0.306214</td>\n",
       "      <td>-0.254303</td>\n",
       "      <td>0.253673</td>\n",
       "      <td>-0.533680</td>\n",
       "      <td>-0.269355</td>\n",
       "      <td>0.150939</td>\n",
       "      <td>-0.229323</td>\n",
       "      <td>-1.078991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042979</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>-0.356661</td>\n",
       "      <td>-0.381828</td>\n",
       "      <td>-0.638338</td>\n",
       "      <td>0.077176</td>\n",
       "      <td>-0.788312</td>\n",
       "      <td>-0.683442</td>\n",
       "      <td>1.087031</td>\n",
       "      <td>-0.593092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62042</th>\n",
       "      <td>GO:2001315</td>\n",
       "      <td>0.027134</td>\n",
       "      <td>0.241391</td>\n",
       "      <td>-0.227353</td>\n",
       "      <td>0.317366</td>\n",
       "      <td>-0.726657</td>\n",
       "      <td>-0.197968</td>\n",
       "      <td>0.045653</td>\n",
       "      <td>0.038912</td>\n",
       "      <td>-0.954113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349853</td>\n",
       "      <td>0.370059</td>\n",
       "      <td>-0.144606</td>\n",
       "      <td>-0.493184</td>\n",
       "      <td>-0.655063</td>\n",
       "      <td>0.217335</td>\n",
       "      <td>-0.841272</td>\n",
       "      <td>-0.821077</td>\n",
       "      <td>1.036363</td>\n",
       "      <td>-0.836614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62043</th>\n",
       "      <td>GO:2001316</td>\n",
       "      <td>0.139543</td>\n",
       "      <td>0.028883</td>\n",
       "      <td>0.899480</td>\n",
       "      <td>0.152932</td>\n",
       "      <td>0.576852</td>\n",
       "      <td>0.330342</td>\n",
       "      <td>0.916943</td>\n",
       "      <td>0.012306</td>\n",
       "      <td>-0.020316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.354748</td>\n",
       "      <td>-0.083168</td>\n",
       "      <td>0.043640</td>\n",
       "      <td>-0.663565</td>\n",
       "      <td>0.543016</td>\n",
       "      <td>-0.652230</td>\n",
       "      <td>-1.427882</td>\n",
       "      <td>-0.985257</td>\n",
       "      <td>1.673561</td>\n",
       "      <td>0.109659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62044</th>\n",
       "      <td>GO:2001317</td>\n",
       "      <td>0.083064</td>\n",
       "      <td>0.090899</td>\n",
       "      <td>0.888541</td>\n",
       "      <td>0.309920</td>\n",
       "      <td>0.403966</td>\n",
       "      <td>0.202783</td>\n",
       "      <td>0.706517</td>\n",
       "      <td>-0.017584</td>\n",
       "      <td>-0.171057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.544680</td>\n",
       "      <td>-0.046654</td>\n",
       "      <td>0.262865</td>\n",
       "      <td>-0.767305</td>\n",
       "      <td>0.753788</td>\n",
       "      <td>-0.577503</td>\n",
       "      <td>-1.194910</td>\n",
       "      <td>-0.799556</td>\n",
       "      <td>1.519368</td>\n",
       "      <td>0.263210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62045 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          protein  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
       "0             FES  0.339602 -0.030744 -0.901381  0.100888  0.886443  0.383596   \n",
       "1          HADHA  -0.131799 -0.025745 -0.677301 -0.053545  0.971046  0.180315   \n",
       "2          SLC7A7  0.385693 -0.070692 -0.847796 -0.022054  0.959772  0.085487   \n",
       "3            LCK   0.650428  0.014479 -0.866163  0.053508  0.951529  0.269402   \n",
       "4           HSPA2  0.322262  0.017484 -0.849302  0.046401  0.920429  0.463832   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "62040  GO:2001313  0.174428  0.194728 -0.284376  0.282102 -0.713190 -0.272055   \n",
       "62041  GO:2001314  0.025886  0.306214 -0.254303  0.253673 -0.533680 -0.269355   \n",
       "62042  GO:2001315  0.027134  0.241391 -0.227353  0.317366 -0.726657 -0.197968   \n",
       "62043  GO:2001316  0.139543  0.028883  0.899480  0.152932  0.576852  0.330342   \n",
       "62044  GO:2001317  0.083064  0.090899  0.888541  0.309920  0.403966  0.202783   \n",
       "\n",
       "       feature7  feature8  feature9  ...  feature759  feature760  feature761  \\\n",
       "0     -0.192082 -0.032063 -0.154869  ...   -0.549204   -0.856123    0.714672   \n",
       "1     -0.028189 -0.077389 -0.095152  ...    0.927885   -0.817812    0.809631   \n",
       "2      0.076455 -0.003006 -0.032268  ...    0.941094   -0.912443    0.789828   \n",
       "3     -0.214788  0.045179 -0.506429  ...   -0.576739   -0.969558    0.916549   \n",
       "4     -0.050414 -0.033398  0.387791  ...    0.387301   -0.860696    0.678607   \n",
       "...         ...       ...       ...  ...         ...         ...         ...   \n",
       "62040  0.121190  0.129901 -0.983496  ...    0.500545    0.429651   -0.292929   \n",
       "62041  0.150939 -0.229323 -1.078991  ...    0.042979    0.134560   -0.356661   \n",
       "62042  0.045653  0.038912 -0.954113  ...    0.349853    0.370059   -0.144606   \n",
       "62043  0.916943  0.012306 -0.020316  ...   -0.354748   -0.083168    0.043640   \n",
       "62044  0.706517 -0.017584 -0.171057  ...   -0.544680   -0.046654    0.262865   \n",
       "\n",
       "       feature762  feature763  feature764  feature765  feature766  feature767  \\\n",
       "0       -0.046649   -0.894424   -0.001815    0.739485    0.015581   -0.023863   \n",
       "1       -0.005827   -0.848839    0.024516    0.526404   -0.039926   -0.102787   \n",
       "2        0.046979   -0.715636    0.085842    0.150494    0.025392   -0.066035   \n",
       "3       -0.080332   -0.927649   -0.047398    0.741663   -0.000096   -0.096318   \n",
       "4       -0.060695   -0.945793    0.040472    0.831079   -0.001711   -0.079842   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "62040   -0.464941   -0.740187    0.179149   -0.960807   -0.746958    1.069112   \n",
       "62041   -0.381828   -0.638338    0.077176   -0.788312   -0.683442    1.087031   \n",
       "62042   -0.493184   -0.655063    0.217335   -0.841272   -0.821077    1.036363   \n",
       "62043   -0.663565    0.543016   -0.652230   -1.427882   -0.985257    1.673561   \n",
       "62044   -0.767305    0.753788   -0.577503   -1.194910   -0.799556    1.519368   \n",
       "\n",
       "       feature768  \n",
       "0       -0.022002  \n",
       "1       -0.026980  \n",
       "2       -0.028283  \n",
       "3       -0.056501  \n",
       "4       -0.011189  \n",
       "...           ...  \n",
       "62040   -0.848182  \n",
       "62041   -0.593092  \n",
       "62042   -0.836614  \n",
       "62043    0.109659  \n",
       "62044    0.263210  \n",
       "\n",
       "[62045 rows x 769 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(go_features_df))\n",
    "go_features_df.rename(columns={'GO': 'protein'}, inplace=True)\n",
    "combined_features = pd.concat([gene_features_df, go_features_df]).reset_index(drop=True)\n",
    "combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b45b65-b1e9-4c4a-817a-73e191c2a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_edges_df = pd.read_csv('GNN/protein_interactions.csv', usecols=[0, 1], names=col_name).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2644823-5f49-432b-b88d-fffd713fe833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MT-TF</td>\n",
       "      <td>GO:0030533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MT-TF</td>\n",
       "      <td>GO:0006412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MT-RNR2</td>\n",
       "      <td>GO:0003735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MT-RNR2</td>\n",
       "      <td>GO:0005840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MT-TL1</td>\n",
       "      <td>GO:0030533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13715124</th>\n",
       "      <td>LDB1</td>\n",
       "      <td>SAMD14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13715125</th>\n",
       "      <td>LDB1</td>\n",
       "      <td>KDM6B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13715126</th>\n",
       "      <td>LDB1</td>\n",
       "      <td>WWP2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13715127</th>\n",
       "      <td>LDB1</td>\n",
       "      <td>VPS33B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13715128</th>\n",
       "      <td>LDB1</td>\n",
       "      <td>NDST2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13544448 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Target      Source\n",
       "0           MT-TF  GO:0030533\n",
       "1           MT-TF  GO:0006412\n",
       "4         MT-RNR2  GO:0003735\n",
       "5         MT-RNR2  GO:0005840\n",
       "6          MT-TL1  GO:0030533\n",
       "...           ...         ...\n",
       "13715124     LDB1      SAMD14\n",
       "13715125     LDB1       KDM6B\n",
       "13715126     LDB1        WWP2\n",
       "13715127     LDB1      VPS33B\n",
       "13715128     LDB1       NDST2\n",
       "\n",
       "[13544448 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_edges = pd.concat([go_protein_df, go_edges_df, gene_edges_df])\n",
    "#combined_edges = combined_edges[['Source', 'Target']]\n",
    "combined_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "049b6ea0-0ee7-4002-bfb1-32bf8c25ede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_in_features = set(combined_features['protein'])\n",
    "\n",
    "filtered_edges_df = combined_edges[\n",
    "    combined_edges['Source'].isin(nodes_in_features) & combined_edges['Target'].isin(nodes_in_features)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fd8258c-b6da-4923-9fdf-ce1f72d1cf22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24323, 17460, 27132,  ...,   947,  6874, 13222],\n",
       "        [ 2077,  2077,  2077,  ..., 10107, 10107, 10107]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_id_to_index = {node_id: i for i, node_id in enumerate(combined_features['protein'])}\n",
    "# 确保edge_index是按照这个新的索引顺序排列的\n",
    "source_indices = [node_id_to_index[node_id] for node_id in filtered_edges_df['Source']]\n",
    "target_indices = [node_id_to_index[node_id] for node_id in filtered_edges_df['Target']]\n",
    "edge_index = torch.tensor([source_indices, target_indices], dtype=torch.long)\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c3bc3ad-3483-4e45-b174-5e3cc910f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_labels_df = labels_df[\n",
    "    labels_df['protein'].isin(nodes_in_features)]\n",
    "filtered_labels_df = filtered_labels_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95f2ea9c-5199-4d01-99e6-5ab0a4e179dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein</th>\n",
       "      <th>Subcellular_location</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORC5</td>\n",
       "      <td>Nucleus</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SFI1</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SUCNR1</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ELK3</td>\n",
       "      <td>Nucleus</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CPEB3</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7698</th>\n",
       "      <td>RPS6KB2</td>\n",
       "      <td>Cytoplasm</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7699</th>\n",
       "      <td>INSR</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7700</th>\n",
       "      <td>SLC7A1</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7701</th>\n",
       "      <td>SLC44A2</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7702</th>\n",
       "      <td>PARM1</td>\n",
       "      <td>Cell membrane</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7703 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      protein Subcellular_location  Label\n",
       "0        ORC5              Nucleus      1\n",
       "1        SFI1            Cytoplasm      2\n",
       "2      SUCNR1        Cell membrane      0\n",
       "3        ELK3              Nucleus      1\n",
       "4       CPEB3            Cytoplasm      2\n",
       "...       ...                  ...    ...\n",
       "7698  RPS6KB2            Cytoplasm      2\n",
       "7699     INSR        Cell membrane      0\n",
       "7700   SLC7A1        Cell membrane      0\n",
       "7701  SLC44A2        Cell membrane      0\n",
       "7702    PARM1        Cell membrane      0\n",
       "\n",
       "[7703 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d792ff08-3120-43c8-886c-4e9656ddf453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1224800/3260523373.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_tensor = torch.tensor(labels, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "label_indices = [node_id_to_index[node_id] for node_id in filtered_labels_df['protein']]\n",
    "#print(label_indices)\n",
    "num_nodes = len(combined_features)\n",
    "labels = torch.full((num_nodes,), -1, dtype=torch.long)\n",
    "for i, index in enumerate(filtered_labels_df['Label']):\n",
    "    labels[label_indices[i]] = index\n",
    "\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ca93ddb-87e4-4ed2-8cea-0cacfe498b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2, -1, -1,  ..., -1, -1, -1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d25264f3-36e6-4d32-a0a3-a9c220aabcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.y 中有 3 种不同的元素（不包括 -1）\n",
      "data.y 中的不同元素有（不包括 -1）：\n",
      "tensor([0, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1224800/306967558.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_tensor = torch.tensor(labels, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# 获取 tensor 中不包括 -1 的不同元素\n",
    "unique_elements = torch.unique(labels_tensor[labels_tensor != -1])\n",
    "num_unique_elements = unique_elements.numel()\n",
    "\n",
    "print(f\"data.y 中有 {num_unique_elements} 种不同的元素（不包括 -1）\")\n",
    "print(\"data.y 中的不同元素有（不包括 -1）：\")\n",
    "print(unique_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0222c264-d7f2-443a-8fab-998124afc361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33960226, -0.03074448, -0.90138096, ...,  0.01558092,\n",
       "        -0.02386307, -0.02200161],\n",
       "       [-0.13179901, -0.02574519, -0.67730105, ..., -0.03992649,\n",
       "        -0.10278717, -0.02697964],\n",
       "       [ 0.38569278, -0.07069244, -0.8477959 , ...,  0.0253919 ,\n",
       "        -0.06603534, -0.02828273],\n",
       "       ...,\n",
       "       [ 0.02713387,  0.24139147, -0.22735251, ..., -0.82107705,\n",
       "         1.036363  , -0.83661443],\n",
       "       [ 0.13954346,  0.02888298,  0.89947975, ..., -0.9852566 ,\n",
       "         1.6735605 ,  0.10965873],\n",
       "       [ 0.08306409,  0.09089889,  0.8885408 , ..., -0.79955566,\n",
       "         1.5193683 ,  0.2632099 ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = combined_features.iloc[:, 1:].values\n",
    "features_tensor = torch.tensor(features, dtype=torch.float)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "223cb4ce-69e0-4549-a65a-c6abee33ec84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([62045, 768]) torch.float32\n",
      "edge_index: torch.Size([2, 9914754]) torch.int64\n",
      "labels: torch.Size([62045]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "data = Data(x=features_tensor, edge_index=edge_index, y=labels_tensor)\n",
    "\n",
    "print(\"x:\", data.x.shape, data.x.dtype)\n",
    "print(\"edge_index:\", data.edge_index.shape, data.edge_index.dtype)\n",
    "print(\"labels:\", data.y.shape, data.y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1a8a850-0d05-4bdf-870e-579376dbb82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def accuracy(pred, target):\n",
    "    r\"\"\"Computes the accuracy of correct predictions.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    return (pred == target).sum().item() / target.numel()\n",
    "\n",
    "\n",
    "\n",
    "def true_positive(pred, target, num_classes):\n",
    "    r\"\"\"Computes the number of true positive predictions.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`LongTensor`\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred == i) & (target == i)).sum())\n",
    "\n",
    "    return torch.tensor(out)\n",
    "\n",
    "\n",
    "\n",
    "def true_negative(pred, target, num_classes):\n",
    "    r\"\"\"Computes the number of true negative predictions.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`LongTensor`\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred != i) & (target != i)).sum())\n",
    "\n",
    "    return torch.tensor(out)\n",
    "\n",
    "\n",
    "\n",
    "def false_positive(pred, target, num_classes):\n",
    "    r\"\"\"Computes the number of false positive predictions.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`LongTensor`\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred == i) & (target != i)).sum())\n",
    "\n",
    "    return torch.tensor(out)\n",
    "\n",
    "\n",
    "\n",
    "def false_negative(pred, target, num_classes):\n",
    "    r\"\"\"Computes the number of false negative predictions.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`LongTensor`\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred != i) & (target == i)).sum())\n",
    "\n",
    "    return torch.tensor(out)\n",
    "\n",
    "\n",
    "\n",
    "def precision(pred, target, num_classes):\n",
    "    r\"\"\"Computes the precision:\n",
    "    :math:`\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}}`.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`Tensor`\n",
    "    \"\"\"\n",
    "    tp = true_positive(pred, target, num_classes).to(torch.float)\n",
    "    fp = false_positive(pred, target, num_classes).to(torch.float)\n",
    "\n",
    "    out = tp / (tp + fp)\n",
    "    out[torch.isnan(out)] = 0\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def recall(pred, target, num_classes):\n",
    "    r\"\"\"Computes the recall:\n",
    "    :math:`\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}`.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`Tensor`\n",
    "    \"\"\"\n",
    "    tp = true_positive(pred, target, num_classes).to(torch.float)\n",
    "    fn = false_negative(pred, target, num_classes).to(torch.float)\n",
    "\n",
    "    out = tp / (tp + fn)\n",
    "    out[torch.isnan(out)] = 0\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def f1_score(pred, target, num_classes):\n",
    "    r\"\"\"Computes the :math:`F_1` score:\n",
    "    :math:`2 \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}\n",
    "    {\\mathrm{precision}+\\mathrm{recall}}`.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): The predictions.\n",
    "        target (Tensor): The targets.\n",
    "        num_classes (int): The number of classes.\n",
    "\n",
    "    :rtype: :class:`Tensor`\n",
    "    \"\"\"\n",
    "    prec = precision(pred, target, num_classes)\n",
    "    rec = recall(pred, target, num_classes)\n",
    "\n",
    "    score = 2 * (prec * rec) / (prec + rec)\n",
    "    score[torch.isnan(score)] = 0\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85335c36-e157-412b-9156-871dc2ef9752",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import ModuleList, Dropout, Linear\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, num_classes, num_layers, activation, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.convs = ModuleList([GCNConv(hidden_dim, hidden_dim) for _ in range(num_layers - 2)])\n",
    "        self.conv_last = GCNConv(hidden_dim, num_classes)\n",
    "        self.activation = activation\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.num_layers = num_layers\n",
    "        self.fc = torch.nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # 输入层\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 隐藏层\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # 输出层\n",
    "        #x = self.conv_last(x, edge_index)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    def get_embedding(self, x, edge_index):\n",
    "        # 输入层\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 隐藏层\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "576d0c4c-8d51-4d5c-bf80-f5f5475d7017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "def train_model_scheduler(model, masked_features, labels, edge_index, optimizer, criterion, scheduler, train_mask):\n",
    "    model.train()  # 设置模型为训练模\n",
    "    optimizer.zero_grad()  # 清空梯度\n",
    "    out = model(masked_features, edge_index)  # 获取模型输出\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])  # 计算损失值，只针对训练集的节点\n",
    "    loss.backward()  # 反向传播\n",
    "    optimizer.step()  # 更新模型参数\n",
    "    scheduler.step(loss)\n",
    "    return loss.item()\n",
    "\n",
    "def train_model(model, masked_features, labels, edge_index, optimizer, criterion, train_mask):\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    optimizer.zero_grad()  # 清空梯度\n",
    "    out = model(masked_features, edge_index) # 获取模型输出\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])  # 计算损失值，只针对训练集的节点\n",
    "    loss.backward()  # 反向传播\n",
    "    optimizer.step()  # 更新模型参数\n",
    "    return loss.item()\n",
    "\"\"\"\n",
    "def evaluate_model(model, features, labels, edge_index, mask):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    with torch.no_grad():  # 关闭梯度计算\n",
    "        # 获取模型输出，这里假设输出已经是经过sigmoid的概率\n",
    "        probabilities = model(features, edge_index)\n",
    "        predictions = torch.argmax(probabilities[mask], dim=1)  # 获取预测类别\n",
    "        \n",
    "        # F1 分数 - 宏平均\n",
    "        val_f1 = f1_score(labels[mask].cpu().numpy(), predictions.cpu().numpy(), average='macro')\n",
    "        \n",
    "        # 多分类AUC计算（一对多方法）\n",
    "        auc_score = 0\n",
    "        for class_idx in range(probabilities.shape[1]):  # 遍历每个类别\n",
    "            true_binary = (labels[mask] == class_idx).cpu().numpy()  # 当前类别为正类，其他类别为负类\n",
    "            pred_probs = probabilities[mask, class_idx].cpu().numpy()  # 当前类别的预测概率\n",
    "            auc_score += roc_auc_score(true_binary, pred_probs)  # 计算当前类别的AUC\n",
    "        \n",
    "        # 计算平均AUC\n",
    "        auc_score /= probabilities.shape[1]\n",
    "\n",
    "    return val_f1, auc_score\"\"\"\n",
    "def evaluate_model(model, features, labels, edge_index, mask):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    with torch.no_grad():  # 关闭梯度计算\n",
    "        # 获取模型输出，这里假设输出已经是经过sigmoid的概率\n",
    "        probabilities = model(features, edge_index)\n",
    "        predictions = torch.argmax(probabilities[mask], dim=1)  # 获取预测类别\n",
    "        auc_score = 0\n",
    "        for class_idx in range(probabilities.shape[1]):  # 遍历每个类别\n",
    "            true_binary = (labels[mask] == class_idx).cpu().numpy()  # 当前类别为正类，其他类别为负类\n",
    "            pred_probs = probabilities[mask, class_idx].cpu().numpy()  # 当前类别的预测概率\n",
    "            auc_score += roc_auc_score(true_binary, pred_probs)  # 计算当前类别的AUC\n",
    "        \n",
    "        # 计算平均AUC\n",
    "        auc_score /= probabilities.shape[1]\n",
    "\n",
    "        val_f1 = torch.mean(f1_score(torch.argmax(probabilities[mask],dim=1), labels[mask], num_classes=3)).cpu().numpy()\n",
    "        #auc_score = roc_auc_score(labels[mask].cpu().numpy(), positive_probs.cpu().numpy())\n",
    "\n",
    "    return val_f1, auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84209f0a-23fe-4d9f-8535-041c7a651ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1541\n",
      "tensor([ True, False, False,  ..., False, False, False])\n",
      "1541\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "1541\n",
      "tensor([ True, False, False,  ..., False, False, False])\n",
      "1541\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "1541\n",
      "tensor([ True, False, False,  ..., False, False, False])\n"
     ]
    }
   ],
   "source": [
    "#localization\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "resilt_localization = []\n",
    "for x in range(5):\n",
    "    # 实例化模型\n",
    "    device = torch.device('cuda:1')\n",
    "    data = data.to(device)\n",
    "    model = GCN(num_features=768, hidden_dim=64, num_classes=3, num_layers=3, activation=F.relu, dropout=0.5)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=50, verbose=True)\n",
    "    \n",
    "    labeled_indices = label_indices\n",
    "    random.shuffle(labeled_indices)\n",
    "    num_labeled = len(labeled_indices)\n",
    "    num_train = int(num_labeled * 0.8)\n",
    "    num_test = num_labeled - num_train\n",
    "    print(num_test)\n",
    "    \n",
    "    # 创建训练和测试掩码\n",
    "    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    \n",
    "    train_mask[labeled_indices[:num_train]] = True\n",
    "    test_mask[labeled_indices[num_train:num_train+num_test]] = True\n",
    "    print(test_mask)\n",
    "    #fully connection\n",
    "    \"\"\"\n",
    "    num_epochs = 1000\n",
    "    x_type1 = data.x[:14450]\n",
    "    x_type2 = data.x[14450:]\n",
    "    x_list = [x_type1, x_type2]\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model_scheduler(model, data.x, data.y, data.edge_index, optimizer, loss_fn, scheduler, train_mask)\n",
    "        test_f1, test_auc = evaluate_model(model, data.x, data.y, data.edge_index, test_mask)\n",
    "        \n",
    "        if (epoch + 1) % 50 == 0: \n",
    "            print(f'Epoch {epoch}: Train Loss:{train_loss:.4f}, Macro_F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "    resilt_localization.append({'F1': test_f1,\n",
    "                    'AUC_score': test_auc})\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a68d4656-0bca-49e5-ac55-f863867a2782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'F1': array(0.8575546, dtype=float32), 'AUC_score': 0.9388644856089505},\n",
       " {'F1': array(0.86176014, dtype=float32), 'AUC_score': 0.9444933915799559},\n",
       " {'F1': array(0.8658428, dtype=float32), 'AUC_score': 0.946029501826635},\n",
       " {'F1': array(0.881814, dtype=float32), 'AUC_score': 0.950830882457491},\n",
       " {'F1': array(0.8532157, dtype=float32), 'AUC_score': 0.9430605067229267}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resilt_localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53458772-13a0-4faf-b505-178806e0cbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 49: Train Loss:0.3885, Macro_F1: 0.8178, AUC_score: 0.8721\n",
      "Epoch 99: Train Loss:0.3355, Macro_F1: 0.8348, AUC_score: 0.8818\n",
      "Epoch 149: Train Loss:0.2737, Macro_F1: 0.8149, AUC_score: 0.8922\n",
      "Epoch 199: Train Loss:0.2311, Macro_F1: 0.8492, AUC_score: 0.8942\n",
      "Epoch 249: Train Loss:0.1804, Macro_F1: 0.8492, AUC_score: 0.8766\n",
      "Epoch 299: Train Loss:0.1427, Macro_F1: 0.8201, AUC_score: 0.9110\n",
      "Epoch 349: Train Loss:0.1444, Macro_F1: 0.8519, AUC_score: 0.8987\n",
      "Epoch 399: Train Loss:0.1165, Macro_F1: 0.8519, AUC_score: 0.8935\n",
      "Epoch 449: Train Loss:0.1203, Macro_F1: 0.8348, AUC_score: 0.8987\n",
      "Epoch 499: Train Loss:0.0841, Macro_F1: 0.8693, AUC_score: 0.8935\n",
      "Epoch 549: Train Loss:0.1089, Macro_F1: 0.8348, AUC_score: 0.8929\n",
      "Epoch 599: Train Loss:0.0920, Macro_F1: 0.8519, AUC_score: 0.8890\n",
      "Epoch 649: Train Loss:0.0754, Macro_F1: 0.8519, AUC_score: 0.8844\n",
      "Epoch 00679: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 699: Train Loss:0.0768, Macro_F1: 0.8371, AUC_score: 0.8903\n",
      "Epoch 00739: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 749: Train Loss:0.0569, Macro_F1: 0.8348, AUC_score: 0.8851\n",
      "Epoch 00790: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 799: Train Loss:0.0491, Macro_F1: 0.8519, AUC_score: 0.8831\n",
      "Epoch 00841: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 849: Train Loss:0.0622, Macro_F1: 0.8519, AUC_score: 0.8812\n",
      "Epoch 00892: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 899: Train Loss:0.0506, Macro_F1: 0.8519, AUC_score: 0.8818\n",
      "Epoch 00943: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 949: Train Loss:0.0526, Macro_F1: 0.8519, AUC_score: 0.8818\n",
      "Epoch 999: Train Loss:0.0747, Macro_F1: 0.8519, AUC_score: 0.8818\n",
      "57\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 49: Train Loss:0.4095, Macro_F1: 0.7799, AUC_score: 0.8532\n",
      "Epoch 99: Train Loss:0.2893, Macro_F1: 0.8145, AUC_score: 0.9117\n",
      "Epoch 149: Train Loss:0.2333, Macro_F1: 0.8286, AUC_score: 0.9364\n",
      "Epoch 199: Train Loss:0.1985, Macro_F1: 0.8376, AUC_score: 0.9305\n",
      "Epoch 249: Train Loss:0.1699, Macro_F1: 0.8459, AUC_score: 0.9455\n",
      "Epoch 299: Train Loss:0.1469, Macro_F1: 0.8667, AUC_score: 0.9623\n",
      "Epoch 349: Train Loss:0.1154, Macro_F1: 0.8667, AUC_score: 0.9558\n",
      "Epoch 399: Train Loss:0.1199, Macro_F1: 0.8715, AUC_score: 0.9617\n",
      "Epoch 449: Train Loss:0.0714, Macro_F1: 0.8459, AUC_score: 0.9552\n",
      "Epoch 499: Train Loss:0.0697, Macro_F1: 0.8459, AUC_score: 0.9435\n",
      "Epoch 00506: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 549: Train Loss:0.0621, Macro_F1: 0.8459, AUC_score: 0.9513\n",
      "Epoch 599: Train Loss:0.0674, Macro_F1: 0.8459, AUC_score: 0.9409\n",
      "Epoch 00615: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 649: Train Loss:0.0584, Macro_F1: 0.8459, AUC_score: 0.9519\n",
      "Epoch 699: Train Loss:0.0586, Macro_F1: 0.8459, AUC_score: 0.9526\n",
      "Epoch 749: Train Loss:0.0591, Macro_F1: 0.8459, AUC_score: 0.9513\n",
      "Epoch 00765: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 799: Train Loss:0.0534, Macro_F1: 0.8459, AUC_score: 0.9513\n",
      "Epoch 00816: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 849: Train Loss:0.0505, Macro_F1: 0.8459, AUC_score: 0.9513\n",
      "Epoch 00867: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 899: Train Loss:0.0641, Macro_F1: 0.8459, AUC_score: 0.9513\n",
      "Epoch 00943: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 949: Train Loss:0.0612, Macro_F1: 0.8459, AUC_score: 0.9513\n",
      "Epoch 00994: reducing learning rate of group 0 to 1.2800e-08.\n",
      "Epoch 999: Train Loss:0.0613, Macro_F1: 0.8459, AUC_score: 0.9513\n",
      "57\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 49: Train Loss:0.4076, Macro_F1: 0.7331, AUC_score: 0.8147\n",
      "Epoch 99: Train Loss:0.3144, Macro_F1: 0.7159, AUC_score: 0.8184\n",
      "Epoch 149: Train Loss:0.2280, Macro_F1: 0.7799, AUC_score: 0.9037\n",
      "Epoch 199: Train Loss:0.2029, Macro_F1: 0.7856, AUC_score: 0.9110\n",
      "Epoch 249: Train Loss:0.1497, Macro_F1: 0.7904, AUC_score: 0.9044\n",
      "Epoch 299: Train Loss:0.1151, Macro_F1: 0.7856, AUC_score: 0.9088\n",
      "Epoch 349: Train Loss:0.1289, Macro_F1: 0.8145, AUC_score: 0.9022\n",
      "Epoch 399: Train Loss:0.0949, Macro_F1: 0.8507, AUC_score: 0.9059\n",
      "Epoch 449: Train Loss:0.0855, Macro_F1: 0.8081, AUC_score: 0.9022\n",
      "Epoch 499: Train Loss:0.0828, Macro_F1: 0.8026, AUC_score: 0.9037\n",
      "Epoch 549: Train Loss:0.0799, Macro_F1: 0.7738, AUC_score: 0.8912\n",
      "Epoch 00592: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 599: Train Loss:0.0661, Macro_F1: 0.8026, AUC_score: 0.9125\n",
      "Epoch 649: Train Loss:0.0789, Macro_F1: 0.8026, AUC_score: 0.9066\n",
      "Epoch 699: Train Loss:0.0534, Macro_F1: 0.8026, AUC_score: 0.9088\n",
      "Epoch 749: Train Loss:0.0521, Macro_F1: 0.7799, AUC_score: 0.9007\n",
      "Epoch 799: Train Loss:0.0701, Macro_F1: 0.8324, AUC_score: 0.9007\n",
      "Epoch 00838: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 849: Train Loss:0.0524, Macro_F1: 0.8026, AUC_score: 0.9022\n",
      "Epoch 00900: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 899: Train Loss:0.0416, Macro_F1: 0.8026, AUC_score: 0.9022\n",
      "Epoch 949: Train Loss:0.0479, Macro_F1: 0.8026, AUC_score: 0.9029\n",
      "Epoch 999: Train Loss:0.0465, Macro_F1: 0.8026, AUC_score: 0.9029\n",
      "57\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 49: Train Loss:0.4075, Macro_F1: 0.7779, AUC_score: 0.8688\n",
      "Epoch 99: Train Loss:0.3159, Macro_F1: 0.8286, AUC_score: 0.9045\n",
      "Epoch 149: Train Loss:0.2472, Macro_F1: 0.7946, AUC_score: 0.9221\n",
      "Epoch 199: Train Loss:0.1795, Macro_F1: 0.8561, AUC_score: 0.9253\n",
      "Epoch 249: Train Loss:0.1512, Macro_F1: 0.7946, AUC_score: 0.9156\n",
      "Epoch 299: Train Loss:0.1342, Macro_F1: 0.8149, AUC_score: 0.9175\n",
      "Epoch 349: Train Loss:0.1407, Macro_F1: 0.8542, AUC_score: 0.9143\n",
      "Epoch 399: Train Loss:0.0950, Macro_F1: 0.8149, AUC_score: 0.9058\n",
      "Epoch 449: Train Loss:0.0773, Macro_F1: 0.7905, AUC_score: 0.8955\n",
      "Epoch 00491: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 499: Train Loss:0.0843, Macro_F1: 0.8348, AUC_score: 0.9052\n",
      "Epoch 549: Train Loss:0.0936, Macro_F1: 0.8348, AUC_score: 0.9039\n",
      "Epoch 599: Train Loss:0.0664, Macro_F1: 0.8348, AUC_score: 0.9019\n",
      "Epoch 00620: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 649: Train Loss:0.0608, Macro_F1: 0.8348, AUC_score: 0.8974\n",
      "Epoch 00677: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 699: Train Loss:0.0829, Macro_F1: 0.8348, AUC_score: 0.8974\n",
      "Epoch 749: Train Loss:0.0627, Macro_F1: 0.8348, AUC_score: 0.8974\n",
      "Epoch 00797: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 799: Train Loss:0.0696, Macro_F1: 0.8348, AUC_score: 0.8974\n",
      "Epoch 849: Train Loss:0.0745, Macro_F1: 0.8348, AUC_score: 0.8974\n",
      "Epoch 00878: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 899: Train Loss:0.0713, Macro_F1: 0.8348, AUC_score: 0.8974\n",
      "Epoch 00929: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 949: Train Loss:0.0750, Macro_F1: 0.8348, AUC_score: 0.8974\n",
      "Epoch 00980: reducing learning rate of group 0 to 1.2800e-08.\n",
      "Epoch 999: Train Loss:0.0608, Macro_F1: 0.8348, AUC_score: 0.8974\n",
      "57\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 49: Train Loss:0.4094, Macro_F1: 0.7905, AUC_score: 0.8491\n",
      "Epoch 99: Train Loss:0.2823, Macro_F1: 0.8031, AUC_score: 0.8651\n",
      "Epoch 149: Train Loss:0.2093, Macro_F1: 0.7614, AUC_score: 0.8779\n",
      "Epoch 199: Train Loss:0.1712, Macro_F1: 0.8049, AUC_score: 0.8734\n",
      "Epoch 249: Train Loss:0.1654, Macro_F1: 0.7199, AUC_score: 0.8517\n",
      "Epoch 299: Train Loss:0.1229, Macro_F1: 0.8049, AUC_score: 0.8625\n",
      "Epoch 349: Train Loss:0.1190, Macro_F1: 0.7084, AUC_score: 0.8587\n",
      "Epoch 399: Train Loss:0.0766, Macro_F1: 0.8049, AUC_score: 0.8734\n",
      "Epoch 449: Train Loss:0.1724, Macro_F1: 0.7246, AUC_score: 0.8478\n",
      "Epoch 499: Train Loss:0.1222, Macro_F1: 0.7449, AUC_score: 0.8638\n",
      "Epoch 00534: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 549: Train Loss:0.0668, Macro_F1: 0.7246, AUC_score: 0.8402\n",
      "Epoch 599: Train Loss:0.0490, Macro_F1: 0.7246, AUC_score: 0.8472\n",
      "Epoch 00611: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 649: Train Loss:0.0556, Macro_F1: 0.7246, AUC_score: 0.8478\n",
      "Epoch 699: Train Loss:0.0634, Macro_F1: 0.7449, AUC_score: 0.8497\n",
      "Epoch 00711: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 749: Train Loss:0.0458, Macro_F1: 0.7449, AUC_score: 0.8542\n",
      "Epoch 799: Train Loss:0.0555, Macro_F1: 0.7449, AUC_score: 0.8529\n",
      "Epoch 849: Train Loss:0.0533, Macro_F1: 0.7449, AUC_score: 0.8536\n",
      "Epoch 00864: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 899: Train Loss:0.0471, Macro_F1: 0.7449, AUC_score: 0.8523\n",
      "Epoch 00915: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 949: Train Loss:0.0552, Macro_F1: 0.7449, AUC_score: 0.8523\n",
      "Epoch 00966: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 999: Train Loss:0.0553, Macro_F1: 0.7449, AUC_score: 0.8523\n",
      "[{'F1': array(0.851948, dtype=float32), 'AUC_score': 0.8818181818181818}, {'F1': array(0.84594595, dtype=float32), 'AUC_score': 0.9512987012987013}, {'F1': array(0.8026316, dtype=float32), 'AUC_score': 0.9029411764705882}, {'F1': array(0.8347826, dtype=float32), 'AUC_score': 0.8974025974025974}, {'F1': array(0.7448849, dtype=float32), 'AUC_score': 0.8523017902813299}]\n"
     ]
    }
   ],
   "source": [
    "#DosageSensitivity\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "result_DosageSensitivity = []\n",
    "for x in range(5):\n",
    "    # 实例化模型\n",
    "    device = torch.device('cuda:1')\n",
    "    data = data.to(device)\n",
    "    model = GCN(num_features=768, hidden_dim=64, num_classes=2, num_layers=3, activation=F.relu, dropout=0.5)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=50, verbose=True)\n",
    "    \n",
    "    labeled_indices = label_indices\n",
    "    random.shuffle(labeled_indices)\n",
    "    num_labeled = len(labeled_indices)\n",
    "    num_train = int(num_labeled * 0.8)\n",
    "    num_test = num_labeled - num_train\n",
    "    print(num_test)\n",
    "    \n",
    "    # 创建训练和测试掩码\n",
    "    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    \n",
    "    train_mask[labeled_indices[:num_train]] = True\n",
    "    test_mask[labeled_indices[num_train:num_train+num_test]] = True\n",
    "    print(test_mask)\n",
    "    #fully connection\n",
    "    num_epochs = 1000\n",
    "    x_type1 = data.x[:14450]\n",
    "    x_type2 = data.x[14450:]\n",
    "    x_list = [x_type1, x_type2]\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model_scheduler(model, data.x, data.y, data.edge_index, optimizer, loss_fn, scheduler, train_mask)\n",
    "        test_f1, test_auc = evaluate_model(model, data.x, data.y, data.edge_index, test_mask)\n",
    "        \n",
    "        if (epoch + 1) % 50 == 0: \n",
    "            print(f'Epoch {epoch}: Train Loss:{train_loss:.4f}, Macro_F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "    result_DosageSensitivity.append({'F1': test_f1,\n",
    "                    'AUC_score': test_auc})\n",
    "print(result_DosageSensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "056b0d2a-3c81-4a6b-9e9c-8efebbd0494f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 49: Train Loss:0.3229, Macro_F1: 0.9699, AUC_score: 0.9930\n",
      "Epoch 99: Train Loss:0.1598, Macro_F1: 0.9699, AUC_score: 0.9439\n",
      "Epoch 149: Train Loss:0.1060, Macro_F1: 0.9111, AUC_score: 0.9386\n",
      "Epoch 199: Train Loss:0.0430, Macro_F1: 0.9393, AUC_score: 0.9228\n",
      "Epoch 249: Train Loss:0.0374, Macro_F1: 0.9393, AUC_score: 0.9211\n",
      "Epoch 299: Train Loss:0.0734, Macro_F1: 0.9393, AUC_score: 0.9035\n",
      "Epoch 00320: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 349: Train Loss:0.0199, Macro_F1: 0.9098, AUC_score: 0.9140\n",
      "Epoch 00387: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 399: Train Loss:0.0224, Macro_F1: 0.9393, AUC_score: 0.9070\n",
      "Epoch 00441: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 449: Train Loss:0.0100, Macro_F1: 0.9393, AUC_score: 0.9053\n",
      "Epoch 00492: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 499: Train Loss:0.0181, Macro_F1: 0.9393, AUC_score: 0.9035\n",
      "Epoch 00543: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 549: Train Loss:0.0180, Macro_F1: 0.9393, AUC_score: 0.9035\n",
      "Epoch 00594: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 599: Train Loss:0.0263, Macro_F1: 0.9393, AUC_score: 0.9035\n",
      "Epoch 00645: reducing learning rate of group 0 to 1.2800e-08.\n",
      "Epoch 649: Train Loss:0.0166, Macro_F1: 0.9393, AUC_score: 0.9035\n",
      "Epoch 00696: reducing learning rate of group 0 to 2.5600e-09.\n",
      "Epoch 699: Train Loss:0.0231, Macro_F1: 0.9393, AUC_score: 0.9035\n",
      "Epoch 749: Train Loss:0.0109, Macro_F1: 0.9393, AUC_score: 0.9035\n",
      "Epoch 799: Train Loss:0.0169, Macro_F1: 0.9393, AUC_score: 0.9035\n",
      "Epoch 849: Train Loss:0.0256, Macro_F1: 0.9393, AUC_score: 0.9035\n",
      "Epoch 899: Train Loss:0.0113, Macro_F1: 0.9393, AUC_score: 0.9035\n",
      "Epoch 949: Train Loss:0.0234, Macro_F1: 0.9393, AUC_score: 0.9035\n",
      "Epoch 999: Train Loss:0.0146, Macro_F1: 0.9393, AUC_score: 0.9035\n",
      "34\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 49: Train Loss:0.2620, Macro_F1: 0.9345, AUC_score: 0.9782\n",
      "Epoch 99: Train Loss:0.1253, Macro_F1: 0.9345, AUC_score: 0.9623\n",
      "Epoch 149: Train Loss:0.1271, Macro_F1: 0.9345, AUC_score: 0.9444\n",
      "Epoch 199: Train Loss:0.0486, Macro_F1: 0.9345, AUC_score: 0.9147\n",
      "Epoch 249: Train Loss:0.0467, Macro_F1: 0.9345, AUC_score: 0.9127\n",
      "Epoch 299: Train Loss:0.0166, Macro_F1: 0.9345, AUC_score: 0.9187\n",
      "Epoch 349: Train Loss:0.0201, Macro_F1: 0.9345, AUC_score: 0.9206\n",
      "Epoch 399: Train Loss:0.0127, Macro_F1: 0.9345, AUC_score: 0.9087\n",
      "Epoch 449: Train Loss:0.0050, Macro_F1: 0.9345, AUC_score: 0.9087\n",
      "Epoch 499: Train Loss:0.0457, Macro_F1: 0.9345, AUC_score: 0.9008\n",
      "Epoch 00513: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 549: Train Loss:0.0080, Macro_F1: 0.9345, AUC_score: 0.9067\n",
      "Epoch 00564: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 599: Train Loss:0.0127, Macro_F1: 0.9345, AUC_score: 0.9067\n",
      "Epoch 00639: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 649: Train Loss:0.0057, Macro_F1: 0.9345, AUC_score: 0.9067\n",
      "Epoch 00690: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 699: Train Loss:0.0112, Macro_F1: 0.9345, AUC_score: 0.9067\n",
      "Epoch 00741: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 749: Train Loss:0.0091, Macro_F1: 0.9345, AUC_score: 0.9067\n",
      "Epoch 00792: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 799: Train Loss:0.0063, Macro_F1: 0.9345, AUC_score: 0.9067\n",
      "Epoch 00843: reducing learning rate of group 0 to 1.2800e-08.\n",
      "Epoch 849: Train Loss:0.0118, Macro_F1: 0.9345, AUC_score: 0.9067\n",
      "Epoch 00894: reducing learning rate of group 0 to 2.5600e-09.\n",
      "Epoch 899: Train Loss:0.0073, Macro_F1: 0.9345, AUC_score: 0.9067\n",
      "Epoch 949: Train Loss:0.0045, Macro_F1: 0.9345, AUC_score: 0.9067\n",
      "Epoch 999: Train Loss:0.0057, Macro_F1: 0.9345, AUC_score: 0.9067\n",
      "34\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 49: Train Loss:0.2233, Macro_F1: 0.8755, AUC_score: 0.9375\n",
      "Epoch 99: Train Loss:0.1340, Macro_F1: 0.8755, AUC_score: 0.9518\n",
      "Epoch 149: Train Loss:0.0651, Macro_F1: 0.9079, AUC_score: 0.9357\n",
      "Epoch 199: Train Loss:0.0234, Macro_F1: 0.9079, AUC_score: 0.9321\n",
      "Epoch 249: Train Loss:0.0261, Macro_F1: 0.9079, AUC_score: 0.9357\n",
      "Epoch 299: Train Loss:0.0085, Macro_F1: 0.9079, AUC_score: 0.9375\n",
      "Epoch 00345: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 349: Train Loss:0.0098, Macro_F1: 0.9079, AUC_score: 0.9321\n",
      "Epoch 399: Train Loss:0.0087, Macro_F1: 0.9079, AUC_score: 0.9339\n",
      "Epoch 00405: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 449: Train Loss:0.0159, Macro_F1: 0.9079, AUC_score: 0.9321\n",
      "Epoch 499: Train Loss:0.0134, Macro_F1: 0.9079, AUC_score: 0.9321\n",
      "Epoch 00503: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 549: Train Loss:0.0119, Macro_F1: 0.9079, AUC_score: 0.9321\n",
      "Epoch 00554: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 599: Train Loss:0.0050, Macro_F1: 0.9079, AUC_score: 0.9321\n",
      "Epoch 00605: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 649: Train Loss:0.0059, Macro_F1: 0.9079, AUC_score: 0.9321\n",
      "Epoch 00656: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 699: Train Loss:0.0159, Macro_F1: 0.9079, AUC_score: 0.9321\n",
      "Epoch 00716: reducing learning rate of group 0 to 1.2800e-08.\n",
      "Epoch 749: Train Loss:0.0064, Macro_F1: 0.9079, AUC_score: 0.9321\n",
      "Epoch 00767: reducing learning rate of group 0 to 2.5600e-09.\n",
      "Epoch 799: Train Loss:0.0099, Macro_F1: 0.9079, AUC_score: 0.9321\n",
      "Epoch 849: Train Loss:0.0142, Macro_F1: 0.9079, AUC_score: 0.9321\n",
      "Epoch 899: Train Loss:0.0053, Macro_F1: 0.9079, AUC_score: 0.9321\n",
      "Epoch 949: Train Loss:0.0041, Macro_F1: 0.9079, AUC_score: 0.9321\n",
      "Epoch 999: Train Loss:0.0097, Macro_F1: 0.9079, AUC_score: 0.9321\n",
      "34\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 49: Train Loss:0.2988, Macro_F1: 0.9283, AUC_score: 0.9876\n",
      "Epoch 99: Train Loss:0.1653, Macro_F1: 0.9283, AUC_score: 0.9752\n",
      "Epoch 149: Train Loss:0.0799, Macro_F1: 0.8891, AUC_score: 0.9649\n",
      "Epoch 199: Train Loss:0.0587, Macro_F1: 0.8891, AUC_score: 0.9690\n",
      "Epoch 249: Train Loss:0.0290, Macro_F1: 0.8565, AUC_score: 0.9566\n",
      "Epoch 299: Train Loss:0.0147, Macro_F1: 0.8565, AUC_score: 0.9566\n",
      "Epoch 349: Train Loss:0.0115, Macro_F1: 0.8565, AUC_score: 0.9380\n",
      "Epoch 399: Train Loss:0.0072, Macro_F1: 0.8565, AUC_score: 0.9236\n",
      "Epoch 449: Train Loss:0.0173, Macro_F1: 0.8565, AUC_score: 0.9483\n",
      "Epoch 499: Train Loss:0.0086, Macro_F1: 0.8565, AUC_score: 0.9752\n",
      "Epoch 00504: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 549: Train Loss:0.0065, Macro_F1: 0.8565, AUC_score: 0.9545\n",
      "Epoch 00594: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 599: Train Loss:0.0046, Macro_F1: 0.8565, AUC_score: 0.9380\n",
      "Epoch 00645: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 649: Train Loss:0.0057, Macro_F1: 0.8565, AUC_score: 0.9360\n",
      "Epoch 699: Train Loss:0.0075, Macro_F1: 0.8565, AUC_score: 0.9360\n",
      "Epoch 00746: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 749: Train Loss:0.0053, Macro_F1: 0.8565, AUC_score: 0.9360\n",
      "Epoch 00797: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 799: Train Loss:0.0075, Macro_F1: 0.8565, AUC_score: 0.9360\n",
      "Epoch 00848: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 849: Train Loss:0.0049, Macro_F1: 0.8565, AUC_score: 0.9360\n",
      "Epoch 00899: reducing learning rate of group 0 to 1.2800e-08.\n",
      "Epoch 899: Train Loss:0.0045, Macro_F1: 0.8565, AUC_score: 0.9360\n",
      "Epoch 00950: reducing learning rate of group 0 to 2.5600e-09.\n",
      "Epoch 949: Train Loss:0.0100, Macro_F1: 0.8565, AUC_score: 0.9360\n",
      "Epoch 999: Train Loss:0.0067, Macro_F1: 0.8565, AUC_score: 0.9360\n",
      "34\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 49: Train Loss:0.3273, Macro_F1: 0.9345, AUC_score: 0.9827\n",
      "Epoch 99: Train Loss:0.1737, Macro_F1: 0.9365, AUC_score: 0.9788\n",
      "Epoch 149: Train Loss:0.1013, Macro_F1: 0.9365, AUC_score: 0.9481\n",
      "Epoch 199: Train Loss:0.0428, Macro_F1: 0.9678, AUC_score: 0.9442\n",
      "Epoch 249: Train Loss:0.0504, Macro_F1: 0.9365, AUC_score: 0.9327\n",
      "Epoch 299: Train Loss:0.0499, Macro_F1: 0.9365, AUC_score: 0.9308\n",
      "Epoch 349: Train Loss:0.0333, Macro_F1: 0.9365, AUC_score: 0.9308\n",
      "Epoch 399: Train Loss:0.0081, Macro_F1: 0.9365, AUC_score: 0.9288\n",
      "Epoch 00421: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 449: Train Loss:0.0115, Macro_F1: 0.9365, AUC_score: 0.9288\n",
      "Epoch 00487: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 499: Train Loss:0.0071, Macro_F1: 0.9365, AUC_score: 0.9288\n",
      "Epoch 00538: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 549: Train Loss:0.0119, Macro_F1: 0.9365, AUC_score: 0.9288\n",
      "Epoch 00595: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 599: Train Loss:0.0100, Macro_F1: 0.9365, AUC_score: 0.9288\n",
      "Epoch 00646: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 649: Train Loss:0.0096, Macro_F1: 0.9365, AUC_score: 0.9288\n",
      "Epoch 00697: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 699: Train Loss:0.0114, Macro_F1: 0.9365, AUC_score: 0.9288\n",
      "Epoch 00748: reducing learning rate of group 0 to 1.2800e-08.\n",
      "Epoch 749: Train Loss:0.0089, Macro_F1: 0.9365, AUC_score: 0.9288\n",
      "Epoch 00799: reducing learning rate of group 0 to 2.5600e-09.\n",
      "Epoch 799: Train Loss:0.0131, Macro_F1: 0.9365, AUC_score: 0.9288\n",
      "Epoch 849: Train Loss:0.0121, Macro_F1: 0.9365, AUC_score: 0.9288\n",
      "Epoch 899: Train Loss:0.0111, Macro_F1: 0.9365, AUC_score: 0.9288\n",
      "Epoch 949: Train Loss:0.0086, Macro_F1: 0.9365, AUC_score: 0.9288\n",
      "Epoch 999: Train Loss:0.0066, Macro_F1: 0.9365, AUC_score: 0.9288\n",
      "[{'F1': array(0.93928576, dtype=float32), 'AUC_score': 0.9035087719298246}, {'F1': array(0.9345238, dtype=float32), 'AUC_score': 0.9067460317460316}, {'F1': array(0.9078591, dtype=float32), 'AUC_score': 0.9321428571428572}, {'F1': array(0.8565217, dtype=float32), 'AUC_score': 0.9359504132231404}, {'F1': array(0.93653846, dtype=float32), 'AUC_score': 0.9288461538461539}]\n"
     ]
    }
   ],
   "source": [
    "#BivalentVsLys4\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "result_BivalentVsLys4 = []\n",
    "for x in range(5):\n",
    "    # 实例化模型\n",
    "    device = torch.device('cuda:1')\n",
    "    data = data.to(device)\n",
    "    model = GCN(num_features=768, hidden_dim=64, num_classes=2, num_layers=3, activation=F.relu, dropout=0.5)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=50, verbose=True)\n",
    "    \n",
    "    labeled_indices = label_indices\n",
    "    random.shuffle(labeled_indices)\n",
    "    num_labeled = len(labeled_indices)\n",
    "    num_train = int(num_labeled * 0.8)\n",
    "    num_test = num_labeled - num_train\n",
    "    print(num_test)\n",
    "    \n",
    "    # 创建训练和测试掩码\n",
    "    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    \n",
    "    train_mask[labeled_indices[:num_train]] = True\n",
    "    test_mask[labeled_indices[num_train:num_train+num_test]] = True\n",
    "    print(test_mask)\n",
    "    #fully connection\n",
    "    num_epochs = 1000\n",
    "    x_type1 = data.x[:14450]\n",
    "    x_type2 = data.x[14450:]\n",
    "    x_list = [x_type1, x_type2]\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model_scheduler(model, data.x, data.y, data.edge_index, optimizer, loss_fn, scheduler, train_mask)\n",
    "        test_f1, test_auc = evaluate_model(model, data.x, data.y, data.edge_index, test_mask)\n",
    "        \n",
    "        if (epoch + 1) % 50 == 0: \n",
    "            print(f'Epoch {epoch}: Train Loss:{train_loss:.4f}, Macro_F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "    result_BivalentVsLys4.append({'F1': test_f1,\n",
    "                    'AUC_score': test_auc})\n",
    "print(result_BivalentVsLys4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5d0ae45e-1392-4aba-b20b-c6fe153515d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 49: Train Loss:0.3054, Macro_F1: 0.7984, AUC_score: 0.9458\n",
      "Epoch 99: Train Loss:0.1780, Macro_F1: 0.8762, AUC_score: 0.9250\n",
      "Epoch 149: Train Loss:0.0599, Macro_F1: 0.7984, AUC_score: 0.9667\n",
      "Epoch 199: Train Loss:0.0575, Macro_F1: 0.7984, AUC_score: 0.9875\n",
      "Epoch 249: Train Loss:0.0186, Macro_F1: 1.0000, AUC_score: 1.0000\n",
      "Epoch 299: Train Loss:0.0104, Macro_F1: 0.9424, AUC_score: 1.0000\n",
      "Epoch 349: Train Loss:0.0176, Macro_F1: 0.9424, AUC_score: 1.0000\n",
      "Epoch 399: Train Loss:0.0136, Macro_F1: 0.9424, AUC_score: 1.0000\n",
      "Epoch 449: Train Loss:0.0049, Macro_F1: 0.9424, AUC_score: 1.0000\n",
      "Epoch 00489: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 499: Train Loss:0.0048, Macro_F1: 0.9424, AUC_score: 1.0000\n",
      "Epoch 549: Train Loss:0.0059, Macro_F1: 0.9424, AUC_score: 1.0000\n",
      "Epoch 599: Train Loss:0.0066, Macro_F1: 0.9424, AUC_score: 1.0000\n",
      "Epoch 649: Train Loss:0.0078, Macro_F1: 0.9424, AUC_score: 1.0000\n",
      "26\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 49: Train Loss:0.2555, Macro_F1: 0.8689, AUC_score: 0.9132\n",
      "Epoch 99: Train Loss:0.1510, Macro_F1: 0.6410, AUC_score: 0.8889\n",
      "Epoch 149: Train Loss:0.0858, Macro_F1: 0.6750, AUC_score: 0.9028\n",
      "Epoch 199: Train Loss:0.0404, Macro_F1: 0.8689, AUC_score: 0.9271\n",
      "Epoch 249: Train Loss:0.0199, Macro_F1: 0.7658, AUC_score: 0.9236\n",
      "Epoch 299: Train Loss:0.0184, Macro_F1: 0.6750, AUC_score: 0.9340\n",
      "Epoch 349: Train Loss:0.0105, Macro_F1: 0.8194, AUC_score: 0.9306\n",
      "Epoch 399: Train Loss:0.0096, Macro_F1: 0.6750, AUC_score: 0.9340\n",
      "Epoch 00431: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 449: Train Loss:0.0059, Macro_F1: 0.6750, AUC_score: 0.9375\n",
      "Epoch 499: Train Loss:0.0058, Macro_F1: 0.6750, AUC_score: 0.9375\n",
      "Epoch 00511: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 549: Train Loss:0.0062, Macro_F1: 0.6750, AUC_score: 0.9375\n",
      "Epoch 599: Train Loss:0.0138, Macro_F1: 0.6750, AUC_score: 0.9375\n",
      "Epoch 00612: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 649: Train Loss:0.0069, Macro_F1: 0.6750, AUC_score: 0.9375\n",
      "26\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 49: Train Loss:0.3246, Macro_F1: 0.8663, AUC_score: 0.9367\n",
      "Epoch 99: Train Loss:0.1293, Macro_F1: 0.8162, AUC_score: 0.9200\n",
      "Epoch 149: Train Loss:0.0735, Macro_F1: 0.8663, AUC_score: 0.8933\n",
      "Epoch 199: Train Loss:0.0230, Macro_F1: 0.8663, AUC_score: 0.8600\n",
      "Epoch 249: Train Loss:0.0091, Macro_F1: 0.8663, AUC_score: 0.8600\n",
      "Epoch 299: Train Loss:0.0093, Macro_F1: 0.8663, AUC_score: 0.8600\n",
      "Epoch 00301: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 349: Train Loss:0.0103, Macro_F1: 0.8663, AUC_score: 0.8600\n",
      "Epoch 00386: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 399: Train Loss:0.0078, Macro_F1: 0.8663, AUC_score: 0.8567\n",
      "Epoch 00437: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 449: Train Loss:0.0068, Macro_F1: 0.8663, AUC_score: 0.8567\n",
      "Epoch 00488: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 499: Train Loss:0.0076, Macro_F1: 0.8663, AUC_score: 0.8567\n",
      "Epoch 00547: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 549: Train Loss:0.0127, Macro_F1: 0.8663, AUC_score: 0.8567\n",
      "Epoch 00598: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 599: Train Loss:0.0080, Macro_F1: 0.8663, AUC_score: 0.8567\n",
      "Epoch 00649: reducing learning rate of group 0 to 1.2800e-08.\n",
      "Epoch 649: Train Loss:0.0085, Macro_F1: 0.8663, AUC_score: 0.8567\n",
      "26\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 49: Train Loss:0.2711, Macro_F1: 0.8045, AUC_score: 0.8708\n",
      "Epoch 99: Train Loss:0.1526, Macro_F1: 0.7833, AUC_score: 0.8583\n",
      "Epoch 149: Train Loss:0.0770, Macro_F1: 0.7658, AUC_score: 0.8583\n",
      "Epoch 199: Train Loss:0.0366, Macro_F1: 0.8045, AUC_score: 0.8875\n",
      "Epoch 249: Train Loss:0.0161, Macro_F1: 0.8045, AUC_score: 0.8875\n",
      "Epoch 299: Train Loss:0.0100, Macro_F1: 0.8045, AUC_score: 0.8833\n",
      "Epoch 349: Train Loss:0.0124, Macro_F1: 0.8045, AUC_score: 0.8833\n",
      "Epoch 00369: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 399: Train Loss:0.0058, Macro_F1: 0.8045, AUC_score: 0.8833\n",
      "Epoch 00443: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 449: Train Loss:0.0072, Macro_F1: 0.8045, AUC_score: 0.8833\n",
      "Epoch 00494: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 499: Train Loss:0.0097, Macro_F1: 0.8045, AUC_score: 0.8833\n",
      "Epoch 00545: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 549: Train Loss:0.0050, Macro_F1: 0.8045, AUC_score: 0.8875\n",
      "Epoch 00596: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 599: Train Loss:0.0043, Macro_F1: 0.8045, AUC_score: 0.8875\n",
      "Epoch 00647: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 649: Train Loss:0.0069, Macro_F1: 0.8045, AUC_score: 0.8875\n",
      "26\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 49: Train Loss:0.3493, Macro_F1: 0.7815, AUC_score: 0.9028\n",
      "Epoch 99: Train Loss:0.2083, Macro_F1: 0.7815, AUC_score: 0.9201\n",
      "Epoch 149: Train Loss:0.0881, Macro_F1: 0.9097, AUC_score: 0.9514\n",
      "Epoch 199: Train Loss:0.0378, Macro_F1: 0.9563, AUC_score: 0.9653\n",
      "Epoch 249: Train Loss:0.0299, Macro_F1: 0.8756, AUC_score: 0.9618\n",
      "Epoch 299: Train Loss:0.0188, Macro_F1: 0.9150, AUC_score: 0.9653\n",
      "Epoch 349: Train Loss:0.0087, Macro_F1: 0.9150, AUC_score: 0.9583\n",
      "Epoch 399: Train Loss:0.0070, Macro_F1: 0.9563, AUC_score: 0.9583\n",
      "Epoch 00445: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 449: Train Loss:0.0067, Macro_F1: 0.9563, AUC_score: 0.9583\n",
      "Epoch 499: Train Loss:0.0035, Macro_F1: 0.9150, AUC_score: 0.9583\n",
      "Epoch 00526: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 549: Train Loss:0.0049, Macro_F1: 0.9563, AUC_score: 0.9583\n",
      "Epoch 00577: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 599: Train Loss:0.0088, Macro_F1: 0.9563, AUC_score: 0.9583\n",
      "Epoch 00628: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 649: Train Loss:0.0038, Macro_F1: 0.9563, AUC_score: 0.9583\n",
      "[{'F1': array(0.9423504, dtype=float32), 'AUC_score': 1.0}, {'F1': array(0.675, dtype=float32), 'AUC_score': 0.9375}, {'F1': array(0.8663101, dtype=float32), 'AUC_score': 0.8566666666666667}, {'F1': array(0.8045113, dtype=float32), 'AUC_score': 0.8875}, {'F1': array(0.9563025, dtype=float32), 'AUC_score': 0.9583333333333333}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#BivalentVsNonMethylated\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "result_BivalentVsNonMethylated = []\n",
    "for x in range(5):\n",
    "    # 实例化模型\n",
    "    device = torch.device('cuda:1')\n",
    "    data = data.to(device)\n",
    "    model = GCN(num_features=768, hidden_dim=64, num_classes=2, num_layers=3, activation=F.relu, dropout=0.5)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=50, verbose=True)\n",
    "    \n",
    "    labeled_indices = label_indices\n",
    "    random.shuffle(labeled_indices)\n",
    "    num_labeled = len(labeled_indices)\n",
    "    num_train = int(num_labeled * 0.8)\n",
    "    num_test = num_labeled - num_train\n",
    "    print(num_test)\n",
    "    \n",
    "    # 创建训练和测试掩码\n",
    "    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    \n",
    "    train_mask[labeled_indices[:num_train]] = True\n",
    "    test_mask[labeled_indices[num_train:num_train+num_test]] = True\n",
    "    print(test_mask)\n",
    "    #fully connection\n",
    "    num_epochs = 650\n",
    "    x_type1 = data.x[:14450]\n",
    "    x_type2 = data.x[14450:]\n",
    "    x_list = [x_type1, x_type2]\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model_scheduler(model, data.x, data.y, data.edge_index, optimizer, loss_fn, scheduler, train_mask)\n",
    "        test_f1, test_auc = evaluate_model(model, data.x, data.y, data.edge_index, test_mask)\n",
    "        \n",
    "        if (epoch + 1) % 50 == 0: \n",
    "            print(f'Epoch {epoch}: Train Loss:{train_loss:.4f}, Macro_F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "    result_BivalentVsNonMethylated.append({'F1': test_f1,\n",
    "                    'AUC_score': test_auc})\n",
    "print(result_BivalentVsNonMethylated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1201fa6f-9037-45b0-8bc8-ee1a47ef03ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 19: Train Loss:0.5641, Macro_F1: 0.3962, AUC_score: 0.4048\n",
      "Epoch 39: Train Loss:0.5189, Macro_F1: 0.3962, AUC_score: 0.4372\n",
      "Epoch 59: Train Loss:0.4644, Macro_F1: 0.3962, AUC_score: 0.4632\n",
      "Epoch 79: Train Loss:0.3750, Macro_F1: 0.4872, AUC_score: 0.4913\n",
      "Epoch 99: Train Loss:0.3394, Macro_F1: 0.5152, AUC_score: 0.5152\n",
      "Epoch 119: Train Loss:0.3189, Macro_F1: 0.3962, AUC_score: 0.5108\n",
      "Epoch 139: Train Loss:0.3418, Macro_F1: 0.4514, AUC_score: 0.5779\n",
      "Epoch 159: Train Loss:0.1993, Macro_F1: 0.7091, AUC_score: 0.6061\n",
      "Epoch 179: Train Loss:0.1925, Macro_F1: 0.6811, AUC_score: 0.6299\n",
      "Epoch 199: Train Loss:0.1070, Macro_F1: 0.5636, AUC_score: 0.5844\n",
      "Epoch 219: Train Loss:0.0830, Macro_F1: 0.5883, AUC_score: 0.5649\n",
      "Epoch 239: Train Loss:0.0803, Macro_F1: 0.6537, AUC_score: 0.6104\n",
      "Epoch 259: Train Loss:0.0792, Macro_F1: 0.6135, AUC_score: 0.6147\n",
      "Epoch 279: Train Loss:0.0745, Macro_F1: 0.6811, AUC_score: 0.6623\n",
      "Epoch 299: Train Loss:0.1069, Macro_F1: 0.5362, AUC_score: 0.5455\n",
      "Epoch 319: Train Loss:0.0418, Macro_F1: 0.5000, AUC_score: 0.4892\n",
      "Epoch 339: Train Loss:0.0604, Macro_F1: 0.5883, AUC_score: 0.5887\n",
      "Epoch 359: Train Loss:0.0467, Macro_F1: 0.4793, AUC_score: 0.5346\n",
      "Epoch 379: Train Loss:0.0418, Macro_F1: 0.5211, AUC_score: 0.5411\n",
      "Epoch 399: Train Loss:0.0369, Macro_F1: 0.5883, AUC_score: 0.5887\n",
      "Epoch 419: Train Loss:0.0587, Macro_F1: 0.5589, AUC_score: 0.5368\n",
      "Epoch 439: Train Loss:0.0772, Macro_F1: 0.6632, AUC_score: 0.6494\n",
      "Epoch 459: Train Loss:0.1599, Macro_F1: 0.4691, AUC_score: 0.5216\n",
      "Epoch 00467: reducing learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 479: Train Loss:0.0995, Macro_F1: 0.6000, AUC_score: 0.6082\n",
      "Epoch 499: Train Loss:0.0498, Macro_F1: 0.5636, AUC_score: 0.6061\n",
      "Epoch 00518: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch 519: Train Loss:0.0481, Macro_F1: 0.5883, AUC_score: 0.6212\n",
      "Epoch 539: Train Loss:0.0317, Macro_F1: 0.6135, AUC_score: 0.6320\n",
      "Epoch 559: Train Loss:0.0414, Macro_F1: 0.6135, AUC_score: 0.6320\n",
      "Epoch 00569: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Epoch 579: Train Loss:0.0425, Macro_F1: 0.5594, AUC_score: 0.6212\n",
      "Epoch 599: Train Loss:0.0568, Macro_F1: 0.6135, AUC_score: 0.6299\n",
      "Epoch 00620: reducing learning rate of group 0 to 1.6000e-05.\n",
      "Epoch 619: Train Loss:0.0453, Macro_F1: 0.6135, AUC_score: 0.6320\n",
      "Epoch 639: Train Loss:0.0508, Macro_F1: 0.6135, AUC_score: 0.6320\n",
      "32\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 19: Train Loss:0.5663, Macro_F1: 0.4286, AUC_score: 0.5260\n",
      "Epoch 39: Train Loss:0.5002, Macro_F1: 0.5475, AUC_score: 0.5677\n",
      "Epoch 59: Train Loss:0.5381, Macro_F1: 0.4286, AUC_score: 0.5286\n",
      "Epoch 79: Train Loss:0.4339, Macro_F1: 0.4459, AUC_score: 0.4844\n",
      "Epoch 99: Train Loss:0.4666, Macro_F1: 0.5475, AUC_score: 0.5104\n",
      "Epoch 119: Train Loss:0.2740, Macro_F1: 0.4182, AUC_score: 0.4141\n",
      "Epoch 139: Train Loss:0.2893, Macro_F1: 0.4386, AUC_score: 0.4349\n",
      "Epoch 159: Train Loss:0.2047, Macro_F1: 0.5060, AUC_score: 0.4635\n",
      "Epoch 179: Train Loss:0.1537, Macro_F1: 0.4514, AUC_score: 0.4740\n",
      "Epoch 199: Train Loss:0.1578, Macro_F1: 0.5211, AUC_score: 0.4427\n",
      "Epoch 219: Train Loss:0.1166, Macro_F1: 0.4793, AUC_score: 0.4531\n",
      "Epoch 239: Train Loss:0.0634, Macro_F1: 0.4872, AUC_score: 0.4401\n",
      "Epoch 259: Train Loss:0.0706, Macro_F1: 0.4872, AUC_score: 0.5443\n",
      "Epoch 279: Train Loss:0.1458, Macro_F1: 0.5211, AUC_score: 0.4375\n",
      "Epoch 299: Train Loss:0.0954, Macro_F1: 0.6395, AUC_score: 0.6068\n",
      "Epoch 319: Train Loss:0.1027, Macro_F1: 0.4074, AUC_score: 0.4089\n",
      "Epoch 00328: reducing learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 339: Train Loss:0.1280, Macro_F1: 0.4340, AUC_score: 0.5130\n",
      "Epoch 359: Train Loss:0.0586, Macro_F1: 0.4691, AUC_score: 0.4896\n",
      "Epoch 379: Train Loss:0.0372, Macro_F1: 0.4872, AUC_score: 0.4896\n",
      "Epoch 399: Train Loss:0.0375, Macro_F1: 0.5429, AUC_score: 0.5234\n",
      "Epoch 419: Train Loss:0.0748, Macro_F1: 0.4872, AUC_score: 0.5130\n",
      "Epoch 439: Train Loss:0.0459, Macro_F1: 0.5429, AUC_score: 0.5130\n",
      "Epoch 459: Train Loss:0.0229, Macro_F1: 0.5656, AUC_score: 0.5234\n",
      "Epoch 479: Train Loss:0.0298, Macro_F1: 0.5060, AUC_score: 0.5286\n",
      "Epoch 499: Train Loss:0.0347, Macro_F1: 0.4872, AUC_score: 0.5026\n",
      "Epoch 519: Train Loss:0.0427, Macro_F1: 0.4691, AUC_score: 0.5130\n",
      "Epoch 539: Train Loss:0.0527, Macro_F1: 0.5060, AUC_score: 0.5052\n",
      "Epoch 00554: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch 559: Train Loss:0.0374, Macro_F1: 0.5897, AUC_score: 0.5156\n",
      "Epoch 579: Train Loss:0.0328, Macro_F1: 0.5656, AUC_score: 0.5000\n",
      "Epoch 599: Train Loss:0.0203, Macro_F1: 0.5897, AUC_score: 0.5026\n",
      "Epoch 619: Train Loss:0.0432, Macro_F1: 0.5656, AUC_score: 0.4948\n",
      "Epoch 639: Train Loss:0.0222, Macro_F1: 0.4872, AUC_score: 0.5156\n",
      "32\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 19: Train Loss:0.5693, Macro_F1: 0.5475, AUC_score: 0.4829\n",
      "Epoch 39: Train Loss:0.4542, Macro_F1: 0.5259, AUC_score: 0.5600\n",
      "Epoch 59: Train Loss:0.3731, Macro_F1: 0.5362, AUC_score: 0.6000\n",
      "Epoch 79: Train Loss:0.3824, Macro_F1: 0.5656, AUC_score: 0.5886\n",
      "Epoch 99: Train Loss:0.2071, Macro_F1: 0.5833, AUC_score: 0.5829\n",
      "Epoch 119: Train Loss:0.2007, Macro_F1: 0.5060, AUC_score: 0.5143\n",
      "Epoch 139: Train Loss:0.1034, Macro_F1: 0.5656, AUC_score: 0.5400\n",
      "Epoch 159: Train Loss:0.1465, Macro_F1: 0.5000, AUC_score: 0.5743\n",
      "Epoch 179: Train Loss:0.0833, Macro_F1: 0.5362, AUC_score: 0.6114\n",
      "Epoch 199: Train Loss:0.1037, Macro_F1: 0.5060, AUC_score: 0.4971\n",
      "Epoch 00206: reducing learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 219: Train Loss:0.0630, Macro_F1: 0.4793, AUC_score: 0.5686\n",
      "Epoch 239: Train Loss:0.0964, Macro_F1: 0.4793, AUC_score: 0.5657\n",
      "Epoch 259: Train Loss:0.0789, Macro_F1: 0.5000, AUC_score: 0.5486\n",
      "Epoch 279: Train Loss:0.0344, Macro_F1: 0.4793, AUC_score: 0.5714\n",
      "Epoch 299: Train Loss:0.0430, Macro_F1: 0.4793, AUC_score: 0.5371\n",
      "Epoch 319: Train Loss:0.0209, Macro_F1: 0.4793, AUC_score: 0.5343\n",
      "Epoch 339: Train Loss:0.0412, Macro_F1: 0.4793, AUC_score: 0.5514\n",
      "Epoch 359: Train Loss:0.0539, Macro_F1: 0.4386, AUC_score: 0.5771\n",
      "Epoch 379: Train Loss:0.0412, Macro_F1: 0.5000, AUC_score: 0.5543\n",
      "Epoch 00382: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch 399: Train Loss:0.0402, Macro_F1: 0.4793, AUC_score: 0.5543\n",
      "Epoch 419: Train Loss:0.0204, Macro_F1: 0.5000, AUC_score: 0.5314\n",
      "Epoch 00436: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Epoch 439: Train Loss:0.0261, Macro_F1: 0.4793, AUC_score: 0.5400\n",
      "Epoch 459: Train Loss:0.0695, Macro_F1: 0.4793, AUC_score: 0.5371\n",
      "Epoch 479: Train Loss:0.0302, Macro_F1: 0.5000, AUC_score: 0.5257\n",
      "Epoch 00487: reducing learning rate of group 0 to 1.6000e-05.\n",
      "Epoch 499: Train Loss:0.0156, Macro_F1: 0.5000, AUC_score: 0.5343\n",
      "Epoch 519: Train Loss:0.0278, Macro_F1: 0.5000, AUC_score: 0.5343\n",
      "Epoch 00538: reducing learning rate of group 0 to 3.2000e-06.\n",
      "Epoch 539: Train Loss:0.0305, Macro_F1: 0.5000, AUC_score: 0.5343\n",
      "Epoch 559: Train Loss:0.0217, Macro_F1: 0.5000, AUC_score: 0.5343\n",
      "Epoch 579: Train Loss:0.0161, Macro_F1: 0.5000, AUC_score: 0.5343\n",
      "Epoch 00589: reducing learning rate of group 0 to 6.4000e-07.\n",
      "Epoch 599: Train Loss:0.0365, Macro_F1: 0.5000, AUC_score: 0.5343\n",
      "Epoch 619: Train Loss:0.0302, Macro_F1: 0.5000, AUC_score: 0.5343\n",
      "Epoch 00640: reducing learning rate of group 0 to 1.2800e-07.\n",
      "Epoch 639: Train Loss:0.0333, Macro_F1: 0.5000, AUC_score: 0.5343\n",
      "32\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 19: Train Loss:0.6097, Macro_F1: 0.4386, AUC_score: 0.4057\n",
      "Epoch 39: Train Loss:0.5856, Macro_F1: 0.4386, AUC_score: 0.5286\n",
      "Epoch 59: Train Loss:0.5439, Macro_F1: 0.4182, AUC_score: 0.6543\n",
      "Epoch 79: Train Loss:0.4997, Macro_F1: 0.5475, AUC_score: 0.6629\n",
      "Epoch 99: Train Loss:0.4693, Macro_F1: 0.5259, AUC_score: 0.6457\n",
      "Epoch 119: Train Loss:0.3855, Macro_F1: 0.5060, AUC_score: 0.6686\n",
      "Epoch 139: Train Loss:0.2301, Macro_F1: 0.6135, AUC_score: 0.6600\n",
      "Epoch 159: Train Loss:0.3583, Macro_F1: 0.5362, AUC_score: 0.6657\n",
      "Epoch 179: Train Loss:0.1509, Macro_F1: 0.6135, AUC_score: 0.6714\n",
      "Epoch 199: Train Loss:0.1456, Macro_F1: 0.5077, AUC_score: 0.6400\n",
      "Epoch 219: Train Loss:0.1795, Macro_F1: 0.5362, AUC_score: 0.6286\n",
      "Epoch 239: Train Loss:0.2265, Macro_F1: 0.6667, AUC_score: 0.6371\n",
      "Epoch 259: Train Loss:0.1109, Macro_F1: 0.5000, AUC_score: 0.6171\n",
      "Epoch 279: Train Loss:0.0630, Macro_F1: 0.4793, AUC_score: 0.5543\n",
      "Epoch 299: Train Loss:0.0764, Macro_F1: 0.5135, AUC_score: 0.6286\n",
      "Epoch 319: Train Loss:0.0761, Macro_F1: 0.5211, AUC_score: 0.5543\n",
      "Epoch 339: Train Loss:0.0669, Macro_F1: 0.5656, AUC_score: 0.5429\n",
      "Epoch 359: Train Loss:0.0751, Macro_F1: 0.5000, AUC_score: 0.5371\n",
      "Epoch 00372: reducing learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 379: Train Loss:0.0960, Macro_F1: 0.5211, AUC_score: 0.5229\n",
      "Epoch 399: Train Loss:0.0420, Macro_F1: 0.5211, AUC_score: 0.5629\n",
      "Epoch 419: Train Loss:0.0653, Macro_F1: 0.4589, AUC_score: 0.5457\n",
      "Epoch 439: Train Loss:0.0547, Macro_F1: 0.5211, AUC_score: 0.5514\n",
      "Epoch 459: Train Loss:0.0401, Macro_F1: 0.5000, AUC_score: 0.5600\n",
      "Epoch 479: Train Loss:0.0599, Macro_F1: 0.5000, AUC_score: 0.5343\n",
      "Epoch 00482: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch 499: Train Loss:0.0568, Macro_F1: 0.5000, AUC_score: 0.5257\n",
      "Epoch 519: Train Loss:0.0415, Macro_F1: 0.5000, AUC_score: 0.5343\n",
      "Epoch 539: Train Loss:0.0344, Macro_F1: 0.5000, AUC_score: 0.5257\n",
      "Epoch 559: Train Loss:0.0329, Macro_F1: 0.5000, AUC_score: 0.5314\n",
      "Epoch 579: Train Loss:0.0450, Macro_F1: 0.5000, AUC_score: 0.5343\n",
      "Epoch 00586: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Epoch 599: Train Loss:0.0260, Macro_F1: 0.5000, AUC_score: 0.5314\n",
      "Epoch 619: Train Loss:0.0406, Macro_F1: 0.5000, AUC_score: 0.5286\n",
      "Epoch 00637: reducing learning rate of group 0 to 1.6000e-05.\n",
      "Epoch 639: Train Loss:0.0432, Macro_F1: 0.5211, AUC_score: 0.5257\n",
      "32\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 19: Train Loss:0.6284, Macro_F1: 0.4182, AUC_score: 0.5700\n",
      "Epoch 39: Train Loss:0.5886, Macro_F1: 0.4182, AUC_score: 0.7319\n",
      "Epoch 59: Train Loss:0.5581, Macro_F1: 0.4182, AUC_score: 0.7053\n",
      "Epoch 79: Train Loss:0.4798, Macro_F1: 0.5259, AUC_score: 0.7802\n",
      "Epoch 99: Train Loss:0.5235, Macro_F1: 0.5259, AUC_score: 0.7729\n",
      "Epoch 119: Train Loss:0.3560, Macro_F1: 0.4872, AUC_score: 0.7415\n",
      "Epoch 139: Train Loss:0.3090, Macro_F1: 0.5060, AUC_score: 0.7778\n",
      "Epoch 159: Train Loss:0.1834, Macro_F1: 0.5897, AUC_score: 0.8116\n",
      "Epoch 179: Train Loss:0.1922, Macro_F1: 0.6158, AUC_score: 0.8285\n",
      "Epoch 199: Train Loss:0.4490, Macro_F1: 0.5060, AUC_score: 0.8406\n",
      "Epoch 219: Train Loss:0.1470, Macro_F1: 0.5897, AUC_score: 0.7826\n",
      "Epoch 239: Train Loss:0.1164, Macro_F1: 0.5656, AUC_score: 0.7826\n",
      "Epoch 259: Train Loss:0.1252, Macro_F1: 0.5429, AUC_score: 0.7295\n",
      "Epoch 279: Train Loss:0.0615, Macro_F1: 0.5833, AUC_score: 0.6594\n",
      "Epoch 299: Train Loss:0.0780, Macro_F1: 0.5833, AUC_score: 0.6473\n",
      "Epoch 319: Train Loss:0.0580, Macro_F1: 0.5897, AUC_score: 0.7633\n",
      "Epoch 339: Train Loss:0.0693, Macro_F1: 0.5897, AUC_score: 0.7295\n",
      "Epoch 359: Train Loss:0.0833, Macro_F1: 0.5897, AUC_score: 0.7246\n",
      "Epoch 00373: reducing learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 379: Train Loss:0.0449, Macro_F1: 0.5656, AUC_score: 0.7488\n",
      "Epoch 399: Train Loss:0.0516, Macro_F1: 0.5897, AUC_score: 0.7222\n",
      "Epoch 419: Train Loss:0.0543, Macro_F1: 0.5429, AUC_score: 0.6884\n",
      "Epoch 439: Train Loss:0.0469, Macro_F1: 0.5429, AUC_score: 0.6739\n",
      "Epoch 00450: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch 459: Train Loss:0.0362, Macro_F1: 0.5656, AUC_score: 0.7295\n",
      "Epoch 479: Train Loss:0.0310, Macro_F1: 0.5656, AUC_score: 0.7295\n",
      "Epoch 499: Train Loss:0.0425, Macro_F1: 0.5656, AUC_score: 0.7319\n",
      "Epoch 00501: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Epoch 519: Train Loss:0.0436, Macro_F1: 0.5656, AUC_score: 0.7319\n",
      "Epoch 539: Train Loss:0.0307, Macro_F1: 0.5656, AUC_score: 0.7319\n",
      "Epoch 00559: reducing learning rate of group 0 to 1.6000e-05.\n",
      "Epoch 559: Train Loss:0.0757, Macro_F1: 0.5656, AUC_score: 0.7319\n",
      "Epoch 579: Train Loss:0.0307, Macro_F1: 0.5656, AUC_score: 0.7319\n",
      "Epoch 599: Train Loss:0.0469, Macro_F1: 0.5656, AUC_score: 0.7319\n",
      "Epoch 00610: reducing learning rate of group 0 to 3.2000e-06.\n",
      "Epoch 619: Train Loss:0.0397, Macro_F1: 0.5656, AUC_score: 0.7319\n",
      "Epoch 639: Train Loss:0.0383, Macro_F1: 0.5656, AUC_score: 0.7319\n",
      "[{'F1': array(0.6135266, dtype=float32), 'AUC_score': 0.6298701298701299}, {'F1': array(0.4871795, dtype=float32), 'AUC_score': 0.5182291666666666}, {'F1': array(0.5, dtype=float32), 'AUC_score': 0.5342857142857143}, {'F1': array(0.5, dtype=float32), 'AUC_score': 0.5257142857142857}, {'F1': array(0.5656109, dtype=float32), 'AUC_score': 0.7318840579710144}]\n"
     ]
    }
   ],
   "source": [
    "#Tf_range\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "result_Tf_range = []\n",
    "for x in range(5):\n",
    "    # 实例化模型\n",
    "    device = torch.device('cuda:1')\n",
    "    data = data.to(device)\n",
    "    model = GCN(num_features=768, hidden_dim=64, num_classes=2, num_layers=4, activation=F.relu, dropout=0.5)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=50, verbose=True)\n",
    "    \n",
    "    labeled_indices = label_indices\n",
    "    random.shuffle(labeled_indices)\n",
    "    num_labeled = len(labeled_indices)\n",
    "    num_train = int(num_labeled * 0.8)\n",
    "    num_test = num_labeled - num_train\n",
    "    print(num_test)\n",
    "    \n",
    "    # 创建训练和测试掩码\n",
    "    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    \n",
    "    train_mask[labeled_indices[:num_train]] = True\n",
    "    test_mask[labeled_indices[num_train:num_train+num_test]] = True\n",
    "    print(test_mask)\n",
    "    \n",
    "    num_epochs = 650\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model_scheduler(model, data.x, data.y, data.edge_index, optimizer, loss_fn, scheduler, train_mask)\n",
    "        test_f1, test_auc = evaluate_model(model, data.x, data.y, data.edge_index, test_mask)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0: \n",
    "            print(f'Epoch {epoch}: Train Loss:{train_loss:.4f}, Macro_F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "    result_Tf_range.append({'F1': test_f1,\n",
    "                    'AUC_score': test_auc})\n",
    "print(result_Tf_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ca8cdefe-06cf-4dcd-8b58-2818edb2dbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 19: Train Loss:0.6275, Macro_F1: 0.4004, AUC_score: 0.5025\n",
      "Epoch 39: Train Loss:0.6281, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 59: Train Loss:0.6375, Macro_F1: 0.4017, AUC_score: 0.5237\n",
      "Epoch 00068: reducing learning rate of group 0 to 2.0000e-02.\n",
      "Epoch 79: Train Loss:0.6233, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 99: Train Loss:0.6244, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 00119: reducing learning rate of group 0 to 4.0000e-03.\n",
      "Epoch 119: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 139: Train Loss:0.6232, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 159: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 00170: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 179: Train Loss:0.6223, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 199: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 219: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 00221: reducing learning rate of group 0 to 1.6000e-04.\n",
      "Epoch 239: Train Loss:0.6221, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 259: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 00272: reducing learning rate of group 0 to 3.2000e-05.\n",
      "Epoch 279: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 299: Train Loss:0.6262, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 319: Train Loss:0.6259, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 00323: reducing learning rate of group 0 to 6.4000e-06.\n",
      "Epoch 339: Train Loss:0.6231, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 359: Train Loss:0.6231, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 00374: reducing learning rate of group 0 to 1.2800e-06.\n",
      "Epoch 379: Train Loss:0.6248, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 399: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 419: Train Loss:0.6252, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 00425: reducing learning rate of group 0 to 2.5600e-07.\n",
      "Epoch 439: Train Loss:0.6241, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 459: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 00476: reducing learning rate of group 0 to 5.1200e-08.\n",
      "Epoch 479: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 499: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "277\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 19: Train Loss:0.6353, Macro_F1: 0.4229, AUC_score: 0.5773\n",
      "Epoch 39: Train Loss:0.6634, Macro_F1: 0.4229, AUC_score: 0.5110\n",
      "Epoch 59: Train Loss:0.6350, Macro_F1: 0.4229, AUC_score: 0.5523\n",
      "Epoch 79: Train Loss:0.6352, Macro_F1: 0.4229, AUC_score: 0.5727\n",
      "Epoch 99: Train Loss:0.6360, Macro_F1: 0.4229, AUC_score: 0.5254\n",
      "Epoch 119: Train Loss:0.6358, Macro_F1: 0.4229, AUC_score: 0.4993\n",
      "Epoch 139: Train Loss:1.3587, Macro_F1: 0.4229, AUC_score: 0.4288\n",
      "Epoch 00147: reducing learning rate of group 0 to 2.0000e-02.\n",
      "Epoch 159: Train Loss:0.6415, Macro_F1: 0.4229, AUC_score: 0.4265\n",
      "Epoch 179: Train Loss:0.6351, Macro_F1: 0.4229, AUC_score: 0.5740\n",
      "Epoch 00198: reducing learning rate of group 0 to 4.0000e-03.\n",
      "Epoch 199: Train Loss:0.6340, Macro_F1: 0.4229, AUC_score: 0.5734\n",
      "Epoch 219: Train Loss:0.6357, Macro_F1: 0.4229, AUC_score: 0.5738\n",
      "Epoch 239: Train Loss:0.6356, Macro_F1: 0.4229, AUC_score: 0.5745\n",
      "Epoch 00249: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 259: Train Loss:0.6355, Macro_F1: 0.4229, AUC_score: 0.5752\n",
      "Epoch 279: Train Loss:0.6330, Macro_F1: 0.4229, AUC_score: 0.5756\n",
      "Epoch 00300: reducing learning rate of group 0 to 1.6000e-04.\n",
      "Epoch 299: Train Loss:0.6334, Macro_F1: 0.4229, AUC_score: 0.5682\n",
      "Epoch 319: Train Loss:0.6344, Macro_F1: 0.4229, AUC_score: 0.5668\n",
      "Epoch 339: Train Loss:0.6353, Macro_F1: 0.4229, AUC_score: 0.5659\n",
      "Epoch 00351: reducing learning rate of group 0 to 3.2000e-05.\n",
      "Epoch 359: Train Loss:0.6329, Macro_F1: 0.4229, AUC_score: 0.5659\n",
      "Epoch 379: Train Loss:0.6356, Macro_F1: 0.4229, AUC_score: 0.5660\n",
      "Epoch 399: Train Loss:0.6346, Macro_F1: 0.4229, AUC_score: 0.5659\n",
      "Epoch 00402: reducing learning rate of group 0 to 6.4000e-06.\n",
      "Epoch 419: Train Loss:0.6318, Macro_F1: 0.4229, AUC_score: 0.5662\n",
      "Epoch 439: Train Loss:0.6348, Macro_F1: 0.4229, AUC_score: 0.5662\n",
      "Epoch 00453: reducing learning rate of group 0 to 1.2800e-06.\n",
      "Epoch 459: Train Loss:0.6326, Macro_F1: 0.4229, AUC_score: 0.5659\n",
      "Epoch 479: Train Loss:0.6336, Macro_F1: 0.4229, AUC_score: 0.5660\n",
      "Epoch 499: Train Loss:0.6331, Macro_F1: 0.4229, AUC_score: 0.5660\n",
      "277\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 19: Train Loss:0.7067, Macro_F1: 0.4310, AUC_score: 0.4888\n",
      "Epoch 39: Train Loss:0.6713, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 59: Train Loss:0.6280, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 79: Train Loss:0.6244, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 99: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 00107: reducing learning rate of group 0 to 2.0000e-02.\n",
      "Epoch 119: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 139: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 00158: reducing learning rate of group 0 to 4.0000e-03.\n",
      "Epoch 159: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 179: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 199: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 00209: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 219: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 239: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 00260: reducing learning rate of group 0 to 1.6000e-04.\n",
      "Epoch 259: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 279: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 299: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 00311: reducing learning rate of group 0 to 3.2000e-05.\n",
      "Epoch 319: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 339: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 359: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 00362: reducing learning rate of group 0 to 6.4000e-06.\n",
      "Epoch 379: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 399: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 00413: reducing learning rate of group 0 to 1.2800e-06.\n",
      "Epoch 419: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 439: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 459: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 00464: reducing learning rate of group 0 to 2.5600e-07.\n",
      "Epoch 479: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "Epoch 499: Train Loss:0.6242, Macro_F1: 0.4017, AUC_score: 0.5000\n",
      "277\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 19: Train Loss:0.6230, Macro_F1: 0.3872, AUC_score: 0.5412\n",
      "Epoch 39: Train Loss:1.7145, Macro_F1: 0.3872, AUC_score: 0.5413\n",
      "Epoch 59: Train Loss:0.6163, Macro_F1: 0.3872, AUC_score: 0.4914\n",
      "Epoch 79: Train Loss:0.6160, Macro_F1: 0.3872, AUC_score: 0.5355\n",
      "Epoch 00089: reducing learning rate of group 0 to 2.0000e-02.\n",
      "Epoch 99: Train Loss:0.6162, Macro_F1: 0.3872, AUC_score: 0.5263\n",
      "Epoch 119: Train Loss:0.6157, Macro_F1: 0.3872, AUC_score: 0.5485\n",
      "Epoch 139: Train Loss:0.6145, Macro_F1: 0.3872, AUC_score: 0.5685\n",
      "Epoch 159: Train Loss:0.6135, Macro_F1: 0.3872, AUC_score: 0.5727\n",
      "Epoch 179: Train Loss:0.6154, Macro_F1: 0.3872, AUC_score: 0.5597\n",
      "Epoch 199: Train Loss:0.6152, Macro_F1: 0.3872, AUC_score: 0.5777\n",
      "Epoch 00210: reducing learning rate of group 0 to 4.0000e-03.\n",
      "Epoch 219: Train Loss:0.6164, Macro_F1: 0.3872, AUC_score: 0.5540\n",
      "Epoch 239: Train Loss:0.6147, Macro_F1: 0.3872, AUC_score: 0.5540\n",
      "Epoch 259: Train Loss:0.6139, Macro_F1: 0.3872, AUC_score: 0.5630\n",
      "Epoch 00261: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 279: Train Loss:0.6150, Macro_F1: 0.3872, AUC_score: 0.5577\n",
      "Epoch 299: Train Loss:0.6149, Macro_F1: 0.3872, AUC_score: 0.5545\n",
      "Epoch 00312: reducing learning rate of group 0 to 1.6000e-04.\n",
      "Epoch 319: Train Loss:0.6157, Macro_F1: 0.3872, AUC_score: 0.5567\n",
      "Epoch 339: Train Loss:0.6155, Macro_F1: 0.3872, AUC_score: 0.5563\n",
      "Epoch 359: Train Loss:0.6144, Macro_F1: 0.3872, AUC_score: 0.5546\n",
      "Epoch 00363: reducing learning rate of group 0 to 3.2000e-05.\n",
      "Epoch 379: Train Loss:0.6152, Macro_F1: 0.3872, AUC_score: 0.5556\n",
      "Epoch 399: Train Loss:0.6146, Macro_F1: 0.3872, AUC_score: 0.5548\n",
      "Epoch 00414: reducing learning rate of group 0 to 6.4000e-06.\n",
      "Epoch 419: Train Loss:0.6143, Macro_F1: 0.3872, AUC_score: 0.5548\n",
      "Epoch 439: Train Loss:0.6144, Macro_F1: 0.3872, AUC_score: 0.5548\n",
      "Epoch 459: Train Loss:0.6154, Macro_F1: 0.3872, AUC_score: 0.5548\n",
      "Epoch 00465: reducing learning rate of group 0 to 1.2800e-06.\n",
      "Epoch 479: Train Loss:0.6148, Macro_F1: 0.3872, AUC_score: 0.5548\n",
      "Epoch 499: Train Loss:0.6149, Macro_F1: 0.3872, AUC_score: 0.5548\n",
      "277\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 19: Train Loss:0.6444, Macro_F1: 0.4056, AUC_score: 0.4979\n",
      "Epoch 39: Train Loss:0.6995, Macro_F1: 0.4248, AUC_score: 0.5054\n",
      "Epoch 59: Train Loss:0.6393, Macro_F1: 0.4056, AUC_score: 0.5191\n",
      "Epoch 79: Train Loss:0.6364, Macro_F1: 0.4056, AUC_score: 0.4971\n",
      "Epoch 99: Train Loss:0.6275, Macro_F1: 0.4056, AUC_score: 0.5457\n",
      "Epoch 119: Train Loss:0.6321, Macro_F1: 0.4530, AUC_score: 0.4773\n",
      "Epoch 00123: reducing learning rate of group 0 to 2.0000e-02.\n",
      "Epoch 139: Train Loss:0.6263, Macro_F1: 0.4056, AUC_score: 0.5000\n",
      "Epoch 159: Train Loss:0.6262, Macro_F1: 0.4056, AUC_score: 0.5000\n",
      "Epoch 00174: reducing learning rate of group 0 to 4.0000e-03.\n",
      "Epoch 179: Train Loss:0.6263, Macro_F1: 0.4056, AUC_score: 0.5000\n",
      "Epoch 199: Train Loss:0.6261, Macro_F1: 0.4056, AUC_score: 0.5000\n",
      "Epoch 219: Train Loss:0.6262, Macro_F1: 0.4056, AUC_score: 0.5000\n",
      "Epoch 00225: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 239: Train Loss:0.6252, Macro_F1: 0.4056, AUC_score: 0.5000\n",
      "Epoch 259: Train Loss:0.6264, Macro_F1: 0.4056, AUC_score: 0.5000\n",
      "Epoch 00276: reducing learning rate of group 0 to 1.6000e-04.\n",
      "Epoch 279: Train Loss:0.6262, Macro_F1: 0.4056, AUC_score: 0.5000\n",
      "Epoch 299: Train Loss:0.6262, Macro_F1: 0.4056, AUC_score: 0.5000\n",
      "Epoch 319: Train Loss:0.6253, Macro_F1: 0.4056, AUC_score: 0.5000\n",
      "Epoch 00327: reducing learning rate of group 0 to 3.2000e-05.\n",
      "Epoch 339: Train Loss:0.6262, Macro_F1: 0.4056, AUC_score: 0.5000\n",
      "Epoch 359: Train Loss:0.6262, Macro_F1: 0.4056, AUC_score: 0.5000\n",
      "Epoch 00378: reducing learning rate of group 0 to 6.4000e-06.\n",
      "Epoch 379: Train Loss:0.6262, Macro_F1: 0.4056, AUC_score: 0.5000\n",
      "Epoch 399: Train Loss:0.6262, Macro_F1: 0.4056, AUC_score: 0.5000\n",
      "Epoch 419: Train Loss:0.6262, Macro_F1: 0.4056, AUC_score: 0.5000\n",
      "Epoch 00429: reducing learning rate of group 0 to 1.2800e-06.\n",
      "Epoch 439: Train Loss:0.6262, Macro_F1: 0.4056, AUC_score: 0.5000\n",
      "Epoch 459: Train Loss:0.6260, Macro_F1: 0.4056, AUC_score: 0.5000\n",
      "Epoch 00480: reducing learning rate of group 0 to 2.5600e-07.\n",
      "Epoch 479: Train Loss:0.6262, Macro_F1: 0.4056, AUC_score: 0.5000\n",
      "Epoch 499: Train Loss:0.6262, Macro_F1: 0.4056, AUC_score: 0.5000\n",
      "[{'F1': array(0.40172783, dtype=float32), 'AUC_score': 0.5}, {'F1': array(0.42291665, dtype=float32), 'AUC_score': 0.565969910797497}, {'F1': array(0.40172783, dtype=float32), 'AUC_score': 0.5}, {'F1': array(0.38716814, dtype=float32), 'AUC_score': 0.5548459383753501}, {'F1': array(0.4055794, dtype=float32), 'AUC_score': 0.5}]\n"
     ]
    }
   ],
   "source": [
    "#TF_target_type\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "result_Tftarget_type1 = []\n",
    "for x in range(5):\n",
    "# 实例化模型\n",
    "    device = torch.device('cuda:1')\n",
    "    data = data.to(device)\n",
    "    model = GCN(num_features=768, hidden_dim=64, num_classes=2, num_layers=5, activation=F.relu, dropout=0.5)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=0.001)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=50, verbose=True)\n",
    "    \n",
    "    labeled_indices = label_indices\n",
    "    random.shuffle(labeled_indices)\n",
    "    num_labeled = len(labeled_indices)\n",
    "    num_train = int(num_labeled * 0.8)\n",
    "    num_test = num_labeled - num_train\n",
    "    print(num_test)\n",
    "    \n",
    "    # 创建训练和测试掩码\n",
    "    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    \n",
    "    train_mask[labeled_indices[:num_train]] = True\n",
    "    test_mask[labeled_indices[num_train:num_train+num_test]] = True\n",
    "    print(test_mask)\n",
    "    \n",
    "    num_epochs = 500\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model_scheduler(model, data.x, data.y, data.edge_index, optimizer, loss_fn, scheduler, train_mask)\n",
    "        test_f1, test_auc = evaluate_model(model, data.x, data.y, data.edge_index, test_mask)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0: \n",
    "            print(f'Epoch {epoch}: Train Loss:{train_loss:.4f}, Macro_F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "    result_Tftarget_type1.append({'F1': test_f1,\n",
    "                    'AUC_score': test_auc})\n",
    "print(result_Tftarget_type1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bcb9f008-6357-4cda-9c0a-125cd7c556ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351\n",
      "tensor([False, False, False,  ..., False, False, False])\n",
      "Epoch 19: Train Loss:1.0360, Macro_F1: 0.2676, AUC_score: 0.5525\n",
      "Epoch 39: Train Loss:1.0411, Macro_F1: 0.2066, AUC_score: 0.4949\n",
      "Epoch 59: Train Loss:1.0399, Macro_F1: 0.2066, AUC_score: 0.4798\n",
      "Epoch 00071: reducing learning rate of group 0 to 2.0000e-02.\n",
      "Epoch 79: Train Loss:1.0403, Macro_F1: 0.2066, AUC_score: 0.4987\n",
      "Epoch 99: Train Loss:1.0400, Macro_F1: 0.2066, AUC_score: 0.4859\n",
      "Epoch 119: Train Loss:1.0391, Macro_F1: 0.2066, AUC_score: 0.4975\n",
      "Epoch 00122: reducing learning rate of group 0 to 4.0000e-03.\n",
      "Epoch 139: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 159: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 00173: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch 179: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 199: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 219: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 00224: reducing learning rate of group 0 to 1.6000e-04.\n",
      "Epoch 239: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 259: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 00275: reducing learning rate of group 0 to 3.2000e-05.\n",
      "Epoch 279: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 299: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 319: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 00326: reducing learning rate of group 0 to 6.4000e-06.\n",
      "Epoch 339: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 359: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 00377: reducing learning rate of group 0 to 1.2800e-06.\n",
      "Epoch 379: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 399: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 419: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 00428: reducing learning rate of group 0 to 2.5600e-07.\n",
      "Epoch 439: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 459: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 00479: reducing learning rate of group 0 to 5.1200e-08.\n",
      "Epoch 479: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 499: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 519: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 00530: reducing learning rate of group 0 to 1.0240e-08.\n",
      "Epoch 539: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 559: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 579: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 599: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 619: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 639: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 659: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 679: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 699: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 719: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 739: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 759: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 779: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 799: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 819: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 839: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 859: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 879: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 899: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 919: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 939: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 959: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 979: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n",
      "Epoch 999: Train Loss:1.0392, Macro_F1: 0.2066, AUC_score: 0.5000\n"
     ]
    }
   ],
   "source": [
    "#TF_target_type\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "result_Tftarget_type2 = []\n",
    "for x in range(5):\n",
    "# 实例化模型\n",
    "    device = torch.device('cuda:1')\n",
    "    data = data.to(device)\n",
    "    model = GCN(num_features=768, hidden_dim=64, num_classes=3, num_layers=5, activation=F.relu, dropout=0.5)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=0.001)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=50, verbose=True)\n",
    "    \n",
    "    labeled_indices = label_indices\n",
    "    random.shuffle(labeled_indices)\n",
    "    num_labeled = len(labeled_indices)\n",
    "    num_train = int(num_labeled * 0.8)\n",
    "    num_test = num_labeled - num_train\n",
    "    print(num_test)\n",
    "    \n",
    "    # 创建训练和测试掩码\n",
    "    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    \n",
    "    train_mask[labeled_indices[:num_train]] = True\n",
    "    test_mask[labeled_indices[num_train:num_train+num_test]] = True\n",
    "    print(test_mask)\n",
    "    \n",
    "    num_epochs = 500\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model_scheduler(model, data.x, data.y, data.edge_index, optimizer, loss_fn, scheduler, train_mask)\n",
    "        test_f1, test_auc = evaluate_model(model, data.x, data.y, data.edge_index, test_mask)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0: \n",
    "            print(f'Epoch {epoch}: Train Loss:{train_loss:.4f}, Macro_F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "        result_Tftarget_type2.append({'F1': test_f1,\n",
    "                    'AUC_score': test_auc})\n",
    "print(result_Tftarget_type2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68171fe2-74ac-4565-ae87-c6c41e081430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4153,\n",
       " 11097,\n",
       " 3849,\n",
       " 518,\n",
       " 12611,\n",
       " 10339,\n",
       " 10138,\n",
       " 4498,\n",
       " 12436,\n",
       " 9062,\n",
       " 5846,\n",
       " 934,\n",
       " 10114,\n",
       " 6054,\n",
       " 8789,\n",
       " 10620,\n",
       " 7708,\n",
       " 11433,\n",
       " 10756,\n",
       " 13879,\n",
       " 3709,\n",
       " 11443,\n",
       " 2262,\n",
       " 4593,\n",
       " 3594,\n",
       " 2137,\n",
       " 10713,\n",
       " 7868,\n",
       " 742,\n",
       " 13130,\n",
       " 3661,\n",
       " 1110,\n",
       " 2459,\n",
       " 3098,\n",
       " 6765,\n",
       " 5337,\n",
       " 12284,\n",
       " 2695,\n",
       " 3760,\n",
       " 3567,\n",
       " 8094,\n",
       " 4471,\n",
       " 4372,\n",
       " 13021,\n",
       " 12447,\n",
       " 8875,\n",
       " 7800,\n",
       " 9753,\n",
       " 10997,\n",
       " 11250,\n",
       " 14267,\n",
       " 5431,\n",
       " 7828,\n",
       " 13572,\n",
       " 7378,\n",
       " 3437,\n",
       " 593,\n",
       " 10603,\n",
       " 5358,\n",
       " 357,\n",
       " 5268,\n",
       " 8199,\n",
       " 6790,\n",
       " 10924,\n",
       " 1461,\n",
       " 3986,\n",
       " 3701,\n",
       " 9344,\n",
       " 463,\n",
       " 245,\n",
       " 1453,\n",
       " 3607,\n",
       " 2318,\n",
       " 4057,\n",
       " 6998,\n",
       " 4084,\n",
       " 7890,\n",
       " 1972,\n",
       " 2665,\n",
       " 204,\n",
       " 9403,\n",
       " 8449,\n",
       " 13789,\n",
       " 4977,\n",
       " 8530,\n",
       " 10742,\n",
       " 2478,\n",
       " 72,\n",
       " 11298,\n",
       " 12014,\n",
       " 9878,\n",
       " 8007,\n",
       " 8073,\n",
       " 5890,\n",
       " 9935,\n",
       " 11423,\n",
       " 11712,\n",
       " 14054,\n",
       " 1539,\n",
       " 8482,\n",
       " 5598,\n",
       " 11238,\n",
       " 5027,\n",
       " 777,\n",
       " 5759,\n",
       " 6176,\n",
       " 13685,\n",
       " 9112,\n",
       " 4504,\n",
       " 5384,\n",
       " 6352,\n",
       " 5114,\n",
       " 13011,\n",
       " 4722,\n",
       " 12358,\n",
       " 14259,\n",
       " 6109,\n",
       " 5428,\n",
       " 4430,\n",
       " 10711,\n",
       " 4702,\n",
       " 9928,\n",
       " 12990,\n",
       " 8739,\n",
       " 10969,\n",
       " 94,\n",
       " 13067,\n",
       " 10507,\n",
       " 606,\n",
       " 10204,\n",
       " 4017,\n",
       " 3850,\n",
       " 4542,\n",
       " 6368,\n",
       " 5660,\n",
       " 6138,\n",
       " 1343,\n",
       " 13594,\n",
       " 14226,\n",
       " 10914,\n",
       " 11232,\n",
       " 884,\n",
       " 6502,\n",
       " 119,\n",
       " 2769,\n",
       " 4024,\n",
       " 7650,\n",
       " 14100,\n",
       " 1926,\n",
       " 8295,\n",
       " 6915,\n",
       " 8150,\n",
       " 7230,\n",
       " 2996,\n",
       " 7389,\n",
       " 10126,\n",
       " 6434,\n",
       " 11302,\n",
       " 7291,\n",
       " 3102,\n",
       " 12126,\n",
       " 6731,\n",
       " 13100,\n",
       " 2139,\n",
       " 2395,\n",
       " 7140,\n",
       " 4869,\n",
       " 9070,\n",
       " 12884,\n",
       " 4901,\n",
       " 1213,\n",
       " 6889,\n",
       " 6412,\n",
       " 6535,\n",
       " 7605,\n",
       " 9001,\n",
       " 7190,\n",
       " 13965,\n",
       " 3652,\n",
       " 11013,\n",
       " 6437,\n",
       " 598,\n",
       " 3252,\n",
       " 1450,\n",
       " 254,\n",
       " 6449,\n",
       " 2332,\n",
       " 10708,\n",
       " 3931,\n",
       " 3909,\n",
       " 539,\n",
       " 10184,\n",
       " 2512,\n",
       " 14145,\n",
       " 13821,\n",
       " 11402,\n",
       " 7822,\n",
       " 13318,\n",
       " 439,\n",
       " 4193,\n",
       " 10523,\n",
       " 1155,\n",
       " 3012,\n",
       " 12439,\n",
       " 9155,\n",
       " 11062,\n",
       " 10147,\n",
       " 5636,\n",
       " 9836,\n",
       " 2458,\n",
       " 14011,\n",
       " 6202,\n",
       " 4167,\n",
       " 5603,\n",
       " 13376,\n",
       " 7722,\n",
       " 13870,\n",
       " 6805,\n",
       " 9783,\n",
       " 11837,\n",
       " 533,\n",
       " 1348,\n",
       " 5177,\n",
       " 2107,\n",
       " 979,\n",
       " 9425,\n",
       " 8154,\n",
       " 11194,\n",
       " 2396,\n",
       " 7732,\n",
       " 251,\n",
       " 1310,\n",
       " 2282,\n",
       " 11346,\n",
       " 5789,\n",
       " 11872,\n",
       " 11278,\n",
       " 1709,\n",
       " 3228,\n",
       " 11126,\n",
       " 5008,\n",
       " 770,\n",
       " 4823,\n",
       " 12529,\n",
       " 2708,\n",
       " 8233,\n",
       " 6617,\n",
       " 8076,\n",
       " 4712,\n",
       " 7632,\n",
       " 15,\n",
       " 4858,\n",
       " 2054,\n",
       " 11606,\n",
       " 117,\n",
       " 11229,\n",
       " 11356,\n",
       " 7617,\n",
       " 9256,\n",
       " 5181,\n",
       " 12727,\n",
       " 1747,\n",
       " 11754,\n",
       " 13817,\n",
       " 12766,\n",
       " 6944,\n",
       " 10601,\n",
       " 830,\n",
       " 5584,\n",
       " 4562,\n",
       " 9496,\n",
       " 12378,\n",
       " 7267,\n",
       " 290,\n",
       " 723,\n",
       " 9530,\n",
       " 4613,\n",
       " 2036,\n",
       " 4310,\n",
       " 4450,\n",
       " 6482,\n",
       " 11817,\n",
       " 2848,\n",
       " 2090,\n",
       " 11455,\n",
       " 1894,\n",
       " 1232,\n",
       " 6376,\n",
       " 9680,\n",
       " 6696,\n",
       " 13279,\n",
       " 3040,\n",
       " 4667,\n",
       " 13771,\n",
       " 4513,\n",
       " 11181,\n",
       " 12885,\n",
       " 2152,\n",
       " 14177,\n",
       " 588,\n",
       " 5416,\n",
       " 12266,\n",
       " 3694,\n",
       " 7590,\n",
       " 3137,\n",
       " 6515,\n",
       " 2419,\n",
       " 6954,\n",
       " 13605,\n",
       " 2222,\n",
       " 7158,\n",
       " 4435,\n",
       " 168,\n",
       " 12655,\n",
       " 13087,\n",
       " 13501,\n",
       " 7978,\n",
       " 5964,\n",
       " 3802,\n",
       " 5807,\n",
       " 6918,\n",
       " 4366,\n",
       " 2860,\n",
       " 10404,\n",
       " 5339,\n",
       " 14265,\n",
       " 2180,\n",
       " 12714,\n",
       " 855,\n",
       " 5575,\n",
       " 3355,\n",
       " 1874,\n",
       " 11197,\n",
       " 10706,\n",
       " 6842,\n",
       " 4942,\n",
       " 1305,\n",
       " 11211,\n",
       " 5311,\n",
       " 9941,\n",
       " 5210,\n",
       " 10194,\n",
       " 8531,\n",
       " 8797,\n",
       " 3304,\n",
       " 7638,\n",
       " 479,\n",
       " 2847,\n",
       " 10805,\n",
       " 11428,\n",
       " 1753,\n",
       " 6897,\n",
       " 4614,\n",
       " 7388,\n",
       " 6858,\n",
       " 3604,\n",
       " 2803,\n",
       " 8986,\n",
       " 4961,\n",
       " 9774,\n",
       " 2436,\n",
       " 4958,\n",
       " 3186,\n",
       " 1341,\n",
       " 13166,\n",
       " 12631,\n",
       " 4320,\n",
       " 13568,\n",
       " 12444,\n",
       " 12071,\n",
       " 451,\n",
       " 370,\n",
       " 2095,\n",
       " 1003,\n",
       " 11859,\n",
       " 12680,\n",
       " 2672,\n",
       " 9912,\n",
       " 12627,\n",
       " 4523,\n",
       " 2555,\n",
       " 8907,\n",
       " 10580,\n",
       " 10874,\n",
       " 1378,\n",
       " 12547,\n",
       " 12065,\n",
       " 4098,\n",
       " 11812,\n",
       " 8074,\n",
       " 13071,\n",
       " 11992,\n",
       " 10502,\n",
       " 8031,\n",
       " 9129,\n",
       " 3017,\n",
       " 3476,\n",
       " 2928,\n",
       " 6603,\n",
       " 4315,\n",
       " 10213,\n",
       " 219,\n",
       " 50,\n",
       " 8346,\n",
       " 6781,\n",
       " 12486,\n",
       " 8744,\n",
       " 6946,\n",
       " 8420,\n",
       " 4361,\n",
       " 1203,\n",
       " 14194,\n",
       " 2793,\n",
       " 9465,\n",
       " 12790,\n",
       " 8251,\n",
       " 5926,\n",
       " 2253,\n",
       " 4299,\n",
       " 9045,\n",
       " 14186,\n",
       " 4659,\n",
       " 8259,\n",
       " 627,\n",
       " 7717,\n",
       " 6330,\n",
       " 12752,\n",
       " 13474,\n",
       " 6904,\n",
       " 9865,\n",
       " 14235,\n",
       " 7928,\n",
       " 6465,\n",
       " 4059,\n",
       " 12481,\n",
       " 924,\n",
       " 5925,\n",
       " 5205,\n",
       " 3496,\n",
       " 6050,\n",
       " 1727,\n",
       " 3439,\n",
       " 6445,\n",
       " 11362,\n",
       " 8138,\n",
       " 445,\n",
       " 2360,\n",
       " 13590,\n",
       " 14086,\n",
       " 2065,\n",
       " 7878,\n",
       " 11293,\n",
       " 1620,\n",
       " 920,\n",
       " 2455,\n",
       " 4745,\n",
       " 8802,\n",
       " 2394,\n",
       " 856,\n",
       " 5760,\n",
       " 5572,\n",
       " 2937,\n",
       " 5283,\n",
       " 11847,\n",
       " 11145,\n",
       " 6942,\n",
       " 9718,\n",
       " 8423,\n",
       " 10757,\n",
       " 5962,\n",
       " 378,\n",
       " 2176,\n",
       " 7121,\n",
       " 14356,\n",
       " 5044,\n",
       " 12028,\n",
       " 10256,\n",
       " 11396,\n",
       " 8039,\n",
       " 7105,\n",
       " 12282,\n",
       " 8357,\n",
       " 2895,\n",
       " 7761,\n",
       " 13880,\n",
       " 7258,\n",
       " 5948,\n",
       " 13164,\n",
       " 7686,\n",
       " 9694,\n",
       " 6112,\n",
       " 377,\n",
       " 9106,\n",
       " 9897,\n",
       " 2546,\n",
       " 102,\n",
       " 7723,\n",
       " 13256,\n",
       " 8933,\n",
       " 8263,\n",
       " 10239,\n",
       " 5947,\n",
       " 8871,\n",
       " 13532,\n",
       " 4062,\n",
       " 11373,\n",
       " 7556,\n",
       " 2217,\n",
       " 9823,\n",
       " 9744,\n",
       " 11830,\n",
       " 9772,\n",
       " 8240,\n",
       " 8990,\n",
       " 4074,\n",
       " 6744,\n",
       " 7027,\n",
       " 11869,\n",
       " 2440,\n",
       " 12882,\n",
       " 4904,\n",
       " 2823,\n",
       " 6843,\n",
       " 3743,\n",
       " 8222,\n",
       " 9316,\n",
       " 11987,\n",
       " 9575,\n",
       " 6209,\n",
       " 7068,\n",
       " 8219,\n",
       " 9063,\n",
       " 14135,\n",
       " 6517,\n",
       " 5374,\n",
       " 465,\n",
       " 1008,\n",
       " 10441,\n",
       " 1591,\n",
       " 2198,\n",
       " 5250,\n",
       " 5529,\n",
       " 3404,\n",
       " 13339,\n",
       " 3842,\n",
       " 5672,\n",
       " 6631,\n",
       " 8663,\n",
       " 1919,\n",
       " 12522,\n",
       " 8098,\n",
       " 3763,\n",
       " 5393,\n",
       " 12809,\n",
       " 10065,\n",
       " 2516,\n",
       " 12489,\n",
       " 6601,\n",
       " 2264,\n",
       " 4177,\n",
       " 14335,\n",
       " 11842,\n",
       " 1836,\n",
       " 10909,\n",
       " 11548,\n",
       " 3650,\n",
       " 10579,\n",
       " 9657,\n",
       " 781,\n",
       " 11191,\n",
       " 1589,\n",
       " 14291,\n",
       " 7858,\n",
       " 11483,\n",
       " 3506,\n",
       " 8638,\n",
       " 6683,\n",
       " 7535,\n",
       " 4681,\n",
       " 3454,\n",
       " 8517,\n",
       " 3412,\n",
       " 3413,\n",
       " 10437,\n",
       " 1837,\n",
       " 3641,\n",
       " 9342,\n",
       " 3257,\n",
       " 1024,\n",
       " 10484,\n",
       " 11388,\n",
       " 5932,\n",
       " 5649,\n",
       " 6116,\n",
       " 10009,\n",
       " 7511,\n",
       " 5763,\n",
       " 7821,\n",
       " 6716,\n",
       " 6446,\n",
       " 6959,\n",
       " 13746,\n",
       " 7639,\n",
       " 699,\n",
       " 8539,\n",
       " 11500,\n",
       " 9519,\n",
       " 7072,\n",
       " 11918,\n",
       " 6687,\n",
       " 1319,\n",
       " 6911,\n",
       " 2138,\n",
       " 11221,\n",
       " 12718,\n",
       " 2294,\n",
       " 2151,\n",
       " 4500,\n",
       " 5646,\n",
       " 8833,\n",
       " 4468,\n",
       " 1137,\n",
       " 4286,\n",
       " 2380,\n",
       " 11569,\n",
       " 7627,\n",
       " 4336,\n",
       " 11341,\n",
       " 13126,\n",
       " 8461,\n",
       " 4592,\n",
       " 11777,\n",
       " 10807,\n",
       " 4437,\n",
       " 2534,\n",
       " 1995,\n",
       " 6456,\n",
       " 2530,\n",
       " 12806,\n",
       " 5263,\n",
       " 3134,\n",
       " 1077,\n",
       " 2541,\n",
       " 6435,\n",
       " 8596,\n",
       " 5003,\n",
       " 6462,\n",
       " 7215,\n",
       " 5223,\n",
       " 14019,\n",
       " 1703,\n",
       " 4112,\n",
       " 6394,\n",
       " 6201,\n",
       " 8735,\n",
       " 3891,\n",
       " 13342,\n",
       " 2653,\n",
       " 350,\n",
       " 3711,\n",
       " 4309,\n",
       " 2685,\n",
       " 9278,\n",
       " 6143,\n",
       " 11649,\n",
       " 5388,\n",
       " 4948,\n",
       " 9130,\n",
       " 6225,\n",
       " 10481,\n",
       " 4921,\n",
       " 12377,\n",
       " 2363,\n",
       " 8359,\n",
       " 1129,\n",
       " 4612,\n",
       " 12162,\n",
       " 3071,\n",
       " 3856,\n",
       " 12267,\n",
       " 1704,\n",
       " 4346,\n",
       " 9389,\n",
       " 7930,\n",
       " 47,\n",
       " 1214,\n",
       " 4007,\n",
       " 4881,\n",
       " 12352,\n",
       " 11882,\n",
       " 5248,\n",
       " 8662,\n",
       " 623,\n",
       " 3859,\n",
       " 6136,\n",
       " 8218,\n",
       " 977,\n",
       " 10784,\n",
       " 11644,\n",
       " 4499,\n",
       " 5449,\n",
       " 3157,\n",
       " 14256,\n",
       " 13580,\n",
       " 1336,\n",
       " 8450,\n",
       " 11205,\n",
       " 4198,\n",
       " 13227,\n",
       " 5855,\n",
       " 3963,\n",
       " 9879,\n",
       " 9374,\n",
       " 8338,\n",
       " 6200,\n",
       " 4554,\n",
       " 10098,\n",
       " 3370,\n",
       " 12134,\n",
       " 7805,\n",
       " 5828,\n",
       " 4754,\n",
       " 2986,\n",
       " 9720,\n",
       " 10431,\n",
       " 9319,\n",
       " 3149,\n",
       " 5944,\n",
       " 4863,\n",
       " 1191,\n",
       " 1710,\n",
       " 11511,\n",
       " 2241,\n",
       " 12676,\n",
       " 7400,\n",
       " 4284,\n",
       " 1122,\n",
       " 1649,\n",
       " 3396,\n",
       " 10621,\n",
       " 1229,\n",
       " 6140,\n",
       " 7567,\n",
       " 6369,\n",
       " 3785,\n",
       " 12399,\n",
       " 3019,\n",
       " 11924,\n",
       " 13028,\n",
       " 10028,\n",
       " 4780,\n",
       " 5991,\n",
       " 13017,\n",
       " 434,\n",
       " 13800,\n",
       " 10349,\n",
       " 2016,\n",
       " 3274,\n",
       " 10112,\n",
       " 5401,\n",
       " 4561,\n",
       " 10622,\n",
       " 4862,\n",
       " 8349,\n",
       " 8370,\n",
       " 6125,\n",
       " 8079,\n",
       " 8148,\n",
       " 633,\n",
       " 13349,\n",
       " 11168,\n",
       " 9674,\n",
       " 1536,\n",
       " 13398,\n",
       " 7741,\n",
       " 3591,\n",
       " 12595,\n",
       " 5015,\n",
       " 14021,\n",
       " 4792,\n",
       " 1313,\n",
       " 9309,\n",
       " 9861,\n",
       " 11352,\n",
       " 10637,\n",
       " 4967,\n",
       " 3327,\n",
       " 10599,\n",
       " 7891,\n",
       " 6182,\n",
       " 13531,\n",
       " 461,\n",
       " 9171,\n",
       " 1677,\n",
       " 996,\n",
       " 840,\n",
       " 13301,\n",
       " 10436,\n",
       " 6040,\n",
       " 5051,\n",
       " 6984,\n",
       " 11572,\n",
       " 6679,\n",
       " 1385,\n",
       " 12206,\n",
       " 6824,\n",
       " 9298,\n",
       " 4212,\n",
       " 3178,\n",
       " 6808,\n",
       " 10432,\n",
       " 980,\n",
       " 10881,\n",
       " 11887,\n",
       " 12404,\n",
       " 3156,\n",
       " 8014,\n",
       " 7873,\n",
       " 12184,\n",
       " 5217,\n",
       " 5589,\n",
       " 11449,\n",
       " 1741,\n",
       " 9183,\n",
       " 6635,\n",
       " 8212,\n",
       " 450,\n",
       " 12388,\n",
       " 1187,\n",
       " 2802,\n",
       " 3780,\n",
       " 3814,\n",
       " 5602,\n",
       " 13719,\n",
       " 5252,\n",
       " 10338,\n",
       " 8848,\n",
       " 7360,\n",
       " 10766,\n",
       " 7658,\n",
       " 2755,\n",
       " 11214,\n",
       " 8767,\n",
       " 8408,\n",
       " 11625,\n",
       " 7056,\n",
       " 8029,\n",
       " 4091,\n",
       " 1822,\n",
       " 3754,\n",
       " 12504,\n",
       " 10066,\n",
       " 12906,\n",
       " 12452,\n",
       " 13763,\n",
       " 8323,\n",
       " 4087,\n",
       " 10765,\n",
       " 5424,\n",
       " 13962,\n",
       " 5764,\n",
       " 4527,\n",
       " 6219,\n",
       " 8191,\n",
       " 8839,\n",
       " 9537,\n",
       " 10250,\n",
       " 10079,\n",
       " 10624,\n",
       " 4835,\n",
       " 6028,\n",
       " 6271,\n",
       " 2810,\n",
       " 8896,\n",
       " 1298,\n",
       " 9571,\n",
       " 7320,\n",
       " 7154,\n",
       " 4195,\n",
       " 9021,\n",
       " 6751,\n",
       " 2678,\n",
       " 418,\n",
       " 13044,\n",
       " 1792,\n",
       " 6171,\n",
       " 12978,\n",
       " 3434,\n",
       " 2509,\n",
       " 5011,\n",
       " 6799,\n",
       " 5521,\n",
       " 5906,\n",
       " 10560,\n",
       " 7910,\n",
       " 6466,\n",
       " 8155,\n",
       " 5818,\n",
       " 6400,\n",
       " 11906,\n",
       " 10913,\n",
       " 4442,\n",
       " 5909,\n",
       " 7598,\n",
       " 6844,\n",
       " 11242,\n",
       " 10548,\n",
       " 4991,\n",
       " 1391,\n",
       " 13236,\n",
       " 8507,\n",
       " 10318,\n",
       " 5253,\n",
       " 5768,\n",
       " 7558,\n",
       " 2456,\n",
       " 5751,\n",
       " 4436,\n",
       " 11203,\n",
       " 5852,\n",
       " 7432,\n",
       " 1216,\n",
       " 4664,\n",
       " 2125,\n",
       " 10473,\n",
       " 14273,\n",
       " 9497,\n",
       " 2030,\n",
       " 11465,\n",
       " 10364,\n",
       " 8189,\n",
       " 12362,\n",
       " 3523,\n",
       " 10667,\n",
       " 11257,\n",
       " 804,\n",
       " 7667,\n",
       " 9435,\n",
       " 14130,\n",
       " 9352,\n",
       " 11909,\n",
       " 2259,\n",
       " 7867,\n",
       " 10640,\n",
       " 4491,\n",
       " 11673,\n",
       " 4283,\n",
       " 3765,\n",
       " 7024,\n",
       " 6802,\n",
       " 4766,\n",
       " 2452,\n",
       " 2317,\n",
       " 3067,\n",
       " 7870,\n",
       " 4704,\n",
       " 4957,\n",
       " 3427,\n",
       " 4524,\n",
       " 3659,\n",
       " 4204,\n",
       " 55,\n",
       " 1484,\n",
       " 9385,\n",
       " 11186,\n",
       " 7472,\n",
       " 14413,\n",
       " 9979,\n",
       " 13396,\n",
       " 5717,\n",
       " 2104,\n",
       " 9647,\n",
       " 3495,\n",
       " 7954,\n",
       " 8282,\n",
       " 4539,\n",
       " 7945,\n",
       " 12374,\n",
       " 6364,\n",
       " 4610,\n",
       " 12653,\n",
       " 13526,\n",
       " 6780,\n",
       " 5631,\n",
       " 11417,\n",
       " 1879,\n",
       " 9655,\n",
       " 5983,\n",
       " 14428,\n",
       " 2254,\n",
       " 12322,\n",
       " 10691,\n",
       " 2901,\n",
       " 12019,\n",
       " 2494,\n",
       " 5745,\n",
       " 7052,\n",
       " 8526,\n",
       " 1877,\n",
       " 3335,\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_indices = label_indices\n",
    "labeled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "499df631-af1c-47c8-a8cf-b960bd6f689f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4153, 11097, 3849, 518, 12611, 10339, 10138, 4498, 12436, 9062, 5846, 934, 10114, 6054, 8789, 10620, 7708, 11433, 10756, 13879, 3709, 11443, 2262, 4593, 3594, 2137, 10713, 7868, 742, 13130, 3661, 1110, 2459, 3098, 6765, 5337, 12284, 2695, 3760, 3567, 8094, 4471, 4372, 13021, 12447, 8875, 7800, 9753, 10997, 11250, 14267, 5431, 7828, 13572, 7378, 3437, 593, 10603, 5358, 357, 5268, 8199, 6790, 10924, 1461, 3986, 3701, 9344, 463, 245, 1453, 3607, 2318, 4057, 6998, 4084, 7890, 1972, 2665, 204, 9403, 8449, 13789, 4977, 8530, 10742, 2478, 72, 11298, 12014, 9878, 8007, 8073, 5890, 9935, 11423, 11712, 14054, 1539, 8482, 5598, 11238, 5027, 777, 5759, 6176, 13685, 9112, 4504, 5384, 6352, 5114, 13011, 4722, 12358, 14259, 6109, 5428, 4430, 10711, 4702, 9928, 12990, 8739, 10969, 94, 13067, 10507, 606, 10204, 4017, 3850, 4542, 6368, 5660, 6138, 1343, 13594, 14226, 10914, 11232, 884, 6502, 119, 2769, 4024, 7650, 14100, 1926, 8295, 6915, 8150, 7230, 2996, 7389, 10126, 6434, 11302, 7291, 3102, 12126, 6731, 13100, 2139, 2395, 7140, 4869, 9070, 12884, 4901, 1213, 6889, 6412, 6535, 7605, 9001, 7190, 13965, 3652, 11013, 6437, 598, 3252, 1450, 254, 6449, 2332, 10708, 3931, 3909, 539, 10184, 2512, 14145, 13821, 11402, 7822, 13318, 439, 4193, 10523, 1155, 3012, 12439, 9155, 11062, 10147, 5636, 9836, 2458, 14011, 6202, 4167, 5603, 13376, 7722, 13870, 6805, 9783, 11837, 533, 1348, 5177, 2107, 979, 9425, 8154, 11194, 2396, 7732, 251, 1310, 2282, 11346, 5789, 11872, 11278, 1709, 3228, 11126, 5008, 770, 4823, 12529, 2708, 8233, 6617, 8076, 4712, 7632, 15, 4858, 2054, 11606, 117, 11229, 11356, 7617, 9256, 5181, 12727, 1747, 11754, 13817, 12766, 6944, 10601, 830, 5584, 4562, 9496, 12378, 7267, 290, 723, 9530, 4613, 2036, 4310, 4450, 6482, 11817, 2848, 2090, 11455, 1894, 1232, 6376, 9680, 6696, 13279, 3040, 4667, 13771, 4513, 11181, 12885, 2152, 14177, 588, 5416, 12266, 3694, 7590, 3137, 6515, 2419, 6954, 13605, 2222, 7158, 4435, 168, 12655, 13087, 13501, 7978, 5964, 3802, 5807, 6918, 4366, 2860, 10404, 5339, 14265, 2180, 12714, 855, 5575, 3355, 1874, 11197, 10706, 6842, 4942, 1305, 11211, 5311, 9941, 5210, 10194, 8531, 8797, 3304, 7638, 479, 2847, 10805, 11428, 1753, 6897, 4614, 7388, 6858, 3604, 2803, 8986, 4961, 9774, 2436, 4958, 3186, 1341, 13166, 12631, 4320, 13568, 12444, 12071, 451, 370, 2095, 1003, 11859, 12680, 2672, 9912, 12627, 4523, 2555, 8907, 10580, 10874, 1378, 12547, 12065, 4098, 11812, 8074, 13071, 11992, 10502, 8031, 9129, 3017, 3476, 2928, 6603, 4315, 10213, 219, 50, 8346, 6781, 12486, 8744, 6946, 8420, 4361, 1203, 14194, 2793, 9465, 12790, 8251, 5926, 2253, 4299, 9045, 14186, 4659, 8259, 627, 7717, 6330, 12752, 13474, 6904, 9865, 14235, 7928, 6465, 4059, 12481, 924, 5925, 5205, 3496, 6050, 1727, 3439, 6445, 11362, 8138, 445, 2360, 13590, 14086, 2065, 7878, 11293, 1620, 920, 2455, 4745, 8802, 2394, 856, 5760, 5572, 2937, 5283, 11847, 11145, 6942, 9718, 8423, 10757, 5962, 378, 2176, 7121, 14356, 5044, 12028, 10256, 11396, 8039, 7105, 12282, 8357, 2895, 7761, 13880, 7258, 5948, 13164, 7686, 9694, 6112, 377, 9106, 9897, 2546, 102, 7723, 13256, 8933, 8263, 10239, 5947, 8871, 13532, 4062, 11373, 7556, 2217, 9823, 9744, 11830, 9772, 8240, 8990, 4074, 6744, 7027, 11869, 2440, 12882, 4904, 2823, 6843, 3743, 8222, 9316, 11987, 9575, 6209, 7068, 8219, 9063, 14135, 6517, 5374, 465, 1008, 10441, 1591, 2198, 5250, 5529, 3404, 13339, 3842, 5672, 6631, 8663, 1919, 12522, 8098, 3763, 5393, 12809, 10065, 2516, 12489, 6601, 2264, 4177, 14335, 11842, 1836, 10909, 11548, 3650, 10579, 9657, 781, 11191, 1589, 14291, 7858, 11483, 3506, 8638, 6683, 7535, 4681, 3454, 8517, 3412, 3413, 10437, 1837, 3641, 9342, 3257, 1024, 10484, 11388, 5932, 5649, 6116, 10009, 7511, 5763, 7821, 6716, 6446, 6959, 13746, 7639, 699, 8539, 11500, 9519, 7072, 11918, 6687, 1319, 6911, 2138, 11221, 12718, 2294, 2151, 4500, 5646, 8833, 4468, 1137, 4286, 2380, 11569, 7627, 4336, 11341, 13126, 8461, 4592, 11777, 10807, 4437, 2534, 1995, 6456, 2530, 12806, 5263, 3134, 1077, 2541, 6435, 8596, 5003, 6462, 7215, 5223, 14019, 1703, 4112, 6394, 6201, 8735, 3891, 13342, 2653, 350, 3711, 4309, 2685, 9278, 6143, 11649, 5388, 4948, 9130, 6225, 10481, 4921, 12377, 2363, 8359, 1129, 4612, 12162, 3071, 3856, 12267, 1704, 4346, 9389, 7930, 47, 1214, 4007, 4881, 12352, 11882, 5248, 8662, 623, 3859, 6136, 8218, 977, 10784, 11644, 4499, 5449, 3157, 14256, 13580, 1336, 8450, 11205, 4198, 13227, 5855, 3963, 9879, 9374, 8338, 6200, 4554, 10098, 3370, 12134, 7805, 5828, 4754, 2986, 9720, 10431, 9319, 3149, 5944, 4863, 1191, 1710, 11511, 2241, 12676, 7400, 4284, 1122, 1649, 3396, 10621, 1229, 6140, 7567, 6369, 3785, 12399, 3019, 11924, 13028, 10028, 4780, 5991, 13017, 434, 13800, 10349, 2016, 3274, 10112, 5401, 4561, 10622, 4862, 8349, 8370, 6125, 8079, 8148, 633, 13349, 11168, 9674, 1536, 13398, 7741, 3591, 12595, 5015, 14021, 4792, 1313, 9309, 9861, 11352, 10637, 4967, 3327, 10599, 7891, 6182, 13531, 461, 9171, 1677, 996, 840, 13301, 10436, 6040, 5051, 6984, 11572, 6679, 1385, 12206, 6824, 9298, 4212, 3178, 6808, 10432, 980, 10881, 11887, 12404, 3156, 8014, 7873, 12184, 5217, 5589, 11449, 1741, 9183, 6635, 8212, 450, 12388, 1187, 2802, 3780, 3814, 5602, 13719, 5252, 10338, 8848, 7360, 10766, 7658, 2755, 11214, 8767, 8408, 11625, 7056, 8029, 4091, 1822, 3754, 12504, 10066, 12906, 12452, 13763, 8323, 4087, 10765, 5424, 13962, 5764, 4527, 6219, 8191, 8839, 9537, 10250, 10079, 10624, 4835, 6028, 6271, 2810, 8896, 1298, 9571, 7320, 7154, 4195, 9021, 6751, 2678, 418, 13044, 1792, 6171, 12978, 3434, 2509, 5011, 6799, 5521, 5906, 10560, 7910, 6466, 8155, 5818, 6400, 11906, 10913, 4442, 5909, 7598, 6844, 11242, 10548, 4991, 1391, 13236, 8507, 10318, 5253, 5768, 7558, 2456, 5751, 4436, 11203, 5852, 7432, 1216, 4664, 2125, 10473, 14273, 9497, 2030, 11465, 10364, 8189, 12362, 3523, 10667, 11257, 804, 7667, 9435, 14130, 9352, 11909, 2259, 7867, 10640, 4491, 11673, 4283, 3765, 7024, 6802, 4766, 2452, 2317, 3067, 7870, 4704, 4957, 3427, 4524, 3659, 4204, 55, 1484, 9385, 11186, 7472, 14413, 9979, 13396, 5717, 2104, 9647, 3495, 7954, 8282, 4539, 7945, 12374, 6364, 4610, 12653, 13526, 6780, 5631, 11417, 1879, 9655, 5983, 14428, 2254, 12322, 10691, 2901, 12019, 2494, 5745, 7052, 8526, 1877, 3335, 9553, 6602, 4952, 7489, 4232, 11046, 13420, 4936, 13316, 7643, 10422, 13859, 2987, 3341, 4730, 4768, 224, 5553, 8134, 6249, 13151, 13271, 2954, 157, 11595, 3608, 5211, 12678, 7212, 3050, 10769, 8358, 11044, 13715, 2941, 11930, 4324, 3077, 3644, 3807, 4306, 7063, 2165, 11273, 8543, 2188, 5387, 5378, 296, 12695, 12056, 10935, 9690, 8127, 8645, 8914, 4011, 8831, 2814, 10682, 13497, 7825, 3273, 386, 1850, 3139, 10301, 11226, 4200, 1099, 8778, 4660, 13137, 4779, 6096, 11698, 6417, 14295, 6648, 11372, 3529, 10240, 3043, 274, 3372, 13918, 7163, 486, 6455, 9791, 1762, 61, 6191, 123, 9408, 8644, 4052, 1218, 4106, 5028, 9637, 5306, 2539, 3362, 10855, 9820, 6048, 2821, 8693, 6007, 2947, 8613, 10336, 9809, 11163, 4628, 261, 11137, 10569, 10446, 6349, 8476, 5895, 11135, 8236, 12233, 7460, 7520, 4446, 9803, 1471, 6898, 13281, 7242, 5438, 2324, 9230, 14176, 13877, 6947, 10035, 11950, 4088, 3706, 12821, 9418, 4605, 5271, 14105, 2674, 5873, 3548, 14317, 6384, 8391, 212, 10080, 6581, 4764, 14406, 9854, 2087, 8975, 11192, 3242, 7669, 11253, 14147, 9019, 5106, 8562, 1018, 3577, 14209, 1474, 10423, 11614, 1991, 5142, 9455, 9771, 3943, 5345, 5464, 1596, 8182, 13173, 12735, 13637, 5590, 2206, 9378, 8078, 3942, 9015, 2838, 748, 724, 8379, 2573, 7269, 13366, 5194, 202, 1784, 4713, 1616, 2734, 10275, 5839, 5665, 764, 10942, 9663, 1235, 5136, 8798, 12210, 4369, 13441, 7516, 13931, 3962, 11746, 12215, 11381, 8012, 5914, 14005, 5327, 9816, 11552, 8335, 6724, 6552, 4926, 5509, 484, 3930, 10594, 6298, 6865, 10067, 3799, 2047, 8810, 8557, 7139, 7185, 9554, 13893, 12922, 1102, 2112, 7202, 5977, 2820, 8065, 10207, 1377, 5820, 11521, 4810, 2948, 13099, 3518, 9231, 11854, 4699, 5756, 11053, 3669, 12641, 13145, 3379, 14293, 8614, 1296, 4130, 11603, 3494, 1006, 10823, 4914, 5618, 6951, 5750, 406, 8100, 13353, 5452, 12129, 13459, 13202, 410, 10687, 11160, 5129, 13822, 13331, 8364, 13194, 169, 9708, 3882, 1192, 956, 5336, 5986, 2894, 3332, 13106, 4262, 11078, 9288, 1759, 3601, 9124, 4601, 6263, 6184, 6476, 14131, 12846, 5144, 13654, 11651, 7503, 4077, 11161, 6982, 9010, 7785, 7448, 2355, 3895, 8278, 3534, 9705, 12554, 571, 13888, 10588, 488, 9272, 1569, 8946, 8764, 558, 5183, 12424, 8387, 4510, 2669, 7826, 2393, 2012, 11561, 8677, 3958, 751, 3821, 5303, 93, 902, 8727, 3880, 7259, 1009, 2973, 8757, 2890, 11840, 10448, 9162, 127, 1066, 7380, 5000, 3481, 9664, 3488, 7220, 7525, 9498, 6637, 557, 1929, 6038, 6575, 6146, 9147, 7872, 10865, 13842, 8941, 7231, 8217, 1440, 5798, 4490, 7325, 1941, 1270, 3693, 6649, 5542, 12067, 8979, 470, 578, 6697, 3989, 11193, 11880, 5035, 2531, 12205, 949, 6404, 11090, 8054, 13502, 5485, 4272, 2298, 5614, 6868, 10749, 12818, 206, 10217, 13579, 13693, 97, 9180, 6211, 9000, 4050, 3267, 1899, 8904, 68, 10341, 5810, 2976, 9895, 8276, 4883, 6129, 8271, 10042, 9847, 9007, 11108, 11016, 11678, 6485, 13327, 13068, 1021, 5313, 3204, 2426, 9532, 7308, 2559, 3791, 10326, 5841, 8330, 13975, 11289, 4293, 13973, 2170, 5775, 12956, 4933, 2715, 5316, 4090, 10979, 3138, 7764, 2898, 4149, 3904, 4444, 7635, 11100, 8419, 1306, 513, 698, 1089, 5163, 11268, 5065, 2829, 12524, 1070, 8386, 6089, 12455, 2739, 3359, 2854, 5871, 2930, 13131, 6847, 6748, 10759, 13645, 5939, 13864, 628, 8706, 7921, 3573, 11853, 8847, 4550, 4129, 13378, 11151, 8964, 11033, 10724, 13284, 3199, 9289, 9786, 13992, 7148, 12661, 10130, 9868, 5039, 11045, 10792, 2776, 9696, 4462, 9358, 8950, 10783, 2813, 8675, 14084, 11468, 534, 2320, 12117, 9196, 11041, 8674, 7624, 796, 1197, 14037, 7058, 9652, 7473, 9475, 3409, 10018, 13933, 8471, 11361, 11971, 2150, 12931, 10718, 1867, 12532, 5264, 4872, 3456, 96, 10458, 2779, 10386, 5451, 6913, 9379, 2711, 1913, 2304, 9409, 13539, 4770, 8453, 7373, 11139, 14098, 943, 6166, 7918, 3955, 12151, 242, 14112, 5396, 7189, 2889, 5670, 7329, 8051, 7492, 5314, 12417, 10986, 7942, 6261, 5100, 4407, 1262, 1901, 7417, 2597, 11731, 8488, 6197, 6335, 14092, 10725, 7608, 10517, 7742, 13095, 7773, 472, 4632, 5907, 3232, 8451, 9035, 8870, 4694, 9490, 6451, 9080, 3440, 7247, 7987, 1478, 7994, 3165, 13946, 5381, 9581, 11829, 7042, 8129, 10577, 8140, 3553, 12409, 11425, 8173, 10998, 7854, 5809, 4126, 13643, 12780, 8470, 6246, 2424, 13383, 6739, 2786, 12397, 13903, 9830, 3152, 5812, 10518, 7348, 12877, 6609, 13014, 10947, 5158, 2310, 2874, 10686, 1726, 10369, 6341, 1864, 7290, 9350, 3057, 4001, 179, 9701, 6971, 7786, 13039, 8152, 5997, 9976, 7537, 9295, 3954, 1245, 11826, 4082, 8817, 3206, 4794, 4008, 5038, 1181, 9716, 10879, 8322, 11856, 7102, 7477, 5262, 9724, 9838, 11888, 5903, 7019, 839, 6638, 927, 6035, 3562, 4123, 12124, 2687, 536, 8141, 2046, 9321, 5532, 7179, 1400, 10470, 693, 13158, 7694, 9590, 8184, 7151, 193, 4248, 1538, 7820, 1116, 474, 9101, 6320, 4154, 322, 11508, 5787, 235, 2624, 2243, 4878, 2406, 13048, 11252, 8719, 13161, 6367, 5626, 9677, 6710, 8787, 13083, 7355, 1495, 6174, 11170, 8113, 2432, 565, 1439, 6443, 4695, 13229, 9340, 8477, 10529, 7502, 11783, 11258, 2487, 853, 13750, 3386, 4394, 6156, 10511, 7053, 3663, 4920, 599, 6556, 12596, 7888, 13343, 10251, 11121, 5842, 4191, 9410, 60, 1026, 13111, 4798, 8519, 10378, 8177, 5201, 6688, 11390, 5340, 5321, 800, 7665, 7381, 5668, 9576, 3493, 11641, 11961, 2407, 12917, 11894, 6288, 938, 1821, 11849, 10927, 11287, 768, 8145, 4706, 2258, 6458, 4871, 10936, 7898, 4647, 12076, 10953, 4279, 1136, 7469, 11713, 14299, 14326, 10864, 11235, 8763, 6742, 3143, 175, 2120, 7795, 1511, 5497, 7399, 14417, 12716, 11384, 3742, 5596, 4685, 11513, 5735, 776, 7877, 957, 9100, 1210, 3471, 11734, 5312, 3898, 12650, 955, 10085, 4645, 13589, 1458, 12895, 2520, 5753, 1380, 6856, 1304, 10219, 13840, 268, 3170, 13996, 10781, 6175, 7907, 13843, 1826, 9784, 13867, 537, 12593, 8692, 6113, 2213, 8428, 12715, 2155, 5928, 10597, 7022, 8880, 2292, 5840, 8249, 5709, 7876, 8494, 9016, 3847, 2098, 13263, 10299, 9606, 12223, 12614, 11860, 7637, 3216, 10767, 4355, 6377, 3422, 10675, 559, 8542, 14385, 180, 706, 3391, 12080, 5152, 9423, 2754, 24, 8037, 5645, 255, 7191, 6970, 5319, 12836, 6673, 2272, 7436, 7416, 11363, 13652, 9893, 3572, 13753, 3902, 3305, 8769, 12091, 6640, 7425, 5187, 11026, 11359, 10948, 6479, 7169, 9469, 11457, 10183, 7136, 7931, 2574, 7337, 2834, 5124, 7521, 10809, 1123, 4443, 11902, 5207, 11177, 8811, 6797, 10289, 8879, 6080, 753, 8513, 1227, 12384, 1653, 5856, 11611, 13401, 1679, 625, 8024, 10830, 12268, 3097, 3920, 3049, 11921, 11865, 192, 11907, 813, 369, 7599, 2099, 10082, 10954, 3342, 5006, 6076, 5582, 9508, 9561, 908, 8237, 12408, 10653, 11462, 10730, 1429, 5988, 2905, 3044, 9131, 6168, 9673, 11724, 2762, 11980, 6548, 10156, 5355, 6941, 7816, 7368, 8404, 9167, 3210, 7146, 2931, 4849, 1856, 13033, 1338, 5241, 2319, 9375, 12234, 5993, 5175, 7124, 8856, 8687, 4341, 4980, 7698, 1969, 1397, 10155, 4488, 7546, 2809, 11324, 4806, 4002, 12264, 3619, 1530, 6550, 14368, 9326, 3291, 7718, 1430, 4374, 4910, 222, 11010, 10141, 8556, 10901, 9849, 12130, 4385, 13026, 4714, 6454, 8196, 10277, 12976, 7542, 2985, 3728, 3636, 12239, 10843, 12803, 3349, 5607, 10287, 3485, 3910, 5625, 2117, 4981, 1886, 3511, 1702, 11482, 11430, 11217, 11806, 2804, 4966, 1251, 7076, 13713, 7020, 2732, 1748, 11413, 2837, 10567, 4350, 13634, 3927, 8299, 151, 2383, 8407, 4101, 4211, 10561, 2476, 2737, 732, 8670, 133, 1064, 2093, 14122, 7735, 5405, 8790, 2980, 5609, 4525, 3489, 10832, 14296, 5982, 3901, 10892, 5020, 8119, 7085, 6721, 12222, 13421, 10234, 6407, 2397, 6429, 13676, 3892, 9992, 7886, 7534, 7101, 11502, 13128, 5528, 4261, 10787, 7494, 6111, 10145, 6190, 1324, 8772, 4673, 11898, 1418, 11299, 10294, 6756, 6939, 3795, 8844, 8344, 12258, 6453, 10010, 810, 11237, 13429, 2196, 12146, 8040, 11966, 11667, 7855, 6244, 8401, 12197, 6514, 9918, 7459, 318, 5057, 12802, 12654, 3491, 2704, 11485, 8993, 6009, 8130, 1957, 6812, 7507, 8903, 12133, 6814, 4386, 7414, 8665, 4725, 3809, 7621, 4767, 3938, 2201, 2705, 3300, 7536, 10544, 6616, 11040, 4000, 10648, 364, 8545, 7932, 1872, 4557, 10558, 10728, 8168, 12416, 11604, 11079, 5010, 6684, 2554, 8183, 1111, 9925, 5017, 10132, 7884, 11382, 671, 11172, 6299, 1734, 9335, 7205, 8991, 8709, 4519, 4923, 7018, 1605, 10668, 1736, 5904, 11442, 2470, 2707, 11141, 11052, 4960, 8959, 4626, 2908, 1799, 8101, 4972, 12172, 1897, 2775, 11886, 7794, 5701, 4898, 9938, 14181, 988, 12784, 11322, 7126, 9151, 8515, 8187, 13917, 11510, 3000, 7543, 11766, 7062, 7619, 6283, 8818, 9564, 9812, 7413, 8125, 8587, 13915, 5467, 11843, 2127, 13034, 3094, 10267, 5896, 1797, 13231, 4912, 1674, 2551, 13230, 2279, 1817, 13710, 4837, 4097, 12582, 10215, 5032, 6241, 10513, 10390, 1780, 6049, 9065, 2788, 6470, 5740, 10442, 9075, 2296, 11958, 1265, 10242, 2269, 6694, 334, 1379, 11555, 10745, 66, 1810, 9186, 7506, 10899, 12213, 12422, 2405, 12639, 10433, 10868, 13640, 14049, 10906, 961, 7177, 6798, 13862, 12757, 3913, 1415, 3146, 7719, 3632, 10222, 3854, 12232, 12685, 1773, 7253, 2024, 8429, 6503, 2735, 9470, 13119, 13725, 5832, 8642, 10153, 9300, 2710, 10950, 5843, 9758, 9140, 7463, 2002, 6630, 1806, 4064, 4473, 7289, 6722, 9023, 13254, 5549, 10425, 6707, 7745, 13995, 10655, 2643, 11103, 2161, 3557, 8086, 6969, 10302, 11439, 5269, 10091, 489, 6012, 7407, 5125, 7145, 2140, 6223, 1993, 11179, 7657, 4410, 9972, 7649, 10173, 1532, 2606, 13370, 2386, 1758, 6800, 7412, 2994, 5458, 11337, 9874, 8228, 4252, 11586, 4373, 1608, 8457, 2965, 13237, 13878, 4950, 11329, 12584, 6432, 4332, 7685, 3834, 7926, 2668, 9936, 12235, 80, 12343, 8550, 10639, 13767, 5695, 1947, 225, 2855, 4918, 10988, 9942, 3934, 8254, 5629, 9682, 13037, 8069, 7000, 13712, 4697, 11006, 12768, 4989, 11592, 278, 146, 283, 13283, 10238, 11365, 9871, 9665, 8160, 5705, 10149, 5990, 9781, 2549, 12107, 4209, 5866, 2037, 2595, 10356, 3100, 4020, 2322, 2119, 1562, 349, 11027, 692, 13900, 12645, 11605, 9963, 7705, 8023, 5479, 12103, 409, 4885, 9247, 1153, 13510, 9200, 1396, 2503, 12693, 3192, 994, 7583, 5422, 10628, 10804, 11539, 9348, 3848, 11368, 10629, 8369, 9618, 13944, 4132, 1592, 7949, 8536, 5994, 3889, 6821, 718, 4440, 10676, 3112, 1761, 8656, 14416, 6685, 2943, 13403, 11223, 12662, 14059, 5257, 90, 8169, 10714, 2378, 1295, 8825, 11183, 3373, 113, 11056, 14, 10696, 14354, 9837, 7354, 5067, 11128, 2114, 8409, 2263, 6894, 8108, 9060, 2068, 5372, 3197, 2968, 6772, 6625, 11377, 1785, 2410, 11739, 1497, 6227, 673, 5558, 3301, 3937, 14219, 10325, 14438, 13439, 1032, 2638, 14424, 8402, 3969, 9356, 5189, 4271, 6840, 11380, 11271, 2728, 1157, 6870, 10072, 6628, 4817, 3367, 6762, 5122, 8441, 6082, 1542, 9872, 1327, 1342, 7673, 1740, 13742, 4653, 10161, 3122, 9178, 9634, 6571, 14283, 3991, 4116, 8064, 2164, 12025, 11838, 13876, 2462, 240, 669, 1996, 12230, 11821, 4604, 9277, 4953, 1395, 2007, 12152, 4243, 8328, 4214, 7986, 8809, 3059, 7104, 10419, 3174, 7252, 11581, 7671, 2749, 806, 6286, 10869, 4588, 2044, 1105, 9237, 11096, 12021, 2594, 2716, 10700, 12839, 3829, 7237, 12988, 2247, 2336, 11647, 10033, 2913, 12007, 5955, 9593, 4496, 9246, 8627, 5098, 6645, 1966, 8008, 8304, 8260, 3376, 634, 11222, 12308, 355, 14036, 10196, 5229, 3483, 10333, 1355, 13402, 7272, 9931, 4291, 4274, 198, 644, 2174, 7976, 8231, 3648, 5364, 12517, 3113, 5886, 540, 7227, 6478, 8599, 10055, 5465, 11567, 4356, 9066, 3127, 6414, 5616, 7316, 6025, 2903, 8795, 11175, 9764, 3072, 11060, 11306, 13413, 5921, 3682, 9486, 587, 5178, 7423, 3375, 4536, 7902, 3917, 6591, 2361, 4240, 3686, 5256, 8954, 11792, 3993, 12353, 10764, 3354, 10844, 11662, 3505, 7391, 1661, 1365, 13367, 10524, 1506, 1599, 5951, 7127, 13447, 3606, 14024, 11210, 12811, 11277, 1563, 5771, 372, 14447, 8432, 4489, 5427, 2325, 6046, 9315, 5797, 3198, 1796, 10284, 5987, 4006, 4776, 6985, 13489, 6576, 14409, 2026, 7011, 5634, 5894, 9843, 7532, 6520, 311, 5996, 11043, 5368, 3003, 9597, 12599, 12955, 6523, 11231, 5504, 4790, 13219, 6747, 10370, 4314, 6928, 10813, 7815, 10571, 11855, 1038, 7204, 4927, 6653, 3653, 1030, 3757, 8522, 9993, 6615, 7282, 7260, 1575, 13782, 11190, 8548, 5351, 2311, 7043, 9500, 5736, 2457, 181, 2270, 7411, 1322, 3358, 4511, 10937, 7979, 8223, 12808, 4915, 8005, 2382, 3016, 2418, 5661, 10058, 10400, 4417, 6966, 653, 13689, 6574, 4994, 3285, 12534, 10732, 6150, 10741, 1431, 8891, 2300, 12833, 5686, 12298, 10590, 4963, 4089, 13277, 10414, 13025, 7359, 11312, 13019, 4990, 8940, 9126, 10656, 4839, 9635, 2623, 9675, 12470, 11294, 2183, 7470, 11460, 1921, 11844, 9762, 12577, 2379, 3346, 6641, 4908, 2872, 12224, 8266, 7924, 3936, 10179, 10083, 2828, 1053, 7201, 10202, 3265, 12528, 3442, 239, 2062, 2153, 9825, 5851, 10089, 13057, 9113, 1612, 10104, 2702, 7554, 11034, 6692, 12682, 6853, 991, 5776, 10974, 4710, 637, 11158, 3592, 756, 6848, 2350, 7977, 12775, 11621, 14163, 6372, 5891, 5601, 1912, 1975, 6750, 9735, 9829, 10465, 7198, 6612, 8479, 10321, 13251, 11695, 4992, 13881, 13554, 5119, 8383, 11282, 10495, 12412, 12773, 6031, 164, 6860, 8576, 2581, 11851, 10331, 4976, 3207, 4860, 5613, 9275, 2118, 677, 11116, 12985, 6032, 8253, 13596, 8618, 8115, 9482, 5160, 3640, 3312, 4820, 6738, 10022, 341, 4833, 12787, 3423, 13313, 11173, 12758, 4760, 2879, 11497, 823, 12883, 7243, 6678, 11738, 5454, 2070, 2789, 11072, 2291, 7593, 10169, 1687, 7297, 11081, 11416, 14039, 4301, 9813, 12824, 2009, 1723, 9980, 11395, 6501, 814, 3501, 10209, 13744, 1005, 6727, 10709, 10388, 3066, 4054, 5674, 2042, 7577, 14325, 1541, 3798, 10248, 10427, 14063, 616, 7899, 10836, 8248, 1455, 8443, 8898, 4747, 7998, 11927, 19, 1619, 3569, 1275, 13053, 12176, 4526, 6935, 847, 7612, 9427, 2352, 1422, 10919, 1688, 2900, 7327, 966, 14234, 936, 6084, 6100, 6590, 2966, 8813, 4516, 7335, 1492, 5425, 4668, 5773, 9458, 3180, 8590, 11234, 10977, 2808, 2858, 10966, 10129, 3879, 3241, 8300, 3173, 51, 8232, 2195, 3617, 12426, 11000, 12989, 8294, 7626, 7499, 10952, 10790, 8882, 10731, 4937, 3723, 11304, 5883, 9292, 8412, 347, 7054, 5816, 10835, 8500, 12323, 5143, 9176, 9608, 197, 757, 654, 3331, 1725, 12034, 5533, 10733, 13566, 11021, 13981, 6584, 11069, 8777, 3912, 6718, 5554, 1260, 3259, 5817, 9712, 13162, 9956, 1970, 11355, 13451, 5279, 2770, 6619, 7385, 6582, 14026, 9507, 3900, 10026, 7894, 7339, 10680, 4400, 2869, 4928, 9276, 4275, 11899, 7711, 6226, 4051, 3022, 10726, 3029, 4460, 11061, 6104, 10867, 2609, 207, 8424, 6980, 7746, 14314, 13146, 11615, 5804, 7440, 7025, 3005, 8353, 14414, 11331, 6269, 6651, 7111, 3465, 9405, 5155, 9192, 4924, 12297, 5398, 8426, 11597, 11202, 4744, 3378, 3593, 883, 2439, 13632, 2433, 4884, 7860, 8822, 7549, 2934, 9899, 6703, 5938, 3855, 10615, 8771, 11520, 5503, 1321, 6547, 9243, 4033, 1409, 11736, 1638, 10496, 8700, 12745, 10392, 14051, 5801, 6633, 8114, 6558, 4559, 13435, 6760, 11977, 8015, 5415, 5743, 7277, 2346, 14319, 14077, 4974, 8215, 1012, 11464, 8273, 1059, 9732, 2846, 4152, 5404, 8287, 7912, 3547, 1698, 186, 5910, 267, 10630, 8487, 14116, 3380, 8759, 3161, 11696, 3399, 6922, 5888, 10566, 6963, 5197, 8342, 1916, 1496, 10539, 3280, 4349, 14262, 9529, 11409, 4718, 4620, 4788, 9711, 9053, 3348, 13598, 6062, 11747, 5075, 8257, 5715, 8267, 2004, 10889, 13801, 1801, 10985, 12957, 3082, 9937, 10887, 3570, 4143, 10420, 7756, 13848, 12334, 4479, 3651, 1527, 6065, 6381, 7322, 3957, 7628, 1561, 2787, 3394, 3863, 13910, 6060, 4354, 7743, 411, 4906, 11185, 9876, 7372, 6510, 4578, 6022, 8846, 2817, 203, 5046, 13957, 3665, 10933, 1194, 6315, 2839, 10070, 9702, 7981, 7482, 9620, 2015, 6051, 5788, 9653, 9412, 8026, 11531, 629, 2209, 5912, 10381, 6361, 9411, 8951, 4569, 149, 11723, 6709, 7946, 7429, 4344, 6262, 5704, 4832, 10883, 2083, 5966, 10115, 8807, 4778, 793, 9211, 1399, 6310, 6237, 10533, 3532, 2122, 2561, 11403, 1637, 2401, 8615, 10110, 3416, 7213, 3374, 5322, 7187, 10491, 2147, 5204, 12487, 4157, 8082, 8910, 959, 11985, 6711, 11690, 6248, 1449, 8036, 4757, 11017, 1981, 9084, 10995, 11684, 5952, 3490, 1917, 526, 3238, 8971, 6218, 7300, 1041, 8827, 3612, 7015, 5989, 4784, 13907, 6351, 9900, 13139, 3654, 10252, 2904, 3145, 1743, 13444, 2265, 8622, 12677, 8574, 11630, 3325, 2228, 5490, 2375, 7276, 8998, 6680, 11199, 7266, 8033, 7997, 7562, 7249, 3725, 7774, 4797, 7703, 10175, 10411, 3308, 12959, 13069, 2364, 10268, 10233, 1475, 6490, 9103, 5861, 1404, 10971, 11307, 5945, 8963, 11573, 1448, 10276, 1054, 2740, 6312, 13775, 2404, 4674, 7882, 2746, 8422, 6656, 648, 3771, 6910, 1387, 7410, 13779, 11002, 9452, 6428, 5519, 10800, 8987, 10480, 871, 4819, 978, 14048, 10602, 10450, 10415, 895, 5123, 4567, 6120, 13630, 10559, 1983, 12326, 9739, 13851, 7081, 4226, 2639, 3744, 5729, 5213, 5777, 10898, 7216, 7203, 7925, 4357, 14169, 4742, 9217, 13296, 8607, 13261, 2583, 14240, 2181, 13167, 11495, 5880, 107, 14045, 12723, 10456, 4756, 1185, 1193, 1905, 6536, 12728, 1843, 9678, 11466, 4457, 1855, 867, 11522, 9614, 4941, 4222, 4669, 104, 13956, 3708, 11962, 2513, 9811, 13009, 6416, 8992, 8474, 13708, 13530, 8527, 10205, 5725, 12011, 10282, 9193, 10891, 8406, 4678, 14227, 5037, 6829, 1549, 11248, 3735, 14057, 12656, 12569, 382, 11506, 12943, 14435, 5711, 11949, 2126, 2267, 3778, 3340, 8939, 11283, 8025, 9448, 5445, 3418, 2633, 7750, 5581, 3457, 11610, 218, 10306, 11024, 6188, 12237, 8583, 14276, 6644, 5604, 3187, 9683, 10319, 3336, 845, 11366, 7689, 2927, 12393, 13179, 4549, 2445, 9914, 3499, 3554, 3060, 6877, 141, 4606, 5254, 2072, 3885, 11244, 962, 1658, 7302, 3164, 14210, 3941, 1446, 5141, 6393, 13150, 4451, 13845, 9760, 4683, 9641, 12635, 8561, 4621, 12725, 9669, 12193, 10168, 8096, 5149, 546, 14185, 6994, 14257, 829, 13214, 7017, 9601, 9323, 1632, 5061, 8157, 3933, 12973, 7766, 1211, 11353, 14006, 9373, 553, 7138, 3351, 13583, 14332, 10236, 9828, 6701, 10863, 584, 8806, 989, 7094, 3732, 7379, 5579, 2870, 44, 4381, 12257, 4224, 7727, 10575, 10834, 4142, 2338, 3970, 383, 5963, 6914, 11216, 14269, 3270, 8540, 11910, 6682, 8431, 3979, 3320, 8246, 130, 7968, 12604, 6071, 3155, 11666, 8221, 13806, 13546, 5344, 6444, 8741, 13365, 10660, 9948, 8925, 607, 3209, 9533, 2281, 7458, 9158, 57, 2831, 7033, 10293, 5330, 13086, 7125, 6725, 374, 13509, 12270, 3088, 13399, 3225, 7530, 8960, 8203, 6929, 8730, 13987, 2654, 13574, 1334, 10582, 12379, 14367, 1764, 7306, 2204, 14107, 9282, 7, 8111, 4540, 9085, 14217, 7526, 10651, 10328, 7471, 8592, 10008, 3024, 1063, 2532, 702, 10005, 8038, 5501, 13045, 9159, 9108, 11858, 7364, 13050, 2331, 14278, 5140, 903, 471, 4824, 112, 6564, 11730, 4946, 13781, 2618, 10771, 194, 5653, 5357, 7724, 14081, 9040, 227, 3425, 3295, 2924, 3464, 6356, 6306, 4736, 11857, 8658, 12935, 3566, 7369, 10086, 12292, 9009, 6560, 8653, 8760, 9670, 6484, 7165, 1678, 13913, 7653, 1887, 8447, 9370, 459, 82, 3844, 11003, 8137, 1775, 4703, 1945, 6691, 4728, 12419, 8573, 2092, 11167, 10142, 7576, 7602, 9603, 1249, 3360, 6230, 12448, 9888, 7006, 7057, 5025, 2175, 11220, 13820, 910, 1594, 1610, 4121, 12506, 8758, 6974, 4595, 5325, 13051, 11285, 3150, 7747, 8021, 3249, 7060, 3148, 6732, 2226, 9662, 4630, 3587, 4830, 3123, 6115, 1770, 969, 2384, 13043, 3731, 2082, 1667, 444, 7857, 10550, 9429, 13832, 9844, 10498, 9998, 10797, 896, 1339, 10490, 5300, 13571, 2484, 7990, 10466, 6294, 2907, 1903, 3498, 13701, 2000, 12111, 10760, 7901, 12986, 4429, 6551, 8452, 2230, 11519, 12579, 11281, 3361, 12058, 3030, 6945, 8736, 13759, 8955, 14347, 10090, 8774, 3817, 11338, 14306, 1956, 9354, 1683, 4099, 4111, 12701, 6839, 7447, 412, 10514, 4338, 7768, 968, 1491, 13778, 13319, 9082, 9768, 8753, 2956, 6967, 7751, 2620, 6256, 10527, 7740, 10358, 8770, 4061, 1464, 6568, 4339, 10570, 6052, 12328, 1578, 11994, 2022, 13758, 10981, 6440, 1274, 5959, 11438, 2231, 8099, 8116, 3772, 7194, 999, 10355, 7974, 8063, 13829, 10428, 1148, 1445, 5595, 12183, 9431, 3217, 3585, 2576, 9036, 6832, 12568, 647, 9580, 14436, 11646, 14410, 3253, 9908, 7155, 9633, 11679, 13291, 14148, 1499, 3610, 4506, 1290, 5564, 1715, 2447, 6792, 10652, 923, 3716, 2251, 5555, 4266, 7900, 9032, 7538, 10263, 9444, 1114, 4631, 7555, 9790, 7582, 9311, 6098, 6816, 11481, 10774, 7233, 10893, 8001, 11305, 11845, 3947, 12977, 2069, 1633, 10562, 12640, 2812, 2842, 4979, 9493, 6654, 2663, 6968, 11152, 13971, 2791, 5878, 3041, 5678, 1096, 3269, 4160, 331, 11080, 11215, 2341, 13551, 13542, 3668, 13963, 5937, 8776, 9346, 11591, 1350, 2064, 13818, 1769, 3858, 13680, 921, 13307, 6527, 8185, 919, 5755, 9885, 6540, 6433, 963, 10365, 5752, 1121, 6714, 12273, 1869, 1020, 6580, 2351, 3583, 1259, 3479, 10181, 7454, 9301, 13390, 2398, 13355, 3203, 5047, 13152, 13783, 29, 1716, 3121, 13740, 3818, 8830, 1084, 14289, 4027, 12432, 12889, 12597, 10095, 2882, 13232, 9551, 7592, 8823, 10785, 10088, 13901, 4456, 5203, 7672, 45, 12482, 4032, 8922, 8641, 4882, 11241, 12571, 12553, 13895, 12078, 8984, 6427, 10310, 5353, 6242, 1600, 13308, 5517, 11435, 6083, 4787, 12294, 9049, 3487, 2961, 10029, 4897, 13937, 970, 12835, 3703, 8356, 9005, 10227, 10704, 5949, 11628, 5113, 985, 1007, 10463, 8751, 7995, 7656, 4257, 6890, 6634, 5430, 13118, 3510, 1118, 971, 2696, 2721, 11543, 10592, 306, 13464, 9503, 2216, 12087, 8931, 2489, 10788, 4813, 1244, 7319, 6704, 12325, 1752, 5107, 4913, 11260, 4655, 1126, 7110, 10000, 13518, 4922, 9920, 12967, 5093, 14245, 1293, 1559, 7271, 3131, 8647, 9143, 5267, 9770, 5109, 3789, 10896, 2926, 1086, 10692, 7728, 2875, 1255, 6178, 7692, 4230, 438, 750, 9520, 14250, 1277, 9114, 8256, 5723, 5847, 13242, 6774, 1613, 5814, 14242, 5999, 791, 2547, 8918, 3581, 12480, 5064, 4929, 1267, 2536, 7587, 6920, 11311, 13661, 2427, 9988, 14038, 2237, 3810, 3729, 909, 630, 3126, 11099, 3625, 10923, 4560, 7232, 6199, 13016, 1697, 620, 9033, 12269, 389, 2852, 7386, 6245, 8283, 4684, 2123, 7349, 6132, 12919, 8707, 4022, 10258, 3514, 1364, 11249, 9746, 1212, 7782, 7779, 1819, 3719, 1717, 6127, 1351, 299, 8247, 10735, 4078, 6053, 14448, 6183, 13220, 183, 8042, 6657, 13731, 11761, 9369, 9636, 10720, 6629, 2565, 13702, 812, 187, 7296, 3616, 5931, 6396, 11820, 13825, 13234, 11613, 3727, 8347, 1696, 7973, 305, 10228, 10195, 5276, 7107, 9987, 9567, 14096, 11638, 1910, 1299, 515, 7802, 7770, 8372, 1420, 2867, 8773, 13120, 7464, 8411, 11440, 10186, 13604, 3797, 12541, 13921, 3971, 2677, 1735, 6419, 13999, 7363, 7209, 8009, 6034, 11360, 12638, 2920, 5243, 8509, 7683, 3867, 4705, 1058, 4382, 5936, 2579, 7238, 10838, 12474, 1436, 528, 1731, 7850, 11374, 8565, 4890, 3890, 9242, 12911, 12123, 4964, 13649, 5338, 12558, 10189, 7935, 11801, 12365, 9091, 14411, 8435, 13114, 10457, 615, 2673, 4597, 6011, 2344, 1516, 594, 7903, 12366, 6626, 5845, 12938, 6018, 5657, 4917, 3311, 10608, 7351, 3578, 12120, 9038, 8996, 11563, 1902, 9329, 3433, 6345, 907, 7251, 3452, 11574, 5104, 1597, 3201, 1028, 1031, 6085, 13311, 10393, 13819, 4199, 6938, 5198, 11063, 12079, 4687, 7028, 2833, 6002, 5297, 247, 9806, 8497, 7707, 995, 1292, 9244, 5635, 12562, 4399, 13098, 4954, 7660, 12897, 6706, 8976, 1922, 8310, 13163, 1839, 1252, 3667, 2055, 2373, 11538, 384, 3074, 11280, 8046, 767, 11274, 8077, 10120, 10782, 4424, 4709, 9528, 3142, 5757, 3385, 745, 1932, 1398, 13639, 13480, 5400, 7219, 11085, 769, 6663, 6397, 9741, 13564, 12035, 9782, 8317, 10848, 9501, 4267, 1522, 5710, 2877, 5426, 13675, 1093, 6729, 6900, 404, 8318, 6961, 3224, 1500, 4721, 4063, 3975, 6016, 498, 1654, 5260, 2266, 8002, 9137, 12202, 2171, 2959, 162, 7109, 5286, 3543, 8502, 3166, 4733, 12285, 2910, 11187, 6544, 8302, 6295, 4136, 12293, 10810, 13824, 9422, 464, 3172, 9432, 13297, 6973, 1891, 661, 4016, 1711, 7914, 3196, 2280, 7224, 10877, 10308, 6187, 4340, 9308, 3671, 6056, 6883, 5448, 9219, 5965, 5083, 4879, 5120, 7050, 5274, 9800, 4321, 7098, 13830, 6189, 10772, 7338, 4426, 1942, 3477, 4478, 5058, 3545, 1002, 7255, 5599, 5675, 8546, 11744, 12240, 11264, 13211, 585, 8887, 7404, 9228, 12190, 11737, 6674, 4225, 9224, 9592, 4598, 14381, 12043, 1326, 4318, 4795, 2131, 9013, 1139, 514, 10505, 7553, 8397, 4408, 13078, 8060, 2891, 4392, 2423, 11091, 10352, 3184, 5552, 3996, 10127, 4512, 34, 3419, 3916, 10139, 4783, 7706, 947, 4934, 1233, 5266, 4769, 8491, 8362, 9749, 5870, 10859, 10255, 5702, 13790, 10478, 3973, 6887, 12694, 5980, 10283, 13487, 6689, 12557, 199, 10094, 10001, 13839, 4887, 8030, 4726, 11371, 8394, 2982, 941, 11601, 7078, 10489, 10525, 3052, 8685, 10348, 3129, 2694, 13525, 511, 773, 6255, 4984, 12565, 821, 14277, 6596, 12724, 7614, 6668, 3015, 12221, 5691, 13883, 2522, 9589, 8804, 10820, 14350, 1011, 3470, 3449, 2717, 1737, 10410, 264, 9087, 8850, 285, 7647, 11702, 10795, 6081, 6650, 10612, 5342, 2205, 7763, 5784, 8484, 7581, 5173, 4988, 6988, 4483, 8146, 8957, 1141, 1158, 8821, 2998, 4241, 6383, 7982, 9801, 2159, 7578, 2760, 8363, 3175, 4419, 7014, 7487, 14201, 6063, 4715, 1459, 9728, 682, 11209, 4875, 1507, 9382, 7615, 11408, 2358, 7783, 13132, 11345, 3775, 4782, 7468, 13322, 4135, 4675, 8337, 6421, 10224, 3035, 9453, 13977, 1432, 13490, 13177, 596, 2722, 12559, 11227, 8554, 4727, 6632, 8849, 4720, 7780, 3031, 13631, 3344, 9517, 6196, 7478, 1092, 2505, 6343, 6695, 14345, 9115, 10585, 2780, 3141, 2113, 11082, 8305, 5, 4210, 10694, 10939, 6388, 13488, 13453, 10572, 3655, 5437, 8655, 8270, 3522, 4122, 1570, 11463, 11988, 13440, 13575, 8071, 3787, 13865, 13049, 13411, 11138, 3467, 11499, 6247, 7311, 3634, 11688, 2774, 794, 6130, 10477, 1176, 2001, 5586, 10046, 2431, 2314, 1207, 10274, 5689, 783, 13804, 2202, 13952, 657, 11412, 3064, 7697, 10232, 7871, 7630, 13656, 12064, 2974, 532, 10266, 9832, 10664, 9006, 11763, 10600, 1502, 6305, 12791, 2824, 11025, 14064, 380, 5185, 11022, 6008, 10474, 12097, 8456, 2032, 5498, 8403, 2124, 9314, 4520, 10642, 248, 10440, 3107, 4174, 1939, 12974, 2034, 5226, 4896, 12830, 691, 11944, 4330, 9002, 4829, 2499, 7449, 12903, 8505, 9877, 10373, 7080, 11564, 13288, 3704, 7781, 7115, 8284, 10342, 5432, 1358, 8828, 6880, 5502, 6257, 2103, 2699, 10387, 4347, 5808, 11626, 5681, 7398, 11029, 2061, 3432, 10367, 7118, 7382, 6013, 700, 11515, 10223, 2967, 1349, 4935, 6531, 789, 13866, 5713, 13325, 5876, 5099, 441, 1075, 10162, 6291, 10770, 1927, 7467, 5082, 4439, 7936, 8052, 5275, 12000, 10413, 3691, 4343, 5620, 8541, 7749, 13154, 2507, 3702, 6901, 507, 4312, 12386, 6925, 9934, 14359, 4487, 6664, 6195, 2193, 10641, 7807, 10062, 7687, 4546, 5781, 4295, 13512, 752, 1412, 13766, 9144, 4083, 8133, 9400, 2409, 7844, 10182, 9819, 1134, 6804, 5791, 3921, 5041, 5394, 9659, 8275, 4208, 6613, 7211, 11107, 13268, 12060, 7666, 12169, 5610, 4071, 2312, 3513, 4624, 4466, 3387, 4103, 10743, 6281, 4119, 8630, 11682, 13908, 11394, 8598, 9428, 6565, 9622, 5902, 2736, 1862, 5712, 10748, 6788, 12413, 2917, 5968, 407, 4151, 316, 1208, 2971, 10054, 9882, 6015, 11979, 4196, 14215, 6534, 3326, 11663, 5648, 13324, 8186, 13443, 1060, 13412, 8633, 2268, 3822, 4716, 836, 3179, 1521, 14302, 3215, 327, 2826, 10870, 1180, 11378, 13615, 13556, 6279, 10476, 13394, 10530, 5844, 577, 13677, 13197, 6336, 696, 720, 8547, 7428, 1360, 11269, 10538, 10019, 864, 14160, 1602, 5301, 11708, 1441, 6036, 6228, 8312, 1560, 10976, 778, 11275, 7149, 8503, 7402, 4285, 10382, 7733, 8840, 13734, 8808, 10408, 13352, 3978, 1469, 8968, 3338, 2472, 7361, 12182, 9491, 4389, 2893, 7462, 2540, 1040, 9222, 5510, 18, 3051, 1283, 5762, 4139, 4449, 5624, 1529, 11691, 6348, 2277, 8912, 491, 11474, 2977, 4838, 13482, 13722, 33, 1361, 5978, 11401, 11571, 6809, 8390, 10962, 7739, 5619, 6108, 7776, 14322, 393, 11336, 1094, 7394, 5875, 4164, 11379, 2142, 5348, 8070, 13397, 9252, 2807, 422, 5663, 1098, 10829, 7106, 13683, 5565, 5721, 12403, 13147, 872, 3011, 12527, 12870, 4055, 7456, 11932, 4497, 4746, 1555, 4983, 11968, 10434, 8966, 6005, 9194, 9286, 613, 5836, 1732, 1554, 13698, 1648, 4403, 13831, 6702, 13036, 12543, 14399, 7342, 8190, 11484, 5908, 10944, 4406, 3306, 14442, 2105, 4475, 8389, 4680, 10822, 8704, 3099, 4579, 8877, 5121, 11528, 8230, 8049, 13346, 1577, 7082, 6144, 9798, 2240, 12243, 7601, 8934, 10383, 319, 4388, 5697, 7975, 6712, 9371, 5453, 4162, 13970, 297, 2078, 6365, 3756, 11480, 5086, 9714, 801, 3852, 9487, 5071, 4441, 11491, 2089, 5029, 7908, 8664, 1777, 3076, 6908, 12880, 12819, 5016, 10167, 7273, 10744, 2533, 7589, 10377, 8985, 5803, 4213, 7693, 876, 7143, 4069, 10626, 11453, 10397, 1617, 13958, 2349, 6075, 3397, 1574, 10374, 3576, 5360, 8352, 6585, 7497, 11665, 1835, 2664, 13555, 10884, 626, 641, 4127, 7652, 5043, 12165, 3896, 6726, 13101, 5537, 4203, 9254, 5792, 8973, 926, 13105, 10761, 2757, 5126, 11725, 8826, 12896, 11039, 287, 10032, 10467, 7032, 7264, 10609, 1890, 2598, 1568, 9264, 4113, 4615, 2177, 7765, 1282, 7031, 12320, 4679, 6135, 5161, 1170, 5076, 1069, 9777, 11120, 4398, 521, 10736, 6604, 9386, 5545, 12785, 6232, 11328, 2615, 10014, 5694, 4545, 7377, 10522, 13269, 1408, 7346, 7704, 6172, 725, 9047, 176, 5746, 14270, 3768, 6326, 6278, 3531, 2121, 5893, 1990, 6522, 7091, 7848, 1444, 5238, 379, 12979, 11321, 12113, 8442, 6519, 7663, 4641, 12115, 1979, 7940, 13979, 11319, 9995, 4431, 9190, 5882, 4250, 11509, 9024, 10087, 10916, 4748, 294, 2604, 1868, 8308, 6681, 3824, 1849, 5857, 10049, 8200, 12711, 3784, 11393, 2738, 4432, 5070, 7280, 10596, 1712, 4423, 2274, 11937, 6324, 9338, 4469, 8544, 8109, 10856, 6543, 11225, 603, 6743, 8056, 7738, 8640, 8198, 7305, 2359, 111, 5397, 14233, 2578, 6822, 4092, 7866, 9565, 2100, 3254, 9014, 7340, 3323, 10605, 3392, 5935, 2773, 8890, 12073, 12277, 14094, 2018, 139, 3160, 1603, 9273, 13467, 10193, 6418, 12950, 13527, 11810, 4855, 8631, 6311, 8097, 2758, 3089, 6885, 3574, 6907, 2750, 6488, 7505, 2811, 13517, 7690, 2718, 161, 11672, 10416, 14023, 1127, 5496, 12980, 14125, 3676, 13500, 9265, 6823, 10372, 6891, 12996, 9584, 11670, 2911, 5734, 9280, 7700, 1857, 5131, 10591, 4235, 13181, 8843, 3929, 1701, 3853, 12765, 1564, 3990, 9754, 6483, 11284, 9392, 7748, 4342, 8920, 6888, 10812, 10376, 5611, 4563, 1911, 765, 981, 1000, 2679, 9050, 3233, 6099, 4939, 12493, 11019, 9430, 9179, 7371, 3846, 14437, 9504, 8418, 338, 2392, 9160, 12588, 14388, 4584, 8053, 6142, 5774, 1808, 11134, 8118, 2651, 11758, 11369, 3212, 2851, 10689, 6426, 11755, 834, 5806, 1999, 11065, 5305, 13276, 356, 12042, 9583, 9640, 11839, 6001, 9855, 2286, 4420, 4163, 6004, 3586, 2822, 5482, 1904, 14315, 7418, 4183, 13646, 2218, 1042, 6021, 7223, 975, 7045, 13414, 4053, 2782, 9824, 5220, 8732, 10965, 9198, 2308, 43, 6207, 4802, 10921, 5343, 6447, 2466, 3717, 3025, 11655, 13113, 12704, 6236, 7274, 7405, 7777, 14196, 5690, 10557, 13765, 9117, 694, 13469, 480, 1651, 2510, 11996, 11490, 6477, 12469, 8106, 5897, 12332, 12533, 10931, 5145, 10198, 3741, 9118, 6333, 7993, 6342, 12932, 5108, 3598, 10137, 11523, 3924, 6545, 7622, 7437, 9098, 5923, 7678, 6919, 10344, 6593, 1013, 6783, 1509, 2227, 14155, 5092, 12405, 13798, 9169, 1426, 4323, 7620, 6300, 6686, 3925, 9187, 269, 7012, 12463, 736, 5128, 2010, 6903, 5323, 4124, 13733, 4841, 5622, 11580, 7034, 7824, 1598, 10500, 12769, 5334, 10818, 7571, 9721, 12939, 1493, 8538, 11098, 3915, 6775, 10512, 14183, 6566, 2303, 4244, 7493, 10650, 3762, 7767, 3443, 12813, 13885, 8620, 4159, 3730, 11917, 8602, 11075, 7913, 9460, 12475, 5030, 8126, 11636, 10794, 13559, 12888, 6313, 14432, 7654, 8703, 14324, 4956, 8067, 14387, 9518, 11716, 10643, 4141, 7490, 12834, 10777, 12625, 5240, 4826, 14095, 5440, 7451, 1486, 1107, 12501, 5523, 9471, 4648, 4190, 1883, 3408, 11565, 3239, 7787, 7426, 10786, 3159, 14053, 13898, 14427, 9853, 3395, 11489, 9596, 3401, 9111, 5785, 5703, 12356, 7836, 3279, 7114, 2275, 8107, 1254, 4, 5414, 3888, 11964, 7037, 4259, 931, 8444, 3584, 10958, 10821, 5524, 11671, 2327, 942, 8604, 3826, 5924, 4654, 9467, 14379, 8970, 8956, 8628, 1466, 13400, 517, 708, 10967, 5811, 1520, 4337, 9965, 9204, 6486, 6391, 4477, 3874, 8090, 3115, 13310, 4646, 13655, 4574, 6344, 14151, 11255, 2290, 10291, 3314, 8427, 2698, 4105, 12256, 3307, 2108, 10073, 7679, 6587, 2481, 4970, 11230, 646, 6357, 6773, 320, 10197, 8315, 7539, 8281, 12908, 6623, 9136, 5350, 10688, 7090, 9586, 6103, 13199, 1844, 3922, 5889, 1246, 1779, 1369, 10172, 6753, 13449, 11532, 11773, 3568, 12658, 2656, 8072, 8149, 12467, 7595, 5684, 5410, 1393, 3410, 10554, 7147, 8714, 5838, 3366, 13418, 8535, 41, 7093, 1156, 1907, 4173, 1221, 13914, 11162, 8724, 1618, 10406, 7285, 12548, 5592, 5436, 10116, 7691, 8242, 7116, 687, 12733, 8469, 4023, 7983, 3737, 13711, 2599, 6221, 2191, 27, 148, 6475, 12786, 11901, 12918, 2556, 5853, 9588, 13223, 8181, 13871, 13610, 5487, 10475, 612, 7832, 4472, 7475, 11113, 5557, 6578, 4903, 7788, 4987, 4056, 5858, 3329, 6876, 307, 5002, 1437, 4049, 2060, 9954, 131, 8439, 1846, 11076, 1923, 8498, 3680, 1915]\n",
      "[6875, 12761, 873, 7951, 3945, 8819, 419, 5329, 8589, 10395, 208, 9355, 13936, 9859, 9233, 3722, 10384, 12612, 5246, 9328, 5700, 253, 13257, 11087, 7221, 8204, 1417, 8296, 8153, 8016, 8625, 3512, 10304, 10350, 14420, 10354, 2261, 1312, 10199, 1225, 826, 7343, 13506, 9648, 10964, 5056, 7495, 8691, 1774, 13614, 10991, 7481, 3877, 3721, 8466, 6962, 4658, 14199, 6932, 12767, 12519, 12713, 2649, 3871, 8570, 7559, 7021, 8331, 7010, 11756, 6937, 7009, 3550, 6672, 6791, 11018, 2415, 6690, 5429, 10324, 9263, 13084, 3083, 1908, 2733, 1078, 8694, 14102, 13578, 11722, 6777, 3524, 8321, 3906, 7609, 8225, 6436, 8506, 6318, 3013, 13673, 889, 8162, 10143, 7864, 7336, 2876, 4114, 6831, 5918, 10003, 4201, 8994, 7003, 6666, 2525, 6660, 10439, 1728, 5632, 987, 4273, 3406, 11694, 1652, 10025, 7141, 795, 6164, 14271, 2798, 14002, 6358, 4251, 4565, 937, 4505, 4231, 14143, 4219, 5587, 2689, 7295, 9970, 10955, 5214, 7616, 10499, 4038, 7248, 3299, 6185, 3666, 8824, 9071, 805, 990, 6145, 5531, 7561, 3117, 5823, 10317, 6321, 3773, 8945, 10117, 11196, 1824, 1477, 11721, 3559, 11642, 12842, 14334, 1882, 8440, 8635, 940, 2641, 2144, 12866, 5805, 13561, 1634, 3649, 7861, 5915, 12175, 3639, 13567, 7089, 8158, 6078, 13724, 6014, 13183, 4791, 9686, 13093, 1029, 5892, 4280, 12619, 317, 6152, 10246, 7540, 8085, 8800, 4691, 11701, 11680, 4465, 10861, 2759, 11891, 7808, 7721, 11498, 12862, 4102, 7239, 13434, 7634, 10402, 3515, 4458, 5484, 3823, 3981, 4282, 10895, 9249, 12981, 8521, 4771, 7344, 9817, 1390, 4507, 13295, 6058, 6530, 9989, 6157, 89, 11668, 3618, 14229, 2642, 3767, 1873, 2449, 8668, 13377, 6737, 6693, 9751, 4492, 8786, 12634, 10716, 7569, 11050, 1881, 4517, 11165, 986, 3697, 8923, 11793, 10779, 2428, 6594, 5514, 8143, 12616, 10430, 271, 13550, 12800, 13751, 7214, 9325, 11863, 2602, 4773, 8110, 11344, 2011, 8285, 9195, 8595, 9120, 8501, 5522, 4724, 9775, 948, 13387, 4940, 10335, 13920, 13838, 8564, 8572, 6852, 7971, 11947, 3264, 4128, 10636, 4014, 3631, 10343, 11631, 12301, 5666, 10926, 12848, 249, 8869, 13691, 3319, 4852, 6017, 7356, 9905, 2245, 11419, 4493, 3183, 8765, 13081, 8004, 7958, 11310, 8486, 1545, 9042, 4412, 3670, 10124, 9443, 1573, 11640, 12406, 1930, 9107, 8936, 14056, 3110, 8121, 13360, 10421, 6976, 5489, 6027, 8720, 8306, 8723, 3193, 5172, 12288, 12003, 7818, 5468, 3595, 14426, 8932, 1088, 2752, 11545, 6340, 1047, 8865, 6759, 8504, 13729, 8726, 6778, 11619, 5714, 13984, 10312, 8761, 6149, 4028, 375, 632, 3836, 7937, 10084, 7439, 5566, 8889, 8815, 3736, 4623, 582, 2628, 12669, 741, 10556, 4548, 6290, 5045, 2035, 11, 12970, 11833, 7550, 3463, 10264, 2508, 875, 13809, 416, 10992, 2021, 10552, 9446, 14404, 827, 3718, 4755, 10684, 685, 780, 5860, 8136, 3502, 6886, 1744, 6658, 12799, 679, 12036, 7893, 1825, 11451, 2287, 8334, 11470, 6026, 1504, 8711, 11536, 1940, 8725, 3733, 12156, 13657, 2777, 1261, 8636, 1082, 10111, 258, 11772, 10107, 1250, 4072, 5597, 12324, 7299, 12610, 3383, 1635, 11558, 6508, 2085, 12607, 12082, 2999, 3213, 6855, 14050, 1268, 2526, 11367, 5877, 6715, 5802, 7250, 2444, 7731, 2726, 344, 5168, 2179, 2211, 10908, 6155, 10487, 4003, 10510, 13861, 9978, 11427, 7793, 13176, 8605, 12971, 8792, 5227, 10678, 6497, 2957, 6586, 7573, 11198, 1601, 12008, 2714, 12296, 9866, 4434, 3263, 5373, 4785, 6728, 10102, 483, 4828, 3976, 1513, 9381, 10347, 7433, 10750, 5461, 6266, 4590, 12546, 2906, 8135, 11681, 10633, 3111, 3803, 7923, 10574, 2412, 6873, 4622, 9291, 7293, 12461, 6229, 3860, 11370, 9018, 6949, 2767, 7813, 12095, 8180, 5418, 11733, 6735, 4576, 4176, 5166, 1937, 13445, 5068, 2983, 6733, 13593, 11593, 7565, 4825, 1494, 3539, 7241, 5559, 6471, 5080, 9152, 10253, 6989, 2632, 13560, 2888, 12525, 8195, 8041, 2866, 13287, 8616, 7095, 10811, 13570, 3974, 3770, 8780, 3046, 3525, 9058, 9488, 11529, 11279, 3028, 12494, 13103, 14272, 11706, 2052, 5687, 11635, 14104, 4995, 7552, 4758, 11588, 14301, 9982, 13581, 8313, 4288, 573, 5561, 6614, 6507, 91, 136, 7244, 8560, 8988, 7074, 4556, 7408, 10610, 12572, 8567, 1363, 10850, 7939, 7625, 7522, 10190, 12454, 993, 1280, 6370, 6093, 11643, 3597, 13847, 9303, 10216, 519, 14197, 165, 14441, 11071, 7153, 892, 6950, 4109, 2883, 5863, 2283, 5815, 12879, 973, 7950, 4777, 398, 2190, 12201, 469, 9138, 10993, 8989, 7350, 11729, 6505, 14027, 6360, 11340, 4147, 10157, 11546, 10978, 3119, 13780, 2588, 5096, 11659, 7734, 4543, 3095, 5349, 12940, 6807, 8750, 11015, 13837, 5724, 7314, 14374, 5326, 8159, 11007, 12863, 10683, 10710, 9737, 7310, 2859, 11122, 5073, 6167, 5192, 5946, 8702, 3310, 10396, 8208, 13834, 729, 13644, 12351, 1694, 5495, 2023, 6810, 6066, 12045, 4880, 6118, 549, 13138, 1168, 1022, 14205, 2342, 3315, 6931, 8775, 10469, 4573, 14128, 1739, 6375, 710, 11657, 6828, 13112, 5913, 12699, 5299, 6024, 401, 11157, 6639, 8117, 2313, 11600, 4845, 8319, 9944, 12420, 4375, 2257, 11265, 185, 12316, 8982, 8713, 14068, 904, 3169, 6940, 4762, 11627, 8937, 7709, 4217, 4997, 8055, 10888, 13027, 12927, 10819, 10746, 3600, 3237, 9923, 2946, 7843, 4229, 9717, 8381, 4104, 6761, 4693, 10564, 2049, 11608, 8398, 8348, 5224, 5049, 12315, 1318, 5233, 7434, 0, 5541, 13912, 7944, 12052, 960, 3698, 13430, 7333, 6055, 1583, 7544, 5481, 6061, 7835, 10583, 9856, 5170, 11077, 4448, 4911, 10329, 7199, 1166, 3282, 4902, 5540, 10515, 5048, 11798, 6745, 2028, 2693, 14342, 11389, 13108, 2856, 13776, 2645, 7103, 7453, 7551, 9578, 9693, 9154, 7610, 14144, 5411, 12142, 1049, 13982, 6493, 1706, 649, 3980, 2703, 8468, 8459, 6430, 7353, 7594, 289, 4900, 1523, 4732, 6382, 6652, 5151, 5972, 11150, 4165, 4276, 4752, 4501, 8286, 5385, 9079, 9027, 11115, 5153, 7841, 7452, 4245, 9719, 1462, 3055, 8022, 7917, 12174, 266, 3558, 4814, 5251, 652, 3675, 11351, 6956, 4319, 4568, 6003, 3940, 13927, 1388, 4041, 5494, 4959, 8393, 14188, 13695, 2203, 9833, 4533, 8384, 260, 5669, 13168, 10332, 6835, 5483, 13157, 2051, 5813, 8032, 1685, 13272, 11999, 7256, 8303, 6158, 12498, 10791, 2878, 2743, 5783, 4759, 12010, 10996, 11405, 10021, 11001, 4218, 7597, 5446, 5658, 4661, 684, 11431, 7175, 5036, 5228, 7480, 10654, 2629, 12689, 2167, 13143, 5799, 5950, 11262, 3085, 3692, 11180, 13484, 4353, 142, 7772, 73, 6730, 4735, 13209, 3363, 1386, 10754, 14307, 10273, 9730, 8120, 664, 2076, 3679, 5833, 10778, 1433, 5259, 809, 2094, 7324, 11618, 9165, 951, 4187, 1330, 7435, 4086, 6827, 1548, 4246, 10616, 2652, 4146, 6169, 4585, 1524, 4019, 6390, 4311, 4973, 3246, 9205, 965, 12743, 1672, 7178, 6675, 6474, 7347, 10983, 9269, 650, 12600, 3521, 11391, 10519, 6936, 8339, 10296, 9685, 11700, 12511, 3120, 5054, 2048, 293, 12471, 774, 10945, 785, 1934, 8697, 10520, 7514, 11884, 5115, 8872, 3275, 6460, 13909, 7298, 5289, 3317, 998, 11398, 11414, 6964, 1052, 984, 1730, 8048, 2497, 8495, 11317, 7100, 1017, 11893, 5795, 3952, 1976, 12674, 4633, 14298, 11410, 11587, 1893, 8752, 3167, 11969, 3819, 105, 4700, 802, 14115, 6766, 11963, 5069, 5901, 10303, 1629, 10659, 2057, 2102, 12421, 13107, 6850, 7696, 10444, 6091, 1680, 5293, 7527, 11579, 11718, 201, 1811, 510, 6525, 8371, 13042, 2914, 10353, 9645, 6267, 5766, 8578, 2562, 3837, 1997, 2744, 4395, 7959, 6837, 6301, 354, 13416, 11411, 1682, 3747, 3081, 5409, 13348, 12251, 7234, 5782, 10758, 14362, 12637, 10662, 11038, 1124, 11550, 5884, 4039, 13226, 189, 788, 6041, 7895, 1547, 4333, 12828, 9494, 10695, 3045, 6276, 10598, 1172, 1142, 2437, 983, 604, 236, 12204, 7120, 10940, 4731, 9573, 14281, 492, 314, 10002, 13354, 9368, 3321, 13874, 1072, 3426, 6995, 9377, 10723, 4035, 1689, 4413, 8733, 2590, 1421, 5231, 11705, 8593, 4847, 1798, 10824, 12753, 1987, 1581, 3614, 2570, 12854, 4529, 191, 8949, 243, 11458, 12137, 5320, 9723, 11195, 12081, 2885, 3007, 12991, 121, 3794, 9270, 5796, 9399, 5421, 4305, 13217, 13253, 2586, 3624, 3811, 7916, 11560, 13431, 9541, 4476, 4737, 3078, 8617, 3630, 1472, 2387, 2390, 10840, 2474, 10799, 10646, 5331, 3688, 12892, 8151, 5118, 10295, 10486, 4765, 7278, 10438, 11441, 4045, 7988, 4115, 2634, 2214, 4335, 10453, 6265, 6504, 3580, 2288, 704, 554, 6282, 1337, 10316, 1333, 726, 11686, 11960, 6043, 7771, 5341, 7829, 3118, 10854, 4048, 449, 8684, 3946, 8921, 674, 4296, 11397, 7004, 14377, 4625, 10272, 9615, 9649, 6768, 5024, 5386, 13902, 4635, 5960, 5292, 5375, 9605, 3678, 12236, 2464, 11895, 5022, 11164, 3104, 4192, 6463, 6567, 11533, 6541, 3528, 2234, 6779, 9527, 10649, 8646, 3552, 12020, 8084, 14067, 8699, 13395, 3350, 13627, 3402, 4596, 3296, 10292, 9250, 13191, 11905, 11123, 10128, 7504, 10526, 9505, 5671, 5005, 432, 1556, 14443, 7580, 3603, 6595, 8745, 6302, 2892, 11436, 12442, 1989, 4297, 5605, 3136, 2697, 7059, 1416, 11653, 9440, 14121, 7752, 11516, 2215, 5466, 13868, 1223, 5769, 14309, 9197, 12788, 3384, 13454, 12782, 4184, 8747, 13012, 2276, 5417, 13991, 1738, 4909, 9671, 6137, 7681, 7512, 5469, 4611, 8722, 11332, 9672, 7513, 11589, 4874, 10677, 8569, 6859, 2239, 2753, 762, 10170, 2184, 12858, 9156, 6618, 6845, 4079, 2978, 5456, 3218, 7270, 3647, 4438, 2005, 2156, 13198, 1955, 7960, 2577, 6339, 14033, 4044, 2189, 11456, 1572, 11125, 13478, 6206, 8220, 2862, 3753, 1644, 12801, 976, 6461, 1215, 3881, 10876, 6147, 8814, 7419, 6148, 10703, 5608, 2630, 5719, 9241, 9251, 4278, 5302, 8883, 7629, 4895, 8983, 8112, 1372, 9333, 7636, 8667, 14225, 9056, 2515, 13553, 9789, 4270, 12964, 8350, 12122, 12975, 6669, 2815, 7092, 4570, 9093, 2667, 8577, 2832, 2921, 6636, 11073, 8291, 12536, 1389, 2039, 4663, 11291, 4521, 2136, 8202, 1476, 11496, 4821, 9600, 8166, 12313, 10203, 8207, 2616, 8909, 1383, 10280, 2958, 6222, 7651, 8563, 4508, 7757, 10123, 8425, 4831, 2538, 8123, 1200, 6165, 5930, 10775, 6857, 3657, 6978, 9466, 10053, 7964, 6431, 13356, 619, 8899, 7262, 3287, 195, 13723, 12030, 865, 8325]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "train_indices = labeled_indices[:num_train]\n",
    "test_indices = labeled_indices[num_train:num_train+num_test]\n",
    "print(train_indices)\n",
    "print(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39811420-bdfb-41c8-bd76-f46ce33b94cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature759</th>\n",
       "      <th>feature760</th>\n",
       "      <th>feature761</th>\n",
       "      <th>feature762</th>\n",
       "      <th>feature763</th>\n",
       "      <th>feature764</th>\n",
       "      <th>feature765</th>\n",
       "      <th>feature766</th>\n",
       "      <th>feature767</th>\n",
       "      <th>feature768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FES</td>\n",
       "      <td>0.339602</td>\n",
       "      <td>-0.030744</td>\n",
       "      <td>-0.901381</td>\n",
       "      <td>0.100888</td>\n",
       "      <td>0.886443</td>\n",
       "      <td>0.383596</td>\n",
       "      <td>-0.192082</td>\n",
       "      <td>-0.032063</td>\n",
       "      <td>-0.154869</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.549204</td>\n",
       "      <td>-0.856123</td>\n",
       "      <td>0.714672</td>\n",
       "      <td>-0.046649</td>\n",
       "      <td>-0.894424</td>\n",
       "      <td>-0.001815</td>\n",
       "      <td>0.739485</td>\n",
       "      <td>0.015581</td>\n",
       "      <td>-0.023863</td>\n",
       "      <td>-0.022002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HADHA</td>\n",
       "      <td>-0.131799</td>\n",
       "      <td>-0.025745</td>\n",
       "      <td>-0.677301</td>\n",
       "      <td>-0.053545</td>\n",
       "      <td>0.971046</td>\n",
       "      <td>0.180315</td>\n",
       "      <td>-0.028189</td>\n",
       "      <td>-0.077389</td>\n",
       "      <td>-0.095152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.927885</td>\n",
       "      <td>-0.817812</td>\n",
       "      <td>0.809631</td>\n",
       "      <td>-0.005827</td>\n",
       "      <td>-0.848839</td>\n",
       "      <td>0.024516</td>\n",
       "      <td>0.526404</td>\n",
       "      <td>-0.039926</td>\n",
       "      <td>-0.102787</td>\n",
       "      <td>-0.026980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLC7A7</td>\n",
       "      <td>0.385693</td>\n",
       "      <td>-0.070692</td>\n",
       "      <td>-0.847796</td>\n",
       "      <td>-0.022054</td>\n",
       "      <td>0.959772</td>\n",
       "      <td>0.085487</td>\n",
       "      <td>0.076455</td>\n",
       "      <td>-0.003006</td>\n",
       "      <td>-0.032268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.941094</td>\n",
       "      <td>-0.912443</td>\n",
       "      <td>0.789828</td>\n",
       "      <td>0.046979</td>\n",
       "      <td>-0.715636</td>\n",
       "      <td>0.085842</td>\n",
       "      <td>0.150494</td>\n",
       "      <td>0.025392</td>\n",
       "      <td>-0.066035</td>\n",
       "      <td>-0.028283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LCK</td>\n",
       "      <td>0.650428</td>\n",
       "      <td>0.014479</td>\n",
       "      <td>-0.866163</td>\n",
       "      <td>0.053508</td>\n",
       "      <td>0.951529</td>\n",
       "      <td>0.269402</td>\n",
       "      <td>-0.214788</td>\n",
       "      <td>0.045179</td>\n",
       "      <td>-0.506429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.576739</td>\n",
       "      <td>-0.969558</td>\n",
       "      <td>0.916549</td>\n",
       "      <td>-0.080332</td>\n",
       "      <td>-0.927649</td>\n",
       "      <td>-0.047398</td>\n",
       "      <td>0.741663</td>\n",
       "      <td>-0.000096</td>\n",
       "      <td>-0.096318</td>\n",
       "      <td>-0.056501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HSPA2</td>\n",
       "      <td>0.322262</td>\n",
       "      <td>0.017484</td>\n",
       "      <td>-0.849302</td>\n",
       "      <td>0.046401</td>\n",
       "      <td>0.920429</td>\n",
       "      <td>0.463832</td>\n",
       "      <td>-0.050414</td>\n",
       "      <td>-0.033398</td>\n",
       "      <td>0.387791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387301</td>\n",
       "      <td>-0.860696</td>\n",
       "      <td>0.678607</td>\n",
       "      <td>-0.060695</td>\n",
       "      <td>-0.945793</td>\n",
       "      <td>0.040472</td>\n",
       "      <td>0.831079</td>\n",
       "      <td>-0.001711</td>\n",
       "      <td>-0.079842</td>\n",
       "      <td>-0.011189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62040</th>\n",
       "      <td>GO:2001313</td>\n",
       "      <td>0.174428</td>\n",
       "      <td>0.194728</td>\n",
       "      <td>-0.284376</td>\n",
       "      <td>0.282102</td>\n",
       "      <td>-0.713190</td>\n",
       "      <td>-0.272055</td>\n",
       "      <td>0.121190</td>\n",
       "      <td>0.129901</td>\n",
       "      <td>-0.983496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500545</td>\n",
       "      <td>0.429651</td>\n",
       "      <td>-0.292929</td>\n",
       "      <td>-0.464941</td>\n",
       "      <td>-0.740187</td>\n",
       "      <td>0.179149</td>\n",
       "      <td>-0.960807</td>\n",
       "      <td>-0.746958</td>\n",
       "      <td>1.069112</td>\n",
       "      <td>-0.848182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62041</th>\n",
       "      <td>GO:2001314</td>\n",
       "      <td>0.025886</td>\n",
       "      <td>0.306214</td>\n",
       "      <td>-0.254303</td>\n",
       "      <td>0.253673</td>\n",
       "      <td>-0.533680</td>\n",
       "      <td>-0.269355</td>\n",
       "      <td>0.150939</td>\n",
       "      <td>-0.229323</td>\n",
       "      <td>-1.078991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042979</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>-0.356661</td>\n",
       "      <td>-0.381828</td>\n",
       "      <td>-0.638338</td>\n",
       "      <td>0.077176</td>\n",
       "      <td>-0.788312</td>\n",
       "      <td>-0.683442</td>\n",
       "      <td>1.087031</td>\n",
       "      <td>-0.593092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62042</th>\n",
       "      <td>GO:2001315</td>\n",
       "      <td>0.027134</td>\n",
       "      <td>0.241391</td>\n",
       "      <td>-0.227353</td>\n",
       "      <td>0.317366</td>\n",
       "      <td>-0.726657</td>\n",
       "      <td>-0.197968</td>\n",
       "      <td>0.045653</td>\n",
       "      <td>0.038912</td>\n",
       "      <td>-0.954113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349853</td>\n",
       "      <td>0.370059</td>\n",
       "      <td>-0.144606</td>\n",
       "      <td>-0.493184</td>\n",
       "      <td>-0.655063</td>\n",
       "      <td>0.217335</td>\n",
       "      <td>-0.841272</td>\n",
       "      <td>-0.821077</td>\n",
       "      <td>1.036363</td>\n",
       "      <td>-0.836614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62043</th>\n",
       "      <td>GO:2001316</td>\n",
       "      <td>0.139543</td>\n",
       "      <td>0.028883</td>\n",
       "      <td>0.899480</td>\n",
       "      <td>0.152932</td>\n",
       "      <td>0.576852</td>\n",
       "      <td>0.330342</td>\n",
       "      <td>0.916943</td>\n",
       "      <td>0.012306</td>\n",
       "      <td>-0.020316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.354748</td>\n",
       "      <td>-0.083168</td>\n",
       "      <td>0.043640</td>\n",
       "      <td>-0.663565</td>\n",
       "      <td>0.543016</td>\n",
       "      <td>-0.652230</td>\n",
       "      <td>-1.427882</td>\n",
       "      <td>-0.985257</td>\n",
       "      <td>1.673561</td>\n",
       "      <td>0.109659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62044</th>\n",
       "      <td>GO:2001317</td>\n",
       "      <td>0.083064</td>\n",
       "      <td>0.090899</td>\n",
       "      <td>0.888541</td>\n",
       "      <td>0.309920</td>\n",
       "      <td>0.403966</td>\n",
       "      <td>0.202783</td>\n",
       "      <td>0.706517</td>\n",
       "      <td>-0.017584</td>\n",
       "      <td>-0.171057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.544680</td>\n",
       "      <td>-0.046654</td>\n",
       "      <td>0.262865</td>\n",
       "      <td>-0.767305</td>\n",
       "      <td>0.753788</td>\n",
       "      <td>-0.577503</td>\n",
       "      <td>-1.194910</td>\n",
       "      <td>-0.799556</td>\n",
       "      <td>1.519368</td>\n",
       "      <td>0.263210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62045 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          protein  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
       "0             FES  0.339602 -0.030744 -0.901381  0.100888  0.886443  0.383596   \n",
       "1          HADHA  -0.131799 -0.025745 -0.677301 -0.053545  0.971046  0.180315   \n",
       "2          SLC7A7  0.385693 -0.070692 -0.847796 -0.022054  0.959772  0.085487   \n",
       "3            LCK   0.650428  0.014479 -0.866163  0.053508  0.951529  0.269402   \n",
       "4           HSPA2  0.322262  0.017484 -0.849302  0.046401  0.920429  0.463832   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "62040  GO:2001313  0.174428  0.194728 -0.284376  0.282102 -0.713190 -0.272055   \n",
       "62041  GO:2001314  0.025886  0.306214 -0.254303  0.253673 -0.533680 -0.269355   \n",
       "62042  GO:2001315  0.027134  0.241391 -0.227353  0.317366 -0.726657 -0.197968   \n",
       "62043  GO:2001316  0.139543  0.028883  0.899480  0.152932  0.576852  0.330342   \n",
       "62044  GO:2001317  0.083064  0.090899  0.888541  0.309920  0.403966  0.202783   \n",
       "\n",
       "       feature7  feature8  feature9  ...  feature759  feature760  feature761  \\\n",
       "0     -0.192082 -0.032063 -0.154869  ...   -0.549204   -0.856123    0.714672   \n",
       "1     -0.028189 -0.077389 -0.095152  ...    0.927885   -0.817812    0.809631   \n",
       "2      0.076455 -0.003006 -0.032268  ...    0.941094   -0.912443    0.789828   \n",
       "3     -0.214788  0.045179 -0.506429  ...   -0.576739   -0.969558    0.916549   \n",
       "4     -0.050414 -0.033398  0.387791  ...    0.387301   -0.860696    0.678607   \n",
       "...         ...       ...       ...  ...         ...         ...         ...   \n",
       "62040  0.121190  0.129901 -0.983496  ...    0.500545    0.429651   -0.292929   \n",
       "62041  0.150939 -0.229323 -1.078991  ...    0.042979    0.134560   -0.356661   \n",
       "62042  0.045653  0.038912 -0.954113  ...    0.349853    0.370059   -0.144606   \n",
       "62043  0.916943  0.012306 -0.020316  ...   -0.354748   -0.083168    0.043640   \n",
       "62044  0.706517 -0.017584 -0.171057  ...   -0.544680   -0.046654    0.262865   \n",
       "\n",
       "       feature762  feature763  feature764  feature765  feature766  feature767  \\\n",
       "0       -0.046649   -0.894424   -0.001815    0.739485    0.015581   -0.023863   \n",
       "1       -0.005827   -0.848839    0.024516    0.526404   -0.039926   -0.102787   \n",
       "2        0.046979   -0.715636    0.085842    0.150494    0.025392   -0.066035   \n",
       "3       -0.080332   -0.927649   -0.047398    0.741663   -0.000096   -0.096318   \n",
       "4       -0.060695   -0.945793    0.040472    0.831079   -0.001711   -0.079842   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "62040   -0.464941   -0.740187    0.179149   -0.960807   -0.746958    1.069112   \n",
       "62041   -0.381828   -0.638338    0.077176   -0.788312   -0.683442    1.087031   \n",
       "62042   -0.493184   -0.655063    0.217335   -0.841272   -0.821077    1.036363   \n",
       "62043   -0.663565    0.543016   -0.652230   -1.427882   -0.985257    1.673561   \n",
       "62044   -0.767305    0.753788   -0.577503   -1.194910   -0.799556    1.519368   \n",
       "\n",
       "       feature768  \n",
       "0       -0.022002  \n",
       "1       -0.026980  \n",
       "2       -0.028283  \n",
       "3       -0.056501  \n",
       "4       -0.011189  \n",
       "...           ...  \n",
       "62040   -0.848182  \n",
       "62041   -0.593092  \n",
       "62042   -0.836614  \n",
       "62043    0.109659  \n",
       "62044    0.263210  \n",
       "\n",
       "[62045 rows x 769 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2dcfc1e5-6826-4b8d-97b9-6daba04b77d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62045"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aeaefa35-3881-431e-a529-8db8b1b322ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62045\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 100\n",
    "count = 1 / num_iterations\n",
    "\n",
    "# 总节点数\n",
    "num_nodes = data.x.shape[0]\n",
    "# 所有节点的索引\n",
    "all_indices = np.arange(num_nodes)\n",
    "mask_out = torch.ones(num_nodes, dtype=torch.bool)\n",
    "# 将测试集索引处的掩码设为False\n",
    "#mask_out[test_indices] = False\n",
    "end = len(combined_features) #- 47595\n",
    "#mask_out[-47595:] = False\n",
    "mask_out[0:end] = False\n",
    "# 使用掩码获取剩余的索引\n",
    "remaining_indices = all_indices[mask_out]\n",
    "complement_mask = ~mask_out\n",
    "#complement_mask[test_indices] = False\n",
    "\n",
    "# 得到既不在 remaining_indices 也不在 test_indices 中的索引\n",
    "complement_indices = all_indices[complement_mask]\n",
    "print(len(complement_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3885a81-229a-4ecb-bce5-2e10e313f0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: 62045\n",
      "s: 0\n",
      "c: 61425\n",
      "s: 620\n",
      "c: 60805\n",
      "s: 620\n",
      "c: 60185\n",
      "s: 620\n",
      "c: 59565\n",
      "s: 620\n",
      "c: 58945\n",
      "s: 620\n",
      "c: 58325\n",
      "s: 620\n",
      "c: 57705\n",
      "s: 620\n",
      "c: 57085\n",
      "s: 620\n",
      "c: 56465\n",
      "s: 620\n",
      "c: 55845\n",
      "s: 620\n",
      "c: 55225\n",
      "s: 620\n",
      "c: 54605\n",
      "s: 620\n",
      "c: 53985\n",
      "s: 620\n",
      "c: 53365\n",
      "s: 620\n",
      "c: 52745\n",
      "s: 620\n",
      "c: 52125\n",
      "s: 620\n",
      "c: 51505\n",
      "s: 620\n",
      "c: 50885\n",
      "s: 620\n",
      "c: 50265\n",
      "s: 620\n",
      "c: 49645\n",
      "s: 620\n",
      "c: 49025\n",
      "s: 620\n",
      "c: 48405\n",
      "s: 620\n",
      "c: 47785\n",
      "s: 620\n",
      "c: 47165\n",
      "s: 620\n",
      "c: 46545\n",
      "s: 620\n",
      "c: 45925\n",
      "s: 620\n",
      "c: 45305\n",
      "s: 620\n",
      "c: 44685\n",
      "s: 620\n",
      "c: 44065\n",
      "s: 620\n",
      "c: 43445\n",
      "s: 620\n",
      "c: 42825\n",
      "s: 620\n",
      "c: 42205\n",
      "s: 620\n",
      "c: 41585\n",
      "s: 620\n",
      "c: 40965\n",
      "s: 620\n",
      "c: 40345\n",
      "s: 620\n",
      "c: 39725\n",
      "s: 620\n",
      "c: 39105\n",
      "s: 620\n",
      "c: 38485\n",
      "s: 620\n",
      "c: 37865\n",
      "s: 620\n",
      "c: 37245\n",
      "s: 620\n",
      "c: 36625\n",
      "s: 620\n",
      "c: 36005\n",
      "s: 620\n",
      "c: 35385\n",
      "s: 620\n",
      "c: 34765\n",
      "s: 620\n",
      "c: 34145\n",
      "s: 620\n",
      "c: 33525\n",
      "s: 620\n",
      "c: 32905\n",
      "s: 620\n",
      "c: 32285\n",
      "s: 620\n",
      "c: 31665\n",
      "s: 620\n",
      "c: 31045\n",
      "s: 620\n",
      "c: 30425\n",
      "s: 620\n",
      "c: 29805\n",
      "s: 620\n",
      "c: 29185\n",
      "s: 620\n",
      "c: 28565\n",
      "s: 620\n",
      "c: 27945\n",
      "s: 620\n",
      "c: 27325\n",
      "s: 620\n",
      "c: 26705\n",
      "s: 620\n",
      "c: 26085\n",
      "s: 620\n",
      "c: 25465\n",
      "s: 620\n",
      "c: 24845\n",
      "s: 620\n",
      "c: 24225\n",
      "s: 620\n",
      "c: 23605\n",
      "s: 620\n",
      "c: 22985\n",
      "s: 620\n",
      "c: 22365\n",
      "s: 620\n",
      "c: 21745\n",
      "s: 620\n",
      "c: 21125\n",
      "s: 620\n",
      "c: 20505\n",
      "s: 620\n",
      "c: 19885\n",
      "s: 620\n",
      "c: 19265\n",
      "s: 620\n",
      "c: 18645\n",
      "s: 620\n",
      "c: 18025\n",
      "s: 620\n",
      "c: 17405\n",
      "s: 620\n",
      "c: 16785\n",
      "s: 620\n",
      "c: 16165\n",
      "s: 620\n",
      "c: 15545\n",
      "s: 620\n",
      "c: 14925\n",
      "s: 620\n",
      "c: 14305\n",
      "s: 620\n",
      "c: 13685\n",
      "s: 620\n",
      "c: 13065\n",
      "s: 620\n",
      "c: 12445\n",
      "s: 620\n",
      "c: 11825\n",
      "s: 620\n",
      "c: 11205\n",
      "s: 620\n",
      "c: 10585\n",
      "s: 620\n",
      "c: 9965\n",
      "s: 620\n",
      "c: 9345\n",
      "s: 620\n",
      "c: 8725\n",
      "s: 620\n",
      "c: 8105\n",
      "s: 620\n",
      "c: 7485\n",
      "s: 620\n",
      "c: 6865\n",
      "s: 620\n",
      "c: 6245\n",
      "s: 620\n",
      "c: 5625\n",
      "s: 620\n",
      "c: 5005\n",
      "s: 620\n",
      "c: 4385\n",
      "s: 620\n",
      "c: 3765\n",
      "s: 620\n",
      "c: 3145\n",
      "s: 620\n",
      "c: 2525\n",
      "s: 620\n",
      "c: 1905\n",
      "s: 620\n",
      "c: 1285\n",
      "s: 620\n",
      "c: 665\n",
      "s: 620\n",
      "c: 45\n",
      "s: 620\n",
      "所有迭代的selected_indices和remaining_indices已保存到文件。\n"
     ]
    }
   ],
   "source": [
    "num_nodes_out = len(complement_indices)\n",
    "num_to_select = int(num_nodes_out * count)\n",
    "for i in range(num_iterations+1):\n",
    "    if i > 0:\n",
    "    # 随机选择节点\n",
    "        selected_indices = np.random.choice(complement_indices, num_to_select, replace=False)\n",
    "        # 更新剩余节点列表\n",
    "        complement_indices = np.setdiff1d(complement_indices, selected_indices)\n",
    "    else:\n",
    "        selected_indices = np.random.choice(complement_indices, 0, replace=False)\n",
    "        complement_indices = np.setdiff1d(complement_indices, selected_indices)\n",
    "    print(\"c:\",len(complement_indices))\n",
    "    # 保存到文件\n",
    "    print(\"s:\",len(selected_indices))\n",
    "    np.save(f'DIVIDED_DATA/GOwith_{i}%.npy', complement_indices)  # 修改保存路径\n",
    "\n",
    "print(\"所有迭代的selected_indices和remaining_indices已保存到文件。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b602171-d1ee-43ea-a782-b73096707afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"早停机制，用于在验证损失停止改善时终止训练。\"\"\"\n",
    "    def __init__(self, patience=200, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            patience (int): 损失没有改善的迭代次数，在这之后训练将会被停止。\n",
    "            verbose (bool): 如果为True，则打印一条消息表明早停被触发。\n",
    "            delta (float): 损失的最小改变，被认为是改善。\n",
    "            path (str): 最佳模型保存路径。\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    print(\"Early stopping triggered\")\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "    \n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        '''保存模型当验证损失减少时'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9178a332-2a41-4e7c-9374-21a94e64d23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_indices = np.load(f'DIVIDED_DATA/GOwith_0%.npy')\n",
    "    \n",
    "    # 根据selected_indices和remaining_indices调整特征\n",
    "masked_features = features\n",
    "masked_features[torch.tensor(selected_indices)] = 0\n",
    "masked_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "410ad8aa-2de5-472e-83c8-cd72f4b840b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 1.1129, Macro_F1: 0.1140, AUC_score: 0.5000\n",
      "Validation loss decreased (1.112925 --> 1.112925).\n",
      "Validation loss decreased (1.111366 --> 1.111366).\n",
      "Validation loss decreased (1.109829 --> 1.109829).\n",
      "Validation loss decreased (1.108318 --> 1.108318).\n",
      "Validation loss decreased (1.106830 --> 1.106830).\n",
      "Validation loss decreased (1.105366 --> 1.105366).\n",
      "Validation loss decreased (1.103927 --> 1.103927).\n",
      "Validation loss decreased (1.102512 --> 1.102512).\n",
      "Validation loss decreased (1.101122 --> 1.101122).\n",
      "Validation loss decreased (1.099758 --> 1.099758).\n",
      "Validation loss decreased (1.098420 --> 1.098420).\n",
      "Validation loss decreased (1.097107 --> 1.097107).\n",
      "Validation loss decreased (1.095820 --> 1.095820).\n",
      "Validation loss decreased (1.094558 --> 1.094558).\n",
      "Validation loss decreased (1.093320 --> 1.093320).\n",
      "Validation loss decreased (1.092113 --> 1.092113).\n",
      "Validation loss decreased (1.090926 --> 1.090926).\n",
      "Validation loss decreased (1.089764 --> 1.089764).\n",
      "Validation loss decreased (1.088631 --> 1.088631).\n",
      "Validation loss decreased (1.087524 --> 1.087524).\n",
      "Validation loss decreased (1.086437 --> 1.086437).\n",
      "Validation loss decreased (1.085379 --> 1.085379).\n",
      "Validation loss decreased (1.084347 --> 1.084347).\n",
      "Validation loss decreased (1.083338 --> 1.083338).\n",
      "Validation loss decreased (1.082353 --> 1.082353).\n",
      "Validation loss decreased (1.081392 --> 1.081392).\n",
      "Validation loss decreased (1.080454 --> 1.080454).\n",
      "Validation loss decreased (1.079540 --> 1.079540).\n",
      "Validation loss decreased (1.078652 --> 1.078652).\n",
      "Validation loss decreased (1.077786 --> 1.077786).\n",
      "Validation loss decreased (1.076942 --> 1.076942).\n",
      "Validation loss decreased (1.076118 --> 1.076118).\n",
      "Validation loss decreased (1.075320 --> 1.075320).\n",
      "Validation loss decreased (1.074542 --> 1.074542).\n",
      "Validation loss decreased (1.073784 --> 1.073784).\n",
      "Validation loss decreased (1.073048 --> 1.073048).\n",
      "Validation loss decreased (1.072335 --> 1.072335).\n",
      "Validation loss decreased (1.071638 --> 1.071638).\n",
      "Validation loss decreased (1.070964 --> 1.070964).\n",
      "Validation loss decreased (1.070305 --> 1.070305).\n",
      "Validation loss decreased (1.069667 --> 1.069667).\n",
      "Validation loss decreased (1.069048 --> 1.069048).\n",
      "Validation loss decreased (1.068448 --> 1.068448).\n",
      "Validation loss decreased (1.067864 --> 1.067864).\n",
      "Validation loss decreased (1.067298 --> 1.067298).\n",
      "Validation loss decreased (1.066749 --> 1.066749).\n",
      "Validation loss decreased (1.066216 --> 1.066216).\n",
      "Validation loss decreased (1.065698 --> 1.065698).\n",
      "Validation loss decreased (1.065197 --> 1.065197).\n",
      "Validation loss decreased (1.064711 --> 1.064711).\n",
      "Epoch 50: Train Loss: 1.0642, Macro_F1: 0.1908, AUC_score: 0.5000\n",
      "Validation loss decreased (1.064242 --> 1.064242).\n",
      "Validation loss decreased (1.063787 --> 1.063787).\n",
      "Validation loss decreased (1.063345 --> 1.063345).\n",
      "Validation loss decreased (1.062916 --> 1.062916).\n",
      "Validation loss decreased (1.062502 --> 1.062502).\n",
      "Validation loss decreased (1.062100 --> 1.062100).\n",
      "Validation loss decreased (1.061711 --> 1.061711).\n",
      "Validation loss decreased (1.061335 --> 1.061335).\n",
      "Validation loss decreased (1.060972 --> 1.060972).\n",
      "Validation loss decreased (1.060617 --> 1.060617).\n",
      "Validation loss decreased (1.060279 --> 1.060279).\n",
      "Validation loss decreased (1.059951 --> 1.059951).\n",
      "Validation loss decreased (1.059633 --> 1.059633).\n",
      "Validation loss decreased (1.059325 --> 1.059325).\n",
      "Validation loss decreased (1.059029 --> 1.059029).\n",
      "Validation loss decreased (1.058739 --> 1.058739).\n",
      "Validation loss decreased (1.058465 --> 1.058465).\n",
      "Validation loss decreased (1.058196 --> 1.058196).\n",
      "Validation loss decreased (1.057937 --> 1.057937).\n",
      "Validation loss decreased (1.057689 --> 1.057689).\n",
      "Validation loss decreased (1.057447 --> 1.057447).\n",
      "Validation loss decreased (1.057214 --> 1.057214).\n",
      "Validation loss decreased (1.056991 --> 1.056991).\n",
      "Validation loss decreased (1.056777 --> 1.056777).\n",
      "Validation loss decreased (1.056568 --> 1.056568).\n",
      "Validation loss decreased (1.056366 --> 1.056366).\n",
      "Validation loss decreased (1.056173 --> 1.056173).\n",
      "Validation loss decreased (1.055987 --> 1.055987).\n",
      "Validation loss decreased (1.055808 --> 1.055808).\n",
      "Validation loss decreased (1.055636 --> 1.055636).\n",
      "Validation loss decreased (1.055470 --> 1.055470).\n",
      "Validation loss decreased (1.055310 --> 1.055310).\n",
      "Validation loss decreased (1.055156 --> 1.055156).\n",
      "Validation loss decreased (1.055009 --> 1.055009).\n",
      "Validation loss decreased (1.054867 --> 1.054867).\n",
      "Validation loss decreased (1.054730 --> 1.054730).\n",
      "Validation loss decreased (1.054601 --> 1.054601).\n",
      "Validation loss decreased (1.054473 --> 1.054473).\n",
      "Validation loss decreased (1.054354 --> 1.054354).\n",
      "Validation loss decreased (1.054235 --> 1.054235).\n",
      "Validation loss decreased (1.054126 --> 1.054126).\n",
      "Validation loss decreased (1.054017 --> 1.054017).\n",
      "Validation loss decreased (1.053913 --> 1.053913).\n",
      "Validation loss decreased (1.053817 --> 1.053817).\n",
      "Validation loss decreased (1.053723 --> 1.053723).\n",
      "Validation loss decreased (1.053632 --> 1.053632).\n",
      "Validation loss decreased (1.053546 --> 1.053546).\n",
      "Validation loss decreased (1.053462 --> 1.053462).\n",
      "Validation loss decreased (1.053380 --> 1.053380).\n",
      "Validation loss decreased (1.053305 --> 1.053305).\n",
      "Epoch 100: Train Loss: 1.0532, Macro_F1: 0.1908, AUC_score: 0.5000\n",
      "Validation loss decreased (1.053232 --> 1.053232).\n",
      "Validation loss decreased (1.053162 --> 1.053162).\n",
      "Validation loss decreased (1.053095 --> 1.053095).\n",
      "Validation loss decreased (1.053030 --> 1.053030).\n",
      "Validation loss decreased (1.052969 --> 1.052969).\n",
      "Validation loss decreased (1.052911 --> 1.052911).\n",
      "Validation loss decreased (1.052856 --> 1.052856).\n",
      "Validation loss decreased (1.052801 --> 1.052801).\n",
      "Validation loss decreased (1.052749 --> 1.052749).\n",
      "Validation loss decreased (1.052701 --> 1.052701).\n",
      "Validation loss decreased (1.052653 --> 1.052653).\n",
      "Validation loss decreased (1.052610 --> 1.052610).\n",
      "Validation loss decreased (1.052566 --> 1.052566).\n",
      "Validation loss decreased (1.052527 --> 1.052527).\n",
      "Validation loss decreased (1.052486 --> 1.052486).\n",
      "Validation loss decreased (1.052449 --> 1.052449).\n",
      "Validation loss decreased (1.052416 --> 1.052416).\n",
      "Validation loss decreased (1.052382 --> 1.052382).\n",
      "Validation loss decreased (1.052350 --> 1.052350).\n",
      "Validation loss decreased (1.052318 --> 1.052318).\n",
      "Validation loss decreased (1.052288 --> 1.052288).\n",
      "Validation loss decreased (1.052262 --> 1.052262).\n",
      "Validation loss decreased (1.052236 --> 1.052236).\n",
      "Validation loss decreased (1.052210 --> 1.052210).\n",
      "Validation loss decreased (1.052187 --> 1.052187).\n",
      "Validation loss decreased (1.052163 --> 1.052163).\n",
      "Validation loss decreased (1.052143 --> 1.052143).\n",
      "Validation loss decreased (1.052121 --> 1.052121).\n",
      "Validation loss decreased (1.052100 --> 1.052100).\n",
      "Validation loss decreased (1.052081 --> 1.052081).\n",
      "Validation loss decreased (1.052064 --> 1.052064).\n",
      "Validation loss decreased (1.052046 --> 1.052046).\n",
      "Validation loss decreased (1.052032 --> 1.052032).\n",
      "Validation loss decreased (1.052017 --> 1.052017).\n",
      "Validation loss decreased (1.052002 --> 1.052002).\n",
      "Validation loss decreased (1.051990 --> 1.051990).\n",
      "Validation loss decreased (1.051976 --> 1.051976).\n",
      "Validation loss decreased (1.051963 --> 1.051963).\n",
      "Validation loss decreased (1.051951 --> 1.051951).\n",
      "Validation loss decreased (1.051940 --> 1.051940).\n",
      "Validation loss decreased (1.051929 --> 1.051929).\n",
      "Validation loss decreased (1.051919 --> 1.051919).\n",
      "Validation loss decreased (1.051909 --> 1.051909).\n",
      "Validation loss decreased (1.051894 --> 1.051894).\n",
      "Validation loss decreased (1.051878 --> 1.051878).\n",
      "Validation loss decreased (1.051864 --> 1.051864).\n",
      "Epoch 150: Train Loss: 1.0519, Macro_F1: 0.1908, AUC_score: 0.5000\n",
      "Validation loss decreased (1.051852 --> 1.051852).\n",
      "Validation loss decreased (1.051841 --> 1.051841).\n",
      "Validation loss decreased (1.051829 --> 1.051829).\n",
      "Validation loss decreased (1.051817 --> 1.051817).\n",
      "Validation loss decreased (1.051806 --> 1.051806).\n",
      "Validation loss decreased (1.051794 --> 1.051794).\n",
      "Validation loss decreased (1.051783 --> 1.051783).\n",
      "Validation loss decreased (1.051771 --> 1.051771).\n",
      "Validation loss decreased (1.051761 --> 1.051761).\n",
      "Epoch 200: Train Loss: 1.0518, Macro_F1: 0.1908, AUC_score: 0.5000\n",
      "Epoch 00205: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 250: Train Loss: 1.0518, Macro_F1: 0.1908, AUC_score: 0.5000\n",
      "Epoch 00256: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 1.0518, Macro_F1: 0.1908, AUC_score: 0.5000\n",
      "Epoch 00307: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 1.0518, Macro_F1: 0.1908, AUC_score: 0.5000\n",
      "Epoch 00358: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Early stopping triggered\n",
      "acc save\n",
      "99.0% node features transform to 0: F1: 0.1908, AUC_score: 0.5000\n",
      "Epoch 0: Train Loss: 1.0518, Macro_F1: 0.1908, AUC_score: 0.5000\n",
      "Validation loss decreased (1.051752 --> 1.051752).\n",
      "Epoch 50: Train Loss: 1.0518, Macro_F1: 0.1908, AUC_score: 0.5324\n",
      "Validation loss decreased (1.051740 --> 1.051740).\n",
      "Validation loss decreased (1.051721 --> 1.051721).\n",
      "Validation loss decreased (1.051710 --> 1.051710).\n",
      "Validation loss decreased (1.051681 --> 1.051681).\n",
      "Validation loss decreased (1.051657 --> 1.051657).\n",
      "Validation loss decreased (1.051629 --> 1.051629).\n",
      "Validation loss decreased (1.051595 --> 1.051595).\n",
      "Validation loss decreased (1.051512 --> 1.051512).\n",
      "Validation loss decreased (1.051449 --> 1.051449).\n",
      "Validation loss decreased (1.051337 --> 1.051337).\n",
      "Validation loss decreased (1.051224 --> 1.051224).\n",
      "Validation loss decreased (1.051035 --> 1.051035).\n",
      "Validation loss decreased (1.050857 --> 1.050857).\n",
      "Validation loss decreased (1.050545 --> 1.050545).\n",
      "Validation loss decreased (1.050189 --> 1.050189).\n",
      "Validation loss decreased (1.049901 --> 1.049901).\n",
      "Validation loss decreased (1.049492 --> 1.049492).\n",
      "Validation loss decreased (1.048946 --> 1.048946).\n",
      "Validation loss decreased (1.048205 --> 1.048205).\n",
      "Validation loss decreased (1.047397 --> 1.047397).\n",
      "Validation loss decreased (1.046395 --> 1.046395).\n",
      "Validation loss decreased (1.045436 --> 1.045436).\n",
      "Validation loss decreased (1.044605 --> 1.044605).\n",
      "Validation loss decreased (1.043066 --> 1.043066).\n",
      "Validation loss decreased (1.041504 --> 1.041504).\n",
      "Epoch 100: Train Loss: 1.0394, Macro_F1: 0.1908, AUC_score: 0.6200\n",
      "Validation loss decreased (1.039411 --> 1.039411).\n",
      "Validation loss decreased (1.038458 --> 1.038458).\n",
      "Validation loss decreased (1.037226 --> 1.037226).\n",
      "Validation loss decreased (1.034915 --> 1.034915).\n",
      "Validation loss decreased (1.032894 --> 1.032894).\n",
      "Validation loss decreased (1.030247 --> 1.030247).\n",
      "Validation loss decreased (1.029647 --> 1.029647).\n",
      "Validation loss decreased (1.025493 --> 1.025493).\n",
      "Validation loss decreased (1.024804 --> 1.024804).\n",
      "Validation loss decreased (1.022421 --> 1.022421).\n",
      "Validation loss decreased (1.019764 --> 1.019764).\n",
      "Validation loss decreased (1.018329 --> 1.018329).\n",
      "Validation loss decreased (1.014436 --> 1.014436).\n",
      "Validation loss decreased (1.013259 --> 1.013259).\n",
      "Validation loss decreased (1.011017 --> 1.011017).\n",
      "Validation loss decreased (1.010157 --> 1.010157).\n",
      "Validation loss decreased (1.005316 --> 1.005316).\n",
      "Validation loss decreased (1.004512 --> 1.004512).\n",
      "Validation loss decreased (1.000569 --> 1.000569).\n",
      "Validation loss decreased (0.999425 --> 0.999425).\n",
      "Validation loss decreased (0.996284 --> 0.996284).\n",
      "Validation loss decreased (0.995018 --> 0.995018).\n",
      "Validation loss decreased (0.991630 --> 0.991630).\n",
      "Validation loss decreased (0.990282 --> 0.990282).\n",
      "Validation loss decreased (0.988040 --> 0.988040).\n",
      "Validation loss decreased (0.986099 --> 0.986099).\n",
      "Validation loss decreased (0.983447 --> 0.983447).\n",
      "Validation loss decreased (0.980621 --> 0.980621).\n",
      "Validation loss decreased (0.980073 --> 0.980073).\n",
      "Validation loss decreased (0.976606 --> 0.976606).\n",
      "Validation loss decreased (0.975123 --> 0.975123).\n",
      "Validation loss decreased (0.972777 --> 0.972777).\n",
      "Validation loss decreased (0.969074 --> 0.969074).\n",
      "Validation loss decreased (0.968803 --> 0.968803).\n",
      "Validation loss decreased (0.965792 --> 0.965792).\n",
      "Validation loss decreased (0.963701 --> 0.963701).\n",
      "Validation loss decreased (0.963626 --> 0.963626).\n",
      "Validation loss decreased (0.960958 --> 0.960958).\n",
      "Validation loss decreased (0.956637 --> 0.956637).\n",
      "Validation loss decreased (0.953901 --> 0.953901).\n",
      "Validation loss decreased (0.952653 --> 0.952653).\n",
      "Validation loss decreased (0.950728 --> 0.950728).\n",
      "Validation loss decreased (0.948127 --> 0.948127).\n",
      "Validation loss decreased (0.944983 --> 0.944983).\n",
      "Epoch 150: Train Loss: 0.9461, Macro_F1: 0.4096, AUC_score: 0.7049\n",
      "Validation loss decreased (0.944822 --> 0.944822).\n",
      "Validation loss decreased (0.943230 --> 0.943230).\n",
      "Validation loss decreased (0.942651 --> 0.942651).\n",
      "Validation loss decreased (0.941040 --> 0.941040).\n",
      "Validation loss decreased (0.937209 --> 0.937209).\n",
      "Validation loss decreased (0.936560 --> 0.936560).\n",
      "Validation loss decreased (0.935726 --> 0.935726).\n",
      "Validation loss decreased (0.931943 --> 0.931943).\n",
      "Validation loss decreased (0.930756 --> 0.930756).\n",
      "Validation loss decreased (0.928590 --> 0.928590).\n",
      "Validation loss decreased (0.925332 --> 0.925332).\n",
      "Validation loss decreased (0.925082 --> 0.925082).\n",
      "Validation loss decreased (0.923760 --> 0.923760).\n",
      "Validation loss decreased (0.923637 --> 0.923637).\n",
      "Validation loss decreased (0.923553 --> 0.923553).\n",
      "Validation loss decreased (0.920877 --> 0.920877).\n",
      "Validation loss decreased (0.919504 --> 0.919504).\n",
      "Validation loss decreased (0.918815 --> 0.918815).\n",
      "Validation loss decreased (0.916004 --> 0.916004).\n",
      "Validation loss decreased (0.915746 --> 0.915746).\n",
      "Validation loss decreased (0.915608 --> 0.915608).\n",
      "Epoch 200: Train Loss: 0.9136, Macro_F1: 0.4136, AUC_score: 0.7205\n",
      "Validation loss decreased (0.913555 --> 0.913555).\n",
      "Validation loss decreased (0.912784 --> 0.912784).\n",
      "Validation loss decreased (0.910523 --> 0.910523).\n",
      "Validation loss decreased (0.908897 --> 0.908897).\n",
      "Validation loss decreased (0.906209 --> 0.906209).\n",
      "Validation loss decreased (0.905431 --> 0.905431).\n",
      "Validation loss decreased (0.905269 --> 0.905269).\n",
      "Validation loss decreased (0.904806 --> 0.904806).\n",
      "Validation loss decreased (0.903730 --> 0.903730).\n",
      "Validation loss decreased (0.902659 --> 0.902659).\n",
      "Epoch 250: Train Loss: 0.9010, Macro_F1: 0.4140, AUC_score: 0.7268\n",
      "Validation loss decreased (0.900994 --> 0.900994).\n",
      "Validation loss decreased (0.899179 --> 0.899179).\n",
      "Validation loss decreased (0.897421 --> 0.897421).\n",
      "Validation loss decreased (0.897261 --> 0.897261).\n",
      "Validation loss decreased (0.897203 --> 0.897203).\n",
      "Validation loss decreased (0.896546 --> 0.896546).\n",
      "Validation loss decreased (0.896014 --> 0.896014).\n",
      "Validation loss decreased (0.893941 --> 0.893941).\n",
      "Validation loss decreased (0.892943 --> 0.892943).\n",
      "Validation loss decreased (0.892887 --> 0.892887).\n",
      "Epoch 300: Train Loss: 0.8949, Macro_F1: 0.4318, AUC_score: 0.7292\n",
      "Validation loss decreased (0.892276 --> 0.892276).\n",
      "Validation loss decreased (0.890401 --> 0.890401).\n",
      "Validation loss decreased (0.890345 --> 0.890345).\n",
      "Validation loss decreased (0.890271 --> 0.890271).\n",
      "Validation loss decreased (0.889836 --> 0.889836).\n",
      "Validation loss decreased (0.889691 --> 0.889691).\n",
      "Validation loss decreased (0.887841 --> 0.887841).\n",
      "Validation loss decreased (0.887154 --> 0.887154).\n",
      "Validation loss decreased (0.887060 --> 0.887060).\n",
      "Epoch 350: Train Loss: 0.8857, Macro_F1: 0.4503, AUC_score: 0.7338\n",
      "Validation loss decreased (0.885748 --> 0.885748).\n",
      "Validation loss decreased (0.885230 --> 0.885230).\n",
      "Validation loss decreased (0.883815 --> 0.883815).\n",
      "Validation loss decreased (0.883600 --> 0.883600).\n",
      "Validation loss decreased (0.882280 --> 0.882280).\n",
      "Validation loss decreased (0.877327 --> 0.877327).\n",
      "Validation loss decreased (0.876507 --> 0.876507).\n",
      "Validation loss decreased (0.876470 --> 0.876470).\n",
      "Validation loss decreased (0.874296 --> 0.874296).\n",
      "Validation loss decreased (0.870872 --> 0.870872).\n",
      "Epoch 400: Train Loss: 0.8712, Macro_F1: 0.4727, AUC_score: 0.7432\n",
      "Validation loss decreased (0.867841 --> 0.867841).\n",
      "Validation loss decreased (0.865536 --> 0.865536).\n",
      "Validation loss decreased (0.862765 --> 0.862765).\n",
      "Validation loss decreased (0.857159 --> 0.857159).\n",
      "Validation loss decreased (0.856647 --> 0.856647).\n",
      "Validation loss decreased (0.856049 --> 0.856049).\n",
      "Validation loss decreased (0.851629 --> 0.851629).\n",
      "Epoch 450: Train Loss: 0.8501, Macro_F1: 0.5347, AUC_score: 0.7586\n",
      "Validation loss decreased (0.850123 --> 0.850123).\n",
      "Validation loss decreased (0.849288 --> 0.849288).\n",
      "Validation loss decreased (0.847676 --> 0.847676).\n",
      "Validation loss decreased (0.845119 --> 0.845119).\n",
      "Validation loss decreased (0.844237 --> 0.844237).\n",
      "Validation loss decreased (0.839139 --> 0.839139).\n",
      "Validation loss decreased (0.838873 --> 0.838873).\n",
      "Validation loss decreased (0.837660 --> 0.837660).\n",
      "Validation loss decreased (0.835722 --> 0.835722).\n",
      "Validation loss decreased (0.832970 --> 0.832970).\n",
      "Epoch 500: Train Loss: 0.8390, Macro_F1: 0.5900, AUC_score: 0.7737\n",
      "Validation loss decreased (0.829657 --> 0.829657).\n",
      "Validation loss decreased (0.828711 --> 0.828711).\n",
      "Validation loss decreased (0.828250 --> 0.828250).\n",
      "Validation loss decreased (0.823879 --> 0.823879).\n",
      "Validation loss decreased (0.820786 --> 0.820786).\n",
      "Validation loss decreased (0.818850 --> 0.818850).\n",
      "Validation loss decreased (0.806749 --> 0.806749).\n",
      "Validation loss decreased (0.800035 --> 0.800035).\n",
      "Validation loss decreased (0.799177 --> 0.799177).\n",
      "Validation loss decreased (0.797976 --> 0.797976).\n",
      "Validation loss decreased (0.797756 --> 0.797756).\n",
      "Validation loss decreased (0.787319 --> 0.787319).\n",
      "Validation loss decreased (0.782392 --> 0.782392).\n",
      "Validation loss decreased (0.781522 --> 0.781522).\n",
      "Validation loss decreased (0.775620 --> 0.775620).\n",
      "Validation loss decreased (0.773520 --> 0.773520).\n",
      "Validation loss decreased (0.768677 --> 0.768677).\n",
      "Validation loss decreased (0.762771 --> 0.762771).\n",
      "Validation loss decreased (0.759197 --> 0.759197).\n",
      "Validation loss decreased (0.756294 --> 0.756294).\n",
      "Epoch 550: Train Loss: 0.7594, Macro_F1: 0.6908, AUC_score: 0.8087\n",
      "Validation loss decreased (0.748759 --> 0.748759).\n",
      "Validation loss decreased (0.741255 --> 0.741255).\n",
      "Validation loss decreased (0.736956 --> 0.736956).\n",
      "Validation loss decreased (0.734234 --> 0.734234).\n",
      "Validation loss decreased (0.729430 --> 0.729430).\n",
      "Validation loss decreased (0.728359 --> 0.728359).\n",
      "Validation loss decreased (0.720965 --> 0.720965).\n",
      "Validation loss decreased (0.715684 --> 0.715684).\n",
      "Validation loss decreased (0.711428 --> 0.711428).\n",
      "Validation loss decreased (0.700942 --> 0.700942).\n",
      "Epoch 600: Train Loss: 0.7202, Macro_F1: 0.6953, AUC_score: 0.8270\n",
      "Validation loss decreased (0.697471 --> 0.697471).\n",
      "Epoch 650: Train Loss: 0.7312, Macro_F1: 0.7044, AUC_score: 0.8333\n",
      "Validation loss decreased (0.695340 --> 0.695340).\n",
      "Validation loss decreased (0.694882 --> 0.694882).\n",
      "Validation loss decreased (0.693193 --> 0.693193).\n",
      "Validation loss decreased (0.689778 --> 0.689778).\n",
      "Validation loss decreased (0.688324 --> 0.688324).\n",
      "Validation loss decreased (0.684331 --> 0.684331).\n",
      "Epoch 700: Train Loss: 0.6967, Macro_F1: 0.6955, AUC_score: 0.8411\n",
      "Validation loss decreased (0.683403 --> 0.683403).\n",
      "Validation loss decreased (0.683115 --> 0.683115).\n",
      "Validation loss decreased (0.676972 --> 0.676972).\n",
      "Validation loss decreased (0.672275 --> 0.672275).\n",
      "Epoch 750: Train Loss: 0.7025, Macro_F1: 0.7064, AUC_score: 0.8440\n",
      "Validation loss decreased (0.670628 --> 0.670628).\n",
      "Validation loss decreased (0.664050 --> 0.664050).\n",
      "Epoch 800: Train Loss: 0.6884, Macro_F1: 0.7191, AUC_score: 0.8465\n",
      "Validation loss decreased (0.662107 --> 0.662107).\n",
      "Epoch 850: Train Loss: 0.6760, Macro_F1: 0.7090, AUC_score: 0.8505\n",
      "Validation loss decreased (0.657341 --> 0.657341).\n",
      "Validation loss decreased (0.655286 --> 0.655286).\n",
      "Epoch 900: Train Loss: 0.6730, Macro_F1: 0.7007, AUC_score: 0.8527\n",
      "Validation loss decreased (0.651978 --> 0.651978).\n",
      "Epoch 950: Train Loss: 0.6724, Macro_F1: 0.7229, AUC_score: 0.8548\n",
      "Validation loss decreased (0.649104 --> 0.649104).\n",
      "Validation loss decreased (0.646403 --> 0.646403).\n",
      "acc save\n",
      "98.0% node features transform to 0: F1: 0.7207, AUC_score: 0.8564\n",
      "Epoch 0: Train Loss: 0.6491, Macro_F1: 0.7219, AUC_score: 0.8772\n",
      "Validation loss decreased (0.649086 --> 0.649086).\n",
      "Validation loss decreased (0.630713 --> 0.630713).\n",
      "Validation loss decreased (0.623753 --> 0.623753).\n",
      "Validation loss decreased (0.620135 --> 0.620135).\n",
      "Validation loss decreased (0.592477 --> 0.592477).\n",
      "Validation loss decreased (0.585092 --> 0.585092).\n",
      "Validation loss decreased (0.575501 --> 0.575501).\n",
      "Validation loss decreased (0.562812 --> 0.562812).\n",
      "Validation loss decreased (0.557179 --> 0.557179).\n",
      "Validation loss decreased (0.550050 --> 0.550050).\n",
      "Validation loss decreased (0.544734 --> 0.544734).\n",
      "Validation loss decreased (0.544678 --> 0.544678).\n",
      "Validation loss decreased (0.537149 --> 0.537149).\n",
      "Epoch 50: Train Loss: 0.5676, Macro_F1: 0.7932, AUC_score: 0.9025\n",
      "Validation loss decreased (0.518906 --> 0.518906).\n",
      "Validation loss decreased (0.518084 --> 0.518084).\n",
      "Validation loss decreased (0.513282 --> 0.513282).\n",
      "Validation loss decreased (0.509494 --> 0.509494).\n",
      "Validation loss decreased (0.488826 --> 0.488826).\n",
      "Epoch 100: Train Loss: 0.5422, Macro_F1: 0.8142, AUC_score: 0.9111\n",
      "Validation loss decreased (0.486848 --> 0.486848).\n",
      "Validation loss decreased (0.478145 --> 0.478145).\n",
      "Epoch 150: Train Loss: 0.5443, Macro_F1: 0.8094, AUC_score: 0.9148\n",
      "Validation loss decreased (0.474475 --> 0.474475).\n",
      "Validation loss decreased (0.471144 --> 0.471144).\n",
      "Validation loss decreased (0.470839 --> 0.470839).\n",
      "Epoch 200: Train Loss: 0.5222, Macro_F1: 0.8180, AUC_score: 0.9183\n",
      "Validation loss decreased (0.465192 --> 0.465192).\n",
      "Validation loss decreased (0.461684 --> 0.461684).\n",
      "Epoch 250: Train Loss: 0.4804, Macro_F1: 0.8158, AUC_score: 0.9205\n",
      "Validation loss decreased (0.448143 --> 0.448143).\n",
      "Epoch 300: Train Loss: 0.4685, Macro_F1: 0.8184, AUC_score: 0.9218\n",
      "Validation loss decreased (0.445048 --> 0.445048).\n",
      "Validation loss decreased (0.443865 --> 0.443865).\n",
      "Epoch 350: Train Loss: 0.4501, Macro_F1: 0.8313, AUC_score: 0.9243\n",
      "Validation loss decreased (0.443697 --> 0.443697).\n",
      "Validation loss decreased (0.442040 --> 0.442040).\n",
      "Validation loss decreased (0.440066 --> 0.440066).\n",
      "Epoch 400: Train Loss: 0.4439, Macro_F1: 0.8291, AUC_score: 0.9264\n",
      "Validation loss decreased (0.431382 --> 0.431382).\n",
      "Validation loss decreased (0.431274 --> 0.431274).\n",
      "Epoch 450: Train Loss: 0.4576, Macro_F1: 0.8315, AUC_score: 0.9279\n",
      "Validation loss decreased (0.424173 --> 0.424173).\n",
      "Epoch 500: Train Loss: 0.4771, Macro_F1: 0.8261, AUC_score: 0.9280\n",
      "Validation loss decreased (0.420035 --> 0.420035).\n",
      "Epoch 550: Train Loss: 0.4637, Macro_F1: 0.8298, AUC_score: 0.9293\n",
      "Validation loss decreased (0.419872 --> 0.419872).\n",
      "Epoch 600: Train Loss: 0.4353, Macro_F1: 0.8360, AUC_score: 0.9313\n",
      "Validation loss decreased (0.416818 --> 0.416818).\n",
      "Epoch 650: Train Loss: 0.4660, Macro_F1: 0.8335, AUC_score: 0.9317\n",
      "Validation loss decreased (0.414058 --> 0.414058).\n",
      "Epoch 700: Train Loss: 0.4723, Macro_F1: 0.8309, AUC_score: 0.9333\n",
      "Validation loss decreased (0.405797 --> 0.405797).\n",
      "Validation loss decreased (0.404508 --> 0.404508).\n",
      "Epoch 750: Train Loss: 0.4362, Macro_F1: 0.8370, AUC_score: 0.9345\n",
      "Validation loss decreased (0.398296 --> 0.398296).\n",
      "Epoch 800: Train Loss: 0.5380, Macro_F1: 0.8344, AUC_score: 0.9358\n",
      "Epoch 850: Train Loss: 0.4430, Macro_F1: 0.8366, AUC_score: 0.9362\n",
      "Epoch 00864: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.397865 --> 0.397865).\n",
      "Validation loss decreased (0.394836 --> 0.394836).\n",
      "Epoch 900: Train Loss: 0.4585, Macro_F1: 0.8355, AUC_score: 0.9374\n",
      "Validation loss decreased (0.390542 --> 0.390542).\n",
      "Epoch 950: Train Loss: 0.4128, Macro_F1: 0.8347, AUC_score: 0.9374\n",
      "acc save\n",
      "97.0% node features transform to 0: F1: 0.8346, AUC_score: 0.9375\n",
      "Epoch 0: Train Loss: 0.4560, Macro_F1: 0.7960, AUC_score: 0.9221\n",
      "Validation loss decreased (0.455992 --> 0.455992).\n",
      "Validation loss decreased (0.449876 --> 0.449876).\n",
      "Validation loss decreased (0.423300 --> 0.423300).\n",
      "Validation loss decreased (0.414138 --> 0.414138).\n",
      "Validation loss decreased (0.413662 --> 0.413662).\n",
      "Validation loss decreased (0.406284 --> 0.406284).\n",
      "Validation loss decreased (0.396464 --> 0.396464).\n",
      "Epoch 50: Train Loss: 0.4775, Macro_F1: 0.8378, AUC_score: 0.9368\n",
      "Validation loss decreased (0.384372 --> 0.384372).\n",
      "Epoch 100: Train Loss: 0.4193, Macro_F1: 0.8436, AUC_score: 0.9400\n",
      "Validation loss decreased (0.383627 --> 0.383627).\n",
      "Epoch 150: Train Loss: 0.3826, Macro_F1: 0.8482, AUC_score: 0.9415\n",
      "Validation loss decreased (0.382629 --> 0.382629).\n",
      "Validation loss decreased (0.374921 --> 0.374921).\n",
      "Epoch 200: Train Loss: 0.4371, Macro_F1: 0.8517, AUC_score: 0.9429\n",
      "Epoch 250: Train Loss: 0.3645, Macro_F1: 0.8479, AUC_score: 0.9428\n",
      "Validation loss decreased (0.364471 --> 0.364471).\n",
      "Validation loss decreased (0.363733 --> 0.363733).\n",
      "Epoch 300: Train Loss: 0.4141, Macro_F1: 0.8514, AUC_score: 0.9440\n",
      "Epoch 350: Train Loss: 0.4115, Macro_F1: 0.8526, AUC_score: 0.9450\n",
      "Epoch 00383: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 400: Train Loss: 0.3739, Macro_F1: 0.8550, AUC_score: 0.9451\n",
      "Epoch 450: Train Loss: 0.3906, Macro_F1: 0.8525, AUC_score: 0.9455\n",
      "Early stopping triggered\n",
      "acc save\n",
      "96.0% node features transform to 0: F1: 0.8557, AUC_score: 0.9452\n",
      "Epoch 0: Train Loss: 0.4619, Macro_F1: 0.8034, AUC_score: 0.9400\n",
      "Validation loss decreased (0.461913 --> 0.461913).\n",
      "Validation loss decreased (0.447607 --> 0.447607).\n",
      "Validation loss decreased (0.436033 --> 0.436033).\n",
      "Validation loss decreased (0.429182 --> 0.429182).\n",
      "Validation loss decreased (0.409124 --> 0.409124).\n",
      "Validation loss decreased (0.390946 --> 0.390946).\n",
      "Validation loss decreased (0.385285 --> 0.385285).\n",
      "Validation loss decreased (0.384485 --> 0.384485).\n",
      "Validation loss decreased (0.377823 --> 0.377823).\n",
      "Validation loss decreased (0.362107 --> 0.362107).\n",
      "Epoch 50: Train Loss: 0.4034, Macro_F1: 0.8476, AUC_score: 0.9457\n",
      "Epoch 100: Train Loss: 0.3917, Macro_F1: 0.8523, AUC_score: 0.9466\n",
      "Validation loss decreased (0.347477 --> 0.347477).\n",
      "Epoch 150: Train Loss: 0.3937, Macro_F1: 0.8485, AUC_score: 0.9476\n",
      "Epoch 200: Train Loss: 0.4459, Macro_F1: 0.8505, AUC_score: 0.9490\n",
      "Epoch 00245: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.3831, Macro_F1: 0.8505, AUC_score: 0.9498\n",
      "Epoch 300: Train Loss: 0.3661, Macro_F1: 0.8570, AUC_score: 0.9501\n",
      "Validation loss decreased (0.345569 --> 0.345569).\n",
      "Epoch 350: Train Loss: 0.4059, Macro_F1: 0.8499, AUC_score: 0.9503\n",
      "Epoch 400: Train Loss: 0.3662, Macro_F1: 0.8522, AUC_score: 0.9506\n",
      "Epoch 00403: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.343887 --> 0.343887).\n",
      "Validation loss decreased (0.342301 --> 0.342301).\n",
      "Validation loss decreased (0.341747 --> 0.341747).\n",
      "Epoch 450: Train Loss: 0.3789, Macro_F1: 0.8560, AUC_score: 0.9506\n",
      "Epoch 500: Train Loss: 0.3712, Macro_F1: 0.8552, AUC_score: 0.9506\n",
      "Validation loss decreased (0.336182 --> 0.336182).\n",
      "Epoch 550: Train Loss: 0.3793, Macro_F1: 0.8555, AUC_score: 0.9507\n",
      "Epoch 600: Train Loss: 0.3774, Macro_F1: 0.8549, AUC_score: 0.9507\n",
      "Epoch 00647: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 650: Train Loss: 0.3955, Macro_F1: 0.8560, AUC_score: 0.9507\n",
      "Epoch 700: Train Loss: 0.3904, Macro_F1: 0.8560, AUC_score: 0.9508\n",
      "Validation loss decreased (0.335735 --> 0.335735).\n",
      "Epoch 750: Train Loss: 0.3632, Macro_F1: 0.8566, AUC_score: 0.9507\n",
      "Epoch 800: Train Loss: 0.3605, Macro_F1: 0.8571, AUC_score: 0.9508\n",
      "Epoch 00810: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 850: Train Loss: 0.4111, Macro_F1: 0.8560, AUC_score: 0.9508\n",
      "Validation loss decreased (0.334676 --> 0.334676).\n",
      "Epoch 900: Train Loss: 0.3600, Macro_F1: 0.8571, AUC_score: 0.9508\n",
      "Epoch 950: Train Loss: 0.4149, Macro_F1: 0.8566, AUC_score: 0.9507\n",
      "Epoch 00969: reducing learning rate of group 0 to 3.2000e-07.\n",
      "acc save\n",
      "95.0% node features transform to 0: F1: 0.8566, AUC_score: 0.9507\n",
      "Epoch 0: Train Loss: 0.3787, Macro_F1: 0.8094, AUC_score: 0.9300\n",
      "Validation loss decreased (0.378667 --> 0.378667).\n",
      "Validation loss decreased (0.378289 --> 0.378289).\n",
      "Validation loss decreased (0.362154 --> 0.362154).\n",
      "Validation loss decreased (0.346392 --> 0.346392).\n",
      "Epoch 50: Train Loss: 0.3660, Macro_F1: 0.8452, AUC_score: 0.9473\n",
      "Validation loss decreased (0.345586 --> 0.345586).\n",
      "Epoch 100: Train Loss: 0.3629, Macro_F1: 0.8456, AUC_score: 0.9489\n",
      "Epoch 150: Train Loss: 0.3956, Macro_F1: 0.8306, AUC_score: 0.9493\n",
      "Validation loss decreased (0.341709 --> 0.341709).\n",
      "Validation loss decreased (0.339357 --> 0.339357).\n",
      "Validation loss decreased (0.326757 --> 0.326757).\n",
      "Epoch 200: Train Loss: 0.3736, Macro_F1: 0.8561, AUC_score: 0.9509\n",
      "Validation loss decreased (0.325373 --> 0.325373).\n",
      "Epoch 250: Train Loss: 0.3372, Macro_F1: 0.8497, AUC_score: 0.9502\n",
      "Epoch 300: Train Loss: 0.3548, Macro_F1: 0.8504, AUC_score: 0.9518\n",
      "Epoch 00314: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 350: Train Loss: 0.3353, Macro_F1: 0.8547, AUC_score: 0.9525\n",
      "Validation loss decreased (0.324493 --> 0.324493).\n",
      "Epoch 400: Train Loss: 0.3316, Macro_F1: 0.8576, AUC_score: 0.9520\n",
      "Epoch 450: Train Loss: 0.3739, Macro_F1: 0.8590, AUC_score: 0.9522\n",
      "Epoch 00462: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.3431, Macro_F1: 0.8568, AUC_score: 0.9527\n",
      "Validation loss decreased (0.322617 --> 0.322617).\n",
      "Epoch 550: Train Loss: 0.3437, Macro_F1: 0.8588, AUC_score: 0.9525\n",
      "Validation loss decreased (0.318465 --> 0.318465).\n",
      "Epoch 600: Train Loss: 0.3422, Macro_F1: 0.8569, AUC_score: 0.9526\n",
      "Epoch 650: Train Loss: 0.3341, Macro_F1: 0.8541, AUC_score: 0.9527\n",
      "Epoch 00692: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 700: Train Loss: 0.3741, Macro_F1: 0.8571, AUC_score: 0.9527\n",
      "Epoch 750: Train Loss: 0.3268, Macro_F1: 0.8577, AUC_score: 0.9527\n",
      "Early stopping triggered\n",
      "acc save\n",
      "94.0% node features transform to 0: F1: 0.8577, AUC_score: 0.9528\n",
      "Epoch 0: Train Loss: 0.3823, Macro_F1: 0.7951, AUC_score: 0.9437\n",
      "Validation loss decreased (0.382335 --> 0.382335).\n",
      "Validation loss decreased (0.346458 --> 0.346458).\n",
      "Validation loss decreased (0.339463 --> 0.339463).\n",
      "Validation loss decreased (0.327847 --> 0.327847).\n",
      "Epoch 50: Train Loss: 0.3790, Macro_F1: 0.8463, AUC_score: 0.9470\n",
      "Validation loss decreased (0.320476 --> 0.320476).\n",
      "Epoch 100: Train Loss: 0.3338, Macro_F1: 0.8505, AUC_score: 0.9490\n",
      "Epoch 150: Train Loss: 0.3446, Macro_F1: 0.8498, AUC_score: 0.9502\n",
      "Validation loss decreased (0.318719 --> 0.318719).\n",
      "Epoch 200: Train Loss: 0.3840, Macro_F1: 0.8452, AUC_score: 0.9507\n",
      "Validation loss decreased (0.315931 --> 0.315931).\n",
      "Epoch 250: Train Loss: 0.3282, Macro_F1: 0.8226, AUC_score: 0.9495\n",
      "Validation loss decreased (0.315459 --> 0.315459).\n",
      "Epoch 300: Train Loss: 0.3674, Macro_F1: 0.8614, AUC_score: 0.9511\n",
      "Validation loss decreased (0.308448 --> 0.308448).\n",
      "Epoch 350: Train Loss: 0.3174, Macro_F1: 0.8470, AUC_score: 0.9510\n",
      "Epoch 400: Train Loss: 0.3264, Macro_F1: 0.8598, AUC_score: 0.9515\n",
      "Epoch 00447: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 450: Train Loss: 0.3278, Macro_F1: 0.8528, AUC_score: 0.9522\n",
      "Epoch 500: Train Loss: 0.3562, Macro_F1: 0.8581, AUC_score: 0.9530\n",
      "Early stopping triggered\n",
      "acc save\n",
      "93.0% node features transform to 0: F1: 0.8565, AUC_score: 0.9526\n",
      "Epoch 0: Train Loss: 0.3542, Macro_F1: 0.8278, AUC_score: 0.9511\n",
      "Validation loss decreased (0.354218 --> 0.354218).\n",
      "Validation loss decreased (0.354148 --> 0.354148).\n",
      "Validation loss decreased (0.332977 --> 0.332977).\n",
      "Validation loss decreased (0.326498 --> 0.326498).\n",
      "Validation loss decreased (0.324363 --> 0.324363).\n",
      "Validation loss decreased (0.315409 --> 0.315409).\n",
      "Validation loss decreased (0.314879 --> 0.314879).\n",
      "Validation loss decreased (0.311589 --> 0.311589).\n",
      "Epoch 50: Train Loss: 0.3121, Macro_F1: 0.8548, AUC_score: 0.9541\n",
      "Validation loss decreased (0.306786 --> 0.306786).\n",
      "Epoch 100: Train Loss: 0.3088, Macro_F1: 0.8542, AUC_score: 0.9532\n",
      "Validation loss decreased (0.297288 --> 0.297288).\n",
      "Epoch 150: Train Loss: 0.3382, Macro_F1: 0.8579, AUC_score: 0.9526\n",
      "Epoch 200: Train Loss: 0.3197, Macro_F1: 0.8425, AUC_score: 0.9534\n",
      "Validation loss decreased (0.296152 --> 0.296152).\n",
      "Epoch 250: Train Loss: 0.2952, Macro_F1: 0.8536, AUC_score: 0.9548\n",
      "Validation loss decreased (0.295172 --> 0.295172).\n",
      "Epoch 300: Train Loss: 0.4273, Macro_F1: 0.8311, AUC_score: 0.9537\n",
      "Validation loss decreased (0.294749 --> 0.294749).\n",
      "Validation loss decreased (0.293059 --> 0.293059).\n",
      "Epoch 350: Train Loss: 0.3568, Macro_F1: 0.8531, AUC_score: 0.9555\n",
      "Validation loss decreased (0.291250 --> 0.291250).\n",
      "Epoch 400: Train Loss: 0.3266, Macro_F1: 0.8629, AUC_score: 0.9553\n",
      "Epoch 450: Train Loss: 0.3694, Macro_F1: 0.8525, AUC_score: 0.9564\n",
      "Epoch 00460: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 500: Train Loss: 0.3512, Macro_F1: 0.8618, AUC_score: 0.9557\n",
      "Validation loss decreased (0.289505 --> 0.289505).\n",
      "Epoch 550: Train Loss: 0.2998, Macro_F1: 0.8593, AUC_score: 0.9559\n",
      "Epoch 600: Train Loss: 0.3447, Macro_F1: 0.8592, AUC_score: 0.9563\n",
      "Epoch 00624: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.286287 --> 0.286287).\n",
      "Epoch 650: Train Loss: 0.3362, Macro_F1: 0.8567, AUC_score: 0.9563\n",
      "Validation loss decreased (0.283767 --> 0.283767).\n",
      "Epoch 700: Train Loss: 0.2956, Macro_F1: 0.8556, AUC_score: 0.9566\n",
      "Epoch 750: Train Loss: 0.3143, Macro_F1: 0.8592, AUC_score: 0.9563\n",
      "Epoch 00766: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 800: Train Loss: 0.3429, Macro_F1: 0.8595, AUC_score: 0.9564\n",
      "Epoch 850: Train Loss: 0.3560, Macro_F1: 0.8565, AUC_score: 0.9566\n",
      "Early stopping triggered\n",
      "acc save\n",
      "92.0% node features transform to 0: F1: 0.8576, AUC_score: 0.9565\n",
      "Epoch 0: Train Loss: 0.3021, Macro_F1: 0.8431, AUC_score: 0.9512\n",
      "Validation loss decreased (0.302076 --> 0.302076).\n",
      "Validation loss decreased (0.300450 --> 0.300450).\n",
      "Validation loss decreased (0.300180 --> 0.300180).\n",
      "Validation loss decreased (0.299057 --> 0.299057).\n",
      "Validation loss decreased (0.294496 --> 0.294496).\n",
      "Epoch 50: Train Loss: 0.3759, Macro_F1: 0.8471, AUC_score: 0.9562\n",
      "Validation loss decreased (0.294149 --> 0.294149).\n",
      "Validation loss decreased (0.293626 --> 0.293626).\n",
      "Epoch 100: Train Loss: 0.3003, Macro_F1: 0.8531, AUC_score: 0.9566\n",
      "Validation loss decreased (0.291974 --> 0.291974).\n",
      "Epoch 150: Train Loss: 0.2984, Macro_F1: 0.8327, AUC_score: 0.9553\n",
      "Validation loss decreased (0.285580 --> 0.285580).\n",
      "Epoch 200: Train Loss: 0.3037, Macro_F1: 0.8587, AUC_score: 0.9567\n",
      "Validation loss decreased (0.285309 --> 0.285309).\n",
      "Epoch 250: Train Loss: 0.3002, Macro_F1: 0.8500, AUC_score: 0.9563\n",
      "Validation loss decreased (0.283729 --> 0.283729).\n",
      "Epoch 300: Train Loss: 0.3126, Macro_F1: 0.8576, AUC_score: 0.9568\n",
      "Validation loss decreased (0.283307 --> 0.283307).\n",
      "Epoch 350: Train Loss: 0.3190, Macro_F1: 0.8526, AUC_score: 0.9568\n",
      "Epoch 400: Train Loss: 0.3275, Macro_F1: 0.8467, AUC_score: 0.9563\n",
      "Validation loss decreased (0.279979 --> 0.279979).\n",
      "Epoch 450: Train Loss: 0.3889, Macro_F1: 0.8567, AUC_score: 0.9563\n",
      "Epoch 500: Train Loss: 0.2936, Macro_F1: 0.8562, AUC_score: 0.9572\n",
      "Epoch 00518: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.274366 --> 0.274366).\n",
      "Epoch 550: Train Loss: 0.2865, Macro_F1: 0.8621, AUC_score: 0.9575\n",
      "Validation loss decreased (0.274300 --> 0.274300).\n",
      "Epoch 600: Train Loss: 0.2843, Macro_F1: 0.8583, AUC_score: 0.9575\n",
      "Validation loss decreased (0.274015 --> 0.274015).\n",
      "Epoch 650: Train Loss: 0.2895, Macro_F1: 0.8551, AUC_score: 0.9578\n",
      "Epoch 700: Train Loss: 0.3057, Macro_F1: 0.8537, AUC_score: 0.9574\n",
      "Epoch 00735: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 750: Train Loss: 0.3669, Macro_F1: 0.8603, AUC_score: 0.9575\n",
      "Epoch 800: Train Loss: 0.2890, Macro_F1: 0.8543, AUC_score: 0.9576\n",
      "Early stopping triggered\n",
      "acc save\n",
      "91.0% node features transform to 0: F1: 0.8571, AUC_score: 0.9574\n",
      "Epoch 0: Train Loss: 0.2977, Macro_F1: 0.8500, AUC_score: 0.9525\n",
      "Validation loss decreased (0.297676 --> 0.297676).\n",
      "Validation loss decreased (0.294321 --> 0.294321).\n",
      "Validation loss decreased (0.285926 --> 0.285926).\n",
      "Validation loss decreased (0.284328 --> 0.284328).\n",
      "Validation loss decreased (0.282122 --> 0.282122).\n",
      "Epoch 50: Train Loss: 0.3398, Macro_F1: 0.8507, AUC_score: 0.9560\n",
      "Validation loss decreased (0.274597 --> 0.274597).\n",
      "Epoch 100: Train Loss: 0.3447, Macro_F1: 0.8590, AUC_score: 0.9550\n",
      "Epoch 150: Train Loss: 0.3046, Macro_F1: 0.8407, AUC_score: 0.9556\n",
      "Epoch 00169: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.2896, Macro_F1: 0.8556, AUC_score: 0.9560\n",
      "Epoch 250: Train Loss: 0.3231, Macro_F1: 0.8569, AUC_score: 0.9561\n",
      "Early stopping triggered\n",
      "acc save\n",
      "90.0% node features transform to 0: F1: 0.8583, AUC_score: 0.9558\n",
      "Epoch 0: Train Loss: 0.2950, Macro_F1: 0.8529, AUC_score: 0.9523\n",
      "Validation loss decreased (0.295003 --> 0.295003).\n",
      "Validation loss decreased (0.294824 --> 0.294824).\n",
      "Validation loss decreased (0.287456 --> 0.287456).\n",
      "Epoch 50: Train Loss: 0.3041, Macro_F1: 0.8446, AUC_score: 0.9553\n",
      "Validation loss decreased (0.282685 --> 0.282685).\n",
      "Epoch 100: Train Loss: 0.2888, Macro_F1: 0.8614, AUC_score: 0.9554\n",
      "Validation loss decreased (0.280415 --> 0.280415).\n",
      "Validation loss decreased (0.275092 --> 0.275092).\n",
      "Epoch 150: Train Loss: 0.2956, Macro_F1: 0.8596, AUC_score: 0.9567\n",
      "Epoch 200: Train Loss: 0.3220, Macro_F1: 0.8465, AUC_score: 0.9558\n",
      "Epoch 00240: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.2858, Macro_F1: 0.8575, AUC_score: 0.9567\n",
      "Epoch 300: Train Loss: 0.2821, Macro_F1: 0.8569, AUC_score: 0.9568\n",
      "Validation loss decreased (0.268141 --> 0.268141).\n",
      "Epoch 350: Train Loss: 0.3215, Macro_F1: 0.8574, AUC_score: 0.9569\n",
      "Epoch 400: Train Loss: 0.3036, Macro_F1: 0.8553, AUC_score: 0.9563\n",
      "Epoch 00435: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.3173, Macro_F1: 0.8603, AUC_score: 0.9566\n",
      "Epoch 500: Train Loss: 0.2903, Macro_F1: 0.8619, AUC_score: 0.9569\n",
      "Early stopping triggered\n",
      "acc save\n",
      "89.0% node features transform to 0: F1: 0.8622, AUC_score: 0.9569\n",
      "Epoch 0: Train Loss: 0.3137, Macro_F1: 0.8283, AUC_score: 0.9519\n",
      "Validation loss decreased (0.313686 --> 0.313686).\n",
      "Validation loss decreased (0.299940 --> 0.299940).\n",
      "Validation loss decreased (0.296045 --> 0.296045).\n",
      "Validation loss decreased (0.291789 --> 0.291789).\n",
      "Validation loss decreased (0.290688 --> 0.290688).\n",
      "Validation loss decreased (0.282914 --> 0.282914).\n",
      "Validation loss decreased (0.282322 --> 0.282322).\n",
      "Epoch 50: Train Loss: 0.2917, Macro_F1: 0.8617, AUC_score: 0.9556\n",
      "Validation loss decreased (0.281970 --> 0.281970).\n",
      "Validation loss decreased (0.279514 --> 0.279514).\n",
      "Validation loss decreased (0.278464 --> 0.278464).\n",
      "Validation loss decreased (0.274645 --> 0.274645).\n",
      "Validation loss decreased (0.270129 --> 0.270129).\n",
      "Epoch 100: Train Loss: 0.2895, Macro_F1: 0.8550, AUC_score: 0.9567\n",
      "Epoch 150: Train Loss: 0.2933, Macro_F1: 0.8577, AUC_score: 0.9566\n",
      "Epoch 00187: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.2807, Macro_F1: 0.8573, AUC_score: 0.9573\n",
      "Epoch 250: Train Loss: 0.2835, Macro_F1: 0.8634, AUC_score: 0.9569\n",
      "Validation loss decreased (0.266617 --> 0.266617).\n",
      "Validation loss decreased (0.266327 --> 0.266327).\n",
      "Epoch 300: Train Loss: 0.3009, Macro_F1: 0.8633, AUC_score: 0.9571\n",
      "Epoch 350: Train Loss: 0.2827, Macro_F1: 0.8540, AUC_score: 0.9567\n",
      "Epoch 00393: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.3147, Macro_F1: 0.8529, AUC_score: 0.9564\n",
      "Epoch 450: Train Loss: 0.2731, Macro_F1: 0.8621, AUC_score: 0.9569\n",
      "Validation loss decreased (0.264182 --> 0.264182).\n",
      "Epoch 500: Train Loss: 0.3027, Macro_F1: 0.8615, AUC_score: 0.9569\n",
      "Epoch 550: Train Loss: 0.2852, Macro_F1: 0.8626, AUC_score: 0.9570\n",
      "Epoch 00554: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 600: Train Loss: 0.3266, Macro_F1: 0.8591, AUC_score: 0.9570\n",
      "Epoch 650: Train Loss: 0.3141, Macro_F1: 0.8626, AUC_score: 0.9570\n",
      "Early stopping triggered\n",
      "acc save\n",
      "88.0% node features transform to 0: F1: 0.8626, AUC_score: 0.9570\n",
      "Epoch 0: Train Loss: 0.2747, Macro_F1: 0.8513, AUC_score: 0.9534\n",
      "Validation loss decreased (0.274675 --> 0.274675).\n",
      "Validation loss decreased (0.267976 --> 0.267976).\n",
      "Epoch 50: Train Loss: 0.2791, Macro_F1: 0.8642, AUC_score: 0.9561\n",
      "Epoch 100: Train Loss: 0.2819, Macro_F1: 0.8559, AUC_score: 0.9566\n",
      "Validation loss decreased (0.264670 --> 0.264670).\n",
      "Validation loss decreased (0.263653 --> 0.263653).\n",
      "Epoch 150: Train Loss: 0.3147, Macro_F1: 0.8554, AUC_score: 0.9550\n",
      "Epoch 200: Train Loss: 0.3132, Macro_F1: 0.8562, AUC_score: 0.9533\n",
      "Epoch 00244: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.3146, Macro_F1: 0.8593, AUC_score: 0.9557\n",
      "Validation loss decreased (0.262060 --> 0.262060).\n",
      "Epoch 300: Train Loss: 0.2963, Macro_F1: 0.8590, AUC_score: 0.9559\n",
      "Epoch 350: Train Loss: 0.2711, Macro_F1: 0.8612, AUC_score: 0.9566\n",
      "Epoch 00375: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.261844 --> 0.261844).\n",
      "Validation loss decreased (0.261271 --> 0.261271).\n",
      "Epoch 400: Train Loss: 0.3043, Macro_F1: 0.8629, AUC_score: 0.9565\n",
      "Validation loss decreased (0.260710 --> 0.260710).\n",
      "Epoch 450: Train Loss: 0.2966, Macro_F1: 0.8629, AUC_score: 0.9565\n",
      "Validation loss decreased (0.260158 --> 0.260158).\n",
      "Epoch 500: Train Loss: 0.3008, Macro_F1: 0.8590, AUC_score: 0.9564\n",
      "Validation loss decreased (0.259191 --> 0.259191).\n",
      "Epoch 550: Train Loss: 0.3121, Macro_F1: 0.8601, AUC_score: 0.9566\n",
      "Validation loss decreased (0.258675 --> 0.258675).\n",
      "Validation loss decreased (0.257636 --> 0.257636).\n",
      "Epoch 600: Train Loss: 0.2976, Macro_F1: 0.8631, AUC_score: 0.9564\n",
      "Epoch 650: Train Loss: 0.2658, Macro_F1: 0.8635, AUC_score: 0.9564\n",
      "Epoch 00672: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 700: Train Loss: 0.2673, Macro_F1: 0.8638, AUC_score: 0.9563\n",
      "Epoch 750: Train Loss: 0.3054, Macro_F1: 0.8627, AUC_score: 0.9564\n",
      "Early stopping triggered\n",
      "acc save\n",
      "87.0% node features transform to 0: F1: 0.8635, AUC_score: 0.9564\n",
      "Epoch 0: Train Loss: 0.3004, Macro_F1: 0.8132, AUC_score: 0.9526\n",
      "Validation loss decreased (0.300433 --> 0.300433).\n",
      "Validation loss decreased (0.298077 --> 0.298077).\n",
      "Validation loss decreased (0.291270 --> 0.291270).\n",
      "Validation loss decreased (0.286986 --> 0.286986).\n",
      "Validation loss decreased (0.272579 --> 0.272579).\n",
      "Epoch 50: Train Loss: 0.2765, Macro_F1: 0.8538, AUC_score: 0.9550\n",
      "Validation loss decreased (0.262638 --> 0.262638).\n",
      "Epoch 100: Train Loss: 0.3415, Macro_F1: 0.8473, AUC_score: 0.9556\n",
      "Validation loss decreased (0.259318 --> 0.259318).\n",
      "Epoch 150: Train Loss: 0.2940, Macro_F1: 0.8625, AUC_score: 0.9546\n",
      "Epoch 200: Train Loss: 0.2794, Macro_F1: 0.8571, AUC_score: 0.9561\n",
      "Epoch 00208: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.2821, Macro_F1: 0.8652, AUC_score: 0.9562\n",
      "Epoch 300: Train Loss: 0.2995, Macro_F1: 0.8607, AUC_score: 0.9561\n",
      "Early stopping triggered\n",
      "acc save\n",
      "86.0% node features transform to 0: F1: 0.8612, AUC_score: 0.9563\n",
      "Epoch 0: Train Loss: 0.2963, Macro_F1: 0.8249, AUC_score: 0.9511\n",
      "Validation loss decreased (0.296306 --> 0.296306).\n",
      "Validation loss decreased (0.292920 --> 0.292920).\n",
      "Validation loss decreased (0.282819 --> 0.282819).\n",
      "Validation loss decreased (0.279902 --> 0.279902).\n",
      "Validation loss decreased (0.277079 --> 0.277079).\n",
      "Validation loss decreased (0.275071 --> 0.275071).\n",
      "Validation loss decreased (0.273674 --> 0.273674).\n",
      "Validation loss decreased (0.271625 --> 0.271625).\n",
      "Validation loss decreased (0.261260 --> 0.261260).\n",
      "Epoch 50: Train Loss: 0.3054, Macro_F1: 0.8548, AUC_score: 0.9542\n",
      "Validation loss decreased (0.257802 --> 0.257802).\n",
      "Epoch 100: Train Loss: 0.2745, Macro_F1: 0.8576, AUC_score: 0.9528\n",
      "Epoch 150: Train Loss: 0.2899, Macro_F1: 0.8514, AUC_score: 0.9540\n",
      "Epoch 00157: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.2924, Macro_F1: 0.8580, AUC_score: 0.9544\n",
      "Validation loss decreased (0.254186 --> 0.254186).\n",
      "Epoch 250: Train Loss: 0.2570, Macro_F1: 0.8566, AUC_score: 0.9547\n",
      "Validation loss decreased (0.252090 --> 0.252090).\n",
      "Epoch 300: Train Loss: 0.2769, Macro_F1: 0.8536, AUC_score: 0.9547\n",
      "Epoch 350: Train Loss: 0.3041, Macro_F1: 0.8562, AUC_score: 0.9549\n",
      "Epoch 00382: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.2584, Macro_F1: 0.8567, AUC_score: 0.9546\n",
      "Epoch 450: Train Loss: 0.2670, Macro_F1: 0.8610, AUC_score: 0.9548\n",
      "Early stopping triggered\n",
      "acc save\n",
      "85.0% node features transform to 0: F1: 0.8586, AUC_score: 0.9549\n",
      "Epoch 0: Train Loss: 0.2964, Macro_F1: 0.8490, AUC_score: 0.9516\n",
      "Validation loss decreased (0.296351 --> 0.296351).\n",
      "Validation loss decreased (0.293698 --> 0.293698).\n",
      "Validation loss decreased (0.290144 --> 0.290144).\n",
      "Validation loss decreased (0.275258 --> 0.275258).\n",
      "Validation loss decreased (0.274225 --> 0.274225).\n",
      "Validation loss decreased (0.267585 --> 0.267585).\n",
      "Epoch 50: Train Loss: 0.2650, Macro_F1: 0.8565, AUC_score: 0.9530\n",
      "Validation loss decreased (0.264983 --> 0.264983).\n",
      "Validation loss decreased (0.263005 --> 0.263005).\n",
      "Validation loss decreased (0.260015 --> 0.260015).\n",
      "Validation loss decreased (0.257049 --> 0.257049).\n",
      "Epoch 100: Train Loss: 0.2811, Macro_F1: 0.8441, AUC_score: 0.9535\n",
      "Epoch 150: Train Loss: 0.2855, Macro_F1: 0.8568, AUC_score: 0.9531\n",
      "Epoch 00188: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.253836 --> 0.253836).\n",
      "Epoch 200: Train Loss: 0.2722, Macro_F1: 0.8583, AUC_score: 0.9547\n",
      "Validation loss decreased (0.252088 --> 0.252088).\n",
      "Epoch 250: Train Loss: 0.2607, Macro_F1: 0.8564, AUC_score: 0.9546\n",
      "Epoch 300: Train Loss: 0.2905, Macro_F1: 0.8624, AUC_score: 0.9544\n",
      "Epoch 00351: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.2585, Macro_F1: 0.8610, AUC_score: 0.9547\n",
      "Epoch 400: Train Loss: 0.2671, Macro_F1: 0.8621, AUC_score: 0.9547\n",
      "Validation loss decreased (0.247082 --> 0.247082).\n",
      "Epoch 450: Train Loss: 0.2842, Macro_F1: 0.8613, AUC_score: 0.9547\n",
      "Epoch 500: Train Loss: 0.2642, Macro_F1: 0.8599, AUC_score: 0.9550\n",
      "Epoch 00545: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 550: Train Loss: 0.2482, Macro_F1: 0.8635, AUC_score: 0.9549\n",
      "Epoch 600: Train Loss: 0.3109, Macro_F1: 0.8635, AUC_score: 0.9549\n",
      "Early stopping triggered\n",
      "acc save\n",
      "84.0% node features transform to 0: F1: 0.8621, AUC_score: 0.9549\n",
      "Epoch 0: Train Loss: 0.2660, Macro_F1: 0.8580, AUC_score: 0.9508\n",
      "Validation loss decreased (0.266010 --> 0.266010).\n",
      "Validation loss decreased (0.265653 --> 0.265653).\n",
      "Epoch 50: Train Loss: 0.2949, Macro_F1: 0.8560, AUC_score: 0.9537\n",
      "Validation loss decreased (0.263624 --> 0.263624).\n",
      "Validation loss decreased (0.261901 --> 0.261901).\n",
      "Validation loss decreased (0.260392 --> 0.260392).\n",
      "Epoch 100: Train Loss: 0.3341, Macro_F1: 0.8588, AUC_score: 0.9543\n",
      "Validation loss decreased (0.257278 --> 0.257278).\n",
      "Epoch 150: Train Loss: 0.2589, Macro_F1: 0.8650, AUC_score: 0.9542\n",
      "Epoch 200: Train Loss: 0.2813, Macro_F1: 0.8623, AUC_score: 0.9548\n",
      "Validation loss decreased (0.255120 --> 0.255120).\n",
      "Epoch 250: Train Loss: 0.3352, Macro_F1: 0.8421, AUC_score: 0.9527\n",
      "Validation loss decreased (0.252187 --> 0.252187).\n",
      "Validation loss decreased (0.249431 --> 0.249431).\n",
      "Epoch 300: Train Loss: 0.3223, Macro_F1: 0.8631, AUC_score: 0.9547\n",
      "Epoch 350: Train Loss: 0.2661, Macro_F1: 0.8616, AUC_score: 0.9542\n",
      "Epoch 00401: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 400: Train Loss: 0.3022, Macro_F1: 0.8619, AUC_score: 0.9548\n",
      "Epoch 450: Train Loss: 0.3011, Macro_F1: 0.8636, AUC_score: 0.9548\n",
      "Validation loss decreased (0.248004 --> 0.248004).\n",
      "Epoch 500: Train Loss: 0.2568, Macro_F1: 0.8648, AUC_score: 0.9554\n",
      "Validation loss decreased (0.243649 --> 0.243649).\n",
      "Epoch 550: Train Loss: 0.2582, Macro_F1: 0.8629, AUC_score: 0.9553\n",
      "Epoch 600: Train Loss: 0.2798, Macro_F1: 0.8653, AUC_score: 0.9555\n",
      "Epoch 00613: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 650: Train Loss: 0.2482, Macro_F1: 0.8646, AUC_score: 0.9554\n",
      "Validation loss decreased (0.242402 --> 0.242402).\n",
      "Validation loss decreased (0.239869 --> 0.239869).\n",
      "Epoch 700: Train Loss: 0.2673, Macro_F1: 0.8643, AUC_score: 0.9554\n",
      "Epoch 750: Train Loss: 0.2771, Macro_F1: 0.8661, AUC_score: 0.9555\n",
      "Epoch 00765: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 800: Train Loss: 0.2908, Macro_F1: 0.8656, AUC_score: 0.9555\n",
      "Epoch 850: Train Loss: 0.2622, Macro_F1: 0.8651, AUC_score: 0.9555\n",
      "Early stopping triggered\n",
      "acc save\n",
      "83.0% node features transform to 0: F1: 0.8656, AUC_score: 0.9555\n",
      "Epoch 0: Train Loss: 0.2503, Macro_F1: 0.8418, AUC_score: 0.9530\n",
      "Validation loss decreased (0.250251 --> 0.250251).\n",
      "Validation loss decreased (0.242263 --> 0.242263).\n",
      "Epoch 50: Train Loss: 0.2707, Macro_F1: 0.8662, AUC_score: 0.9559\n",
      "Epoch 100: Train Loss: 0.2825, Macro_F1: 0.8513, AUC_score: 0.9547\n",
      "Validation loss decreased (0.240498 --> 0.240498).\n",
      "Epoch 150: Train Loss: 0.2846, Macro_F1: 0.8565, AUC_score: 0.9554\n",
      "Epoch 200: Train Loss: 0.2474, Macro_F1: 0.8660, AUC_score: 0.9564\n",
      "Epoch 00215: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.2615, Macro_F1: 0.8691, AUC_score: 0.9564\n",
      "Validation loss decreased (0.237650 --> 0.237650).\n",
      "Epoch 300: Train Loss: 0.2853, Macro_F1: 0.8635, AUC_score: 0.9561\n",
      "Epoch 350: Train Loss: 0.2723, Macro_F1: 0.8660, AUC_score: 0.9563\n",
      "Epoch 00364: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.2862, Macro_F1: 0.8686, AUC_score: 0.9566\n",
      "Epoch 450: Train Loss: 0.2525, Macro_F1: 0.8694, AUC_score: 0.9566\n",
      "Early stopping triggered\n",
      "acc save\n",
      "82.0% node features transform to 0: F1: 0.8699, AUC_score: 0.9566\n",
      "Epoch 0: Train Loss: 0.2673, Macro_F1: 0.8458, AUC_score: 0.9542\n",
      "Validation loss decreased (0.267300 --> 0.267300).\n",
      "Validation loss decreased (0.265978 --> 0.265978).\n",
      "Validation loss decreased (0.242610 --> 0.242610).\n",
      "Validation loss decreased (0.242469 --> 0.242469).\n",
      "Validation loss decreased (0.241330 --> 0.241330).\n",
      "Epoch 50: Train Loss: 0.2530, Macro_F1: 0.8665, AUC_score: 0.9557\n",
      "Validation loss decreased (0.240835 --> 0.240835).\n",
      "Epoch 100: Train Loss: 0.2502, Macro_F1: 0.8648, AUC_score: 0.9557\n",
      "Validation loss decreased (0.240136 --> 0.240136).\n",
      "Validation loss decreased (0.239796 --> 0.239796).\n",
      "Epoch 150: Train Loss: 0.2561, Macro_F1: 0.8644, AUC_score: 0.9561\n",
      "Epoch 200: Train Loss: 0.2896, Macro_F1: 0.8491, AUC_score: 0.9553\n",
      "Epoch 00216: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.237494 --> 0.237494).\n",
      "Epoch 250: Train Loss: 0.2503, Macro_F1: 0.8673, AUC_score: 0.9568\n",
      "Validation loss decreased (0.232745 --> 0.232745).\n",
      "Epoch 300: Train Loss: 0.2465, Macro_F1: 0.8629, AUC_score: 0.9568\n",
      "Epoch 350: Train Loss: 0.2496, Macro_F1: 0.8672, AUC_score: 0.9563\n",
      "Epoch 00391: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.2650, Macro_F1: 0.8665, AUC_score: 0.9569\n",
      "Epoch 450: Train Loss: 0.3042, Macro_F1: 0.8686, AUC_score: 0.9568\n",
      "Early stopping triggered\n",
      "acc save\n",
      "81.0% node features transform to 0: F1: 0.8665, AUC_score: 0.9569\n",
      "Epoch 0: Train Loss: 0.2566, Macro_F1: 0.8523, AUC_score: 0.9527\n",
      "Validation loss decreased (0.256635 --> 0.256635).\n",
      "Validation loss decreased (0.252562 --> 0.252562).\n",
      "Validation loss decreased (0.245652 --> 0.245652).\n",
      "Epoch 50: Train Loss: 0.2779, Macro_F1: 0.8616, AUC_score: 0.9546\n",
      "Validation loss decreased (0.245556 --> 0.245556).\n",
      "Validation loss decreased (0.242963 --> 0.242963).\n",
      "Validation loss decreased (0.241544 --> 0.241544).\n",
      "Validation loss decreased (0.236532 --> 0.236532).\n",
      "Epoch 100: Train Loss: 0.3344, Macro_F1: 0.8697, AUC_score: 0.9547\n",
      "Validation loss decreased (0.236064 --> 0.236064).\n",
      "Epoch 150: Train Loss: 0.2569, Macro_F1: 0.8589, AUC_score: 0.9557\n",
      "Epoch 200: Train Loss: 0.2872, Macro_F1: 0.8646, AUC_score: 0.9549\n",
      "Epoch 00233: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.2409, Macro_F1: 0.8665, AUC_score: 0.9551\n",
      "Epoch 300: Train Loss: 0.3100, Macro_F1: 0.8651, AUC_score: 0.9557\n",
      "Validation loss decreased (0.235936 --> 0.235936).\n",
      "Validation loss decreased (0.233232 --> 0.233232).\n",
      "Validation loss decreased (0.229031 --> 0.229031).\n",
      "Epoch 350: Train Loss: 0.2708, Macro_F1: 0.8699, AUC_score: 0.9560\n",
      "Epoch 400: Train Loss: 0.2472, Macro_F1: 0.8670, AUC_score: 0.9559\n",
      "Validation loss decreased (0.228138 --> 0.228138).\n",
      "Epoch 450: Train Loss: 0.2486, Macro_F1: 0.8624, AUC_score: 0.9561\n",
      "Validation loss decreased (0.227892 --> 0.227892).\n",
      "Epoch 500: Train Loss: 0.2562, Macro_F1: 0.8671, AUC_score: 0.9561\n",
      "Epoch 550: Train Loss: 0.2339, Macro_F1: 0.8627, AUC_score: 0.9559\n",
      "Epoch 00567: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 600: Train Loss: 0.2645, Macro_F1: 0.8670, AUC_score: 0.9562\n",
      "Epoch 650: Train Loss: 0.2512, Macro_F1: 0.8668, AUC_score: 0.9562\n",
      "Early stopping triggered\n",
      "acc save\n",
      "80.0% node features transform to 0: F1: 0.8655, AUC_score: 0.9562\n",
      "Epoch 0: Train Loss: 0.2582, Macro_F1: 0.8632, AUC_score: 0.9545\n",
      "Validation loss decreased (0.258196 --> 0.258196).\n",
      "Validation loss decreased (0.250441 --> 0.250441).\n",
      "Validation loss decreased (0.240592 --> 0.240592).\n",
      "Validation loss decreased (0.236421 --> 0.236421).\n",
      "Validation loss decreased (0.233014 --> 0.233014).\n",
      "Epoch 50: Train Loss: 0.2592, Macro_F1: 0.8593, AUC_score: 0.9552\n",
      "Validation loss decreased (0.231671 --> 0.231671).\n",
      "Epoch 100: Train Loss: 0.2596, Macro_F1: 0.8670, AUC_score: 0.9566\n",
      "Validation loss decreased (0.230950 --> 0.230950).\n",
      "Validation loss decreased (0.230286 --> 0.230286).\n",
      "Epoch 150: Train Loss: 0.2668, Macro_F1: 0.8621, AUC_score: 0.9563\n",
      "Validation loss decreased (0.229890 --> 0.229890).\n",
      "Validation loss decreased (0.228403 --> 0.228403).\n",
      "Epoch 200: Train Loss: 0.2862, Macro_F1: 0.8559, AUC_score: 0.9548\n",
      "Epoch 250: Train Loss: 0.2417, Macro_F1: 0.8641, AUC_score: 0.9558\n",
      "Epoch 00295: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.2399, Macro_F1: 0.8645, AUC_score: 0.9577\n",
      "Validation loss decreased (0.227675 --> 0.227675).\n",
      "Epoch 350: Train Loss: 0.2275, Macro_F1: 0.8648, AUC_score: 0.9566\n",
      "Validation loss decreased (0.227454 --> 0.227454).\n",
      "Validation loss decreased (0.226541 --> 0.226541).\n",
      "Epoch 400: Train Loss: 0.2304, Macro_F1: 0.8665, AUC_score: 0.9564\n",
      "Validation loss decreased (0.225088 --> 0.225088).\n",
      "Epoch 450: Train Loss: 0.2396, Macro_F1: 0.8616, AUC_score: 0.9568\n",
      "Validation loss decreased (0.222012 --> 0.222012).\n",
      "Epoch 500: Train Loss: 0.2619, Macro_F1: 0.8657, AUC_score: 0.9569\n",
      "Validation loss decreased (0.218573 --> 0.218573).\n",
      "Epoch 550: Train Loss: 0.2304, Macro_F1: 0.8597, AUC_score: 0.9570\n",
      "Epoch 600: Train Loss: 0.2405, Macro_F1: 0.8607, AUC_score: 0.9569\n",
      "Epoch 00627: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 650: Train Loss: 0.2369, Macro_F1: 0.8619, AUC_score: 0.9573\n",
      "Epoch 700: Train Loss: 0.2338, Macro_F1: 0.8616, AUC_score: 0.9572\n",
      "Early stopping triggered\n",
      "acc save\n",
      "79.0% node features transform to 0: F1: 0.8627, AUC_score: 0.9573\n",
      "Epoch 0: Train Loss: 0.2527, Macro_F1: 0.8463, AUC_score: 0.9541\n",
      "Validation loss decreased (0.252666 --> 0.252666).\n",
      "Validation loss decreased (0.240216 --> 0.240216).\n",
      "Validation loss decreased (0.235220 --> 0.235220).\n",
      "Validation loss decreased (0.234070 --> 0.234070).\n",
      "Validation loss decreased (0.230095 --> 0.230095).\n",
      "Validation loss decreased (0.227613 --> 0.227613).\n",
      "Epoch 50: Train Loss: 0.2652, Macro_F1: 0.8611, AUC_score: 0.9561\n",
      "Validation loss decreased (0.223080 --> 0.223080).\n",
      "Epoch 100: Train Loss: 0.2397, Macro_F1: 0.8610, AUC_score: 0.9556\n",
      "Validation loss decreased (0.222125 --> 0.222125).\n",
      "Epoch 150: Train Loss: 0.2672, Macro_F1: 0.8491, AUC_score: 0.9557\n",
      "Epoch 200: Train Loss: 0.2583, Macro_F1: 0.8590, AUC_score: 0.9552\n",
      "Epoch 00212: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.2435, Macro_F1: 0.8670, AUC_score: 0.9565\n",
      "Validation loss decreased (0.220645 --> 0.220645).\n",
      "Validation loss decreased (0.219851 --> 0.219851).\n",
      "Epoch 300: Train Loss: 0.2304, Macro_F1: 0.8617, AUC_score: 0.9568\n",
      "Validation loss decreased (0.217289 --> 0.217289).\n",
      "Validation loss decreased (0.217126 --> 0.217126).\n",
      "Epoch 350: Train Loss: 0.2285, Macro_F1: 0.8642, AUC_score: 0.9566\n",
      "Epoch 400: Train Loss: 0.2787, Macro_F1: 0.8626, AUC_score: 0.9561\n",
      "Epoch 00450: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.2616, Macro_F1: 0.8629, AUC_score: 0.9566\n",
      "Epoch 500: Train Loss: 0.2351, Macro_F1: 0.8654, AUC_score: 0.9565\n",
      "Early stopping triggered\n",
      "acc save\n",
      "78.0% node features transform to 0: F1: 0.8622, AUC_score: 0.9566\n",
      "Epoch 0: Train Loss: 0.2340, Macro_F1: 0.8582, AUC_score: 0.9542\n",
      "Validation loss decreased (0.233951 --> 0.233951).\n",
      "Validation loss decreased (0.230953 --> 0.230953).\n",
      "Epoch 50: Train Loss: 0.2432, Macro_F1: 0.8585, AUC_score: 0.9566\n",
      "Validation loss decreased (0.230913 --> 0.230913).\n",
      "Validation loss decreased (0.219761 --> 0.219761).\n",
      "Epoch 100: Train Loss: 0.2561, Macro_F1: 0.8621, AUC_score: 0.9570\n",
      "Epoch 150: Train Loss: 0.2375, Macro_F1: 0.8609, AUC_score: 0.9562\n",
      "Epoch 00183: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.2384, Macro_F1: 0.8600, AUC_score: 0.9572\n",
      "Validation loss decreased (0.215721 --> 0.215721).\n",
      "Epoch 250: Train Loss: 0.2236, Macro_F1: 0.8622, AUC_score: 0.9569\n",
      "Epoch 300: Train Loss: 0.2643, Macro_F1: 0.8615, AUC_score: 0.9570\n",
      "Epoch 00348: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.2301, Macro_F1: 0.8580, AUC_score: 0.9571\n",
      "Epoch 400: Train Loss: 0.2270, Macro_F1: 0.8605, AUC_score: 0.9573\n",
      "Early stopping triggered\n",
      "acc save\n",
      "77.0% node features transform to 0: F1: 0.8611, AUC_score: 0.9573\n",
      "Epoch 0: Train Loss: 0.2229, Macro_F1: 0.8485, AUC_score: 0.9536\n",
      "Validation loss decreased (0.222912 --> 0.222912).\n",
      "Epoch 50: Train Loss: 0.2376, Macro_F1: 0.8648, AUC_score: 0.9563\n",
      "Validation loss decreased (0.219084 --> 0.219084).\n",
      "Validation loss decreased (0.217232 --> 0.217232).\n",
      "Epoch 100: Train Loss: 0.2332, Macro_F1: 0.8692, AUC_score: 0.9560\n",
      "Epoch 150: Train Loss: 0.2508, Macro_F1: 0.8642, AUC_score: 0.9547\n",
      "Epoch 00176: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.2318, Macro_F1: 0.8619, AUC_score: 0.9561\n",
      "Validation loss decreased (0.215386 --> 0.215386).\n",
      "Epoch 250: Train Loss: 0.3018, Macro_F1: 0.8618, AUC_score: 0.9561\n",
      "Validation loss decreased (0.208567 --> 0.208567).\n",
      "Epoch 300: Train Loss: 0.2568, Macro_F1: 0.8629, AUC_score: 0.9563\n",
      "Epoch 350: Train Loss: 0.2724, Macro_F1: 0.8618, AUC_score: 0.9560\n",
      "Epoch 00383: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.2514, Macro_F1: 0.8619, AUC_score: 0.9563\n",
      "Epoch 450: Train Loss: 0.2680, Macro_F1: 0.8630, AUC_score: 0.9562\n",
      "Early stopping triggered\n",
      "acc save\n",
      "76.0% node features transform to 0: F1: 0.8619, AUC_score: 0.9563\n",
      "Epoch 0: Train Loss: 0.2333, Macro_F1: 0.8570, AUC_score: 0.9518\n",
      "Validation loss decreased (0.233303 --> 0.233303).\n",
      "Validation loss decreased (0.232091 --> 0.232091).\n",
      "Validation loss decreased (0.230067 --> 0.230067).\n",
      "Validation loss decreased (0.222515 --> 0.222515).\n",
      "Validation loss decreased (0.220802 --> 0.220802).\n",
      "Epoch 50: Train Loss: 0.2308, Macro_F1: 0.8635, AUC_score: 0.9546\n",
      "Validation loss decreased (0.218463 --> 0.218463).\n",
      "Epoch 100: Train Loss: 0.3054, Macro_F1: 0.8550, AUC_score: 0.9526\n",
      "Validation loss decreased (0.216706 --> 0.216706).\n",
      "Epoch 150: Train Loss: 0.2288, Macro_F1: 0.8622, AUC_score: 0.9538\n",
      "Epoch 200: Train Loss: 0.2256, Macro_F1: 0.8670, AUC_score: 0.9547\n",
      "Epoch 00223: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.216190 --> 0.216190).\n",
      "Epoch 250: Train Loss: 0.2253, Macro_F1: 0.8619, AUC_score: 0.9557\n",
      "Validation loss decreased (0.215989 --> 0.215989).\n",
      "Epoch 300: Train Loss: 0.2240, Macro_F1: 0.8613, AUC_score: 0.9555\n",
      "Validation loss decreased (0.214260 --> 0.214260).\n",
      "Epoch 350: Train Loss: 0.2446, Macro_F1: 0.8604, AUC_score: 0.9551\n",
      "Validation loss decreased (0.209814 --> 0.209814).\n",
      "Epoch 400: Train Loss: 0.2310, Macro_F1: 0.8627, AUC_score: 0.9554\n",
      "Validation loss decreased (0.208148 --> 0.208148).\n",
      "Epoch 450: Train Loss: 0.2316, Macro_F1: 0.8638, AUC_score: 0.9554\n",
      "Epoch 500: Train Loss: 0.2109, Macro_F1: 0.8619, AUC_score: 0.9555\n",
      "Epoch 00509: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 550: Train Loss: 0.2411, Macro_F1: 0.8622, AUC_score: 0.9555\n",
      "Epoch 600: Train Loss: 0.2260, Macro_F1: 0.8635, AUC_score: 0.9554\n",
      "Early stopping triggered\n",
      "acc save\n",
      "75.0% node features transform to 0: F1: 0.8627, AUC_score: 0.9554\n",
      "Epoch 0: Train Loss: 0.2309, Macro_F1: 0.8610, AUC_score: 0.9543\n",
      "Validation loss decreased (0.230875 --> 0.230875).\n",
      "Validation loss decreased (0.221727 --> 0.221727).\n",
      "Validation loss decreased (0.212895 --> 0.212895).\n",
      "Epoch 50: Train Loss: 0.2228, Macro_F1: 0.8669, AUC_score: 0.9548\n",
      "Validation loss decreased (0.209831 --> 0.209831).\n",
      "Epoch 100: Train Loss: 0.2486, Macro_F1: 0.8669, AUC_score: 0.9552\n",
      "Epoch 150: Train Loss: 0.2215, Macro_F1: 0.8628, AUC_score: 0.9540\n",
      "Epoch 00156: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.2293, Macro_F1: 0.8643, AUC_score: 0.9554\n",
      "Epoch 250: Train Loss: 0.2266, Macro_F1: 0.8689, AUC_score: 0.9544\n",
      "Early stopping triggered\n",
      "acc save\n",
      "74.0% node features transform to 0: F1: 0.8665, AUC_score: 0.9546\n",
      "Epoch 0: Train Loss: 0.2571, Macro_F1: 0.8511, AUC_score: 0.9518\n",
      "Validation loss decreased (0.257130 --> 0.257130).\n",
      "Validation loss decreased (0.254808 --> 0.254808).\n",
      "Validation loss decreased (0.235348 --> 0.235348).\n",
      "Validation loss decreased (0.216904 --> 0.216904).\n",
      "Validation loss decreased (0.214955 --> 0.214955).\n",
      "Validation loss decreased (0.214867 --> 0.214867).\n",
      "Epoch 50: Train Loss: 0.2247, Macro_F1: 0.8588, AUC_score: 0.9556\n",
      "Validation loss decreased (0.214830 --> 0.214830).\n",
      "Epoch 100: Train Loss: 0.2323, Macro_F1: 0.8699, AUC_score: 0.9550\n",
      "Validation loss decreased (0.210168 --> 0.210168).\n",
      "Validation loss decreased (0.209372 --> 0.209372).\n",
      "Epoch 150: Train Loss: 0.2140, Macro_F1: 0.8641, AUC_score: 0.9549\n",
      "Epoch 200: Train Loss: 0.2077, Macro_F1: 0.8600, AUC_score: 0.9533\n",
      "Validation loss decreased (0.207667 --> 0.207667).\n",
      "Epoch 250: Train Loss: 0.2294, Macro_F1: 0.8666, AUC_score: 0.9543\n",
      "Epoch 300: Train Loss: 0.2160, Macro_F1: 0.8699, AUC_score: 0.9523\n",
      "Epoch 00302: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.207512 --> 0.207512).\n",
      "Epoch 350: Train Loss: 0.2393, Macro_F1: 0.8654, AUC_score: 0.9547\n",
      "Validation loss decreased (0.206982 --> 0.206982).\n",
      "Epoch 400: Train Loss: 0.2160, Macro_F1: 0.8651, AUC_score: 0.9549\n",
      "Epoch 450: Train Loss: 0.2110, Macro_F1: 0.8638, AUC_score: 0.9548\n",
      "Validation loss decreased (0.206091 --> 0.206091).\n",
      "Validation loss decreased (0.205419 --> 0.205419).\n",
      "Epoch 500: Train Loss: 0.2113, Macro_F1: 0.8640, AUC_score: 0.9550\n",
      "Validation loss decreased (0.201864 --> 0.201864).\n",
      "Epoch 550: Train Loss: 0.2240, Macro_F1: 0.8645, AUC_score: 0.9542\n",
      "Epoch 600: Train Loss: 0.2215, Macro_F1: 0.8638, AUC_score: 0.9547\n",
      "Epoch 00623: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.201779 --> 0.201779).\n",
      "Epoch 650: Train Loss: 0.2220, Macro_F1: 0.8632, AUC_score: 0.9551\n",
      "Epoch 700: Train Loss: 0.2426, Macro_F1: 0.8624, AUC_score: 0.9551\n",
      "Epoch 00729: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 750: Train Loss: 0.2200, Macro_F1: 0.8644, AUC_score: 0.9552\n",
      "Validation loss decreased (0.201073 --> 0.201073).\n",
      "Epoch 800: Train Loss: 0.2120, Macro_F1: 0.8627, AUC_score: 0.9551\n",
      "Epoch 850: Train Loss: 0.2344, Macro_F1: 0.8660, AUC_score: 0.9551\n",
      "Epoch 00866: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 900: Train Loss: 0.2074, Macro_F1: 0.8652, AUC_score: 0.9551\n",
      "Epoch 950: Train Loss: 0.2261, Macro_F1: 0.8641, AUC_score: 0.9551\n",
      "Early stopping triggered\n",
      "acc save\n",
      "73.0% node features transform to 0: F1: 0.8633, AUC_score: 0.9551\n",
      "Epoch 0: Train Loss: 0.2470, Macro_F1: 0.8464, AUC_score: 0.9469\n",
      "Validation loss decreased (0.247034 --> 0.247034).\n",
      "Validation loss decreased (0.232533 --> 0.232533).\n",
      "Validation loss decreased (0.228904 --> 0.228904).\n",
      "Validation loss decreased (0.219771 --> 0.219771).\n",
      "Epoch 50: Train Loss: 0.2451, Macro_F1: 0.8622, AUC_score: 0.9549\n",
      "Validation loss decreased (0.213524 --> 0.213524).\n",
      "Epoch 100: Train Loss: 0.2278, Macro_F1: 0.8654, AUC_score: 0.9537\n",
      "Validation loss decreased (0.210051 --> 0.210051).\n",
      "Validation loss decreased (0.204591 --> 0.204591).\n",
      "Epoch 150: Train Loss: 0.2245, Macro_F1: 0.8636, AUC_score: 0.9533\n",
      "Epoch 200: Train Loss: 0.2122, Macro_F1: 0.8754, AUC_score: 0.9536\n",
      "Epoch 00233: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.2184, Macro_F1: 0.8692, AUC_score: 0.9539\n",
      "Validation loss decreased (0.201673 --> 0.201673).\n",
      "Epoch 300: Train Loss: 0.2154, Macro_F1: 0.8675, AUC_score: 0.9536\n",
      "Epoch 350: Train Loss: 0.2231, Macro_F1: 0.8673, AUC_score: 0.9546\n",
      "Epoch 00358: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.200635 --> 0.200635).\n",
      "Epoch 400: Train Loss: 0.2191, Macro_F1: 0.8676, AUC_score: 0.9549\n",
      "Epoch 450: Train Loss: 0.2272, Macro_F1: 0.8697, AUC_score: 0.9547\n",
      "Validation loss decreased (0.197687 --> 0.197687).\n",
      "Epoch 500: Train Loss: 0.2656, Macro_F1: 0.8671, AUC_score: 0.9547\n",
      "Epoch 550: Train Loss: 0.2350, Macro_F1: 0.8694, AUC_score: 0.9547\n",
      "Epoch 00590: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 600: Train Loss: 0.2242, Macro_F1: 0.8700, AUC_score: 0.9547\n",
      "Epoch 650: Train Loss: 0.2147, Macro_F1: 0.8700, AUC_score: 0.9548\n",
      "Early stopping triggered\n",
      "acc save\n",
      "72.0% node features transform to 0: F1: 0.8687, AUC_score: 0.9549\n",
      "Epoch 0: Train Loss: 0.2764, Macro_F1: 0.8501, AUC_score: 0.9518\n",
      "Validation loss decreased (0.276379 --> 0.276379).\n",
      "Validation loss decreased (0.273698 --> 0.273698).\n",
      "Validation loss decreased (0.241907 --> 0.241907).\n",
      "Validation loss decreased (0.231773 --> 0.231773).\n",
      "Validation loss decreased (0.218577 --> 0.218577).\n",
      "Validation loss decreased (0.218472 --> 0.218472).\n",
      "Validation loss decreased (0.206030 --> 0.206030).\n",
      "Epoch 50: Train Loss: 0.2172, Macro_F1: 0.8651, AUC_score: 0.9535\n",
      "Epoch 100: Train Loss: 0.2145, Macro_F1: 0.8706, AUC_score: 0.9539\n",
      "Epoch 00121: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.2155, Macro_F1: 0.8665, AUC_score: 0.9544\n",
      "Validation loss decreased (0.204602 --> 0.204602).\n",
      "Validation loss decreased (0.198536 --> 0.198536).\n",
      "Epoch 200: Train Loss: 0.2092, Macro_F1: 0.8711, AUC_score: 0.9548\n",
      "Epoch 250: Train Loss: 0.2353, Macro_F1: 0.8685, AUC_score: 0.9544\n",
      "Epoch 00275: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 300: Train Loss: 0.2117, Macro_F1: 0.8692, AUC_score: 0.9553\n",
      "Epoch 350: Train Loss: 0.2227, Macro_F1: 0.8667, AUC_score: 0.9550\n",
      "Early stopping triggered\n",
      "acc save\n",
      "71.0% node features transform to 0: F1: 0.8710, AUC_score: 0.9551\n",
      "Epoch 0: Train Loss: 0.2135, Macro_F1: 0.8426, AUC_score: 0.9501\n",
      "Validation loss decreased (0.213473 --> 0.213473).\n",
      "Validation loss decreased (0.210493 --> 0.210493).\n",
      "Validation loss decreased (0.201928 --> 0.201928).\n",
      "Epoch 50: Train Loss: 0.2207, Macro_F1: 0.8675, AUC_score: 0.9537\n",
      "Epoch 100: Train Loss: 0.2282, Macro_F1: 0.8673, AUC_score: 0.9537\n",
      "Epoch 00126: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.200553 --> 0.200553).\n",
      "Epoch 150: Train Loss: 0.2200, Macro_F1: 0.8679, AUC_score: 0.9544\n",
      "Epoch 200: Train Loss: 0.2282, Macro_F1: 0.8680, AUC_score: 0.9542\n",
      "Epoch 00249: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 250: Train Loss: 0.2282, Macro_F1: 0.8664, AUC_score: 0.9540\n",
      "Epoch 300: Train Loss: 0.2140, Macro_F1: 0.8700, AUC_score: 0.9547\n",
      "Early stopping triggered\n",
      "acc save\n",
      "70.0% node features transform to 0: F1: 0.8700, AUC_score: 0.9548\n",
      "Epoch 0: Train Loss: 0.2071, Macro_F1: 0.8618, AUC_score: 0.9536\n",
      "Validation loss decreased (0.207055 --> 0.207055).\n",
      "Validation loss decreased (0.206291 --> 0.206291).\n",
      "Epoch 50: Train Loss: 0.2227, Macro_F1: 0.8697, AUC_score: 0.9544\n",
      "Validation loss decreased (0.203221 --> 0.203221).\n",
      "Epoch 100: Train Loss: 0.2317, Macro_F1: 0.8674, AUC_score: 0.9545\n",
      "Validation loss decreased (0.202815 --> 0.202815).\n",
      "Epoch 150: Train Loss: 0.2382, Macro_F1: 0.8585, AUC_score: 0.9514\n",
      "Validation loss decreased (0.201270 --> 0.201270).\n",
      "Epoch 200: Train Loss: 0.2626, Macro_F1: 0.8707, AUC_score: 0.9557\n",
      "Epoch 250: Train Loss: 0.2497, Macro_F1: 0.8743, AUC_score: 0.9535\n",
      "Epoch 00261: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.201113 --> 0.201113).\n",
      "Validation loss decreased (0.197872 --> 0.197872).\n",
      "Epoch 300: Train Loss: 0.2148, Macro_F1: 0.8699, AUC_score: 0.9543\n",
      "Epoch 350: Train Loss: 0.2152, Macro_F1: 0.8705, AUC_score: 0.9541\n",
      "Epoch 00381: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.2092, Macro_F1: 0.8721, AUC_score: 0.9545\n",
      "Validation loss decreased (0.197198 --> 0.197198).\n",
      "Epoch 450: Train Loss: 0.2038, Macro_F1: 0.8700, AUC_score: 0.9548\n",
      "Validation loss decreased (0.195405 --> 0.195405).\n",
      "Validation loss decreased (0.194840 --> 0.194840).\n",
      "Epoch 500: Train Loss: 0.2063, Macro_F1: 0.8699, AUC_score: 0.9542\n",
      "Validation loss decreased (0.190911 --> 0.190911).\n",
      "Epoch 550: Train Loss: 0.2204, Macro_F1: 0.8718, AUC_score: 0.9547\n",
      "Epoch 600: Train Loss: 0.2409, Macro_F1: 0.8719, AUC_score: 0.9549\n",
      "Epoch 00631: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 650: Train Loss: 0.2147, Macro_F1: 0.8707, AUC_score: 0.9545\n",
      "Epoch 700: Train Loss: 0.1939, Macro_F1: 0.8716, AUC_score: 0.9547\n",
      "Early stopping triggered\n",
      "acc save\n",
      "69.0% node features transform to 0: F1: 0.8716, AUC_score: 0.9547\n",
      "Epoch 0: Train Loss: 0.2171, Macro_F1: 0.8476, AUC_score: 0.9506\n",
      "Validation loss decreased (0.217132 --> 0.217132).\n",
      "Validation loss decreased (0.207455 --> 0.207455).\n",
      "Validation loss decreased (0.206814 --> 0.206814).\n",
      "Validation loss decreased (0.205615 --> 0.205615).\n",
      "Validation loss decreased (0.201059 --> 0.201059).\n",
      "Epoch 50: Train Loss: 0.2187, Macro_F1: 0.8637, AUC_score: 0.9517\n",
      "Validation loss decreased (0.200198 --> 0.200198).\n",
      "Validation loss decreased (0.198492 --> 0.198492).\n",
      "Validation loss decreased (0.197577 --> 0.197577).\n",
      "Epoch 100: Train Loss: 0.2132, Macro_F1: 0.8716, AUC_score: 0.9542\n",
      "Epoch 150: Train Loss: 0.2451, Macro_F1: 0.8632, AUC_score: 0.9499\n",
      "Epoch 00190: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.2039, Macro_F1: 0.8699, AUC_score: 0.9524\n",
      "Epoch 250: Train Loss: 0.2267, Macro_F1: 0.8724, AUC_score: 0.9537\n",
      "Early stopping triggered\n",
      "acc save\n",
      "68.0% node features transform to 0: F1: 0.8667, AUC_score: 0.9528\n",
      "Epoch 0: Train Loss: 0.2094, Macro_F1: 0.8573, AUC_score: 0.9519\n",
      "Validation loss decreased (0.209442 --> 0.209442).\n",
      "Validation loss decreased (0.207708 --> 0.207708).\n",
      "Validation loss decreased (0.204733 --> 0.204733).\n",
      "Validation loss decreased (0.203226 --> 0.203226).\n",
      "Epoch 50: Train Loss: 0.2131, Macro_F1: 0.8639, AUC_score: 0.9522\n",
      "Validation loss decreased (0.201874 --> 0.201874).\n",
      "Validation loss decreased (0.194063 --> 0.194063).\n",
      "Validation loss decreased (0.193155 --> 0.193155).\n",
      "Epoch 100: Train Loss: 0.2102, Macro_F1: 0.8677, AUC_score: 0.9529\n",
      "Epoch 150: Train Loss: 0.3460, Macro_F1: 0.8721, AUC_score: 0.9532\n",
      "Epoch 00172: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.189560 --> 0.189560).\n",
      "Epoch 200: Train Loss: 0.2156, Macro_F1: 0.8672, AUC_score: 0.9532\n",
      "Epoch 250: Train Loss: 0.2079, Macro_F1: 0.8691, AUC_score: 0.9529\n",
      "Epoch 00291: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 300: Train Loss: 0.1963, Macro_F1: 0.8675, AUC_score: 0.9527\n",
      "Epoch 350: Train Loss: 0.2065, Macro_F1: 0.8697, AUC_score: 0.9536\n",
      "Early stopping triggered\n",
      "acc save\n",
      "67.0% node features transform to 0: F1: 0.8721, AUC_score: 0.9536\n",
      "Epoch 0: Train Loss: 0.2002, Macro_F1: 0.8676, AUC_score: 0.9504\n",
      "Validation loss decreased (0.200217 --> 0.200217).\n",
      "Validation loss decreased (0.196511 --> 0.196511).\n",
      "Epoch 50: Train Loss: 0.2125, Macro_F1: 0.8680, AUC_score: 0.9524\n",
      "Epoch 100: Train Loss: 0.1961, Macro_F1: 0.8686, AUC_score: 0.9525\n",
      "Validation loss decreased (0.196144 --> 0.196144).\n",
      "Validation loss decreased (0.195898 --> 0.195898).\n",
      "Validation loss decreased (0.188892 --> 0.188892).\n",
      "Epoch 150: Train Loss: 0.2022, Macro_F1: 0.8658, AUC_score: 0.9520\n",
      "Epoch 200: Train Loss: 0.2503, Macro_F1: 0.8746, AUC_score: 0.9531\n",
      "Epoch 00235: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.2309, Macro_F1: 0.8703, AUC_score: 0.9535\n",
      "Epoch 300: Train Loss: 0.1950, Macro_F1: 0.8721, AUC_score: 0.9538\n",
      "Early stopping triggered\n",
      "acc save\n",
      "65.99999999999999% node features transform to 0: F1: 0.8725, AUC_score: 0.9539\n",
      "Epoch 0: Train Loss: 0.2617, Macro_F1: 0.8533, AUC_score: 0.9506\n",
      "Validation loss decreased (0.261711 --> 0.261711).\n",
      "Validation loss decreased (0.196501 --> 0.196501).\n",
      "Epoch 50: Train Loss: 0.2491, Macro_F1: 0.8699, AUC_score: 0.9530\n",
      "Validation loss decreased (0.195998 --> 0.195998).\n",
      "Validation loss decreased (0.193802 --> 0.193802).\n",
      "Epoch 100: Train Loss: 0.2070, Macro_F1: 0.8633, AUC_score: 0.9498\n",
      "Validation loss decreased (0.193741 --> 0.193741).\n",
      "Validation loss decreased (0.193300 --> 0.193300).\n",
      "Epoch 150: Train Loss: 0.2030, Macro_F1: 0.8671, AUC_score: 0.9511\n",
      "Validation loss decreased (0.192510 --> 0.192510).\n",
      "Epoch 200: Train Loss: 0.1976, Macro_F1: 0.8735, AUC_score: 0.9538\n",
      "Validation loss decreased (0.185309 --> 0.185309).\n",
      "Epoch 250: Train Loss: 0.2104, Macro_F1: 0.8688, AUC_score: 0.9541\n",
      "Validation loss decreased (0.184190 --> 0.184190).\n",
      "Epoch 300: Train Loss: 0.2187, Macro_F1: 0.8689, AUC_score: 0.9515\n",
      "Epoch 350: Train Loss: 0.2230, Macro_F1: 0.8692, AUC_score: 0.9522\n",
      "Epoch 00368: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 400: Train Loss: 0.2148, Macro_F1: 0.8732, AUC_score: 0.9538\n",
      "Epoch 450: Train Loss: 0.2154, Macro_F1: 0.8743, AUC_score: 0.9539\n",
      "Early stopping triggered\n",
      "acc save\n",
      "64.99999999999999% node features transform to 0: F1: 0.8724, AUC_score: 0.9533\n",
      "Epoch 0: Train Loss: 0.2040, Macro_F1: 0.8680, AUC_score: 0.9520\n",
      "Validation loss decreased (0.204014 --> 0.204014).\n",
      "Validation loss decreased (0.199986 --> 0.199986).\n",
      "Validation loss decreased (0.194211 --> 0.194211).\n",
      "Epoch 50: Train Loss: 0.1954, Macro_F1: 0.8705, AUC_score: 0.9531\n",
      "Validation loss decreased (0.193018 --> 0.193018).\n",
      "Validation loss decreased (0.189615 --> 0.189615).\n",
      "Epoch 100: Train Loss: 0.2046, Macro_F1: 0.8677, AUC_score: 0.9536\n",
      "Validation loss decreased (0.186990 --> 0.186990).\n",
      "Epoch 150: Train Loss: 0.2048, Macro_F1: 0.8682, AUC_score: 0.9519\n",
      "Validation loss decreased (0.184192 --> 0.184192).\n",
      "Epoch 200: Train Loss: 0.2396, Macro_F1: 0.8646, AUC_score: 0.9506\n",
      "Epoch 250: Train Loss: 0.2044, Macro_F1: 0.8668, AUC_score: 0.9532\n",
      "Epoch 00268: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.2286, Macro_F1: 0.8726, AUC_score: 0.9540\n",
      "Validation loss decreased (0.183625 --> 0.183625).\n",
      "Epoch 350: Train Loss: 0.1984, Macro_F1: 0.8748, AUC_score: 0.9539\n",
      "Validation loss decreased (0.181946 --> 0.181946).\n",
      "Epoch 400: Train Loss: 0.2183, Macro_F1: 0.8727, AUC_score: 0.9539\n",
      "Validation loss decreased (0.180967 --> 0.180967).\n",
      "Epoch 450: Train Loss: 0.1889, Macro_F1: 0.8732, AUC_score: 0.9539\n",
      "Epoch 500: Train Loss: 0.2144, Macro_F1: 0.8727, AUC_score: 0.9536\n",
      "Validation loss decreased (0.179567 --> 0.179567).\n",
      "Epoch 550: Train Loss: 0.2023, Macro_F1: 0.8748, AUC_score: 0.9539\n",
      "Epoch 600: Train Loss: 0.2378, Macro_F1: 0.8724, AUC_score: 0.9538\n",
      "Epoch 00614: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 650: Train Loss: 0.1915, Macro_F1: 0.8737, AUC_score: 0.9536\n",
      "Epoch 700: Train Loss: 0.2012, Macro_F1: 0.8732, AUC_score: 0.9536\n",
      "Early stopping triggered\n",
      "acc save\n",
      "64.0% node features transform to 0: F1: 0.8716, AUC_score: 0.9537\n",
      "Epoch 0: Train Loss: 0.2239, Macro_F1: 0.8459, AUC_score: 0.9455\n",
      "Validation loss decreased (0.223867 --> 0.223867).\n",
      "Validation loss decreased (0.189611 --> 0.189611).\n",
      "Validation loss decreased (0.189042 --> 0.189042).\n",
      "Epoch 50: Train Loss: 0.1944, Macro_F1: 0.8727, AUC_score: 0.9532\n",
      "Validation loss decreased (0.188911 --> 0.188911).\n",
      "Validation loss decreased (0.181367 --> 0.181367).\n",
      "Epoch 100: Train Loss: 0.2039, Macro_F1: 0.8721, AUC_score: 0.9538\n",
      "Validation loss decreased (0.179341 --> 0.179341).\n",
      "Epoch 150: Train Loss: 0.2828, Macro_F1: 0.8721, AUC_score: 0.9526\n",
      "Epoch 200: Train Loss: 0.2073, Macro_F1: 0.8740, AUC_score: 0.9531\n",
      "Epoch 00210: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.2036, Macro_F1: 0.8721, AUC_score: 0.9528\n",
      "Epoch 300: Train Loss: 0.1845, Macro_F1: 0.8722, AUC_score: 0.9526\n",
      "Early stopping triggered\n",
      "acc save\n",
      "63.0% node features transform to 0: F1: 0.8746, AUC_score: 0.9531\n",
      "Epoch 0: Train Loss: 0.1895, Macro_F1: 0.8531, AUC_score: 0.9493\n",
      "Validation loss decreased (0.189532 --> 0.189532).\n",
      "Validation loss decreased (0.186722 --> 0.186722).\n",
      "Validation loss decreased (0.186302 --> 0.186302).\n",
      "Epoch 50: Train Loss: 0.2633, Macro_F1: 0.8707, AUC_score: 0.9519\n",
      "Validation loss decreased (0.183104 --> 0.183104).\n",
      "Epoch 100: Train Loss: 0.2505, Macro_F1: 0.8609, AUC_score: 0.9513\n",
      "Validation loss decreased (0.181057 --> 0.181057).\n",
      "Epoch 150: Train Loss: 0.1940, Macro_F1: 0.8638, AUC_score: 0.9522\n",
      "Validation loss decreased (0.179402 --> 0.179402).\n",
      "Epoch 200: Train Loss: 0.3667, Macro_F1: 0.8718, AUC_score: 0.9516\n",
      "Epoch 250: Train Loss: 0.1871, Macro_F1: 0.8709, AUC_score: 0.9528\n",
      "Epoch 00295: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.2791, Macro_F1: 0.8707, AUC_score: 0.9510\n",
      "Epoch 350: Train Loss: 0.1949, Macro_F1: 0.8721, AUC_score: 0.9531\n",
      "Validation loss decreased (0.177604 --> 0.177604).\n",
      "Validation loss decreased (0.177509 --> 0.177509).\n",
      "Epoch 400: Train Loss: 0.1800, Macro_F1: 0.8708, AUC_score: 0.9533\n",
      "Validation loss decreased (0.176001 --> 0.176001).\n",
      "Epoch 450: Train Loss: 0.2056, Macro_F1: 0.8705, AUC_score: 0.9534\n",
      "Validation loss decreased (0.175109 --> 0.175109).\n",
      "Validation loss decreased (0.174740 --> 0.174740).\n",
      "Epoch 500: Train Loss: 0.1870, Macro_F1: 0.8699, AUC_score: 0.9531\n",
      "Epoch 550: Train Loss: 0.2021, Macro_F1: 0.8702, AUC_score: 0.9533\n",
      "Epoch 00599: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 600: Train Loss: 0.2140, Macro_F1: 0.8705, AUC_score: 0.9527\n",
      "Validation loss decreased (0.173502 --> 0.173502).\n",
      "Epoch 650: Train Loss: 0.1893, Macro_F1: 0.8759, AUC_score: 0.9534\n",
      "Epoch 700: Train Loss: 0.1860, Macro_F1: 0.8705, AUC_score: 0.9532\n",
      "Epoch 00712: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 750: Train Loss: 0.1875, Macro_F1: 0.8729, AUC_score: 0.9533\n",
      "Epoch 800: Train Loss: 0.1799, Macro_F1: 0.8724, AUC_score: 0.9533\n",
      "Early stopping triggered\n",
      "acc save\n",
      "62.0% node features transform to 0: F1: 0.8732, AUC_score: 0.9533\n",
      "Epoch 0: Train Loss: 0.2002, Macro_F1: 0.8335, AUC_score: 0.9485\n",
      "Validation loss decreased (0.200230 --> 0.200230).\n",
      "Validation loss decreased (0.194401 --> 0.194401).\n",
      "Validation loss decreased (0.184547 --> 0.184547).\n",
      "Validation loss decreased (0.182540 --> 0.182540).\n",
      "Epoch 50: Train Loss: 0.1983, Macro_F1: 0.8667, AUC_score: 0.9515\n",
      "Validation loss decreased (0.182280 --> 0.182280).\n",
      "Validation loss decreased (0.175876 --> 0.175876).\n",
      "Epoch 100: Train Loss: 0.2160, Macro_F1: 0.8607, AUC_score: 0.9515\n",
      "Epoch 150: Train Loss: 0.2128, Macro_F1: 0.8672, AUC_score: 0.9518\n",
      "Epoch 00180: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.175779 --> 0.175779).\n",
      "Epoch 200: Train Loss: 0.1925, Macro_F1: 0.8714, AUC_score: 0.9529\n",
      "Epoch 250: Train Loss: 0.1841, Macro_F1: 0.8734, AUC_score: 0.9529\n",
      "Epoch 00294: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 300: Train Loss: 0.1826, Macro_F1: 0.8702, AUC_score: 0.9523\n",
      "Validation loss decreased (0.172344 --> 0.172344).\n",
      "Epoch 350: Train Loss: 0.1893, Macro_F1: 0.8721, AUC_score: 0.9528\n",
      "Epoch 400: Train Loss: 0.1997, Macro_F1: 0.8713, AUC_score: 0.9528\n",
      "Epoch 00428: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 450: Train Loss: 0.2011, Macro_F1: 0.8707, AUC_score: 0.9529\n",
      "Epoch 500: Train Loss: 0.1733, Macro_F1: 0.8707, AUC_score: 0.9528\n",
      "Early stopping triggered\n",
      "acc save\n",
      "61.0% node features transform to 0: F1: 0.8707, AUC_score: 0.9528\n",
      "Epoch 0: Train Loss: 0.1931, Macro_F1: 0.8345, AUC_score: 0.9435\n",
      "Validation loss decreased (0.193147 --> 0.193147).\n",
      "Validation loss decreased (0.187694 --> 0.187694).\n",
      "Validation loss decreased (0.186440 --> 0.186440).\n",
      "Validation loss decreased (0.182955 --> 0.182955).\n",
      "Validation loss decreased (0.181665 --> 0.181665).\n",
      "Validation loss decreased (0.179258 --> 0.179258).\n",
      "Epoch 50: Train Loss: 0.2068, Macro_F1: 0.8634, AUC_score: 0.9522\n",
      "Validation loss decreased (0.177110 --> 0.177110).\n",
      "Epoch 100: Train Loss: 0.2976, Macro_F1: 0.8662, AUC_score: 0.9524\n",
      "Epoch 150: Train Loss: 0.2198, Macro_F1: 0.8675, AUC_score: 0.9529\n",
      "Epoch 00194: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.2700, Macro_F1: 0.8672, AUC_score: 0.9535\n",
      "Validation loss decreased (0.176312 --> 0.176312).\n",
      "Validation loss decreased (0.175698 --> 0.175698).\n",
      "Epoch 250: Train Loss: 0.1913, Macro_F1: 0.8718, AUC_score: 0.9532\n",
      "Validation loss decreased (0.174365 --> 0.174365).\n",
      "Epoch 300: Train Loss: 0.1946, Macro_F1: 0.8675, AUC_score: 0.9532\n",
      "Validation loss decreased (0.174275 --> 0.174275).\n",
      "Validation loss decreased (0.172555 --> 0.172555).\n",
      "Epoch 350: Train Loss: 0.1941, Macro_F1: 0.8721, AUC_score: 0.9526\n",
      "Epoch 400: Train Loss: 0.1868, Macro_F1: 0.8694, AUC_score: 0.9530\n",
      "Epoch 00419: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.2241, Macro_F1: 0.8726, AUC_score: 0.9530\n",
      "Epoch 500: Train Loss: 0.1886, Macro_F1: 0.8710, AUC_score: 0.9529\n",
      "Early stopping triggered\n",
      "acc save\n",
      "60.0% node features transform to 0: F1: 0.8710, AUC_score: 0.9528\n",
      "Epoch 0: Train Loss: 0.2014, Macro_F1: 0.8449, AUC_score: 0.9504\n",
      "Validation loss decreased (0.201356 --> 0.201356).\n",
      "Validation loss decreased (0.201154 --> 0.201154).\n",
      "Validation loss decreased (0.197127 --> 0.197127).\n",
      "Validation loss decreased (0.194892 --> 0.194892).\n",
      "Validation loss decreased (0.180729 --> 0.180729).\n",
      "Validation loss decreased (0.180714 --> 0.180714).\n",
      "Validation loss decreased (0.177408 --> 0.177408).\n",
      "Epoch 50: Train Loss: 0.2384, Macro_F1: 0.8695, AUC_score: 0.9537\n",
      "Validation loss decreased (0.174026 --> 0.174026).\n",
      "Epoch 100: Train Loss: 0.1866, Macro_F1: 0.8708, AUC_score: 0.9533\n",
      "Epoch 150: Train Loss: 0.1974, Macro_F1: 0.8691, AUC_score: 0.9535\n",
      "Epoch 00163: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.173391 --> 0.173391).\n",
      "Epoch 200: Train Loss: 0.1787, Macro_F1: 0.8730, AUC_score: 0.9540\n",
      "Validation loss decreased (0.173377 --> 0.173377).\n",
      "Epoch 250: Train Loss: 0.2247, Macro_F1: 0.8754, AUC_score: 0.9538\n",
      "Validation loss decreased (0.170627 --> 0.170627).\n",
      "Epoch 300: Train Loss: 0.1907, Macro_F1: 0.8732, AUC_score: 0.9537\n",
      "Epoch 350: Train Loss: 0.1899, Macro_F1: 0.8688, AUC_score: 0.9534\n",
      "Epoch 00381: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.1851, Macro_F1: 0.8719, AUC_score: 0.9533\n",
      "Epoch 450: Train Loss: 0.1901, Macro_F1: 0.8738, AUC_score: 0.9535\n",
      "Early stopping triggered\n",
      "acc save\n",
      "59.0% node features transform to 0: F1: 0.8709, AUC_score: 0.9536\n",
      "Epoch 0: Train Loss: 0.1945, Macro_F1: 0.8302, AUC_score: 0.9480\n",
      "Validation loss decreased (0.194481 --> 0.194481).\n",
      "Validation loss decreased (0.182023 --> 0.182023).\n",
      "Validation loss decreased (0.181640 --> 0.181640).\n",
      "Validation loss decreased (0.181620 --> 0.181620).\n",
      "Validation loss decreased (0.181331 --> 0.181331).\n",
      "Validation loss decreased (0.177806 --> 0.177806).\n",
      "Epoch 50: Train Loss: 0.1885, Macro_F1: 0.8711, AUC_score: 0.9533\n",
      "Validation loss decreased (0.175394 --> 0.175394).\n",
      "Validation loss decreased (0.173001 --> 0.173001).\n",
      "Epoch 100: Train Loss: 0.1765, Macro_F1: 0.8724, AUC_score: 0.9540\n",
      "Epoch 150: Train Loss: 0.2001, Macro_F1: 0.8713, AUC_score: 0.9535\n",
      "Validation loss decreased (0.172833 --> 0.172833).\n",
      "Validation loss decreased (0.172741 --> 0.172741).\n",
      "Epoch 200: Train Loss: 0.1755, Macro_F1: 0.8705, AUC_score: 0.9532\n",
      "Epoch 250: Train Loss: 0.2199, Macro_F1: 0.8702, AUC_score: 0.9497\n",
      "Epoch 00296: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.1851, Macro_F1: 0.8718, AUC_score: 0.9535\n",
      "Epoch 350: Train Loss: 0.1864, Macro_F1: 0.8671, AUC_score: 0.9531\n",
      "Validation loss decreased (0.172429 --> 0.172429).\n",
      "Epoch 400: Train Loss: 0.1838, Macro_F1: 0.8722, AUC_score: 0.9532\n",
      "Epoch 450: Train Loss: 0.1746, Macro_F1: 0.8719, AUC_score: 0.9535\n",
      "Epoch 00472: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.168373 --> 0.168373).\n",
      "Epoch 500: Train Loss: 0.1710, Macro_F1: 0.8732, AUC_score: 0.9532\n",
      "Validation loss decreased (0.164762 --> 0.164762).\n",
      "Epoch 550: Train Loss: 0.1756, Macro_F1: 0.8724, AUC_score: 0.9533\n",
      "Validation loss decreased (0.163910 --> 0.163910).\n",
      "Epoch 600: Train Loss: 0.1863, Macro_F1: 0.8735, AUC_score: 0.9532\n",
      "Epoch 650: Train Loss: 0.1921, Macro_F1: 0.8740, AUC_score: 0.9531\n",
      "Epoch 00701: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 700: Train Loss: 0.1684, Macro_F1: 0.8743, AUC_score: 0.9530\n",
      "Epoch 750: Train Loss: 0.1894, Macro_F1: 0.8727, AUC_score: 0.9530\n",
      "Early stopping triggered\n",
      "acc save\n",
      "58.00000000000001% node features transform to 0: F1: 0.8727, AUC_score: 0.9531\n",
      "Epoch 0: Train Loss: 0.1877, Macro_F1: 0.8536, AUC_score: 0.9470\n",
      "Validation loss decreased (0.187705 --> 0.187705).\n",
      "Validation loss decreased (0.176129 --> 0.176129).\n",
      "Validation loss decreased (0.175013 --> 0.175013).\n",
      "Epoch 50: Train Loss: 0.2159, Macro_F1: 0.8475, AUC_score: 0.9462\n",
      "Epoch 100: Train Loss: 0.1950, Macro_F1: 0.8684, AUC_score: 0.9524\n",
      "Epoch 00133: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.1990, Macro_F1: 0.8711, AUC_score: 0.9533\n",
      "Epoch 200: Train Loss: 0.1972, Macro_F1: 0.8740, AUC_score: 0.9533\n",
      "Validation loss decreased (0.174030 --> 0.174030).\n",
      "Validation loss decreased (0.172068 --> 0.172068).\n",
      "Epoch 250: Train Loss: 0.1896, Macro_F1: 0.8724, AUC_score: 0.9531\n",
      "Validation loss decreased (0.167064 --> 0.167064).\n",
      "Epoch 300: Train Loss: 0.1810, Macro_F1: 0.8689, AUC_score: 0.9530\n",
      "Epoch 350: Train Loss: 0.1803, Macro_F1: 0.8716, AUC_score: 0.9528\n",
      "Epoch 00353: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.1888, Macro_F1: 0.8727, AUC_score: 0.9530\n",
      "Epoch 450: Train Loss: 0.1982, Macro_F1: 0.8708, AUC_score: 0.9528\n",
      "Early stopping triggered\n",
      "acc save\n",
      "57.00000000000001% node features transform to 0: F1: 0.8708, AUC_score: 0.9528\n",
      "Epoch 0: Train Loss: 0.1803, Macro_F1: 0.8699, AUC_score: 0.9495\n",
      "Validation loss decreased (0.180306 --> 0.180306).\n",
      "Epoch 50: Train Loss: 0.1984, Macro_F1: 0.8700, AUC_score: 0.9519\n",
      "Validation loss decreased (0.176899 --> 0.176899).\n",
      "Validation loss decreased (0.175608 --> 0.175608).\n",
      "Validation loss decreased (0.173584 --> 0.173584).\n",
      "Validation loss decreased (0.172030 --> 0.172030).\n",
      "Epoch 100: Train Loss: 0.1712, Macro_F1: 0.8709, AUC_score: 0.9526\n",
      "Validation loss decreased (0.171166 --> 0.171166).\n",
      "Epoch 150: Train Loss: 0.1922, Macro_F1: 0.8710, AUC_score: 0.9520\n",
      "Epoch 200: Train Loss: 0.1903, Macro_F1: 0.8668, AUC_score: 0.9527\n",
      "Epoch 00202: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.1887, Macro_F1: 0.8733, AUC_score: 0.9528\n",
      "Validation loss decreased (0.168856 --> 0.168856).\n",
      "Epoch 300: Train Loss: 0.1931, Macro_F1: 0.8749, AUC_score: 0.9521\n",
      "Epoch 350: Train Loss: 0.1774, Macro_F1: 0.8730, AUC_score: 0.9523\n",
      "Epoch 00384: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.166953 --> 0.166953).\n",
      "Epoch 400: Train Loss: 0.2162, Macro_F1: 0.8719, AUC_score: 0.9524\n",
      "Epoch 450: Train Loss: 0.1849, Macro_F1: 0.8746, AUC_score: 0.9524\n",
      "Epoch 00495: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Validation loss decreased (0.164347 --> 0.164347).\n",
      "Epoch 500: Train Loss: 0.1945, Macro_F1: 0.8733, AUC_score: 0.9525\n",
      "Epoch 550: Train Loss: 0.1815, Macro_F1: 0.8733, AUC_score: 0.9525\n",
      "Epoch 00600: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 600: Train Loss: 0.1705, Macro_F1: 0.8743, AUC_score: 0.9525\n",
      "Epoch 650: Train Loss: 0.2082, Macro_F1: 0.8749, AUC_score: 0.9525\n",
      "Early stopping triggered\n",
      "acc save\n",
      "56.00000000000001% node features transform to 0: F1: 0.8749, AUC_score: 0.9525\n",
      "Epoch 0: Train Loss: 0.1902, Macro_F1: 0.8362, AUC_score: 0.9484\n",
      "Validation loss decreased (0.190217 --> 0.190217).\n",
      "Validation loss decreased (0.189460 --> 0.189460).\n",
      "Validation loss decreased (0.187362 --> 0.187362).\n",
      "Validation loss decreased (0.183830 --> 0.183830).\n",
      "Validation loss decreased (0.183599 --> 0.183599).\n",
      "Validation loss decreased (0.182705 --> 0.182705).\n",
      "Validation loss decreased (0.170734 --> 0.170734).\n",
      "Validation loss decreased (0.170054 --> 0.170054).\n",
      "Epoch 50: Train Loss: 0.2377, Macro_F1: 0.8722, AUC_score: 0.9520\n",
      "Epoch 100: Train Loss: 0.1875, Macro_F1: 0.8705, AUC_score: 0.9519\n",
      "Epoch 00121: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.1763, Macro_F1: 0.8711, AUC_score: 0.9526\n",
      "Validation loss decreased (0.166833 --> 0.166833).\n",
      "Epoch 200: Train Loss: 0.1818, Macro_F1: 0.8684, AUC_score: 0.9519\n",
      "Epoch 250: Train Loss: 0.2009, Macro_F1: 0.8711, AUC_score: 0.9521\n",
      "Validation loss decreased (0.166007 --> 0.166007).\n",
      "Validation loss decreased (0.165808 --> 0.165808).\n",
      "Epoch 300: Train Loss: 0.1935, Macro_F1: 0.8731, AUC_score: 0.9519\n",
      "Epoch 350: Train Loss: 0.1759, Macro_F1: 0.8742, AUC_score: 0.9518\n",
      "Epoch 00387: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.1775, Macro_F1: 0.8714, AUC_score: 0.9520\n",
      "Validation loss decreased (0.164149 --> 0.164149).\n",
      "Epoch 450: Train Loss: 0.1797, Macro_F1: 0.8717, AUC_score: 0.9519\n",
      "Epoch 500: Train Loss: 0.2029, Macro_F1: 0.8720, AUC_score: 0.9520\n",
      "Validation loss decreased (0.162727 --> 0.162727).\n",
      "Epoch 550: Train Loss: 0.1727, Macro_F1: 0.8736, AUC_score: 0.9519\n",
      "Epoch 600: Train Loss: 0.1803, Macro_F1: 0.8717, AUC_score: 0.9520\n",
      "Epoch 00609: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 650: Train Loss: 0.1755, Macro_F1: 0.8717, AUC_score: 0.9520\n",
      "Epoch 700: Train Loss: 0.2744, Macro_F1: 0.8749, AUC_score: 0.9520\n",
      "Early stopping triggered\n",
      "acc save\n",
      "55.00000000000001% node features transform to 0: F1: 0.8728, AUC_score: 0.9520\n",
      "Epoch 0: Train Loss: 0.1820, Macro_F1: 0.8580, AUC_score: 0.9490\n",
      "Validation loss decreased (0.182038 --> 0.182038).\n",
      "Validation loss decreased (0.179121 --> 0.179121).\n",
      "Validation loss decreased (0.171699 --> 0.171699).\n",
      "Epoch 50: Train Loss: 0.2035, Macro_F1: 0.8741, AUC_score: 0.9541\n",
      "Validation loss decreased (0.168900 --> 0.168900).\n",
      "Epoch 100: Train Loss: 0.1871, Macro_F1: 0.8676, AUC_score: 0.9516\n",
      "Epoch 150: Train Loss: 0.2135, Macro_F1: 0.8639, AUC_score: 0.9516\n",
      "Epoch 00170: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.168655 --> 0.168655).\n",
      "Epoch 200: Train Loss: 0.1729, Macro_F1: 0.8715, AUC_score: 0.9527\n",
      "Validation loss decreased (0.166242 --> 0.166242).\n",
      "Epoch 250: Train Loss: 0.2076, Macro_F1: 0.8684, AUC_score: 0.9528\n",
      "Epoch 300: Train Loss: 0.1858, Macro_F1: 0.8717, AUC_score: 0.9527\n",
      "Validation loss decreased (0.165815 --> 0.165815).\n",
      "Validation loss decreased (0.164689 --> 0.164689).\n",
      "Epoch 350: Train Loss: 0.1706, Macro_F1: 0.8720, AUC_score: 0.9526\n",
      "Validation loss decreased (0.161684 --> 0.161684).\n",
      "Epoch 400: Train Loss: 0.1821, Macro_F1: 0.8712, AUC_score: 0.9525\n",
      "Epoch 450: Train Loss: 0.1748, Macro_F1: 0.8711, AUC_score: 0.9519\n",
      "Epoch 00484: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.2188, Macro_F1: 0.8709, AUC_score: 0.9529\n",
      "Epoch 550: Train Loss: 0.2150, Macro_F1: 0.8714, AUC_score: 0.9527\n",
      "Early stopping triggered\n",
      "acc save\n",
      "54.0% node features transform to 0: F1: 0.8717, AUC_score: 0.9529\n",
      "Epoch 0: Train Loss: 0.1640, Macro_F1: 0.8594, AUC_score: 0.9491\n",
      "Validation loss decreased (0.163975 --> 0.163975).\n",
      "Epoch 50: Train Loss: 0.1940, Macro_F1: 0.8596, AUC_score: 0.9506\n",
      "Epoch 100: Train Loss: 0.2324, Macro_F1: 0.8689, AUC_score: 0.9517\n",
      "Epoch 00102: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.159697 --> 0.159697).\n",
      "Epoch 150: Train Loss: 0.1754, Macro_F1: 0.8701, AUC_score: 0.9521\n",
      "Validation loss decreased (0.159061 --> 0.159061).\n",
      "Epoch 200: Train Loss: 0.1768, Macro_F1: 0.8679, AUC_score: 0.9522\n",
      "Epoch 250: Train Loss: 0.1701, Macro_F1: 0.8725, AUC_score: 0.9525\n",
      "Epoch 00289: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 300: Train Loss: 0.1741, Macro_F1: 0.8680, AUC_score: 0.9526\n",
      "Epoch 350: Train Loss: 0.1922, Macro_F1: 0.8695, AUC_score: 0.9527\n",
      "Early stopping triggered\n",
      "acc save\n",
      "53.0% node features transform to 0: F1: 0.8673, AUC_score: 0.9523\n",
      "Epoch 0: Train Loss: 0.1711, Macro_F1: 0.8609, AUC_score: 0.9481\n",
      "Validation loss decreased (0.171094 --> 0.171094).\n",
      "Validation loss decreased (0.170929 --> 0.170929).\n",
      "Validation loss decreased (0.167965 --> 0.167965).\n",
      "Validation loss decreased (0.164237 --> 0.164237).\n",
      "Epoch 50: Train Loss: 0.2066, Macro_F1: 0.8618, AUC_score: 0.9512\n",
      "Epoch 100: Train Loss: 0.2035, Macro_F1: 0.8681, AUC_score: 0.9532\n",
      "Epoch 00131: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.1863, Macro_F1: 0.8700, AUC_score: 0.9535\n",
      "Validation loss decreased (0.163009 --> 0.163009).\n",
      "Epoch 200: Train Loss: 0.1744, Macro_F1: 0.8626, AUC_score: 0.9523\n",
      "Validation loss decreased (0.160884 --> 0.160884).\n",
      "Epoch 250: Train Loss: 0.1691, Macro_F1: 0.8704, AUC_score: 0.9526\n",
      "Epoch 300: Train Loss: 0.1806, Macro_F1: 0.8714, AUC_score: 0.9526\n",
      "Epoch 00345: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.1686, Macro_F1: 0.8711, AUC_score: 0.9524\n",
      "Validation loss decreased (0.159748 --> 0.159748).\n",
      "Epoch 400: Train Loss: 0.1629, Macro_F1: 0.8706, AUC_score: 0.9524\n",
      "Epoch 450: Train Loss: 0.1675, Macro_F1: 0.8708, AUC_score: 0.9523\n",
      "Epoch 00478: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 500: Train Loss: 0.1848, Macro_F1: 0.8720, AUC_score: 0.9524\n",
      "Validation loss decreased (0.155312 --> 0.155312).\n",
      "Epoch 550: Train Loss: 0.1645, Macro_F1: 0.8706, AUC_score: 0.9525\n",
      "Epoch 600: Train Loss: 0.1749, Macro_F1: 0.8714, AUC_score: 0.9525\n",
      "Epoch 00614: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 650: Train Loss: 0.1825, Macro_F1: 0.8717, AUC_score: 0.9524\n",
      "Epoch 700: Train Loss: 0.1637, Macro_F1: 0.8706, AUC_score: 0.9524\n",
      "Early stopping triggered\n",
      "acc save\n",
      "52.0% node features transform to 0: F1: 0.8706, AUC_score: 0.9524\n",
      "Epoch 0: Train Loss: 0.1766, Macro_F1: 0.8383, AUC_score: 0.9432\n",
      "Validation loss decreased (0.176634 --> 0.176634).\n",
      "Validation loss decreased (0.170741 --> 0.170741).\n",
      "Epoch 50: Train Loss: 0.1916, Macro_F1: 0.8720, AUC_score: 0.9529\n",
      "Validation loss decreased (0.169841 --> 0.169841).\n",
      "Validation loss decreased (0.168443 --> 0.168443).\n",
      "Validation loss decreased (0.166574 --> 0.166574).\n",
      "Epoch 100: Train Loss: 0.1740, Macro_F1: 0.8719, AUC_score: 0.9519\n",
      "Validation loss decreased (0.165907 --> 0.165907).\n",
      "Validation loss decreased (0.159577 --> 0.159577).\n",
      "Epoch 150: Train Loss: 0.1712, Macro_F1: 0.8718, AUC_score: 0.9525\n",
      "Epoch 200: Train Loss: 0.1673, Macro_F1: 0.8652, AUC_score: 0.9522\n",
      "Epoch 00210: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.1964, Macro_F1: 0.8686, AUC_score: 0.9522\n",
      "Validation loss decreased (0.159265 --> 0.159265).\n",
      "Validation loss decreased (0.159139 --> 0.159139).\n",
      "Epoch 300: Train Loss: 0.1706, Macro_F1: 0.8712, AUC_score: 0.9523\n",
      "Validation loss decreased (0.157325 --> 0.157325).\n",
      "Epoch 350: Train Loss: 0.1651, Macro_F1: 0.8733, AUC_score: 0.9521\n",
      "Epoch 400: Train Loss: 0.1672, Macro_F1: 0.8709, AUC_score: 0.9521\n",
      "Epoch 00417: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.1716, Macro_F1: 0.8712, AUC_score: 0.9523\n",
      "Validation loss decreased (0.156823 --> 0.156823).\n",
      "Epoch 500: Train Loss: 0.2329, Macro_F1: 0.8706, AUC_score: 0.9523\n",
      "Epoch 550: Train Loss: 0.1699, Macro_F1: 0.8728, AUC_score: 0.9522\n",
      "Epoch 00574: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Validation loss decreased (0.155386 --> 0.155386).\n",
      "Validation loss decreased (0.154912 --> 0.154912).\n",
      "Epoch 600: Train Loss: 0.1648, Macro_F1: 0.8686, AUC_score: 0.9523\n",
      "Epoch 650: Train Loss: 0.1695, Macro_F1: 0.8720, AUC_score: 0.9523\n",
      "Epoch 00687: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 700: Train Loss: 0.1637, Macro_F1: 0.8720, AUC_score: 0.9524\n",
      "Epoch 750: Train Loss: 0.1751, Macro_F1: 0.8720, AUC_score: 0.9524\n",
      "Early stopping triggered\n",
      "acc save\n",
      "51.0% node features transform to 0: F1: 0.8717, AUC_score: 0.9523\n",
      "Epoch 0: Train Loss: 0.2054, Macro_F1: 0.8374, AUC_score: 0.9419\n",
      "Validation loss decreased (0.205410 --> 0.205410).\n",
      "Validation loss decreased (0.196077 --> 0.196077).\n",
      "Validation loss decreased (0.195586 --> 0.195586).\n",
      "Validation loss decreased (0.189983 --> 0.189983).\n",
      "Validation loss decreased (0.185111 --> 0.185111).\n",
      "Validation loss decreased (0.180471 --> 0.180471).\n",
      "Validation loss decreased (0.178335 --> 0.178335).\n",
      "Validation loss decreased (0.175947 --> 0.175947).\n",
      "Validation loss decreased (0.170594 --> 0.170594).\n",
      "Validation loss decreased (0.169196 --> 0.169196).\n",
      "Epoch 50: Train Loss: 0.1678, Macro_F1: 0.8670, AUC_score: 0.9525\n",
      "Validation loss decreased (0.167795 --> 0.167795).\n",
      "Validation loss decreased (0.166944 --> 0.166944).\n",
      "Validation loss decreased (0.166118 --> 0.166118).\n",
      "Validation loss decreased (0.162280 --> 0.162280).\n",
      "Epoch 100: Train Loss: 0.2092, Macro_F1: 0.8701, AUC_score: 0.9524\n",
      "Epoch 150: Train Loss: 0.2071, Macro_F1: 0.8706, AUC_score: 0.9519\n",
      "Validation loss decreased (0.160285 --> 0.160285).\n",
      "Validation loss decreased (0.158365 --> 0.158365).\n",
      "Epoch 200: Train Loss: 0.1619, Macro_F1: 0.8681, AUC_score: 0.9528\n",
      "Epoch 250: Train Loss: 0.2035, Macro_F1: 0.8724, AUC_score: 0.9518\n",
      "Epoch 00290: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.2041, Macro_F1: 0.8696, AUC_score: 0.9521\n",
      "Validation loss decreased (0.158163 --> 0.158163).\n",
      "Epoch 350: Train Loss: 0.1620, Macro_F1: 0.8674, AUC_score: 0.9520\n",
      "Epoch 400: Train Loss: 0.1621, Macro_F1: 0.8731, AUC_score: 0.9525\n",
      "Validation loss decreased (0.156994 --> 0.156994).\n",
      "Validation loss decreased (0.156705 --> 0.156705).\n",
      "Validation loss decreased (0.156610 --> 0.156610).\n",
      "Validation loss decreased (0.156144 --> 0.156144).\n",
      "Epoch 450: Train Loss: 0.1561, Macro_F1: 0.8701, AUC_score: 0.9525\n",
      "Validation loss decreased (0.156122 --> 0.156122).\n",
      "Epoch 500: Train Loss: 0.1573, Macro_F1: 0.8675, AUC_score: 0.9520\n",
      "Epoch 550: Train Loss: 0.1620, Macro_F1: 0.8655, AUC_score: 0.9524\n",
      "Epoch 00552: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.151322 --> 0.151322).\n",
      "Epoch 600: Train Loss: 0.1666, Macro_F1: 0.8679, AUC_score: 0.9523\n",
      "Epoch 650: Train Loss: 0.2045, Macro_F1: 0.8676, AUC_score: 0.9521\n",
      "Epoch 00701: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 700: Train Loss: 0.1590, Macro_F1: 0.8670, AUC_score: 0.9520\n",
      "Epoch 750: Train Loss: 0.1656, Macro_F1: 0.8662, AUC_score: 0.9520\n",
      "Early stopping triggered\n",
      "acc save\n",
      "50.0% node features transform to 0: F1: 0.8652, AUC_score: 0.9520\n",
      "Epoch 0: Train Loss: 0.1618, Macro_F1: 0.8533, AUC_score: 0.9461\n",
      "Validation loss decreased (0.161793 --> 0.161793).\n",
      "Epoch 50: Train Loss: 0.2096, Macro_F1: 0.8684, AUC_score: 0.9509\n",
      "Validation loss decreased (0.161470 --> 0.161470).\n",
      "Validation loss decreased (0.161410 --> 0.161410).\n",
      "Epoch 100: Train Loss: 0.1665, Macro_F1: 0.8703, AUC_score: 0.9520\n",
      "Epoch 150: Train Loss: 0.1879, Macro_F1: 0.8686, AUC_score: 0.9509\n",
      "Epoch 00189: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.151512 --> 0.151512).\n",
      "Epoch 200: Train Loss: 0.1766, Macro_F1: 0.8687, AUC_score: 0.9531\n",
      "Epoch 250: Train Loss: 0.2093, Macro_F1: 0.8673, AUC_score: 0.9523\n",
      "Epoch 00292: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 300: Train Loss: 0.1788, Macro_F1: 0.8653, AUC_score: 0.9525\n",
      "Epoch 350: Train Loss: 0.1990, Macro_F1: 0.8676, AUC_score: 0.9524\n",
      "Early stopping triggered\n",
      "acc save\n",
      "49.0% node features transform to 0: F1: 0.8651, AUC_score: 0.9522\n",
      "Epoch 0: Train Loss: 0.1927, Macro_F1: 0.8482, AUC_score: 0.9444\n",
      "Validation loss decreased (0.192661 --> 0.192661).\n",
      "Validation loss decreased (0.176322 --> 0.176322).\n",
      "Validation loss decreased (0.173053 --> 0.173053).\n",
      "Validation loss decreased (0.167960 --> 0.167960).\n",
      "Validation loss decreased (0.165689 --> 0.165689).\n",
      "Validation loss decreased (0.164986 --> 0.164986).\n",
      "Epoch 50: Train Loss: 0.2433, Macro_F1: 0.8535, AUC_score: 0.9511\n",
      "Epoch 100: Train Loss: 0.2420, Macro_F1: 0.8617, AUC_score: 0.9486\n",
      "Validation loss decreased (0.164680 --> 0.164680).\n",
      "Epoch 150: Train Loss: 0.1966, Macro_F1: 0.8594, AUC_score: 0.9518\n",
      "Validation loss decreased (0.163601 --> 0.163601).\n",
      "Epoch 200: Train Loss: 0.1848, Macro_F1: 0.8683, AUC_score: 0.9528\n",
      "Validation loss decreased (0.159958 --> 0.159958).\n",
      "Epoch 250: Train Loss: 0.2452, Macro_F1: 0.8756, AUC_score: 0.9531\n",
      "Epoch 300: Train Loss: 0.1730, Macro_F1: 0.8741, AUC_score: 0.9526\n",
      "Epoch 00322: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.155343 --> 0.155343).\n",
      "Epoch 350: Train Loss: 0.1722, Macro_F1: 0.8730, AUC_score: 0.9528\n",
      "Epoch 400: Train Loss: 0.1929, Macro_F1: 0.8744, AUC_score: 0.9527\n",
      "Epoch 00445: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.1707, Macro_F1: 0.8759, AUC_score: 0.9527\n",
      "Epoch 500: Train Loss: 0.1606, Macro_F1: 0.8757, AUC_score: 0.9529\n",
      "Early stopping triggered\n",
      "acc save\n",
      "48.0% node features transform to 0: F1: 0.8697, AUC_score: 0.9529\n",
      "Epoch 0: Train Loss: 0.1672, Macro_F1: 0.8605, AUC_score: 0.9505\n",
      "Validation loss decreased (0.167159 --> 0.167159).\n",
      "Validation loss decreased (0.162672 --> 0.162672).\n",
      "Epoch 50: Train Loss: 0.1832, Macro_F1: 0.8674, AUC_score: 0.9520\n",
      "Validation loss decreased (0.162163 --> 0.162163).\n",
      "Validation loss decreased (0.160031 --> 0.160031).\n",
      "Epoch 100: Train Loss: 0.1695, Macro_F1: 0.8724, AUC_score: 0.9518\n",
      "Validation loss decreased (0.158648 --> 0.158648).\n",
      "Epoch 150: Train Loss: 0.1716, Macro_F1: 0.8749, AUC_score: 0.9533\n",
      "Epoch 200: Train Loss: 0.1884, Macro_F1: 0.8691, AUC_score: 0.9503\n",
      "Epoch 00236: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.154359 --> 0.154359).\n",
      "Epoch 250: Train Loss: 0.1577, Macro_F1: 0.8716, AUC_score: 0.9520\n",
      "Validation loss decreased (0.153115 --> 0.153115).\n",
      "Epoch 300: Train Loss: 0.1618, Macro_F1: 0.8727, AUC_score: 0.9520\n",
      "Validation loss decreased (0.152797 --> 0.152797).\n",
      "Epoch 350: Train Loss: 0.1785, Macro_F1: 0.8664, AUC_score: 0.9525\n",
      "Validation loss decreased (0.152080 --> 0.152080).\n",
      "Epoch 400: Train Loss: 0.1607, Macro_F1: 0.8609, AUC_score: 0.9527\n",
      "Epoch 450: Train Loss: 0.1787, Macro_F1: 0.8724, AUC_score: 0.9523\n",
      "Epoch 00453: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.1761, Macro_F1: 0.8697, AUC_score: 0.9527\n",
      "Epoch 550: Train Loss: 0.1492, Macro_F1: 0.8671, AUC_score: 0.9528\n",
      "Validation loss decreased (0.149209 --> 0.149209).\n",
      "Epoch 600: Train Loss: 0.1684, Macro_F1: 0.8687, AUC_score: 0.9527\n",
      "Epoch 650: Train Loss: 0.1851, Macro_F1: 0.8681, AUC_score: 0.9527\n",
      "Epoch 00652: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Validation loss decreased (0.148485 --> 0.148485).\n",
      "Epoch 700: Train Loss: 0.1781, Macro_F1: 0.8687, AUC_score: 0.9527\n",
      "Epoch 750: Train Loss: 0.1586, Macro_F1: 0.8670, AUC_score: 0.9526\n",
      "Epoch 00785: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 800: Train Loss: 0.1649, Macro_F1: 0.8682, AUC_score: 0.9527\n",
      "Epoch 850: Train Loss: 0.1716, Macro_F1: 0.8681, AUC_score: 0.9527\n",
      "Early stopping triggered\n",
      "acc save\n",
      "47.0% node features transform to 0: F1: 0.8676, AUC_score: 0.9527\n",
      "Epoch 0: Train Loss: 0.1841, Macro_F1: 0.8301, AUC_score: 0.9498\n",
      "Validation loss decreased (0.184072 --> 0.184072).\n",
      "Validation loss decreased (0.171274 --> 0.171274).\n",
      "Validation loss decreased (0.167128 --> 0.167128).\n",
      "Validation loss decreased (0.160080 --> 0.160080).\n",
      "Validation loss decreased (0.155295 --> 0.155295).\n",
      "Epoch 50: Train Loss: 0.2131, Macro_F1: 0.8702, AUC_score: 0.9520\n",
      "Epoch 100: Train Loss: 0.1833, Macro_F1: 0.8751, AUC_score: 0.9526\n",
      "Validation loss decreased (0.152673 --> 0.152673).\n",
      "Epoch 150: Train Loss: 0.1773, Macro_F1: 0.8683, AUC_score: 0.9527\n",
      "Epoch 200: Train Loss: 0.1910, Macro_F1: 0.8672, AUC_score: 0.9526\n",
      "Epoch 00213: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.1666, Macro_F1: 0.8727, AUC_score: 0.9531\n",
      "Epoch 300: Train Loss: 0.1540, Macro_F1: 0.8680, AUC_score: 0.9528\n",
      "Early stopping triggered\n",
      "acc save\n",
      "46.0% node features transform to 0: F1: 0.8743, AUC_score: 0.9524\n",
      "Epoch 0: Train Loss: 0.1774, Macro_F1: 0.8571, AUC_score: 0.9506\n",
      "Validation loss decreased (0.177413 --> 0.177413).\n",
      "Validation loss decreased (0.174530 --> 0.174530).\n",
      "Validation loss decreased (0.174434 --> 0.174434).\n",
      "Validation loss decreased (0.172554 --> 0.172554).\n",
      "Validation loss decreased (0.167575 --> 0.167575).\n",
      "Validation loss decreased (0.163482 --> 0.163482).\n",
      "Epoch 50: Train Loss: 0.1967, Macro_F1: 0.8640, AUC_score: 0.9497\n",
      "Validation loss decreased (0.158513 --> 0.158513).\n",
      "Epoch 100: Train Loss: 0.1781, Macro_F1: 0.8722, AUC_score: 0.9525\n",
      "Validation loss decreased (0.157790 --> 0.157790).\n",
      "Validation loss decreased (0.155396 --> 0.155396).\n",
      "Validation loss decreased (0.151039 --> 0.151039).\n",
      "Epoch 150: Train Loss: 0.1635, Macro_F1: 0.8780, AUC_score: 0.9527\n",
      "Epoch 200: Train Loss: 0.1903, Macro_F1: 0.8720, AUC_score: 0.9511\n",
      "Epoch 00218: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.2053, Macro_F1: 0.8724, AUC_score: 0.9529\n",
      "Epoch 300: Train Loss: 0.1620, Macro_F1: 0.8689, AUC_score: 0.9533\n",
      "Early stopping triggered\n",
      "acc save\n",
      "44.99999999999999% node features transform to 0: F1: 0.8729, AUC_score: 0.9523\n",
      "Epoch 0: Train Loss: 0.1764, Macro_F1: 0.8560, AUC_score: 0.9503\n",
      "Validation loss decreased (0.176411 --> 0.176411).\n",
      "Validation loss decreased (0.165578 --> 0.165578).\n",
      "Validation loss decreased (0.164610 --> 0.164610).\n",
      "Validation loss decreased (0.163866 --> 0.163866).\n",
      "Validation loss decreased (0.162524 --> 0.162524).\n",
      "Validation loss decreased (0.162357 --> 0.162357).\n",
      "Validation loss decreased (0.160754 --> 0.160754).\n",
      "Epoch 50: Train Loss: 0.1612, Macro_F1: 0.8705, AUC_score: 0.9525\n",
      "Validation loss decreased (0.157460 --> 0.157460).\n",
      "Validation loss decreased (0.157000 --> 0.157000).\n",
      "Validation loss decreased (0.156098 --> 0.156098).\n",
      "Epoch 100: Train Loss: 0.1644, Macro_F1: 0.8615, AUC_score: 0.9528\n",
      "Epoch 150: Train Loss: 0.1935, Macro_F1: 0.8718, AUC_score: 0.9540\n",
      "Validation loss decreased (0.155783 --> 0.155783).\n",
      "Epoch 200: Train Loss: 0.2072, Macro_F1: 0.8647, AUC_score: 0.9522\n",
      "Epoch 250: Train Loss: 0.1964, Macro_F1: 0.8694, AUC_score: 0.9528\n",
      "Epoch 00271: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.1727, Macro_F1: 0.8741, AUC_score: 0.9520\n",
      "Validation loss decreased (0.155027 --> 0.155027).\n",
      "Validation loss decreased (0.154274 --> 0.154274).\n",
      "Validation loss decreased (0.153458 --> 0.153458).\n",
      "Validation loss decreased (0.152923 --> 0.152923).\n",
      "Epoch 350: Train Loss: 0.1522, Macro_F1: 0.8749, AUC_score: 0.9520\n",
      "Validation loss decreased (0.152216 --> 0.152216).\n",
      "Validation loss decreased (0.150499 --> 0.150499).\n",
      "Epoch 400: Train Loss: 0.1742, Macro_F1: 0.8700, AUC_score: 0.9525\n",
      "Validation loss decreased (0.149010 --> 0.149010).\n",
      "Epoch 450: Train Loss: 0.1716, Macro_F1: 0.8773, AUC_score: 0.9526\n",
      "Validation loss decreased (0.145343 --> 0.145343).\n",
      "Epoch 500: Train Loss: 0.1585, Macro_F1: 0.8736, AUC_score: 0.9525\n",
      "Epoch 550: Train Loss: 0.1527, Macro_F1: 0.8765, AUC_score: 0.9522\n",
      "Epoch 00582: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 600: Train Loss: 0.1776, Macro_F1: 0.8703, AUC_score: 0.9534\n",
      "Epoch 650: Train Loss: 0.1753, Macro_F1: 0.8697, AUC_score: 0.9533\n",
      "Early stopping triggered\n",
      "acc save\n",
      "43.99999999999999% node features transform to 0: F1: 0.8700, AUC_score: 0.9533\n",
      "Epoch 0: Train Loss: 0.1689, Macro_F1: 0.8271, AUC_score: 0.9477\n",
      "Validation loss decreased (0.168882 --> 0.168882).\n",
      "Validation loss decreased (0.166373 --> 0.166373).\n",
      "Validation loss decreased (0.162902 --> 0.162902).\n",
      "Validation loss decreased (0.162143 --> 0.162143).\n",
      "Validation loss decreased (0.152579 --> 0.152579).\n",
      "Epoch 50: Train Loss: 0.1840, Macro_F1: 0.8746, AUC_score: 0.9526\n",
      "Validation loss decreased (0.151649 --> 0.151649).\n",
      "Epoch 100: Train Loss: 0.1612, Macro_F1: 0.8720, AUC_score: 0.9549\n",
      "Epoch 150: Train Loss: 0.1701, Macro_F1: 0.8663, AUC_score: 0.9514\n",
      "Epoch 00181: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.150519 --> 0.150519).\n",
      "Epoch 200: Train Loss: 0.1697, Macro_F1: 0.8697, AUC_score: 0.9539\n",
      "Validation loss decreased (0.148542 --> 0.148542).\n",
      "Epoch 250: Train Loss: 0.1673, Macro_F1: 0.8768, AUC_score: 0.9529\n",
      "Epoch 300: Train Loss: 0.1481, Macro_F1: 0.8738, AUC_score: 0.9536\n",
      "Validation loss decreased (0.148055 --> 0.148055).\n",
      "Validation loss decreased (0.147199 --> 0.147199).\n",
      "Epoch 350: Train Loss: 0.1529, Macro_F1: 0.8730, AUC_score: 0.9540\n",
      "Epoch 400: Train Loss: 0.1645, Macro_F1: 0.8730, AUC_score: 0.9527\n",
      "Validation loss decreased (0.146473 --> 0.146473).\n",
      "Validation loss decreased (0.145205 --> 0.145205).\n",
      "Epoch 450: Train Loss: 0.1884, Macro_F1: 0.8744, AUC_score: 0.9534\n",
      "Epoch 500: Train Loss: 0.1844, Macro_F1: 0.8752, AUC_score: 0.9536\n",
      "Epoch 00526: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 550: Train Loss: 0.1538, Macro_F1: 0.8719, AUC_score: 0.9531\n",
      "Epoch 600: Train Loss: 0.1665, Macro_F1: 0.8749, AUC_score: 0.9531\n",
      "Early stopping triggered\n",
      "acc save\n",
      "42.99999999999999% node features transform to 0: F1: 0.8762, AUC_score: 0.9532\n",
      "Epoch 0: Train Loss: 0.1605, Macro_F1: 0.8370, AUC_score: 0.9511\n",
      "Validation loss decreased (0.160530 --> 0.160530).\n",
      "Validation loss decreased (0.154672 --> 0.154672).\n",
      "Epoch 50: Train Loss: 0.1657, Macro_F1: 0.8740, AUC_score: 0.9522\n",
      "Validation loss decreased (0.153733 --> 0.153733).\n",
      "Epoch 100: Train Loss: 0.2251, Macro_F1: 0.8663, AUC_score: 0.9517\n",
      "Epoch 150: Train Loss: 0.1843, Macro_F1: 0.8736, AUC_score: 0.9542\n",
      "Epoch 00158: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.152729 --> 0.152729).\n",
      "Validation loss decreased (0.147137 --> 0.147137).\n",
      "Epoch 200: Train Loss: 0.1538, Macro_F1: 0.8735, AUC_score: 0.9536\n",
      "Validation loss decreased (0.145790 --> 0.145790).\n",
      "Epoch 250: Train Loss: 0.1497, Macro_F1: 0.8765, AUC_score: 0.9527\n",
      "Epoch 300: Train Loss: 0.1559, Macro_F1: 0.8741, AUC_score: 0.9539\n",
      "Epoch 00320: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.145554 --> 0.145554).\n",
      "Epoch 350: Train Loss: 0.2040, Macro_F1: 0.8730, AUC_score: 0.9536\n",
      "Epoch 400: Train Loss: 0.1535, Macro_F1: 0.8733, AUC_score: 0.9536\n",
      "Epoch 00432: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 450: Train Loss: 0.1720, Macro_F1: 0.8717, AUC_score: 0.9534\n",
      "Validation loss decreased (0.144602 --> 0.144602).\n",
      "Epoch 500: Train Loss: 0.1575, Macro_F1: 0.8722, AUC_score: 0.9534\n",
      "Epoch 550: Train Loss: 0.1463, Macro_F1: 0.8738, AUC_score: 0.9533\n",
      "Epoch 00577: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 600: Train Loss: 0.1781, Macro_F1: 0.8749, AUC_score: 0.9533\n",
      "Validation loss decreased (0.144067 --> 0.144067).\n",
      "Epoch 650: Train Loss: 0.1689, Macro_F1: 0.8733, AUC_score: 0.9533\n",
      "Epoch 700: Train Loss: 0.1957, Macro_F1: 0.8733, AUC_score: 0.9533\n",
      "Epoch 00739: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 750: Train Loss: 0.1849, Macro_F1: 0.8733, AUC_score: 0.9533\n",
      "Validation loss decreased (0.142990 --> 0.142990).\n",
      "Epoch 800: Train Loss: 0.1515, Macro_F1: 0.8733, AUC_score: 0.9533\n",
      "Epoch 850: Train Loss: 0.1672, Macro_F1: 0.8733, AUC_score: 0.9534\n",
      "Epoch 00866: reducing learning rate of group 0 to 6.4000e-08.\n",
      "Epoch 900: Train Loss: 0.1746, Macro_F1: 0.8733, AUC_score: 0.9534\n",
      "Epoch 950: Train Loss: 0.1836, Macro_F1: 0.8733, AUC_score: 0.9534\n",
      "Early stopping triggered\n",
      "acc save\n",
      "42.00000000000001% node features transform to 0: F1: 0.8733, AUC_score: 0.9534\n",
      "Epoch 0: Train Loss: 0.1640, Macro_F1: 0.8362, AUC_score: 0.9512\n",
      "Validation loss decreased (0.163984 --> 0.163984).\n",
      "Validation loss decreased (0.156977 --> 0.156977).\n",
      "Validation loss decreased (0.152012 --> 0.152012).\n",
      "Validation loss decreased (0.151240 --> 0.151240).\n",
      "Epoch 50: Train Loss: 0.1651, Macro_F1: 0.8759, AUC_score: 0.9529\n",
      "Epoch 100: Train Loss: 0.1847, Macro_F1: 0.8734, AUC_score: 0.9530\n",
      "Epoch 00148: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.1913, Macro_F1: 0.8754, AUC_score: 0.9542\n",
      "Validation loss decreased (0.147127 --> 0.147127).\n",
      "Epoch 200: Train Loss: 0.1662, Macro_F1: 0.8683, AUC_score: 0.9542\n",
      "Epoch 250: Train Loss: 0.1908, Macro_F1: 0.8740, AUC_score: 0.9530\n",
      "Validation loss decreased (0.144404 --> 0.144404).\n",
      "Epoch 300: Train Loss: 0.1512, Macro_F1: 0.8743, AUC_score: 0.9532\n",
      "Validation loss decreased (0.140843 --> 0.140843).\n",
      "Epoch 350: Train Loss: 0.1819, Macro_F1: 0.8725, AUC_score: 0.9539\n",
      "Epoch 400: Train Loss: 0.1662, Macro_F1: 0.8732, AUC_score: 0.9534\n",
      "Epoch 00451: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.1832, Macro_F1: 0.8694, AUC_score: 0.9537\n",
      "Epoch 500: Train Loss: 0.1608, Macro_F1: 0.8711, AUC_score: 0.9536\n",
      "Early stopping triggered\n",
      "acc save\n",
      "41.0% node features transform to 0: F1: 0.8735, AUC_score: 0.9532\n",
      "Epoch 0: Train Loss: 0.1531, Macro_F1: 0.8443, AUC_score: 0.9490\n",
      "Validation loss decreased (0.153095 --> 0.153095).\n",
      "Epoch 50: Train Loss: 0.2348, Macro_F1: 0.8729, AUC_score: 0.9530\n",
      "Validation loss decreased (0.152234 --> 0.152234).\n",
      "Epoch 100: Train Loss: 0.2152, Macro_F1: 0.8710, AUC_score: 0.9522\n",
      "Epoch 150: Train Loss: 0.1731, Macro_F1: 0.8735, AUC_score: 0.9526\n",
      "Epoch 00179: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.151917 --> 0.151917).\n",
      "Epoch 200: Train Loss: 0.1635, Macro_F1: 0.8725, AUC_score: 0.9526\n",
      "Validation loss decreased (0.150104 --> 0.150104).\n",
      "Validation loss decreased (0.147829 --> 0.147829).\n",
      "Validation loss decreased (0.146831 --> 0.146831).\n",
      "Validation loss decreased (0.145116 --> 0.145116).\n",
      "Epoch 250: Train Loss: 0.1701, Macro_F1: 0.8738, AUC_score: 0.9530\n",
      "Epoch 300: Train Loss: 0.1483, Macro_F1: 0.8746, AUC_score: 0.9528\n",
      "Validation loss decreased (0.144254 --> 0.144254).\n",
      "Epoch 350: Train Loss: 0.1984, Macro_F1: 0.8727, AUC_score: 0.9526\n",
      "Epoch 400: Train Loss: 0.1729, Macro_F1: 0.8739, AUC_score: 0.9531\n",
      "Epoch 00446: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.1529, Macro_F1: 0.8713, AUC_score: 0.9528\n",
      "Epoch 500: Train Loss: 0.1633, Macro_F1: 0.8738, AUC_score: 0.9533\n",
      "Validation loss decreased (0.144111 --> 0.144111).\n",
      "Validation loss decreased (0.143589 --> 0.143589).\n",
      "Epoch 550: Train Loss: 0.1509, Macro_F1: 0.8724, AUC_score: 0.9531\n",
      "Validation loss decreased (0.142670 --> 0.142670).\n",
      "Validation loss decreased (0.141253 --> 0.141253).\n",
      "Epoch 600: Train Loss: 0.1977, Macro_F1: 0.8706, AUC_score: 0.9533\n",
      "Epoch 650: Train Loss: 0.1845, Macro_F1: 0.8759, AUC_score: 0.9530\n",
      "Epoch 00700: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 700: Train Loss: 0.1851, Macro_F1: 0.8695, AUC_score: 0.9531\n",
      "Epoch 750: Train Loss: 0.1451, Macro_F1: 0.8711, AUC_score: 0.9529\n",
      "Early stopping triggered\n",
      "acc save\n",
      "40.0% node features transform to 0: F1: 0.8711, AUC_score: 0.9530\n",
      "Epoch 0: Train Loss: 0.1833, Macro_F1: 0.8434, AUC_score: 0.9479\n",
      "Validation loss decreased (0.183268 --> 0.183268).\n",
      "Validation loss decreased (0.178653 --> 0.178653).\n",
      "Validation loss decreased (0.177748 --> 0.177748).\n",
      "Validation loss decreased (0.177609 --> 0.177609).\n",
      "Validation loss decreased (0.172613 --> 0.172613).\n",
      "Validation loss decreased (0.170019 --> 0.170019).\n",
      "Validation loss decreased (0.163742 --> 0.163742).\n",
      "Validation loss decreased (0.154375 --> 0.154375).\n",
      "Epoch 50: Train Loss: 0.2015, Macro_F1: 0.8732, AUC_score: 0.9523\n",
      "Validation loss decreased (0.152910 --> 0.152910).\n",
      "Epoch 100: Train Loss: 0.1513, Macro_F1: 0.8719, AUC_score: 0.9535\n",
      "Validation loss decreased (0.151274 --> 0.151274).\n",
      "Epoch 150: Train Loss: 0.2319, Macro_F1: 0.8687, AUC_score: 0.9514\n",
      "Epoch 200: Train Loss: 0.1502, Macro_F1: 0.8702, AUC_score: 0.9535\n",
      "Validation loss decreased (0.150210 --> 0.150210).\n",
      "Validation loss decreased (0.149983 --> 0.149983).\n",
      "Epoch 250: Train Loss: 0.2175, Macro_F1: 0.8756, AUC_score: 0.9534\n",
      "Epoch 300: Train Loss: 0.1813, Macro_F1: 0.8786, AUC_score: 0.9536\n",
      "Epoch 00307: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 350: Train Loss: 0.1594, Macro_F1: 0.8722, AUC_score: 0.9535\n",
      "Validation loss decreased (0.148797 --> 0.148797).\n",
      "Validation loss decreased (0.147434 --> 0.147434).\n",
      "Validation loss decreased (0.146854 --> 0.146854).\n",
      "Validation loss decreased (0.142163 --> 0.142163).\n",
      "Epoch 400: Train Loss: 0.1497, Macro_F1: 0.8721, AUC_score: 0.9535\n",
      "Epoch 450: Train Loss: 0.1484, Macro_F1: 0.8727, AUC_score: 0.9538\n",
      "Epoch 00495: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.142070 --> 0.142070).\n",
      "Epoch 500: Train Loss: 0.1538, Macro_F1: 0.8743, AUC_score: 0.9534\n",
      "Epoch 550: Train Loss: 0.1624, Macro_F1: 0.8708, AUC_score: 0.9537\n",
      "Epoch 00597: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 600: Train Loss: 0.1644, Macro_F1: 0.8746, AUC_score: 0.9536\n",
      "Epoch 650: Train Loss: 0.1867, Macro_F1: 0.8735, AUC_score: 0.9536\n",
      "Early stopping triggered\n",
      "acc save\n",
      "39.0% node features transform to 0: F1: 0.8724, AUC_score: 0.9536\n",
      "Epoch 0: Train Loss: 0.1797, Macro_F1: 0.8409, AUC_score: 0.9481\n",
      "Validation loss decreased (0.179651 --> 0.179651).\n",
      "Validation loss decreased (0.160067 --> 0.160067).\n",
      "Validation loss decreased (0.156278 --> 0.156278).\n",
      "Validation loss decreased (0.152934 --> 0.152934).\n",
      "Epoch 50: Train Loss: 0.2046, Macro_F1: 0.8725, AUC_score: 0.9526\n",
      "Validation loss decreased (0.152237 --> 0.152237).\n",
      "Epoch 100: Train Loss: 0.1905, Macro_F1: 0.8723, AUC_score: 0.9527\n",
      "Validation loss decreased (0.144202 --> 0.144202).\n",
      "Epoch 150: Train Loss: 0.1793, Macro_F1: 0.8724, AUC_score: 0.9533\n",
      "Epoch 200: Train Loss: 0.1884, Macro_F1: 0.8574, AUC_score: 0.9485\n",
      "Epoch 00215: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.1602, Macro_F1: 0.8724, AUC_score: 0.9538\n",
      "Epoch 300: Train Loss: 0.1495, Macro_F1: 0.8738, AUC_score: 0.9533\n",
      "Early stopping triggered\n",
      "acc save\n",
      "38.0% node features transform to 0: F1: 0.8740, AUC_score: 0.9537\n",
      "Epoch 0: Train Loss: 0.1500, Macro_F1: 0.8443, AUC_score: 0.9495\n",
      "Validation loss decreased (0.149967 --> 0.149967).\n",
      "Epoch 50: Train Loss: 0.1739, Macro_F1: 0.8705, AUC_score: 0.9539\n",
      "Validation loss decreased (0.148612 --> 0.148612).\n",
      "Epoch 100: Train Loss: 0.1863, Macro_F1: 0.8721, AUC_score: 0.9540\n",
      "Validation loss decreased (0.147354 --> 0.147354).\n",
      "Validation loss decreased (0.146835 --> 0.146835).\n",
      "Validation loss decreased (0.145596 --> 0.145596).\n",
      "Epoch 150: Train Loss: 0.1583, Macro_F1: 0.8702, AUC_score: 0.9540\n",
      "Epoch 200: Train Loss: 0.1532, Macro_F1: 0.8681, AUC_score: 0.9519\n",
      "Epoch 00240: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.143390 --> 0.143390).\n",
      "Epoch 250: Train Loss: 0.1502, Macro_F1: 0.8773, AUC_score: 0.9538\n",
      "Validation loss decreased (0.140744 --> 0.140744).\n",
      "Epoch 300: Train Loss: 0.1791, Macro_F1: 0.8684, AUC_score: 0.9544\n",
      "Validation loss decreased (0.140363 --> 0.140363).\n",
      "Epoch 350: Train Loss: 0.1571, Macro_F1: 0.8728, AUC_score: 0.9538\n",
      "Epoch 400: Train Loss: 0.1717, Macro_F1: 0.8717, AUC_score: 0.9534\n",
      "Epoch 00425: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.1858, Macro_F1: 0.8741, AUC_score: 0.9538\n",
      "Epoch 500: Train Loss: 0.1539, Macro_F1: 0.8736, AUC_score: 0.9535\n",
      "Early stopping triggered\n",
      "acc save\n",
      "37.0% node features transform to 0: F1: 0.8735, AUC_score: 0.9536\n",
      "Epoch 0: Train Loss: 0.2008, Macro_F1: 0.8416, AUC_score: 0.9483\n",
      "Validation loss decreased (0.200832 --> 0.200832).\n",
      "Validation loss decreased (0.152048 --> 0.152048).\n",
      "Validation loss decreased (0.150789 --> 0.150789).\n",
      "Epoch 50: Train Loss: 0.1652, Macro_F1: 0.8655, AUC_score: 0.9539\n",
      "Validation loss decreased (0.150523 --> 0.150523).\n",
      "Validation loss decreased (0.145880 --> 0.145880).\n",
      "Validation loss decreased (0.145528 --> 0.145528).\n",
      "Epoch 100: Train Loss: 0.1687, Macro_F1: 0.8677, AUC_score: 0.9512\n",
      "Epoch 150: Train Loss: 0.1852, Macro_F1: 0.8771, AUC_score: 0.9537\n",
      "Epoch 00159: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.143928 --> 0.143928).\n",
      "Epoch 200: Train Loss: 0.1557, Macro_F1: 0.8735, AUC_score: 0.9530\n",
      "Validation loss decreased (0.142586 --> 0.142586).\n",
      "Validation loss decreased (0.140384 --> 0.140384).\n",
      "Epoch 250: Train Loss: 0.1520, Macro_F1: 0.8686, AUC_score: 0.9536\n",
      "Validation loss decreased (0.138427 --> 0.138427).\n",
      "Epoch 300: Train Loss: 0.1622, Macro_F1: 0.8685, AUC_score: 0.9535\n",
      "Epoch 350: Train Loss: 0.1900, Macro_F1: 0.8743, AUC_score: 0.9529\n",
      "Epoch 00382: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.1454, Macro_F1: 0.8719, AUC_score: 0.9537\n",
      "Epoch 450: Train Loss: 0.1563, Macro_F1: 0.8709, AUC_score: 0.9534\n",
      "Early stopping triggered\n",
      "acc save\n",
      "36.0% node features transform to 0: F1: 0.8733, AUC_score: 0.9535\n",
      "Epoch 0: Train Loss: 0.1518, Macro_F1: 0.8397, AUC_score: 0.9492\n",
      "Validation loss decreased (0.151793 --> 0.151793).\n",
      "Epoch 50: Train Loss: 0.1778, Macro_F1: 0.8680, AUC_score: 0.9530\n",
      "Validation loss decreased (0.148134 --> 0.148134).\n",
      "Validation loss decreased (0.145717 --> 0.145717).\n",
      "Validation loss decreased (0.144904 --> 0.144904).\n",
      "Epoch 100: Train Loss: 0.1786, Macro_F1: 0.8745, AUC_score: 0.9519\n",
      "Epoch 150: Train Loss: 0.1697, Macro_F1: 0.8724, AUC_score: 0.9527\n",
      "Epoch 00177: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.2040, Macro_F1: 0.8741, AUC_score: 0.9530\n",
      "Validation loss decreased (0.142365 --> 0.142365).\n",
      "Epoch 250: Train Loss: 0.1549, Macro_F1: 0.8737, AUC_score: 0.9527\n",
      "Validation loss decreased (0.141350 --> 0.141350).\n",
      "Epoch 300: Train Loss: 0.2108, Macro_F1: 0.8721, AUC_score: 0.9536\n",
      "Validation loss decreased (0.140568 --> 0.140568).\n",
      "Epoch 350: Train Loss: 0.1983, Macro_F1: 0.8689, AUC_score: 0.9527\n",
      "Validation loss decreased (0.140433 --> 0.140433).\n",
      "Epoch 400: Train Loss: 0.1435, Macro_F1: 0.8746, AUC_score: 0.9533\n",
      "Epoch 450: Train Loss: 0.1433, Macro_F1: 0.8765, AUC_score: 0.9533\n",
      "Validation loss decreased (0.138157 --> 0.138157).\n",
      "Validation loss decreased (0.136448 --> 0.136448).\n",
      "Epoch 500: Train Loss: 0.1654, Macro_F1: 0.8702, AUC_score: 0.9531\n",
      "Epoch 550: Train Loss: 0.1690, Macro_F1: 0.8738, AUC_score: 0.9531\n",
      "Epoch 00589: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 600: Train Loss: 0.1520, Macro_F1: 0.8686, AUC_score: 0.9535\n",
      "Epoch 650: Train Loss: 0.1433, Macro_F1: 0.8719, AUC_score: 0.9535\n",
      "Early stopping triggered\n",
      "acc save\n",
      "35.0% node features transform to 0: F1: 0.8741, AUC_score: 0.9534\n",
      "Epoch 0: Train Loss: 0.1841, Macro_F1: 0.8393, AUC_score: 0.9473\n",
      "Validation loss decreased (0.184141 --> 0.184141).\n",
      "Validation loss decreased (0.176596 --> 0.176596).\n",
      "Validation loss decreased (0.174483 --> 0.174483).\n",
      "Validation loss decreased (0.174444 --> 0.174444).\n",
      "Validation loss decreased (0.166095 --> 0.166095).\n",
      "Validation loss decreased (0.161213 --> 0.161213).\n",
      "Validation loss decreased (0.157495 --> 0.157495).\n",
      "Validation loss decreased (0.152826 --> 0.152826).\n",
      "Validation loss decreased (0.147646 --> 0.147646).\n",
      "Epoch 50: Train Loss: 0.1501, Macro_F1: 0.8716, AUC_score: 0.9523\n",
      "Validation loss decreased (0.147113 --> 0.147113).\n",
      "Epoch 100: Train Loss: 0.1822, Macro_F1: 0.8721, AUC_score: 0.9526\n",
      "Validation loss decreased (0.146243 --> 0.146243).\n",
      "Epoch 150: Train Loss: 0.1651, Macro_F1: 0.8751, AUC_score: 0.9525\n",
      "Validation loss decreased (0.145807 --> 0.145807).\n",
      "Validation loss decreased (0.144968 --> 0.144968).\n",
      "Epoch 200: Train Loss: 0.1500, Macro_F1: 0.8743, AUC_score: 0.9516\n",
      "Epoch 250: Train Loss: 0.2660, Macro_F1: 0.8698, AUC_score: 0.9503\n",
      "Epoch 00261: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.142976 --> 0.142976).\n",
      "Epoch 300: Train Loss: 0.1485, Macro_F1: 0.8689, AUC_score: 0.9529\n",
      "Epoch 350: Train Loss: 0.1691, Macro_F1: 0.8692, AUC_score: 0.9531\n",
      "Validation loss decreased (0.140132 --> 0.140132).\n",
      "Epoch 400: Train Loss: 0.1626, Macro_F1: 0.8684, AUC_score: 0.9532\n",
      "Validation loss decreased (0.139500 --> 0.139500).\n",
      "Epoch 450: Train Loss: 0.1607, Macro_F1: 0.8705, AUC_score: 0.9533\n",
      "Validation loss decreased (0.139482 --> 0.139482).\n",
      "Epoch 500: Train Loss: 0.1761, Macro_F1: 0.8743, AUC_score: 0.9525\n",
      "Epoch 550: Train Loss: 0.1491, Macro_F1: 0.8713, AUC_score: 0.9526\n",
      "Epoch 00572: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 600: Train Loss: 0.1442, Macro_F1: 0.8741, AUC_score: 0.9529\n",
      "Validation loss decreased (0.137698 --> 0.137698).\n",
      "Epoch 650: Train Loss: 0.1473, Macro_F1: 0.8741, AUC_score: 0.9532\n",
      "Validation loss decreased (0.136984 --> 0.136984).\n",
      "Epoch 700: Train Loss: 0.1418, Macro_F1: 0.8724, AUC_score: 0.9530\n",
      "Epoch 750: Train Loss: 0.1816, Macro_F1: 0.8730, AUC_score: 0.9530\n",
      "Epoch 00760: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Validation loss decreased (0.136892 --> 0.136892).\n",
      "Epoch 800: Train Loss: 0.1536, Macro_F1: 0.8746, AUC_score: 0.9530\n",
      "Epoch 850: Train Loss: 0.1518, Macro_F1: 0.8746, AUC_score: 0.9530\n",
      "Epoch 00882: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 900: Train Loss: 0.1586, Macro_F1: 0.8746, AUC_score: 0.9530\n",
      "Epoch 950: Train Loss: 0.1518, Macro_F1: 0.8746, AUC_score: 0.9530\n",
      "Early stopping triggered\n",
      "acc save\n",
      "34.0% node features transform to 0: F1: 0.8751, AUC_score: 0.9530\n",
      "Epoch 0: Train Loss: 0.1394, Macro_F1: 0.8681, AUC_score: 0.9497\n",
      "Validation loss decreased (0.139402 --> 0.139402).\n",
      "Epoch 50: Train Loss: 0.1634, Macro_F1: 0.8737, AUC_score: 0.9529\n",
      "Epoch 100: Train Loss: 0.1553, Macro_F1: 0.8705, AUC_score: 0.9529\n",
      "Epoch 00102: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.134717 --> 0.134717).\n",
      "Epoch 150: Train Loss: 0.1547, Macro_F1: 0.8695, AUC_score: 0.9535\n",
      "Epoch 200: Train Loss: 0.1611, Macro_F1: 0.8686, AUC_score: 0.9529\n",
      "Epoch 00251: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 250: Train Loss: 0.1668, Macro_F1: 0.8696, AUC_score: 0.9530\n",
      "Epoch 300: Train Loss: 0.1551, Macro_F1: 0.8700, AUC_score: 0.9532\n",
      "Early stopping triggered\n",
      "acc save\n",
      "32.99999999999999% node features transform to 0: F1: 0.8727, AUC_score: 0.9532\n",
      "Epoch 0: Train Loss: 0.1663, Macro_F1: 0.8404, AUC_score: 0.9473\n",
      "Validation loss decreased (0.166334 --> 0.166334).\n",
      "Validation loss decreased (0.149836 --> 0.149836).\n",
      "Validation loss decreased (0.147142 --> 0.147142).\n",
      "Validation loss decreased (0.146425 --> 0.146425).\n",
      "Epoch 50: Train Loss: 0.1668, Macro_F1: 0.8668, AUC_score: 0.9518\n",
      "Validation loss decreased (0.143711 --> 0.143711).\n",
      "Epoch 100: Train Loss: 0.1543, Macro_F1: 0.8745, AUC_score: 0.9531\n",
      "Epoch 150: Train Loss: 0.1500, Macro_F1: 0.8735, AUC_score: 0.9519\n",
      "Validation loss decreased (0.142764 --> 0.142764).\n",
      "Epoch 200: Train Loss: 0.2125, Macro_F1: 0.8709, AUC_score: 0.9520\n",
      "Validation loss decreased (0.139796 --> 0.139796).\n",
      "Epoch 250: Train Loss: 0.1393, Macro_F1: 0.8745, AUC_score: 0.9523\n",
      "Validation loss decreased (0.139294 --> 0.139294).\n",
      "Epoch 300: Train Loss: 0.2155, Macro_F1: 0.8575, AUC_score: 0.9458\n",
      "Epoch 350: Train Loss: 0.1483, Macro_F1: 0.8718, AUC_score: 0.9533\n",
      "Epoch 00352: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.138960 --> 0.138960).\n",
      "Validation loss decreased (0.136253 --> 0.136253).\n",
      "Epoch 400: Train Loss: 0.1690, Macro_F1: 0.8764, AUC_score: 0.9531\n",
      "Validation loss decreased (0.135800 --> 0.135800).\n",
      "Validation loss decreased (0.134770 --> 0.134770).\n",
      "Epoch 450: Train Loss: 0.1498, Macro_F1: 0.8755, AUC_score: 0.9533\n",
      "Validation loss decreased (0.134010 --> 0.134010).\n",
      "Validation loss decreased (0.133709 --> 0.133709).\n",
      "Epoch 500: Train Loss: 0.1867, Macro_F1: 0.8739, AUC_score: 0.9533\n",
      "Epoch 550: Train Loss: 0.1581, Macro_F1: 0.8758, AUC_score: 0.9532\n",
      "Epoch 00557: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 600: Train Loss: 0.1843, Macro_F1: 0.8725, AUC_score: 0.9534\n",
      "Epoch 650: Train Loss: 0.1662, Macro_F1: 0.8746, AUC_score: 0.9533\n",
      "Early stopping triggered\n",
      "acc save\n",
      "31.999999999999996% node features transform to 0: F1: 0.8738, AUC_score: 0.9533\n",
      "Epoch 0: Train Loss: 0.1417, Macro_F1: 0.8682, AUC_score: 0.9515\n",
      "Validation loss decreased (0.141667 --> 0.141667).\n",
      "Epoch 50: Train Loss: 0.2092, Macro_F1: 0.8735, AUC_score: 0.9542\n",
      "Validation loss decreased (0.140779 --> 0.140779).\n",
      "Epoch 100: Train Loss: 0.1903, Macro_F1: 0.8743, AUC_score: 0.9523\n",
      "Epoch 150: Train Loss: 0.1757, Macro_F1: 0.8742, AUC_score: 0.9533\n",
      "Epoch 00167: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.138775 --> 0.138775).\n",
      "Epoch 200: Train Loss: 0.1572, Macro_F1: 0.8741, AUC_score: 0.9528\n",
      "Validation loss decreased (0.137411 --> 0.137411).\n",
      "Epoch 250: Train Loss: 0.1474, Macro_F1: 0.8758, AUC_score: 0.9530\n",
      "Validation loss decreased (0.132160 --> 0.132160).\n",
      "Epoch 300: Train Loss: 0.1479, Macro_F1: 0.8727, AUC_score: 0.9533\n",
      "Validation loss decreased (0.131219 --> 0.131219).\n",
      "Epoch 350: Train Loss: 0.1551, Macro_F1: 0.8718, AUC_score: 0.9524\n",
      "Epoch 400: Train Loss: 0.1380, Macro_F1: 0.8742, AUC_score: 0.9538\n",
      "Epoch 00405: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.1458, Macro_F1: 0.8746, AUC_score: 0.9534\n",
      "Epoch 500: Train Loss: 0.1436, Macro_F1: 0.8752, AUC_score: 0.9533\n",
      "Early stopping triggered\n",
      "acc save\n",
      "30.999999999999993% node features transform to 0: F1: 0.8752, AUC_score: 0.9534\n",
      "Epoch 0: Train Loss: 0.1421, Macro_F1: 0.8668, AUC_score: 0.9509\n",
      "Validation loss decreased (0.142147 --> 0.142147).\n",
      "Validation loss decreased (0.139274 --> 0.139274).\n",
      "Epoch 50: Train Loss: 0.2228, Macro_F1: 0.8719, AUC_score: 0.9535\n",
      "Epoch 100: Train Loss: 0.1545, Macro_F1: 0.8706, AUC_score: 0.9534\n",
      "Epoch 00138: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.1417, Macro_F1: 0.8717, AUC_score: 0.9535\n",
      "Validation loss decreased (0.137550 --> 0.137550).\n",
      "Epoch 200: Train Loss: 0.1658, Macro_F1: 0.8741, AUC_score: 0.9530\n",
      "Validation loss decreased (0.135547 --> 0.135547).\n",
      "Validation loss decreased (0.130680 --> 0.130680).\n",
      "Epoch 250: Train Loss: 0.1826, Macro_F1: 0.8757, AUC_score: 0.9533\n",
      "Epoch 300: Train Loss: 0.1881, Macro_F1: 0.8720, AUC_score: 0.9535\n",
      "Epoch 00319: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.1521, Macro_F1: 0.8750, AUC_score: 0.9536\n",
      "Epoch 400: Train Loss: 0.1562, Macro_F1: 0.8741, AUC_score: 0.9535\n",
      "Early stopping triggered\n",
      "acc save\n",
      "29.999999999999993% node features transform to 0: F1: 0.8736, AUC_score: 0.9536\n",
      "Epoch 0: Train Loss: 0.1641, Macro_F1: 0.8339, AUC_score: 0.9409\n",
      "Validation loss decreased (0.164089 --> 0.164089).\n",
      "Validation loss decreased (0.156272 --> 0.156272).\n",
      "Validation loss decreased (0.154469 --> 0.154469).\n",
      "Validation loss decreased (0.153456 --> 0.153456).\n",
      "Validation loss decreased (0.146337 --> 0.146337).\n",
      "Validation loss decreased (0.145858 --> 0.145858).\n",
      "Validation loss decreased (0.145433 --> 0.145433).\n",
      "Validation loss decreased (0.142572 --> 0.142572).\n",
      "Validation loss decreased (0.137776 --> 0.137776).\n",
      "Epoch 50: Train Loss: 0.1486, Macro_F1: 0.8700, AUC_score: 0.9528\n",
      "Epoch 100: Train Loss: 0.1962, Macro_F1: 0.8647, AUC_score: 0.9520\n",
      "Epoch 00133: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.1969, Macro_F1: 0.8671, AUC_score: 0.9536\n",
      "Validation loss decreased (0.133922 --> 0.133922).\n",
      "Validation loss decreased (0.130998 --> 0.130998).\n",
      "Epoch 200: Train Loss: 0.1762, Macro_F1: 0.8765, AUC_score: 0.9538\n",
      "Epoch 250: Train Loss: 0.1588, Macro_F1: 0.8761, AUC_score: 0.9531\n",
      "Epoch 00284: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 300: Train Loss: 0.1432, Macro_F1: 0.8755, AUC_score: 0.9537\n",
      "Epoch 350: Train Loss: 0.1597, Macro_F1: 0.8732, AUC_score: 0.9538\n",
      "Early stopping triggered\n",
      "acc save\n",
      "29.000000000000004% node features transform to 0: F1: 0.8747, AUC_score: 0.9536\n",
      "Epoch 0: Train Loss: 0.1514, Macro_F1: 0.8378, AUC_score: 0.9487\n",
      "Validation loss decreased (0.151418 --> 0.151418).\n",
      "Validation loss decreased (0.148293 --> 0.148293).\n",
      "Validation loss decreased (0.144882 --> 0.144882).\n",
      "Validation loss decreased (0.144113 --> 0.144113).\n",
      "Validation loss decreased (0.141815 --> 0.141815).\n",
      "Validation loss decreased (0.138810 --> 0.138810).\n",
      "Epoch 50: Train Loss: 0.1572, Macro_F1: 0.8695, AUC_score: 0.9537\n",
      "Epoch 100: Train Loss: 0.1568, Macro_F1: 0.8731, AUC_score: 0.9539\n",
      "Validation loss decreased (0.137486 --> 0.137486).\n",
      "Epoch 150: Train Loss: 0.1816, Macro_F1: 0.8662, AUC_score: 0.9514\n",
      "Epoch 200: Train Loss: 0.1601, Macro_F1: 0.8678, AUC_score: 0.9534\n",
      "Epoch 00240: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.1647, Macro_F1: 0.8765, AUC_score: 0.9540\n",
      "Validation loss decreased (0.137242 --> 0.137242).\n",
      "Epoch 300: Train Loss: 0.1520, Macro_F1: 0.8723, AUC_score: 0.9539\n",
      "Validation loss decreased (0.135827 --> 0.135827).\n",
      "Validation loss decreased (0.135655 --> 0.135655).\n",
      "Validation loss decreased (0.135255 --> 0.135255).\n",
      "Epoch 350: Train Loss: 0.1405, Macro_F1: 0.8721, AUC_score: 0.9539\n",
      "Epoch 400: Train Loss: 0.1725, Macro_F1: 0.8726, AUC_score: 0.9539\n",
      "Epoch 00413: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.135167 --> 0.135167).\n",
      "Validation loss decreased (0.133731 --> 0.133731).\n",
      "Epoch 450: Train Loss: 0.1422, Macro_F1: 0.8750, AUC_score: 0.9535\n",
      "Validation loss decreased (0.130899 --> 0.130899).\n",
      "Epoch 500: Train Loss: 0.1660, Macro_F1: 0.8732, AUC_score: 0.9537\n",
      "Epoch 550: Train Loss: 0.1365, Macro_F1: 0.8752, AUC_score: 0.9534\n",
      "Epoch 00556: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 600: Train Loss: 0.1387, Macro_F1: 0.8725, AUC_score: 0.9536\n",
      "Epoch 650: Train Loss: 0.1747, Macro_F1: 0.8709, AUC_score: 0.9536\n",
      "Early stopping triggered\n",
      "acc save\n",
      "28.000000000000004% node features transform to 0: F1: 0.8709, AUC_score: 0.9536\n",
      "Epoch 0: Train Loss: 0.1433, Macro_F1: 0.8458, AUC_score: 0.9432\n",
      "Validation loss decreased (0.143257 --> 0.143257).\n",
      "Validation loss decreased (0.142450 --> 0.142450).\n",
      "Epoch 50: Train Loss: 0.1449, Macro_F1: 0.8760, AUC_score: 0.9517\n",
      "Validation loss decreased (0.142123 --> 0.142123).\n",
      "Validation loss decreased (0.139549 --> 0.139549).\n",
      "Epoch 100: Train Loss: 0.1552, Macro_F1: 0.8721, AUC_score: 0.9521\n",
      "Validation loss decreased (0.135909 --> 0.135909).\n",
      "Validation loss decreased (0.135797 --> 0.135797).\n",
      "Epoch 150: Train Loss: 0.1960, Macro_F1: 0.8734, AUC_score: 0.9525\n",
      "Epoch 200: Train Loss: 0.1872, Macro_F1: 0.8664, AUC_score: 0.9514\n",
      "Epoch 00211: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.1478, Macro_F1: 0.8709, AUC_score: 0.9529\n",
      "Validation loss decreased (0.131923 --> 0.131923).\n",
      "Validation loss decreased (0.130437 --> 0.130437).\n",
      "Epoch 300: Train Loss: 0.1448, Macro_F1: 0.8723, AUC_score: 0.9529\n",
      "Epoch 350: Train Loss: 0.1764, Macro_F1: 0.8678, AUC_score: 0.9526\n",
      "Epoch 00377: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.1658, Macro_F1: 0.8722, AUC_score: 0.9525\n",
      "Epoch 450: Train Loss: 0.1303, Macro_F1: 0.8688, AUC_score: 0.9525\n",
      "Validation loss decreased (0.130256 --> 0.130256).\n",
      "Epoch 500: Train Loss: 0.1612, Macro_F1: 0.8711, AUC_score: 0.9527\n",
      "Validation loss decreased (0.130149 --> 0.130149).\n",
      "Epoch 550: Train Loss: 0.1309, Macro_F1: 0.8704, AUC_score: 0.9529\n",
      "Validation loss decreased (0.129694 --> 0.129694).\n",
      "Epoch 600: Train Loss: 0.1412, Macro_F1: 0.8726, AUC_score: 0.9528\n",
      "Validation loss decreased (0.126566 --> 0.126566).\n",
      "Epoch 650: Train Loss: 0.1592, Macro_F1: 0.8698, AUC_score: 0.9531\n",
      "Epoch 700: Train Loss: 0.1497, Macro_F1: 0.8723, AUC_score: 0.9529\n",
      "Epoch 00712: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 750: Train Loss: 0.1360, Macro_F1: 0.8712, AUC_score: 0.9527\n",
      "Validation loss decreased (0.126509 --> 0.126509).\n",
      "Epoch 800: Train Loss: 0.1375, Macro_F1: 0.8721, AUC_score: 0.9528\n",
      "Epoch 850: Train Loss: 0.2322, Macro_F1: 0.8706, AUC_score: 0.9527\n",
      "Epoch 00892: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 900: Train Loss: 0.1710, Macro_F1: 0.8706, AUC_score: 0.9527\n",
      "Epoch 950: Train Loss: 0.1567, Macro_F1: 0.8690, AUC_score: 0.9527\n",
      "Early stopping triggered\n",
      "acc save\n",
      "27.0% node features transform to 0: F1: 0.8690, AUC_score: 0.9527\n",
      "Epoch 0: Train Loss: 0.1602, Macro_F1: 0.8437, AUC_score: 0.9494\n",
      "Validation loss decreased (0.160190 --> 0.160190).\n",
      "Validation loss decreased (0.154938 --> 0.154938).\n",
      "Validation loss decreased (0.149617 --> 0.149617).\n",
      "Validation loss decreased (0.147054 --> 0.147054).\n",
      "Validation loss decreased (0.146155 --> 0.146155).\n",
      "Epoch 50: Train Loss: 0.2342, Macro_F1: 0.8597, AUC_score: 0.9478\n",
      "Validation loss decreased (0.144372 --> 0.144372).\n",
      "Validation loss decreased (0.141714 --> 0.141714).\n",
      "Validation loss decreased (0.136811 --> 0.136811).\n",
      "Epoch 100: Train Loss: 0.1613, Macro_F1: 0.8759, AUC_score: 0.9526\n",
      "Epoch 150: Train Loss: 0.1506, Macro_F1: 0.8718, AUC_score: 0.9529\n",
      "Epoch 00192: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.1562, Macro_F1: 0.8727, AUC_score: 0.9537\n",
      "Validation loss decreased (0.132077 --> 0.132077).\n",
      "Epoch 250: Train Loss: 0.1490, Macro_F1: 0.8685, AUC_score: 0.9541\n",
      "Epoch 300: Train Loss: 0.1526, Macro_F1: 0.8731, AUC_score: 0.9532\n",
      "Epoch 00341: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 350: Train Loss: 0.1456, Macro_F1: 0.8753, AUC_score: 0.9531\n",
      "Validation loss decreased (0.128711 --> 0.128711).\n",
      "Epoch 400: Train Loss: 0.1408, Macro_F1: 0.8734, AUC_score: 0.9530\n",
      "Epoch 450: Train Loss: 0.1487, Macro_F1: 0.8725, AUC_score: 0.9530\n",
      "Epoch 00484: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 500: Train Loss: 0.1345, Macro_F1: 0.8712, AUC_score: 0.9530\n",
      "Epoch 550: Train Loss: 0.1388, Macro_F1: 0.8731, AUC_score: 0.9531\n",
      "Early stopping triggered\n",
      "acc save\n",
      "26.0% node features transform to 0: F1: 0.8723, AUC_score: 0.9530\n",
      "Epoch 0: Train Loss: 0.1657, Macro_F1: 0.8386, AUC_score: 0.9406\n",
      "Validation loss decreased (0.165660 --> 0.165660).\n",
      "Validation loss decreased (0.153006 --> 0.153006).\n",
      "Validation loss decreased (0.147044 --> 0.147044).\n",
      "Validation loss decreased (0.141997 --> 0.141997).\n",
      "Validation loss decreased (0.135473 --> 0.135473).\n",
      "Validation loss decreased (0.134080 --> 0.134080).\n",
      "Epoch 50: Train Loss: 0.1402, Macro_F1: 0.8729, AUC_score: 0.9516\n",
      "Validation loss decreased (0.132411 --> 0.132411).\n",
      "Epoch 100: Train Loss: 0.1560, Macro_F1: 0.8662, AUC_score: 0.9515\n",
      "Epoch 150: Train Loss: 0.1498, Macro_F1: 0.8663, AUC_score: 0.9520\n",
      "Epoch 00170: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.1472, Macro_F1: 0.8737, AUC_score: 0.9519\n",
      "Validation loss decreased (0.131098 --> 0.131098).\n",
      "Epoch 250: Train Loss: 0.1514, Macro_F1: 0.8723, AUC_score: 0.9516\n",
      "Epoch 300: Train Loss: 0.1467, Macro_F1: 0.8730, AUC_score: 0.9519\n",
      "Epoch 00320: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.131035 --> 0.131035).\n",
      "Validation loss decreased (0.130879 --> 0.130879).\n",
      "Epoch 350: Train Loss: 0.1454, Macro_F1: 0.8733, AUC_score: 0.9523\n",
      "Validation loss decreased (0.130499 --> 0.130499).\n",
      "Epoch 400: Train Loss: 0.1386, Macro_F1: 0.8737, AUC_score: 0.9524\n",
      "Validation loss decreased (0.130368 --> 0.130368).\n",
      "Epoch 450: Train Loss: 0.1744, Macro_F1: 0.8738, AUC_score: 0.9524\n",
      "Epoch 500: Train Loss: 0.1355, Macro_F1: 0.8729, AUC_score: 0.9527\n",
      "Validation loss decreased (0.129191 --> 0.129191).\n",
      "Epoch 550: Train Loss: 0.1423, Macro_F1: 0.8749, AUC_score: 0.9521\n",
      "Validation loss decreased (0.126735 --> 0.126735).\n",
      "Epoch 600: Train Loss: 0.1410, Macro_F1: 0.8733, AUC_score: 0.9524\n",
      "Epoch 650: Train Loss: 0.1739, Macro_F1: 0.8729, AUC_score: 0.9525\n",
      "Validation loss decreased (0.126525 --> 0.126525).\n",
      "Epoch 700: Train Loss: 0.2295, Macro_F1: 0.8724, AUC_score: 0.9523\n",
      "Epoch 750: Train Loss: 0.1609, Macro_F1: 0.8746, AUC_score: 0.9525\n",
      "Epoch 00762: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 800: Train Loss: 0.1413, Macro_F1: 0.8722, AUC_score: 0.9523\n",
      "Epoch 850: Train Loss: 0.1693, Macro_F1: 0.8738, AUC_score: 0.9525\n",
      "Early stopping triggered\n",
      "acc save\n",
      "25.0% node features transform to 0: F1: 0.8730, AUC_score: 0.9524\n",
      "Epoch 0: Train Loss: 0.1525, Macro_F1: 0.8375, AUC_score: 0.9490\n",
      "Validation loss decreased (0.152521 --> 0.152521).\n",
      "Validation loss decreased (0.142155 --> 0.142155).\n",
      "Validation loss decreased (0.137417 --> 0.137417).\n",
      "Validation loss decreased (0.135371 --> 0.135371).\n",
      "Epoch 50: Train Loss: 0.1415, Macro_F1: 0.8736, AUC_score: 0.9524\n",
      "Epoch 100: Train Loss: 0.1857, Macro_F1: 0.8717, AUC_score: 0.9513\n",
      "Epoch 00144: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.1777, Macro_F1: 0.8737, AUC_score: 0.9531\n",
      "Epoch 200: Train Loss: 0.1512, Macro_F1: 0.8724, AUC_score: 0.9529\n",
      "Validation loss decreased (0.134078 --> 0.134078).\n",
      "Epoch 250: Train Loss: 0.1765, Macro_F1: 0.8700, AUC_score: 0.9530\n",
      "Validation loss decreased (0.133879 --> 0.133879).\n",
      "Validation loss decreased (0.130322 --> 0.130322).\n",
      "Validation loss decreased (0.129627 --> 0.129627).\n",
      "Validation loss decreased (0.129370 --> 0.129370).\n",
      "Epoch 300: Train Loss: 0.1582, Macro_F1: 0.8720, AUC_score: 0.9528\n",
      "Epoch 350: Train Loss: 0.2314, Macro_F1: 0.8689, AUC_score: 0.9535\n",
      "Epoch 00383: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.1269, Macro_F1: 0.8749, AUC_score: 0.9530\n",
      "Validation loss decreased (0.126908 --> 0.126908).\n",
      "Epoch 450: Train Loss: 0.1751, Macro_F1: 0.8775, AUC_score: 0.9528\n",
      "Validation loss decreased (0.126866 --> 0.126866).\n",
      "Validation loss decreased (0.125137 --> 0.125137).\n",
      "Epoch 500: Train Loss: 0.1291, Macro_F1: 0.8749, AUC_score: 0.9526\n",
      "Validation loss decreased (0.124234 --> 0.124234).\n",
      "Epoch 550: Train Loss: 0.1377, Macro_F1: 0.8716, AUC_score: 0.9527\n",
      "Epoch 600: Train Loss: 0.1315, Macro_F1: 0.8731, AUC_score: 0.9530\n",
      "Epoch 00619: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 650: Train Loss: 0.1270, Macro_F1: 0.8763, AUC_score: 0.9527\n",
      "Epoch 700: Train Loss: 0.1576, Macro_F1: 0.8760, AUC_score: 0.9528\n",
      "Early stopping triggered\n",
      "acc save\n",
      "24.0% node features transform to 0: F1: 0.8761, AUC_score: 0.9528\n",
      "Epoch 0: Train Loss: 0.1400, Macro_F1: 0.8287, AUC_score: 0.9467\n",
      "Validation loss decreased (0.140021 --> 0.140021).\n",
      "Validation loss decreased (0.135858 --> 0.135858).\n",
      "Epoch 50: Train Loss: 0.1570, Macro_F1: 0.8691, AUC_score: 0.9507\n",
      "Epoch 100: Train Loss: 0.1636, Macro_F1: 0.8727, AUC_score: 0.9503\n",
      "Validation loss decreased (0.133590 --> 0.133590).\n",
      "Validation loss decreased (0.133130 --> 0.133130).\n",
      "Epoch 150: Train Loss: 0.1493, Macro_F1: 0.8703, AUC_score: 0.9528\n",
      "Epoch 200: Train Loss: 0.1489, Macro_F1: 0.8714, AUC_score: 0.9496\n",
      "Epoch 00228: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.1466, Macro_F1: 0.8771, AUC_score: 0.9525\n",
      "Epoch 300: Train Loss: 0.1861, Macro_F1: 0.8736, AUC_score: 0.9520\n",
      "Early stopping triggered\n",
      "acc save\n",
      "23.0% node features transform to 0: F1: 0.8738, AUC_score: 0.9513\n",
      "Epoch 0: Train Loss: 0.1529, Macro_F1: 0.8500, AUC_score: 0.9478\n",
      "Validation loss decreased (0.152934 --> 0.152934).\n",
      "Validation loss decreased (0.147510 --> 0.147510).\n",
      "Validation loss decreased (0.143492 --> 0.143492).\n",
      "Validation loss decreased (0.143028 --> 0.143028).\n",
      "Validation loss decreased (0.141856 --> 0.141856).\n",
      "Validation loss decreased (0.141599 --> 0.141599).\n",
      "Validation loss decreased (0.141362 --> 0.141362).\n",
      "Epoch 50: Train Loss: 0.1799, Macro_F1: 0.8709, AUC_score: 0.9510\n",
      "Validation loss decreased (0.139457 --> 0.139457).\n",
      "Validation loss decreased (0.138910 --> 0.138910).\n",
      "Validation loss decreased (0.136516 --> 0.136516).\n",
      "Epoch 100: Train Loss: 0.1441, Macro_F1: 0.8662, AUC_score: 0.9499\n",
      "Epoch 150: Train Loss: 0.1758, Macro_F1: 0.8664, AUC_score: 0.9496\n",
      "Validation loss decreased (0.134382 --> 0.134382).\n",
      "Epoch 200: Train Loss: 0.1629, Macro_F1: 0.8687, AUC_score: 0.9491\n",
      "Epoch 250: Train Loss: 0.2676, Macro_F1: 0.8677, AUC_score: 0.9514\n",
      "Epoch 00259: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.1391, Macro_F1: 0.8772, AUC_score: 0.9516\n",
      "Validation loss decreased (0.132274 --> 0.132274).\n",
      "Epoch 350: Train Loss: 0.1372, Macro_F1: 0.8732, AUC_score: 0.9518\n",
      "Epoch 400: Train Loss: 0.1410, Macro_F1: 0.8758, AUC_score: 0.9513\n",
      "Epoch 00406: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Validation loss decreased (0.130374 --> 0.130374).\n",
      "Validation loss decreased (0.129837 --> 0.129837).\n",
      "Epoch 450: Train Loss: 0.1318, Macro_F1: 0.8758, AUC_score: 0.9521\n",
      "Epoch 500: Train Loss: 0.1617, Macro_F1: 0.8748, AUC_score: 0.9522\n",
      "Validation loss decreased (0.129332 --> 0.129332).\n",
      "Validation loss decreased (0.127822 --> 0.127822).\n",
      "Validation loss decreased (0.127439 --> 0.127439).\n",
      "Epoch 550: Train Loss: 0.1402, Macro_F1: 0.8744, AUC_score: 0.9523\n",
      "Epoch 600: Train Loss: 0.1519, Macro_F1: 0.8744, AUC_score: 0.9519\n",
      "Epoch 00636: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 650: Train Loss: 0.1399, Macro_F1: 0.8753, AUC_score: 0.9523\n",
      "Epoch 700: Train Loss: 0.1617, Macro_F1: 0.8753, AUC_score: 0.9521\n",
      "Early stopping triggered\n",
      "acc save\n",
      "21.999999999999996% node features transform to 0: F1: 0.8745, AUC_score: 0.9521\n",
      "Epoch 0: Train Loss: 0.1497, Macro_F1: 0.8567, AUC_score: 0.9456\n",
      "Validation loss decreased (0.149667 --> 0.149667).\n",
      "Validation loss decreased (0.145556 --> 0.145556).\n",
      "Epoch 50: Train Loss: 0.1819, Macro_F1: 0.8609, AUC_score: 0.9431\n",
      "Validation loss decreased (0.141480 --> 0.141480).\n",
      "Validation loss decreased (0.139528 --> 0.139528).\n",
      "Validation loss decreased (0.137487 --> 0.137487).\n",
      "Epoch 100: Train Loss: 0.1478, Macro_F1: 0.8736, AUC_score: 0.9526\n",
      "Epoch 150: Train Loss: 0.1581, Macro_F1: 0.8710, AUC_score: 0.9511\n",
      "Validation loss decreased (0.136694 --> 0.136694).\n",
      "Validation loss decreased (0.127609 --> 0.127609).\n",
      "Epoch 200: Train Loss: 0.1519, Macro_F1: 0.8716, AUC_score: 0.9515\n",
      "Epoch 250: Train Loss: 0.1693, Macro_F1: 0.8762, AUC_score: 0.9505\n",
      "Epoch 00257: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.1624, Macro_F1: 0.8740, AUC_score: 0.9529\n",
      "Epoch 350: Train Loss: 0.1425, Macro_F1: 0.8753, AUC_score: 0.9528\n",
      "Early stopping triggered\n",
      "acc save\n",
      "20.999999999999996% node features transform to 0: F1: 0.8730, AUC_score: 0.9529\n",
      "Epoch 0: Train Loss: 0.1497, Macro_F1: 0.8293, AUC_score: 0.9507\n",
      "Validation loss decreased (0.149734 --> 0.149734).\n",
      "Validation loss decreased (0.144790 --> 0.144790).\n",
      "Validation loss decreased (0.137592 --> 0.137592).\n",
      "Validation loss decreased (0.136860 --> 0.136860).\n",
      "Validation loss decreased (0.135870 --> 0.135870).\n",
      "Epoch 50: Train Loss: 0.1316, Macro_F1: 0.8719, AUC_score: 0.9541\n",
      "Validation loss decreased (0.131556 --> 0.131556).\n",
      "Epoch 100: Train Loss: 0.1586, Macro_F1: 0.8741, AUC_score: 0.9539\n",
      "Validation loss decreased (0.129463 --> 0.129463).\n",
      "Epoch 150: Train Loss: 0.1427, Macro_F1: 0.8743, AUC_score: 0.9539\n",
      "Epoch 200: Train Loss: 0.1834, Macro_F1: 0.8702, AUC_score: 0.9497\n",
      "Epoch 00205: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.1455, Macro_F1: 0.8739, AUC_score: 0.9534\n",
      "Validation loss decreased (0.126272 --> 0.126272).\n",
      "Validation loss decreased (0.124659 --> 0.124659).\n",
      "Validation loss decreased (0.124431 --> 0.124431).\n",
      "Epoch 300: Train Loss: 0.1356, Macro_F1: 0.8747, AUC_score: 0.9532\n",
      "Epoch 350: Train Loss: 0.1525, Macro_F1: 0.8751, AUC_score: 0.9531\n",
      "Epoch 00376: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.1633, Macro_F1: 0.8703, AUC_score: 0.9527\n",
      "Epoch 450: Train Loss: 0.1361, Macro_F1: 0.8722, AUC_score: 0.9530\n",
      "Early stopping triggered\n",
      "acc save\n",
      "19.999999999999996% node features transform to 0: F1: 0.8722, AUC_score: 0.9527\n",
      "Epoch 0: Train Loss: 0.1393, Macro_F1: 0.8390, AUC_score: 0.9484\n",
      "Validation loss decreased (0.139297 --> 0.139297).\n",
      "Epoch 50: Train Loss: 0.1750, Macro_F1: 0.8766, AUC_score: 0.9537\n",
      "Validation loss decreased (0.133579 --> 0.133579).\n",
      "Validation loss decreased (0.130935 --> 0.130935).\n",
      "Epoch 100: Train Loss: 0.1408, Macro_F1: 0.8748, AUC_score: 0.9522\n",
      "Epoch 150: Train Loss: 0.1569, Macro_F1: 0.8646, AUC_score: 0.9478\n",
      "Epoch 00186: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.1843, Macro_F1: 0.8739, AUC_score: 0.9523\n",
      "Validation loss decreased (0.129426 --> 0.129426).\n",
      "Validation loss decreased (0.129318 --> 0.129318).\n",
      "Epoch 250: Train Loss: 0.1341, Macro_F1: 0.8761, AUC_score: 0.9526\n",
      "Validation loss decreased (0.126178 --> 0.126178).\n",
      "Epoch 300: Train Loss: 0.1318, Macro_F1: 0.8760, AUC_score: 0.9518\n",
      "Validation loss decreased (0.125197 --> 0.125197).\n",
      "Epoch 350: Train Loss: 0.1449, Macro_F1: 0.8731, AUC_score: 0.9524\n",
      "Validation loss decreased (0.124984 --> 0.124984).\n",
      "Epoch 400: Train Loss: 0.1522, Macro_F1: 0.8740, AUC_score: 0.9528\n",
      "Validation loss decreased (0.124520 --> 0.124520).\n",
      "Epoch 450: Train Loss: 0.1349, Macro_F1: 0.8747, AUC_score: 0.9531\n",
      "Validation loss decreased (0.120513 --> 0.120513).\n",
      "Epoch 500: Train Loss: 0.1345, Macro_F1: 0.8708, AUC_score: 0.9515\n",
      "Epoch 550: Train Loss: 0.1273, Macro_F1: 0.8753, AUC_score: 0.9534\n",
      "Epoch 00572: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 600: Train Loss: 0.1333, Macro_F1: 0.8743, AUC_score: 0.9523\n",
      "Epoch 650: Train Loss: 0.1327, Macro_F1: 0.8739, AUC_score: 0.9526\n",
      "Early stopping triggered\n",
      "acc save\n",
      "18.999999999999993% node features transform to 0: F1: 0.8757, AUC_score: 0.9528\n",
      "Epoch 0: Train Loss: 0.1502, Macro_F1: 0.8079, AUC_score: 0.9436\n",
      "Validation loss decreased (0.150199 --> 0.150199).\n",
      "Validation loss decreased (0.143856 --> 0.143856).\n",
      "Validation loss decreased (0.134465 --> 0.134465).\n",
      "Validation loss decreased (0.132698 --> 0.132698).\n",
      "Epoch 50: Train Loss: 0.1381, Macro_F1: 0.8747, AUC_score: 0.9527\n",
      "Validation loss decreased (0.132600 --> 0.132600).\n",
      "Validation loss decreased (0.124866 --> 0.124866).\n",
      "Epoch 100: Train Loss: 0.1713, Macro_F1: 0.8733, AUC_score: 0.9513\n",
      "Epoch 150: Train Loss: 0.1705, Macro_F1: 0.8676, AUC_score: 0.9519\n",
      "Epoch 00164: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.1407, Macro_F1: 0.8785, AUC_score: 0.9530\n",
      "Epoch 250: Train Loss: 0.1632, Macro_F1: 0.8733, AUC_score: 0.9516\n",
      "Early stopping triggered\n",
      "acc save\n",
      "17.999999999999993% node features transform to 0: F1: 0.8742, AUC_score: 0.9525\n",
      "Epoch 0: Train Loss: 0.1361, Macro_F1: 0.8567, AUC_score: 0.9500\n",
      "Validation loss decreased (0.136057 --> 0.136057).\n",
      "Validation loss decreased (0.130864 --> 0.130864).\n",
      "Epoch 50: Train Loss: 0.1533, Macro_F1: 0.8697, AUC_score: 0.9495\n",
      "Epoch 100: Train Loss: 0.1580, Macro_F1: 0.8728, AUC_score: 0.9514\n",
      "Epoch 00147: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.1714, Macro_F1: 0.8740, AUC_score: 0.9527\n",
      "Validation loss decreased (0.130683 --> 0.130683).\n",
      "Validation loss decreased (0.129937 --> 0.129937).\n",
      "Epoch 200: Train Loss: 0.1681, Macro_F1: 0.8776, AUC_score: 0.9533\n",
      "Validation loss decreased (0.127814 --> 0.127814).\n",
      "Epoch 250: Train Loss: 0.1804, Macro_F1: 0.8696, AUC_score: 0.9527\n",
      "Validation loss decreased (0.126835 --> 0.126835).\n",
      "Epoch 300: Train Loss: 0.1415, Macro_F1: 0.8740, AUC_score: 0.9524\n",
      "Validation loss decreased (0.125758 --> 0.125758).\n",
      "Validation loss decreased (0.125384 --> 0.125384).\n",
      "Epoch 350: Train Loss: 0.1447, Macro_F1: 0.8749, AUC_score: 0.9524\n",
      "Validation loss decreased (0.123703 --> 0.123703).\n",
      "Validation loss decreased (0.122942 --> 0.122942).\n",
      "Epoch 400: Train Loss: 0.1538, Macro_F1: 0.8736, AUC_score: 0.9533\n",
      "Epoch 450: Train Loss: 0.1725, Macro_F1: 0.8730, AUC_score: 0.9529\n",
      "Epoch 00501: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.1547, Macro_F1: 0.8738, AUC_score: 0.9529\n",
      "Validation loss decreased (0.122369 --> 0.122369).\n",
      "Validation loss decreased (0.120410 --> 0.120410).\n",
      "Epoch 550: Train Loss: 0.1460, Macro_F1: 0.8762, AUC_score: 0.9531\n",
      "Epoch 600: Train Loss: 0.1331, Macro_F1: 0.8738, AUC_score: 0.9524\n",
      "Epoch 00641: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 650: Train Loss: 0.1271, Macro_F1: 0.8735, AUC_score: 0.9525\n",
      "Epoch 700: Train Loss: 0.1751, Macro_F1: 0.8733, AUC_score: 0.9527\n",
      "Validation loss decreased (0.119157 --> 0.119157).\n",
      "Epoch 750: Train Loss: 0.1335, Macro_F1: 0.8738, AUC_score: 0.9527\n",
      "Epoch 800: Train Loss: 0.1377, Macro_F1: 0.8741, AUC_score: 0.9525\n",
      "Epoch 00814: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 850: Train Loss: 0.1364, Macro_F1: 0.8741, AUC_score: 0.9526\n",
      "Epoch 900: Train Loss: 0.1871, Macro_F1: 0.8741, AUC_score: 0.9525\n",
      "Early stopping triggered\n",
      "acc save\n",
      "16.999999999999993% node features transform to 0: F1: 0.8741, AUC_score: 0.9525\n",
      "Epoch 0: Train Loss: 0.1470, Macro_F1: 0.8485, AUC_score: 0.9428\n",
      "Validation loss decreased (0.146991 --> 0.146991).\n",
      "Validation loss decreased (0.141358 --> 0.141358).\n",
      "Validation loss decreased (0.137295 --> 0.137295).\n",
      "Validation loss decreased (0.133025 --> 0.133025).\n",
      "Epoch 50: Train Loss: 0.1580, Macro_F1: 0.8757, AUC_score: 0.9516\n",
      "Validation loss decreased (0.128227 --> 0.128227).\n",
      "Epoch 100: Train Loss: 0.1390, Macro_F1: 0.8739, AUC_score: 0.9538\n",
      "Epoch 150: Train Loss: 0.1489, Macro_F1: 0.8722, AUC_score: 0.9525\n",
      "Epoch 00163: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.1486, Macro_F1: 0.8735, AUC_score: 0.9517\n",
      "Validation loss decreased (0.127700 --> 0.127700).\n",
      "Epoch 250: Train Loss: 0.1452, Macro_F1: 0.8752, AUC_score: 0.9525\n",
      "Validation loss decreased (0.126011 --> 0.126011).\n",
      "Epoch 300: Train Loss: 0.1348, Macro_F1: 0.8763, AUC_score: 0.9521\n",
      "Validation loss decreased (0.123811 --> 0.123811).\n",
      "Validation loss decreased (0.123705 --> 0.123705).\n",
      "Epoch 350: Train Loss: 0.1371, Macro_F1: 0.8778, AUC_score: 0.9526\n",
      "Epoch 400: Train Loss: 0.1498, Macro_F1: 0.8698, AUC_score: 0.9502\n",
      "Validation loss decreased (0.122743 --> 0.122743).\n",
      "Epoch 450: Train Loss: 0.1516, Macro_F1: 0.8753, AUC_score: 0.9521\n",
      "Epoch 500: Train Loss: 0.1697, Macro_F1: 0.8757, AUC_score: 0.9526\n",
      "Epoch 00522: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 550: Train Loss: 0.1581, Macro_F1: 0.8723, AUC_score: 0.9518\n",
      "Validation loss decreased (0.121789 --> 0.121789).\n",
      "Epoch 600: Train Loss: 0.1522, Macro_F1: 0.8735, AUC_score: 0.9527\n",
      "Epoch 650: Train Loss: 0.1330, Macro_F1: 0.8754, AUC_score: 0.9528\n",
      "Validation loss decreased (0.120860 --> 0.120860).\n",
      "Epoch 700: Train Loss: 0.1703, Macro_F1: 0.8744, AUC_score: 0.9526\n",
      "Epoch 750: Train Loss: 0.1373, Macro_F1: 0.8757, AUC_score: 0.9528\n",
      "Epoch 00757: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 800: Train Loss: 0.1536, Macro_F1: 0.8752, AUC_score: 0.9528\n",
      "Epoch 850: Train Loss: 0.1412, Macro_F1: 0.8752, AUC_score: 0.9527\n",
      "Early stopping triggered\n",
      "acc save\n",
      "16.000000000000004% node features transform to 0: F1: 0.8752, AUC_score: 0.9527\n",
      "Epoch 0: Train Loss: 0.1468, Macro_F1: 0.8587, AUC_score: 0.9478\n",
      "Validation loss decreased (0.146848 --> 0.146848).\n",
      "Validation loss decreased (0.141044 --> 0.141044).\n",
      "Validation loss decreased (0.140787 --> 0.140787).\n",
      "Validation loss decreased (0.137439 --> 0.137439).\n",
      "Validation loss decreased (0.135890 --> 0.135890).\n",
      "Validation loss decreased (0.133867 --> 0.133867).\n",
      "Validation loss decreased (0.131752 --> 0.131752).\n",
      "Epoch 50: Train Loss: 0.1526, Macro_F1: 0.8733, AUC_score: 0.9527\n",
      "Validation loss decreased (0.131607 --> 0.131607).\n",
      "Validation loss decreased (0.130156 --> 0.130156).\n",
      "Validation loss decreased (0.126804 --> 0.126804).\n",
      "Validation loss decreased (0.126730 --> 0.126730).\n",
      "Epoch 100: Train Loss: 0.1407, Macro_F1: 0.8665, AUC_score: 0.9520\n",
      "Epoch 150: Train Loss: 0.1750, Macro_F1: 0.8600, AUC_score: 0.9476\n",
      "Epoch 00171: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.1437, Macro_F1: 0.8776, AUC_score: 0.9536\n",
      "Validation loss decreased (0.126607 --> 0.126607).\n",
      "Validation loss decreased (0.122895 --> 0.122895).\n",
      "Epoch 250: Train Loss: 0.1498, Macro_F1: 0.8744, AUC_score: 0.9526\n",
      "Validation loss decreased (0.119806 --> 0.119806).\n",
      "Epoch 300: Train Loss: 0.1291, Macro_F1: 0.8757, AUC_score: 0.9531\n",
      "Epoch 350: Train Loss: 0.1207, Macro_F1: 0.8754, AUC_score: 0.9523\n",
      "Epoch 00382: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.1395, Macro_F1: 0.8765, AUC_score: 0.9532\n",
      "Epoch 450: Train Loss: 0.1307, Macro_F1: 0.8770, AUC_score: 0.9529\n",
      "Early stopping triggered\n",
      "acc save\n",
      "15.000000000000002% node features transform to 0: F1: 0.8717, AUC_score: 0.9524\n",
      "Epoch 0: Train Loss: 0.1360, Macro_F1: 0.8328, AUC_score: 0.9480\n",
      "Validation loss decreased (0.136017 --> 0.136017).\n",
      "Validation loss decreased (0.132871 --> 0.132871).\n",
      "Epoch 50: Train Loss: 0.1492, Macro_F1: 0.8768, AUC_score: 0.9525\n",
      "Validation loss decreased (0.130153 --> 0.130153).\n",
      "Validation loss decreased (0.127720 --> 0.127720).\n",
      "Epoch 100: Train Loss: 0.1381, Macro_F1: 0.8711, AUC_score: 0.9500\n",
      "Epoch 150: Train Loss: 0.1617, Macro_F1: 0.8730, AUC_score: 0.9515\n",
      "Epoch 00197: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.1496, Macro_F1: 0.8791, AUC_score: 0.9529\n",
      "Validation loss decreased (0.126178 --> 0.126178).\n",
      "Validation loss decreased (0.125045 --> 0.125045).\n",
      "Validation loss decreased (0.124240 --> 0.124240).\n",
      "Epoch 250: Train Loss: 0.1416, Macro_F1: 0.8779, AUC_score: 0.9537\n",
      "Validation loss decreased (0.121709 --> 0.121709).\n",
      "Epoch 300: Train Loss: 0.1275, Macro_F1: 0.8741, AUC_score: 0.9526\n",
      "Validation loss decreased (0.120972 --> 0.120972).\n",
      "Validation loss decreased (0.117685 --> 0.117685).\n",
      "Epoch 350: Train Loss: 0.1328, Macro_F1: 0.8738, AUC_score: 0.9529\n",
      "Epoch 400: Train Loss: 0.1211, Macro_F1: 0.8734, AUC_score: 0.9521\n",
      "Epoch 00433: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.1350, Macro_F1: 0.8764, AUC_score: 0.9526\n",
      "Epoch 500: Train Loss: 0.1312, Macro_F1: 0.8761, AUC_score: 0.9532\n",
      "Early stopping triggered\n",
      "acc save\n",
      "14.000000000000002% node features transform to 0: F1: 0.8767, AUC_score: 0.9529\n",
      "Epoch 0: Train Loss: 0.1273, Macro_F1: 0.8313, AUC_score: 0.9495\n",
      "Validation loss decreased (0.127310 --> 0.127310).\n",
      "Validation loss decreased (0.126733 --> 0.126733).\n",
      "Epoch 50: Train Loss: 0.1425, Macro_F1: 0.8653, AUC_score: 0.9497\n",
      "Epoch 100: Train Loss: 0.1725, Macro_F1: 0.8668, AUC_score: 0.9504\n",
      "Epoch 00131: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.1455, Macro_F1: 0.8725, AUC_score: 0.9501\n",
      "Validation loss decreased (0.124380 --> 0.124380).\n",
      "Validation loss decreased (0.124329 --> 0.124329).\n",
      "Epoch 200: Train Loss: 0.1376, Macro_F1: 0.8740, AUC_score: 0.9523\n",
      "Validation loss decreased (0.123718 --> 0.123718).\n",
      "Validation loss decreased (0.121454 --> 0.121454).\n",
      "Epoch 250: Train Loss: 0.1338, Macro_F1: 0.8744, AUC_score: 0.9515\n",
      "Epoch 300: Train Loss: 0.1332, Macro_F1: 0.8733, AUC_score: 0.9526\n",
      "Validation loss decreased (0.121091 --> 0.121091).\n",
      "Epoch 350: Train Loss: 0.1231, Macro_F1: 0.8746, AUC_score: 0.9517\n",
      "Epoch 400: Train Loss: 0.1498, Macro_F1: 0.8733, AUC_score: 0.9513\n",
      "Epoch 00438: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.1321, Macro_F1: 0.8778, AUC_score: 0.9524\n",
      "Validation loss decreased (0.120147 --> 0.120147).\n",
      "Validation loss decreased (0.115679 --> 0.115679).\n",
      "Epoch 500: Train Loss: 0.1684, Macro_F1: 0.8751, AUC_score: 0.9529\n",
      "Epoch 550: Train Loss: 0.1222, Macro_F1: 0.8751, AUC_score: 0.9525\n",
      "Epoch 00578: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 600: Train Loss: 0.1350, Macro_F1: 0.8757, AUC_score: 0.9529\n",
      "Validation loss decreased (0.113571 --> 0.113571).\n",
      "Epoch 650: Train Loss: 0.1300, Macro_F1: 0.8764, AUC_score: 0.9528\n",
      "Epoch 700: Train Loss: 0.1527, Macro_F1: 0.8748, AUC_score: 0.9529\n",
      "Epoch 00725: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 750: Train Loss: 0.1469, Macro_F1: 0.8754, AUC_score: 0.9528\n",
      "Epoch 800: Train Loss: 0.1274, Macro_F1: 0.8754, AUC_score: 0.9528\n",
      "Early stopping triggered\n",
      "acc save\n",
      "13.0% node features transform to 0: F1: 0.8754, AUC_score: 0.9528\n",
      "Epoch 0: Train Loss: 0.1316, Macro_F1: 0.8288, AUC_score: 0.9366\n",
      "Validation loss decreased (0.131617 --> 0.131617).\n",
      "Validation loss decreased (0.128717 --> 0.128717).\n",
      "Epoch 50: Train Loss: 0.1573, Macro_F1: 0.8736, AUC_score: 0.9527\n",
      "Epoch 100: Train Loss: 0.1929, Macro_F1: 0.8650, AUC_score: 0.9519\n",
      "Epoch 00131: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 150: Train Loss: 0.1409, Macro_F1: 0.8781, AUC_score: 0.9516\n",
      "Validation loss decreased (0.125964 --> 0.125964).\n",
      "Epoch 200: Train Loss: 0.1381, Macro_F1: 0.8751, AUC_score: 0.9522\n",
      "Validation loss decreased (0.124932 --> 0.124932).\n",
      "Validation loss decreased (0.124491 --> 0.124491).\n",
      "Validation loss decreased (0.123679 --> 0.123679).\n",
      "Epoch 250: Train Loss: 0.1702, Macro_F1: 0.8741, AUC_score: 0.9520\n",
      "Validation loss decreased (0.122710 --> 0.122710).\n",
      "Epoch 300: Train Loss: 0.1489, Macro_F1: 0.8746, AUC_score: 0.9512\n",
      "Validation loss decreased (0.117188 --> 0.117188).\n",
      "Epoch 350: Train Loss: 0.1447, Macro_F1: 0.8775, AUC_score: 0.9522\n",
      "Epoch 400: Train Loss: 0.1600, Macro_F1: 0.8726, AUC_score: 0.9529\n",
      "Epoch 00412: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.1207, Macro_F1: 0.8781, AUC_score: 0.9525\n",
      "Epoch 500: Train Loss: 0.1403, Macro_F1: 0.8772, AUC_score: 0.9525\n",
      "Early stopping triggered\n",
      "acc save\n",
      "12.0% node features transform to 0: F1: 0.8773, AUC_score: 0.9521\n",
      "Epoch 0: Train Loss: 0.1535, Macro_F1: 0.8227, AUC_score: 0.9468\n",
      "Validation loss decreased (0.153496 --> 0.153496).\n",
      "Validation loss decreased (0.142888 --> 0.142888).\n",
      "Validation loss decreased (0.137762 --> 0.137762).\n",
      "Validation loss decreased (0.134267 --> 0.134267).\n",
      "Validation loss decreased (0.130661 --> 0.130661).\n",
      "Validation loss decreased (0.130453 --> 0.130453).\n",
      "Epoch 50: Train Loss: 0.1385, Macro_F1: 0.8730, AUC_score: 0.9522\n",
      "Validation loss decreased (0.128509 --> 0.128509).\n",
      "Validation loss decreased (0.127622 --> 0.127622).\n",
      "Validation loss decreased (0.126210 --> 0.126210).\n",
      "Epoch 100: Train Loss: 0.2097, Macro_F1: 0.8734, AUC_score: 0.9517\n",
      "Validation loss decreased (0.122958 --> 0.122958).\n",
      "Epoch 150: Train Loss: 0.1488, Macro_F1: 0.8753, AUC_score: 0.9516\n",
      "Epoch 200: Train Loss: 0.1636, Macro_F1: 0.8692, AUC_score: 0.9496\n",
      "Epoch 00238: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.1329, Macro_F1: 0.8740, AUC_score: 0.9494\n",
      "Validation loss decreased (0.121999 --> 0.121999).\n",
      "Epoch 300: Train Loss: 0.1349, Macro_F1: 0.8751, AUC_score: 0.9515\n",
      "Validation loss decreased (0.121516 --> 0.121516).\n",
      "Validation loss decreased (0.120731 --> 0.120731).\n",
      "Epoch 350: Train Loss: 0.1272, Macro_F1: 0.8769, AUC_score: 0.9522\n",
      "Epoch 400: Train Loss: 0.1221, Macro_F1: 0.8758, AUC_score: 0.9519\n",
      "Validation loss decreased (0.119420 --> 0.119420).\n",
      "Epoch 450: Train Loss: 0.1337, Macro_F1: 0.8757, AUC_score: 0.9519\n",
      "Validation loss decreased (0.116866 --> 0.116866).\n",
      "Validation loss decreased (0.116659 --> 0.116659).\n",
      "Epoch 500: Train Loss: 0.1203, Macro_F1: 0.8737, AUC_score: 0.9532\n",
      "Epoch 550: Train Loss: 0.2053, Macro_F1: 0.8756, AUC_score: 0.9528\n",
      "Epoch 00600: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 600: Train Loss: 0.1235, Macro_F1: 0.8738, AUC_score: 0.9515\n",
      "Epoch 650: Train Loss: 0.1386, Macro_F1: 0.8751, AUC_score: 0.9523\n",
      "Early stopping triggered\n",
      "acc save\n",
      "10.999999999999998% node features transform to 0: F1: 0.8759, AUC_score: 0.9522\n",
      "Epoch 0: Train Loss: 0.1683, Macro_F1: 0.8474, AUC_score: 0.9423\n",
      "Validation loss decreased (0.168255 --> 0.168255).\n",
      "Validation loss decreased (0.141323 --> 0.141323).\n",
      "Validation loss decreased (0.140597 --> 0.140597).\n",
      "Validation loss decreased (0.134702 --> 0.134702).\n",
      "Validation loss decreased (0.127595 --> 0.127595).\n",
      "Epoch 50: Train Loss: 0.1266, Macro_F1: 0.8752, AUC_score: 0.9515\n",
      "Validation loss decreased (0.126577 --> 0.126577).\n",
      "Validation loss decreased (0.125987 --> 0.125987).\n",
      "Epoch 100: Train Loss: 0.1458, Macro_F1: 0.8759, AUC_score: 0.9504\n",
      "Validation loss decreased (0.120806 --> 0.120806).\n",
      "Epoch 150: Train Loss: 0.1414, Macro_F1: 0.8724, AUC_score: 0.9502\n",
      "Epoch 200: Train Loss: 0.1394, Macro_F1: 0.8776, AUC_score: 0.9523\n",
      "Epoch 00224: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.1632, Macro_F1: 0.8749, AUC_score: 0.9519\n",
      "Validation loss decreased (0.120339 --> 0.120339).\n",
      "Validation loss decreased (0.116262 --> 0.116262).\n",
      "Epoch 300: Train Loss: 0.1497, Macro_F1: 0.8750, AUC_score: 0.9529\n",
      "Epoch 350: Train Loss: 0.1294, Macro_F1: 0.8752, AUC_score: 0.9531\n",
      "Validation loss decreased (0.114527 --> 0.114527).\n",
      "Epoch 400: Train Loss: 0.1255, Macro_F1: 0.8736, AUC_score: 0.9529\n",
      "Epoch 450: Train Loss: 0.1254, Macro_F1: 0.8747, AUC_score: 0.9524\n",
      "Epoch 00468: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.1701, Macro_F1: 0.8764, AUC_score: 0.9529\n",
      "Epoch 550: Train Loss: 0.1410, Macro_F1: 0.8781, AUC_score: 0.9525\n",
      "Early stopping triggered\n",
      "acc save\n",
      "9.999999999999998% node features transform to 0: F1: 0.8759, AUC_score: 0.9523\n",
      "Epoch 0: Train Loss: 0.1405, Macro_F1: 0.8255, AUC_score: 0.9471\n",
      "Validation loss decreased (0.140487 --> 0.140487).\n",
      "Validation loss decreased (0.130731 --> 0.130731).\n",
      "Validation loss decreased (0.129981 --> 0.129981).\n",
      "Epoch 50: Train Loss: 0.1524, Macro_F1: 0.8715, AUC_score: 0.9537\n",
      "Validation loss decreased (0.128072 --> 0.128072).\n",
      "Validation loss decreased (0.125461 --> 0.125461).\n",
      "Validation loss decreased (0.123834 --> 0.123834).\n",
      "Epoch 100: Train Loss: 0.1481, Macro_F1: 0.8741, AUC_score: 0.9515\n",
      "Epoch 150: Train Loss: 0.1676, Macro_F1: 0.8656, AUC_score: 0.9500\n",
      "Epoch 00184: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.1376, Macro_F1: 0.8740, AUC_score: 0.9506\n",
      "Validation loss decreased (0.120759 --> 0.120759).\n",
      "Epoch 250: Train Loss: 0.1262, Macro_F1: 0.8765, AUC_score: 0.9526\n",
      "Validation loss decreased (0.119838 --> 0.119838).\n",
      "Validation loss decreased (0.119770 --> 0.119770).\n",
      "Epoch 300: Train Loss: 0.1200, Macro_F1: 0.8764, AUC_score: 0.9518\n",
      "Validation loss decreased (0.119352 --> 0.119352).\n",
      "Validation loss decreased (0.118001 --> 0.118001).\n",
      "Epoch 350: Train Loss: 0.1423, Macro_F1: 0.8762, AUC_score: 0.9524\n",
      "Validation loss decreased (0.117692 --> 0.117692).\n",
      "Epoch 400: Train Loss: 0.1221, Macro_F1: 0.8779, AUC_score: 0.9524\n",
      "Epoch 450: Train Loss: 0.1289, Macro_F1: 0.8714, AUC_score: 0.9507\n",
      "Epoch 00470: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.1532, Macro_F1: 0.8776, AUC_score: 0.9526\n",
      "Epoch 550: Train Loss: 0.1331, Macro_F1: 0.8776, AUC_score: 0.9522\n",
      "Validation loss decreased (0.114338 --> 0.114338).\n",
      "Epoch 600: Train Loss: 0.1321, Macro_F1: 0.8762, AUC_score: 0.9525\n",
      "Epoch 650: Train Loss: 0.1395, Macro_F1: 0.8786, AUC_score: 0.9527\n",
      "Epoch 00657: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Validation loss decreased (0.112257 --> 0.112257).\n",
      "Epoch 700: Train Loss: 0.1585, Macro_F1: 0.8781, AUC_score: 0.9523\n",
      "Epoch 750: Train Loss: 0.1237, Macro_F1: 0.8768, AUC_score: 0.9524\n",
      "Epoch 00764: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Validation loss decreased (0.112009 --> 0.112009).\n",
      "Epoch 800: Train Loss: 0.1415, Macro_F1: 0.8768, AUC_score: 0.9524\n",
      "Epoch 850: Train Loss: 0.1474, Macro_F1: 0.8768, AUC_score: 0.9524\n",
      "Epoch 00880: reducing learning rate of group 0 to 3.2000e-07.\n",
      "Epoch 900: Train Loss: 0.1425, Macro_F1: 0.8768, AUC_score: 0.9524\n",
      "Epoch 950: Train Loss: 0.1228, Macro_F1: 0.8768, AUC_score: 0.9523\n",
      "Early stopping triggered\n",
      "acc save\n",
      "8.999999999999996% node features transform to 0: F1: 0.8768, AUC_score: 0.9524\n",
      "Epoch 0: Train Loss: 0.1673, Macro_F1: 0.8462, AUC_score: 0.9414\n",
      "Validation loss decreased (0.167310 --> 0.167310).\n",
      "Validation loss decreased (0.165653 --> 0.165653).\n",
      "Validation loss decreased (0.144095 --> 0.144095).\n",
      "Validation loss decreased (0.137133 --> 0.137133).\n",
      "Validation loss decreased (0.134242 --> 0.134242).\n",
      "Validation loss decreased (0.132491 --> 0.132491).\n",
      "Validation loss decreased (0.131976 --> 0.131976).\n",
      "Epoch 50: Train Loss: 0.1457, Macro_F1: 0.8706, AUC_score: 0.9521\n",
      "Validation loss decreased (0.129553 --> 0.129553).\n",
      "Validation loss decreased (0.124542 --> 0.124542).\n",
      "Epoch 100: Train Loss: 0.1267, Macro_F1: 0.8754, AUC_score: 0.9515\n",
      "Validation loss decreased (0.124401 --> 0.124401).\n",
      "Epoch 150: Train Loss: 0.1953, Macro_F1: 0.8722, AUC_score: 0.9487\n",
      "Epoch 200: Train Loss: 0.1642, Macro_F1: 0.8779, AUC_score: 0.9537\n",
      "Validation loss decreased (0.123553 --> 0.123553).\n",
      "Validation loss decreased (0.122547 --> 0.122547).\n",
      "Epoch 250: Train Loss: 0.1530, Macro_F1: 0.8735, AUC_score: 0.9523\n",
      "Validation loss decreased (0.118682 --> 0.118682).\n",
      "Epoch 300: Train Loss: 0.1263, Macro_F1: 0.8762, AUC_score: 0.9531\n",
      "Epoch 350: Train Loss: 0.1348, Macro_F1: 0.8763, AUC_score: 0.9532\n",
      "Epoch 00398: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 400: Train Loss: 0.1671, Macro_F1: 0.8724, AUC_score: 0.9486\n",
      "Epoch 450: Train Loss: 0.2014, Macro_F1: 0.8776, AUC_score: 0.9514\n",
      "Early stopping triggered\n",
      "acc save\n",
      "7.9999999999999964% node features transform to 0: F1: 0.8734, AUC_score: 0.9528\n",
      "Epoch 0: Train Loss: 0.1255, Macro_F1: 0.8579, AUC_score: 0.9420\n",
      "Validation loss decreased (0.125491 --> 0.125491).\n",
      "Validation loss decreased (0.122686 --> 0.122686).\n",
      "Epoch 50: Train Loss: 0.1456, Macro_F1: 0.8738, AUC_score: 0.9511\n",
      "Epoch 100: Train Loss: 0.1470, Macro_F1: 0.8652, AUC_score: 0.9511\n",
      "Epoch 00132: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Validation loss decreased (0.121524 --> 0.121524).\n",
      "Epoch 150: Train Loss: 0.1249, Macro_F1: 0.8749, AUC_score: 0.9524\n",
      "Validation loss decreased (0.119892 --> 0.119892).\n",
      "Validation loss decreased (0.119668 --> 0.119668).\n",
      "Epoch 200: Train Loss: 0.1555, Macro_F1: 0.8712, AUC_score: 0.9512\n",
      "Validation loss decreased (0.117040 --> 0.117040).\n",
      "Epoch 250: Train Loss: 0.1266, Macro_F1: 0.8726, AUC_score: 0.9515\n",
      "Validation loss decreased (0.116390 --> 0.116390).\n",
      "Epoch 300: Train Loss: 0.1425, Macro_F1: 0.8745, AUC_score: 0.9526\n",
      "Epoch 350: Train Loss: 0.1295, Macro_F1: 0.8729, AUC_score: 0.9519\n",
      "Epoch 00360: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.1628, Macro_F1: 0.8747, AUC_score: 0.9526\n",
      "Validation loss decreased (0.115940 --> 0.115940).\n",
      "Epoch 450: Train Loss: 0.1222, Macro_F1: 0.8738, AUC_score: 0.9527\n",
      "Epoch 500: Train Loss: 0.1309, Macro_F1: 0.8715, AUC_score: 0.9526\n",
      "Epoch 00521: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Validation loss decreased (0.113730 --> 0.113730).\n",
      "Epoch 550: Train Loss: 0.1244, Macro_F1: 0.8743, AUC_score: 0.9525\n",
      "Validation loss decreased (0.111343 --> 0.111343).\n",
      "Epoch 600: Train Loss: 0.1221, Macro_F1: 0.8762, AUC_score: 0.9524\n",
      "Epoch 650: Train Loss: 0.1215, Macro_F1: 0.8748, AUC_score: 0.9525\n",
      "Epoch 00691: reducing learning rate of group 0 to 1.6000e-06.\n",
      "Epoch 700: Train Loss: 0.1290, Macro_F1: 0.8762, AUC_score: 0.9524\n",
      "Epoch 750: Train Loss: 0.1176, Macro_F1: 0.8756, AUC_score: 0.9524\n",
      "Early stopping triggered\n",
      "acc save\n",
      "6.999999999999995% node features transform to 0: F1: 0.8756, AUC_score: 0.9524\n",
      "Epoch 0: Train Loss: 0.1271, Macro_F1: 0.8194, AUC_score: 0.9466\n",
      "Validation loss decreased (0.127092 --> 0.127092).\n",
      "Epoch 50: Train Loss: 0.1666, Macro_F1: 0.8725, AUC_score: 0.9497\n",
      "Validation loss decreased (0.123576 --> 0.123576).\n",
      "Validation loss decreased (0.118893 --> 0.118893).\n",
      "Validation loss decreased (0.116501 --> 0.116501).\n",
      "Epoch 100: Train Loss: 0.1573, Macro_F1: 0.8725, AUC_score: 0.9509\n",
      "Epoch 150: Train Loss: 0.1411, Macro_F1: 0.8672, AUC_score: 0.9490\n",
      "Epoch 00181: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.1419, Macro_F1: 0.8765, AUC_score: 0.9518\n",
      "Epoch 250: Train Loss: 0.1681, Macro_F1: 0.8718, AUC_score: 0.9517\n",
      "Early stopping triggered\n",
      "acc save\n",
      "5.999999999999995% node features transform to 0: F1: 0.8735, AUC_score: 0.9524\n",
      "Epoch 0: Train Loss: 0.1437, Macro_F1: 0.8319, AUC_score: 0.9432\n",
      "Validation loss decreased (0.143668 --> 0.143668).\n",
      "Validation loss decreased (0.133809 --> 0.133809).\n",
      "Validation loss decreased (0.127655 --> 0.127655).\n",
      "Validation loss decreased (0.125564 --> 0.125564).\n",
      "Epoch 50: Train Loss: 0.1571, Macro_F1: 0.8765, AUC_score: 0.9497\n",
      "Validation loss decreased (0.119223 --> 0.119223).\n",
      "Epoch 100: Train Loss: 0.1469, Macro_F1: 0.8729, AUC_score: 0.9499\n",
      "Epoch 150: Train Loss: 0.1283, Macro_F1: 0.8627, AUC_score: 0.9480\n",
      "Epoch 00165: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.1544, Macro_F1: 0.8733, AUC_score: 0.9513\n",
      "Validation loss decreased (0.118357 --> 0.118357).\n",
      "Validation loss decreased (0.118246 --> 0.118246).\n",
      "Validation loss decreased (0.118157 --> 0.118157).\n",
      "Epoch 250: Train Loss: 0.1822, Macro_F1: 0.8713, AUC_score: 0.9511\n",
      "Validation loss decreased (0.115141 --> 0.115141).\n",
      "Epoch 300: Train Loss: 0.1242, Macro_F1: 0.8752, AUC_score: 0.9509\n",
      "Validation loss decreased (0.114840 --> 0.114840).\n",
      "Epoch 350: Train Loss: 0.1499, Macro_F1: 0.8757, AUC_score: 0.9516\n",
      "Epoch 400: Train Loss: 0.1406, Macro_F1: 0.8743, AUC_score: 0.9526\n",
      "Epoch 00410: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 450: Train Loss: 0.1214, Macro_F1: 0.8691, AUC_score: 0.9522\n",
      "Validation loss decreased (0.114747 --> 0.114747).\n",
      "Validation loss decreased (0.114489 --> 0.114489).\n",
      "Epoch 500: Train Loss: 0.1257, Macro_F1: 0.8737, AUC_score: 0.9517\n",
      "Validation loss decreased (0.113733 --> 0.113733).\n",
      "Epoch 550: Train Loss: 0.1907, Macro_F1: 0.8748, AUC_score: 0.9518\n",
      "Epoch 600: Train Loss: 0.1326, Macro_F1: 0.8694, AUC_score: 0.9521\n",
      "Validation loss decreased (0.113086 --> 0.113086).\n",
      "Epoch 650: Train Loss: 0.1392, Macro_F1: 0.8718, AUC_score: 0.9516\n",
      "Validation loss decreased (0.110614 --> 0.110614).\n",
      "Epoch 700: Train Loss: 0.1373, Macro_F1: 0.8715, AUC_score: 0.9515\n",
      "Epoch 750: Train Loss: 0.1223, Macro_F1: 0.8729, AUC_score: 0.9516\n",
      "Epoch 00756: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 800: Train Loss: 0.1164, Macro_F1: 0.8732, AUC_score: 0.9519\n",
      "Epoch 850: Train Loss: 0.1332, Macro_F1: 0.8737, AUC_score: 0.9519\n",
      "Early stopping triggered\n",
      "acc save\n",
      "4.999999999999993% node features transform to 0: F1: 0.8743, AUC_score: 0.9519\n",
      "Epoch 0: Train Loss: 0.1408, Macro_F1: 0.8483, AUC_score: 0.9467\n",
      "Validation loss decreased (0.140765 --> 0.140765).\n",
      "Validation loss decreased (0.130839 --> 0.130839).\n",
      "Validation loss decreased (0.124811 --> 0.124811).\n",
      "Validation loss decreased (0.123672 --> 0.123672).\n",
      "Epoch 50: Train Loss: 0.1367, Macro_F1: 0.8758, AUC_score: 0.9522\n",
      "Validation loss decreased (0.122332 --> 0.122332).\n",
      "Validation loss decreased (0.118429 --> 0.118429).\n",
      "Epoch 100: Train Loss: 0.1431, Macro_F1: 0.8743, AUC_score: 0.9510\n",
      "Epoch 150: Train Loss: 0.1402, Macro_F1: 0.8710, AUC_score: 0.9513\n",
      "Epoch 00166: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.1328, Macro_F1: 0.8729, AUC_score: 0.9507\n",
      "Epoch 250: Train Loss: 0.1511, Macro_F1: 0.8785, AUC_score: 0.9506\n",
      "Validation loss decreased (0.118171 --> 0.118171).\n",
      "Validation loss decreased (0.114495 --> 0.114495).\n",
      "Epoch 300: Train Loss: 0.1495, Macro_F1: 0.8700, AUC_score: 0.9489\n",
      "Validation loss decreased (0.113414 --> 0.113414).\n",
      "Epoch 350: Train Loss: 0.1403, Macro_F1: 0.8727, AUC_score: 0.9515\n",
      "Validation loss decreased (0.112878 --> 0.112878).\n",
      "Epoch 400: Train Loss: 0.1242, Macro_F1: 0.8723, AUC_score: 0.9508\n",
      "Epoch 450: Train Loss: 0.1176, Macro_F1: 0.8714, AUC_score: 0.9514\n",
      "Epoch 00499: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 500: Train Loss: 0.1166, Macro_F1: 0.8740, AUC_score: 0.9512\n",
      "Epoch 550: Train Loss: 0.1392, Macro_F1: 0.8737, AUC_score: 0.9518\n",
      "Validation loss decreased (0.112266 --> 0.112266).\n",
      "Validation loss decreased (0.111730 --> 0.111730).\n",
      "Validation loss decreased (0.107401 --> 0.107401).\n",
      "Epoch 600: Train Loss: 0.1286, Macro_F1: 0.8748, AUC_score: 0.9521\n",
      "Epoch 650: Train Loss: 0.1339, Macro_F1: 0.8746, AUC_score: 0.9519\n",
      "Epoch 00678: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 700: Train Loss: 0.1590, Macro_F1: 0.8756, AUC_score: 0.9515\n",
      "Epoch 750: Train Loss: 0.1239, Macro_F1: 0.8751, AUC_score: 0.9518\n",
      "Early stopping triggered\n",
      "acc save\n",
      "4.0000000000000036% node features transform to 0: F1: 0.8735, AUC_score: 0.9520\n",
      "Epoch 0: Train Loss: 0.1411, Macro_F1: 0.8289, AUC_score: 0.9402\n",
      "Validation loss decreased (0.141062 --> 0.141062).\n",
      "Validation loss decreased (0.140038 --> 0.140038).\n",
      "Validation loss decreased (0.139134 --> 0.139134).\n",
      "Validation loss decreased (0.136502 --> 0.136502).\n",
      "Validation loss decreased (0.129395 --> 0.129395).\n",
      "Validation loss decreased (0.127744 --> 0.127744).\n",
      "Validation loss decreased (0.126406 --> 0.126406).\n",
      "Validation loss decreased (0.123461 --> 0.123461).\n",
      "Epoch 50: Train Loss: 0.1304, Macro_F1: 0.8769, AUC_score: 0.9508\n",
      "Validation loss decreased (0.121616 --> 0.121616).\n",
      "Validation loss decreased (0.121322 --> 0.121322).\n",
      "Epoch 100: Train Loss: 0.1260, Macro_F1: 0.8731, AUC_score: 0.9505\n",
      "Validation loss decreased (0.120909 --> 0.120909).\n",
      "Validation loss decreased (0.117850 --> 0.117850).\n",
      "Epoch 150: Train Loss: 0.1305, Macro_F1: 0.8728, AUC_score: 0.9506\n",
      "Epoch 200: Train Loss: 0.1805, Macro_F1: 0.8733, AUC_score: 0.9511\n",
      "Epoch 00217: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.1489, Macro_F1: 0.8720, AUC_score: 0.9503\n",
      "Validation loss decreased (0.114521 --> 0.114521).\n",
      "Epoch 300: Train Loss: 0.1384, Macro_F1: 0.8751, AUC_score: 0.9507\n",
      "Epoch 350: Train Loss: 0.1272, Macro_F1: 0.8724, AUC_score: 0.9498\n",
      "Epoch 00377: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.1140, Macro_F1: 0.8769, AUC_score: 0.9509\n",
      "Validation loss decreased (0.113964 --> 0.113964).\n",
      "Epoch 450: Train Loss: 0.1316, Macro_F1: 0.8735, AUC_score: 0.9504\n",
      "Validation loss decreased (0.111398 --> 0.111398).\n",
      "Epoch 500: Train Loss: 0.1214, Macro_F1: 0.8753, AUC_score: 0.9509\n",
      "Epoch 550: Train Loss: 0.1140, Macro_F1: 0.8760, AUC_score: 0.9508\n",
      "Epoch 00594: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 600: Train Loss: 0.1118, Macro_F1: 0.8756, AUC_score: 0.9511\n",
      "Epoch 650: Train Loss: 0.1121, Macro_F1: 0.8763, AUC_score: 0.9510\n",
      "Early stopping triggered\n",
      "acc save\n",
      "3.0000000000000027% node features transform to 0: F1: 0.8759, AUC_score: 0.9510\n",
      "Epoch 0: Train Loss: 0.1229, Macro_F1: 0.8569, AUC_score: 0.9466\n",
      "Validation loss decreased (0.122912 --> 0.122912).\n",
      "Epoch 50: Train Loss: 0.1453, Macro_F1: 0.8681, AUC_score: 0.9454\n",
      "Validation loss decreased (0.122853 --> 0.122853).\n",
      "Validation loss decreased (0.122556 --> 0.122556).\n",
      "Validation loss decreased (0.117637 --> 0.117637).\n",
      "Epoch 100: Train Loss: 0.1787, Macro_F1: 0.8756, AUC_score: 0.9510\n",
      "Epoch 150: Train Loss: 0.1420, Macro_F1: 0.8745, AUC_score: 0.9498\n",
      "Epoch 00198: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 200: Train Loss: 0.1511, Macro_F1: 0.8721, AUC_score: 0.9516\n",
      "Validation loss decreased (0.117516 --> 0.117516).\n",
      "Epoch 250: Train Loss: 0.1165, Macro_F1: 0.8717, AUC_score: 0.9515\n",
      "Validation loss decreased (0.116488 --> 0.116488).\n",
      "Validation loss decreased (0.114049 --> 0.114049).\n",
      "Validation loss decreased (0.111349 --> 0.111349).\n",
      "Epoch 300: Train Loss: 0.1234, Macro_F1: 0.8723, AUC_score: 0.9510\n",
      "Epoch 350: Train Loss: 0.1317, Macro_F1: 0.8727, AUC_score: 0.9505\n",
      "Epoch 00386: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 400: Train Loss: 0.1428, Macro_F1: 0.8724, AUC_score: 0.9504\n",
      "Epoch 450: Train Loss: 0.1351, Macro_F1: 0.8753, AUC_score: 0.9513\n",
      "Validation loss decreased (0.109871 --> 0.109871).\n",
      "Epoch 500: Train Loss: 0.1574, Macro_F1: 0.8737, AUC_score: 0.9514\n",
      "Epoch 550: Train Loss: 0.1176, Macro_F1: 0.8758, AUC_score: 0.9508\n",
      "Epoch 00574: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 600: Train Loss: 0.1157, Macro_F1: 0.8750, AUC_score: 0.9511\n",
      "Epoch 650: Train Loss: 0.1188, Macro_F1: 0.8769, AUC_score: 0.9511\n",
      "Early stopping triggered\n",
      "acc save\n",
      "2.0000000000000018% node features transform to 0: F1: 0.8750, AUC_score: 0.9510\n",
      "Epoch 0: Train Loss: 0.1235, Macro_F1: 0.8545, AUC_score: 0.9433\n",
      "Validation loss decreased (0.123467 --> 0.123467).\n",
      "Epoch 50: Train Loss: 0.1408, Macro_F1: 0.8729, AUC_score: 0.9494\n",
      "Validation loss decreased (0.119890 --> 0.119890).\n",
      "Epoch 100: Train Loss: 0.1491, Macro_F1: 0.8710, AUC_score: 0.9503\n",
      "Epoch 150: Train Loss: 0.1372, Macro_F1: 0.8715, AUC_score: 0.9523\n",
      "Validation loss decreased (0.114690 --> 0.114690).\n",
      "Epoch 200: Train Loss: 0.1343, Macro_F1: 0.8732, AUC_score: 0.9497\n",
      "Epoch 250: Train Loss: 0.1516, Macro_F1: 0.8695, AUC_score: 0.9488\n",
      "Epoch 00255: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 300: Train Loss: 0.1394, Macro_F1: 0.8748, AUC_score: 0.9503\n",
      "Epoch 350: Train Loss: 0.1631, Macro_F1: 0.8756, AUC_score: 0.9502\n",
      "Early stopping triggered\n",
      "acc save\n",
      "1.0000000000000009% node features transform to 0: F1: 0.8785, AUC_score: 0.9503\n",
      "Epoch 0: Train Loss: 0.1286, Macro_F1: 0.8093, AUC_score: 0.9432\n",
      "Validation loss decreased (0.128581 --> 0.128581).\n",
      "Validation loss decreased (0.127148 --> 0.127148).\n",
      "Epoch 50: Train Loss: 0.1204, Macro_F1: 0.8727, AUC_score: 0.9506\n",
      "Validation loss decreased (0.120387 --> 0.120387).\n",
      "Epoch 100: Train Loss: 0.1347, Macro_F1: 0.8723, AUC_score: 0.9511\n",
      "Validation loss decreased (0.118441 --> 0.118441).\n",
      "Validation loss decreased (0.111407 --> 0.111407).\n",
      "Epoch 150: Train Loss: 0.1316, Macro_F1: 0.8678, AUC_score: 0.9496\n",
      "Epoch 200: Train Loss: 0.1362, Macro_F1: 0.8630, AUC_score: 0.9446\n",
      "Epoch 00206: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 250: Train Loss: 0.1283, Macro_F1: 0.8770, AUC_score: 0.9524\n",
      "Epoch 300: Train Loss: 0.1375, Macro_F1: 0.8727, AUC_score: 0.9505\n",
      "Early stopping triggered\n",
      "acc save\n",
      "0.0% node features transform to 0: F1: 0.8727, AUC_score: 0.9505\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1')\n",
    "data = data.to(device)\n",
    "results = []\n",
    "num_epochs = 1000\n",
    "# 加载数据\n",
    "features = data.x\n",
    "labels = data.y # 根据你的数据加载函数进行调整\n",
    "num_iterations = 100\n",
    "original_features = features.clone()\n",
    "# 总共需要迭代的次数，这里以逐步增加5%为例，直到100%\n",
    "model = GCN(num_features=data.x.shape[1], hidden_dim=64, num_classes=3, num_layers=3, activation=F.relu, dropout=0.5)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=50, verbose=True)\n",
    "for i in range(num_iterations):\n",
    "    # 加载这次迭代的selected_indices和remaining_indices\n",
    "    selected_indices = np.load(f'DIVIDED_DATA/GOwith_{i}%.npy')\n",
    "    \n",
    "    # 根据selected_indices和remaining_indices调整特征\n",
    "    masked_features = features.clone()\n",
    "    masked_features[torch.tensor(selected_indices)] = 0  # 假设features是一个PyTorch tensor\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=200, verbose=True, delta=0.00001)\n",
    "    \n",
    "    if i > 0:\n",
    "        # 从上一个迭代保存的模型中加载参数\n",
    "        model.load_state_dict(torch.load(f'G-G_DATA/G-G_model_WITH{i-1}.pth'))\n",
    "        # 重新初始化优化器和调度器\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=100, verbose=True)\n",
    "    \n",
    "    # 模型训练和评估逻辑\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model_scheduler(model, masked_features, data.y, data.edge_index, optimizer, loss_fn, scheduler, train_mask)\n",
    "        test_f1, test_auc = evaluate_model(model, masked_features, data.y, data.edge_index, test_mask)\n",
    "        #high_f1, high_auc = evaluate_model(model, data.x, data.y, data.edge_index, test_high)\n",
    "        #low_f1, low_auc = evaluate_model(model, data.x, data.y, data.edge_index, test_low)\n",
    "        if epoch % 50 == 0:  # 每10个epoch打印一次信息\n",
    "            print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Macro_F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "        early_stopping(train_loss, model, i)\n",
    "        if early_stopping.early_stop or epoch == num_epochs - 1:\n",
    "            results.append({\n",
    "                            'Train Loss': train_loss,\n",
    "                            'F1': test_f1,\n",
    "                            'AUC_score': test_auc\n",
    "                        })\n",
    "                            #'highinfo F1': high_f1,                                                            \n",
    "                            #'highinfo AUC_score': high_auc,\n",
    "                            #'lowinfo F1': low_f1,\n",
    "                            #'lowinfo AUC_score': low_auc\n",
    "            print(\"acc save\")\n",
    "            rate = 1 - count * (i + 1)\n",
    "            rate = rate * 100\n",
    "            print(f'{rate}% node features transform to 0: F1: {test_f1:.4f}, AUC_score: {test_auc:.4f}')\n",
    "            torch.save(model.state_dict(), f'G-G_DATA/G-G_model_WITH{i}.pth')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6cb65678-b2c0-4aa2-b232-2f4981378fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Train Loss': 1.0517518520355225,\n",
       "  'F1': array(0.1908291, dtype=float32),\n",
       "  'AUC_score': 0.5},\n",
       " {'Train Loss': 0.6586019396781921,\n",
       "  'F1': array(0.72071403, dtype=float32),\n",
       "  'AUC_score': 0.8563770847548539},\n",
       " {'Train Loss': 0.4226788282394409,\n",
       "  'F1': array(0.8346391, dtype=float32),\n",
       "  'AUC_score': 0.9374641057184944},\n",
       " {'Train Loss': 0.3980135917663574,\n",
       "  'F1': array(0.85567683, dtype=float32),\n",
       "  'AUC_score': 0.9451525963275172},\n",
       " {'Train Loss': 0.3451567590236664,\n",
       "  'F1': array(0.85655564, dtype=float32),\n",
       "  'AUC_score': 0.950747870202958},\n",
       " {'Train Loss': 0.33657386898994446,\n",
       "  'F1': array(0.85765743, dtype=float32),\n",
       "  'AUC_score': 0.952752063199776},\n",
       " {'Train Loss': 0.3947474956512451,\n",
       "  'F1': array(0.8565218, dtype=float32),\n",
       "  'AUC_score': 0.9525994644041996},\n",
       " {'Train Loss': 0.29908865690231323,\n",
       "  'F1': array(0.8575504, dtype=float32),\n",
       "  'AUC_score': 0.9565432867831846},\n",
       " {'Train Loss': 0.30417898297309875,\n",
       "  'F1': array(0.8570633, dtype=float32),\n",
       "  'AUC_score': 0.9574230506878512},\n",
       " {'Train Loss': 0.28212180733680725,\n",
       "  'F1': array(0.85834163, dtype=float32),\n",
       "  'AUC_score': 0.9558074929328798},\n",
       " {'Train Loss': 0.31440311670303345,\n",
       "  'F1': array(0.86216044, dtype=float32),\n",
       "  'AUC_score': 0.9568762984064417},\n",
       " {'Train Loss': 0.2796888053417206,\n",
       "  'F1': array(0.8626084, dtype=float32),\n",
       "  'AUC_score': 0.9570344360537159},\n",
       " {'Train Loss': 0.2855429947376251,\n",
       "  'F1': array(0.8634696, dtype=float32),\n",
       "  'AUC_score': 0.9563806789224193},\n",
       " {'Train Loss': 0.28391867876052856,\n",
       "  'F1': array(0.861237, dtype=float32),\n",
       "  'AUC_score': 0.9563209634887997},\n",
       " {'Train Loss': 0.2794922888278961,\n",
       "  'F1': array(0.8585723, dtype=float32),\n",
       "  'AUC_score': 0.9549045521747529},\n",
       " {'Train Loss': 0.26917487382888794,\n",
       "  'F1': array(0.8621152, dtype=float32),\n",
       "  'AUC_score': 0.954924055957413},\n",
       " {'Train Loss': 0.3090745508670807,\n",
       "  'F1': array(0.8655975, dtype=float32),\n",
       "  'AUC_score': 0.9554650136804042},\n",
       " {'Train Loss': 0.25155109167099,\n",
       "  'F1': array(0.8699052, dtype=float32),\n",
       "  'AUC_score': 0.9566056587912056},\n",
       " {'Train Loss': 0.2550874650478363,\n",
       "  'F1': array(0.866476, dtype=float32),\n",
       "  'AUC_score': 0.9568904753837614},\n",
       " {'Train Loss': 0.2750098705291748,\n",
       "  'F1': array(0.865458, dtype=float32),\n",
       "  'AUC_score': 0.9561561995557538},\n",
       " {'Train Loss': 0.23663686215877533,\n",
       "  'F1': array(0.8627057, dtype=float32),\n",
       "  'AUC_score': 0.9572788294482021},\n",
       " {'Train Loss': 0.24460342526435852,\n",
       "  'F1': array(0.86218053, dtype=float32),\n",
       "  'AUC_score': 0.9565949285868259},\n",
       " {'Train Loss': 0.2372387796640396,\n",
       "  'F1': array(0.86106247, dtype=float32),\n",
       "  'AUC_score': 0.9572775903450856},\n",
       " {'Train Loss': 0.21770313382148743,\n",
       "  'F1': array(0.86186093, dtype=float32),\n",
       "  'AUC_score': 0.9563263645263526},\n",
       " {'Train Loss': 0.2262590229511261,\n",
       "  'F1': array(0.8626688, dtype=float32),\n",
       "  'AUC_score': 0.9553985357646514},\n",
       " {'Train Loss': 0.21231582760810852,\n",
       "  'F1': array(0.8665137, dtype=float32),\n",
       "  'AUC_score': 0.954611324273821},\n",
       " {'Train Loss': 0.23750324547290802,\n",
       "  'F1': array(0.8632739, dtype=float32),\n",
       "  'AUC_score': 0.9550638543906982},\n",
       " {'Train Loss': 0.2136986404657364,\n",
       "  'F1': array(0.8686592, dtype=float32),\n",
       "  'AUC_score': 0.9548826261815982},\n",
       " {'Train Loss': 0.21554706990718842,\n",
       "  'F1': array(0.8710273, dtype=float32),\n",
       "  'AUC_score': 0.9551095471231413},\n",
       " {'Train Loss': 0.21393147110939026,\n",
       "  'F1': array(0.86998266, dtype=float32),\n",
       "  'AUC_score': 0.9547829698242033},\n",
       " {'Train Loss': 0.20472770929336548,\n",
       "  'F1': array(0.87155765, dtype=float32),\n",
       "  'AUC_score': 0.954681187200613},\n",
       " {'Train Loss': 0.21003292500972748,\n",
       "  'F1': array(0.8666746, dtype=float32),\n",
       "  'AUC_score': 0.9528126788673549},\n",
       " {'Train Loss': 0.19753910601139069,\n",
       "  'F1': array(0.87212133, dtype=float32),\n",
       "  'AUC_score': 0.9535904835546748},\n",
       " {'Train Loss': 0.2062893956899643,\n",
       "  'F1': array(0.8724758, dtype=float32),\n",
       "  'AUC_score': 0.9538611248112622},\n",
       " {'Train Loss': 0.2251461297273636,\n",
       "  'F1': array(0.87235665, dtype=float32),\n",
       "  'AUC_score': 0.9533299749749852},\n",
       " {'Train Loss': 0.23611977696418762,\n",
       "  'F1': array(0.8715586, dtype=float32),\n",
       "  'AUC_score': 0.9537079787876596},\n",
       " {'Train Loss': 0.20231842994689941,\n",
       "  'F1': array(0.8746137, dtype=float32),\n",
       "  'AUC_score': 0.9530695844305931},\n",
       " {'Train Loss': 0.19885994493961334,\n",
       "  'F1': array(0.8731968, dtype=float32),\n",
       "  'AUC_score': 0.9532813586715857},\n",
       " {'Train Loss': 0.1810358166694641,\n",
       "  'F1': array(0.87074447, dtype=float32),\n",
       "  'AUC_score': 0.9528416855743699},\n",
       " {'Train Loss': 0.18310853838920593,\n",
       "  'F1': array(0.8710249, dtype=float32),\n",
       "  'AUC_score': 0.9528486377649648},\n",
       " {'Train Loss': 0.18056146800518036,\n",
       "  'F1': array(0.8708572, dtype=float32),\n",
       "  'AUC_score': 0.9536128069790998},\n",
       " {'Train Loss': 0.22183313965797424,\n",
       "  'F1': array(0.8726985, dtype=float32),\n",
       "  'AUC_score': 0.9530568141512018},\n",
       " {'Train Loss': 0.18748076260089874,\n",
       "  'F1': array(0.87078065, dtype=float32),\n",
       "  'AUC_score': 0.9527796296180976},\n",
       " {'Train Loss': 0.18530572950839996,\n",
       "  'F1': array(0.8748612, dtype=float32),\n",
       "  'AUC_score': 0.952497506159462},\n",
       " {'Train Loss': 0.17574596405029297,\n",
       "  'F1': array(0.8728123, dtype=float32),\n",
       "  'AUC_score': 0.952017482011942},\n",
       " {'Train Loss': 0.18373055756092072,\n",
       "  'F1': array(0.87172604, dtype=float32),\n",
       "  'AUC_score': 0.9528925733714676},\n",
       " {'Train Loss': 0.16573123633861542,\n",
       "  'F1': array(0.86734074, dtype=float32),\n",
       "  'AUC_score': 0.9522534338158751},\n",
       " {'Train Loss': 0.18739324808120728,\n",
       "  'F1': array(0.87060004, dtype=float32),\n",
       "  'AUC_score': 0.9524414165838812},\n",
       " {'Train Loss': 0.16736391186714172,\n",
       "  'F1': array(0.87169814, dtype=float32),\n",
       "  'AUC_score': 0.9523416346690917},\n",
       " {'Train Loss': 0.16273722052574158,\n",
       "  'F1': array(0.86516064, dtype=float32),\n",
       "  'AUC_score': 0.9520004488571144},\n",
       " {'Train Loss': 0.20496423542499542,\n",
       "  'F1': array(0.8651478, dtype=float32),\n",
       "  'AUC_score': 0.9522093560294714},\n",
       " {'Train Loss': 0.163970485329628,\n",
       "  'F1': array(0.86974674, dtype=float32),\n",
       "  'AUC_score': 0.9529223605788514},\n",
       " {'Train Loss': 0.16261248290538788,\n",
       "  'F1': array(0.86758417, dtype=float32),\n",
       "  'AUC_score': 0.9526639434566823},\n",
       " {'Train Loss': 0.17710214853286743,\n",
       "  'F1': array(0.8742961, dtype=float32),\n",
       "  'AUC_score': 0.9523516239081712},\n",
       " {'Train Loss': 0.17007754743099213,\n",
       "  'F1': array(0.87289953, dtype=float32),\n",
       "  'AUC_score': 0.9523039528112154},\n",
       " {'Train Loss': 0.15558920800685883,\n",
       "  'F1': array(0.870006, dtype=float32),\n",
       "  'AUC_score': 0.9532954552766055},\n",
       " {'Train Loss': 0.16659995913505554,\n",
       "  'F1': array(0.8762307, dtype=float32),\n",
       "  'AUC_score': 0.9531977731247405},\n",
       " {'Train Loss': 0.17070552706718445,\n",
       "  'F1': array(0.87326336, dtype=float32),\n",
       "  'AUC_score': 0.9533541608905836},\n",
       " {'Train Loss': 0.1668166071176529,\n",
       "  'F1': array(0.8735183, dtype=float32),\n",
       "  'AUC_score': 0.9532426340959009},\n",
       " {'Train Loss': 0.1579710841178894,\n",
       "  'F1': array(0.87112314, dtype=float32),\n",
       "  'AUC_score': 0.9529589848630233},\n",
       " {'Train Loss': 0.1509108543395996,\n",
       "  'F1': array(0.8723943, dtype=float32),\n",
       "  'AUC_score': 0.9536290119801883},\n",
       " {'Train Loss': 0.1530640423297882,\n",
       "  'F1': array(0.8740401, dtype=float32),\n",
       "  'AUC_score': 0.9536646843771733},\n",
       " {'Train Loss': 0.17444998025894165,\n",
       "  'F1': array(0.87353605, dtype=float32),\n",
       "  'AUC_score': 0.9535607037412204},\n",
       " {'Train Loss': 0.15064199268817902,\n",
       "  'F1': array(0.8732848, dtype=float32),\n",
       "  'AUC_score': 0.9534578766625809},\n",
       " {'Train Loss': 0.14358599483966827,\n",
       "  'F1': array(0.87405586, dtype=float32),\n",
       "  'AUC_score': 0.9533702312600757},\n",
       " {'Train Loss': 0.16859908401966095,\n",
       "  'F1': array(0.87513715, dtype=float32),\n",
       "  'AUC_score': 0.9529770336673025},\n",
       " {'Train Loss': 0.16678161919116974,\n",
       "  'F1': array(0.87272483, dtype=float32),\n",
       "  'AUC_score': 0.9531703126893878},\n",
       " {'Train Loss': 0.1420716941356659,\n",
       "  'F1': array(0.8738243, dtype=float32),\n",
       "  'AUC_score': 0.953310810824588},\n",
       " {'Train Loss': 0.1540115624666214,\n",
       "  'F1': array(0.875201, dtype=float32),\n",
       "  'AUC_score': 0.953367064330101},\n",
       " {'Train Loss': 0.14506614208221436,\n",
       "  'F1': array(0.87356585, dtype=float32),\n",
       "  'AUC_score': 0.9535860328649711},\n",
       " {'Train Loss': 0.1699487566947937,\n",
       "  'F1': array(0.87467796, dtype=float32),\n",
       "  'AUC_score': 0.9535689737157985},\n",
       " {'Train Loss': 0.13787241280078888,\n",
       "  'F1': array(0.8709423, dtype=float32),\n",
       "  'AUC_score': 0.9536223092538992},\n",
       " {'Train Loss': 0.15721634030342102,\n",
       "  'F1': array(0.8690416, dtype=float32),\n",
       "  'AUC_score': 0.9527038457110565},\n",
       " {'Train Loss': 0.14395618438720703,\n",
       "  'F1': array(0.87230426, dtype=float32),\n",
       "  'AUC_score': 0.9529885054646043},\n",
       " {'Train Loss': 0.15137270092964172,\n",
       "  'F1': array(0.87304825, dtype=float32),\n",
       "  'AUC_score': 0.9524280973982197},\n",
       " {'Train Loss': 0.23403985798358917,\n",
       "  'F1': array(0.87605554, dtype=float32),\n",
       "  'AUC_score': 0.9527645024436776},\n",
       " {'Train Loss': 0.14647260308265686,\n",
       "  'F1': array(0.8738305, dtype=float32),\n",
       "  'AUC_score': 0.9513228240197072},\n",
       " {'Train Loss': 0.13665562868118286,\n",
       "  'F1': array(0.8745095, dtype=float32),\n",
       "  'AUC_score': 0.9520842311283845},\n",
       " {'Train Loss': 0.15878377854824066,\n",
       "  'F1': array(0.87295765, dtype=float32),\n",
       "  'AUC_score': 0.9528510157752033},\n",
       " {'Train Loss': 0.14875318109989166,\n",
       "  'F1': array(0.8722224, dtype=float32),\n",
       "  'AUC_score': 0.9526644391686631},\n",
       " {'Train Loss': 0.13223499059677124,\n",
       "  'F1': array(0.87574005, dtype=float32),\n",
       "  'AUC_score': 0.9527870316910879},\n",
       " {'Train Loss': 0.1417752057313919,\n",
       "  'F1': array(0.8741662, dtype=float32),\n",
       "  'AUC_score': 0.9524502677369634},\n",
       " {'Train Loss': 0.13091601431369781,\n",
       "  'F1': array(0.87408847, dtype=float32),\n",
       "  'AUC_score': 0.9525474785608794},\n",
       " {'Train Loss': 0.13524235785007477,\n",
       "  'F1': array(0.8751946, dtype=float32),\n",
       "  'AUC_score': 0.9526546282337902},\n",
       " {'Train Loss': 0.14503683149814606,\n",
       "  'F1': array(0.8717069, dtype=float32),\n",
       "  'AUC_score': 0.9524147588303254},\n",
       " {'Train Loss': 0.17066901922225952,\n",
       "  'F1': array(0.8767209, dtype=float32),\n",
       "  'AUC_score': 0.9529233171665389},\n",
       " {'Train Loss': 0.11696072667837143,\n",
       "  'F1': array(0.87541705, dtype=float32),\n",
       "  'AUC_score': 0.952796659218247},\n",
       " {'Train Loss': 0.17785562574863434,\n",
       "  'F1': array(0.8772669, dtype=float32),\n",
       "  'AUC_score': 0.9521205124664712},\n",
       " {'Train Loss': 0.16979224979877472,\n",
       "  'F1': array(0.87586737, dtype=float32),\n",
       "  'AUC_score': 0.9521911017377409},\n",
       " {'Train Loss': 0.13745470345020294,\n",
       "  'F1': array(0.8759391, dtype=float32),\n",
       "  'AUC_score': 0.9523452524674133},\n",
       " {'Train Loss': 0.13182584941387177,\n",
       "  'F1': array(0.8767791, dtype=float32),\n",
       "  'AUC_score': 0.9523535524886992},\n",
       " {'Train Loss': 0.12164511531591415,\n",
       "  'F1': array(0.87344617, dtype=float32),\n",
       "  'AUC_score': 0.9527580144539153},\n",
       " {'Train Loss': 0.13659510016441345,\n",
       "  'F1': array(0.8756194, dtype=float32),\n",
       "  'AUC_score': 0.9523696359342978},\n",
       " {'Train Loss': 0.12112917751073837,\n",
       "  'F1': array(0.87354445, dtype=float32),\n",
       "  'AUC_score': 0.9524251262162536},\n",
       " {'Train Loss': 0.12146910279989243,\n",
       "  'F1': array(0.87425965, dtype=float32),\n",
       "  'AUC_score': 0.9519324585082524},\n",
       " {'Train Loss': 0.13693270087242126,\n",
       "  'F1': array(0.87352467, dtype=float32),\n",
       "  'AUC_score': 0.9519501508178396},\n",
       " {'Train Loss': 0.1255495697259903,\n",
       "  'F1': array(0.87585694, dtype=float32),\n",
       "  'AUC_score': 0.9509820175760706},\n",
       " {'Train Loss': 0.11681928485631943,\n",
       "  'F1': array(0.8750313, dtype=float32),\n",
       "  'AUC_score': 0.9510440050964779},\n",
       " {'Train Loss': 0.12612870335578918,\n",
       "  'F1': array(0.87853175, dtype=float32),\n",
       "  'AUC_score': 0.9503139273552162},\n",
       " {'Train Loss': 0.12086515128612518,\n",
       "  'F1': array(0.87265587, dtype=float32),\n",
       "  'AUC_score': 0.9505424390283972}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a752641-dd08-4f52-b6ad-5deafff3e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from numpy import array, float32\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, np.generic):\n",
    "            return obj.item()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "\n",
    "# 保存为 JSON 文件\n",
    "with open('GNN/zeroed_GO_localization.json', 'w') as f:\n",
    "    json.dump(results, f, cls=NumpyEncoder, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842fcbc2-f8a9-4be8-bb44-d28498cf80e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
